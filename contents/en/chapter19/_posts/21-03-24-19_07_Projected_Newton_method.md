--- layout: post title: 19-07 Projected Newton method chapter: '19' order: '11' owner: Hooncheol Shin categories: - - chapter19 lang: en --- # # What's wrong with projected Newton? When $$h$$ thise indicator function $$h = I_c(x)$$ of convex set $$C$$, the problem can be defined as follows: >$$ \min_{x} \ g(x) \quad \text{subject to} \quad x \in C$$ Therefore, if $$h(x) = I_c(x)$$, then proximal gradient descent becomes **projected gradient descent**. That is, projected gradient descent is a special case of proximal gradient descent. What about the case of proximal Newton when $$h(x) = I_c(x)$$? In this case, the update equation is defined as follows: > >$$ \begin{align} > z^{+} & =\underset{z \in C}{\text{argmin}} \ \frac{1}{2} \parallel x - H^{-1} \nabla g(x) - z \parallel_H^2 \\\\ > &= \underset{z \in C}{\text{argmin}} \ \nabla g(x)^T (z - x) + \frac{1}{2} (z - x)^T H (z - x) \\\\ > \end{align} > $$ If $$H = I$$, then this becomes the result of projecting $$x - \nabla g(x)$$ onto set $$C$$, but for general $$H \neq I$$, it is not a projection. (If $$H = I$$, it would be the $$l_2$$-norm, so if it were the $$l_2$$-norm instead of the H-norm, it would be a projection. ) Therefore, the projected Newton method is not a special case of the proximal Newton method. # # Projected Newton for box constraints For the special case of problems with box constraints, projected Newton can be applied. (Bertsekas, 1982; Kim et al. , 2010; Schmidt et al. , 2011). Let the problem be as follows: >$$ \min_{x} \ g(x) \quad \text{subject to} \quad l \le x \le u $$ Starting with the initial point $$x^{(0)}$$ of the Projected Newton method and a small constant $$\epsilon \gt 0$$, we iterate the following steps ($$k = 1, 2, 3,. . $$). * step1: Binding set definition. > \begin{align} B_{k-1} & = \\{ i: x_i^{(k-1)} \le l_i + \epsilon \quad \text{and} \quad \nabla_i g(x^{(k-1)}) \gt 0 \\} \quad \cup \quad \\{ i: x_i^{(k-1)} \ge u_i - \epsilon \quad \text{and} \quad \nabla_i g(x^{(k-1)}) \lt 0 \\} \end{align} optimization stepat, variablebox of constraint boundaryto,. a lot function. * step2: Free set $$F_{k-1} = \left\{1,. . , n \right\} \backslash B_{k-1}$$ definition. * step3: Free variable therefore, of Hessian of submatrix inverse definition. >$$ S^{(k-1)} = [(\nabla^2 g(x^{(k-1)}))_{F_{k-1}}]^{-1}$$ * step4: Fee variable Newton step execution projection. > >$$ \begin{align} > x_{(k)} = P_{[l, u]} \left( x^{(k-1)} - t_k \begin{bmatrix} S^{(k-1)} & 0 \\ > 0 & I \end{bmatrix} > \begin{bmatrix} \nabla F_{k-1} g(x^{(k-1)}) \\ \nabla B_{k-1} g(x^{(k-1)}) \end{bmatrix} > \right) > \end{align} > $$ from $$P_{[l, u]}$$ $$[l, u] = [l_1, u_1] \times \cdots [l_n, u_n]$$to, is projection. matrix free variableabout, fromNewton step executionbut, binding of variable case, exists. also, , projection box scope outsideto, about, from coordinateabout, appropriate $$l_i$$ or, $$u_i$$ solution is. method problem very (ex, differencelarge case, ) of part variable boundary nearto, from free set very when, optimization is method. some problem box constraint ? as follows: problem very with, informing, exists. * Nonnegative least squares * Support vector machine dual * Graphical lasso dual * Fused lasso (total variation is denoing) dual # # Convergence properties * Bertsekas (1982) appropriate to, projected Newtonwith, iteration appropriate binding constraints exists. that, free variableabout, Newton's methodand,. * Bertsekas (1982) also, superlinear convergence proof. * Kim et al. (2010), Schmidt et al. (2011) BFGS-style update use projected quasi-Newton method inside. 