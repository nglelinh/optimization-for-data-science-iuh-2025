--- layout: post title: 19-05 Notable examples chapter: '19' order: '9' owner: Hooncheol Shin categories: - - chapter19 lang: en --- # # Glmnet and QUIC Proximal newton of method very exists. * **glmnet** (Friedman et al. , 2009): $$l_1$$ penalized generalized linear modelsto, about, prox Newton implementation. Coordinate descent using, from inner problem. * **QUIC** (Hsiesh et al. , 2011): graphical lasso problemto, about, prox Newton implementation. Factorization trick use coordinate descent using, from inner problem. implementation to, from very scope use state-of-the-art exists. Proximal Newton method proximal than gradient of $$g$$ gradient often computation. therefore, , computation proximal newton. also, , inner solver among good performance exists. # # Example: lasso is logtic regression Lee et al. (2012)paperat, yeslet's look at. $$l_1$$ regularized is logtic regressionto, solution next methodabout, from performance evaluation. 1. FISTA: accelerated prox grad 2. spaRSA: spectral projected gradient method 3. PN: proximal Newton # # # Dense hessian X (n=5000, p=6000) yes data n = 5000, feature p = 6000 dense feature matrix $$X$$ problemabout, nextand, performance. Hessian dense because of, very challenging problem exists. <figure class="image" style="align: center; " > <p align="center" > <img src="{{ site. baseurl }}/img/chapter_img/chapter19/09. 05_Lasso_Example1. png" alt="[Fig 1] Dense hessian X (n=5000, p=6000) [2]" width="70%" > <figcaption style="text-align: center; ">[Fig 1] Dense hessian X (n=5000, p=6000) [2]</figcaption > </p > </figure > function with, , time with, evaluation with, from, function with, when, of PN performance very exists. from $$g$$and, $$\nabla g$$ computationtime and is part particularly, $$\exp$$and, $$\log$$function computationtime a lot. # # # Sparse hessian X (n=542, 000, p=47, 000) yes of next case, $$X$$ sparse because of, $$g$$and, $$\nabla g$$ computationtime. <figure class="image" style="align: center; " > <p align="center" > <img src="{{ site. baseurl }}/img/chapter_img/chapter19/09. 05_Lasso_Example-sparse. png" alt="[Fig 2] Sparse hessian X (n=542, 000, p=47, 000) [2]" width="70%" > <figcaption style="text-align: center; ">[Fig 2] Sparse hessian X (n=542, 000, p=47, 000) [2]</figcaption > </p > </figure > # # Inexact prox evaluations Proximal Newton methodat, proximal operation computation when, prox operator closed form because of, accurate computation. thatto, , very high accuracy very good exists. Lee (2014)at, global convergenceand, local superlinear convergence inner of problem stopping rule inside. # # # Three stopping rules Graphical lasso estimation problemto, inner optimizations above stopping rules is comparon. when, , data n = 72 feature p = is 1255. <figure class="image" style="align: center; " > <p align="center" > <img src="{{ site. baseurl }}/img/chapter_img/chapter19/09. 05_Inexact_prox. png" alt="[Fig 3] Three stopping rules [2]" width="70%" > <figcaption style="text-align: center; ">[Fig 3] Three stopping rules [2]</figcaption > </p > </figure > stopping rule adaptive, maxiter = 10, is exact. Maxiter inner iteration maximum 10to, exact accurate solution when, to, iterationis. Proximal newton method quadratic convergence to, exact quadratic satisfies convergence exists. Maxiter=of 10 case, maximum 10inner iterationwith, quadratic convergence but, of adaptive case, quadratic convergence satisfiedly among is fast. # # # Stopping rule of usual newton method general newton's methodat, inner problem $$x^{(k-1)}of $$ $$g$$to, about, quadratic approximation $$\tilde{g}_{k-1}$$ minimization. and, , $$\eta_k, k=1, 2, 3,. . $$ choosing, from next condition, when, among. (forcing sequence. ) > \begin{align} \parallel \nabla \tilde{g}_{k-1}(x^{(k)}) \parallel_2 & \le \eta_k \parallel \nabla g(x^{(k-1)}) \parallel_2 \\\\ \end{align} condition, next positionat, gradient current positionat, than gradient $$\eta_k$$ is small. when, , Quadratic approximation $$\tilde{g}_{k-1}(z) = \nabla g(x)^T (z-x) + \frac{1}{2t} \parallel z - x \parallel_2^2$$is. # # # Stopping rule of proximal gradient method Lee et al. (2012)at, proximal gradientat, gradient instead, generalized gradient useinside. > >$$ \begin{align} \parallel G_{\tilde{f}_{k-1}/M}(x^{(k)}) \parallel_2 & \le \eta_k \parallel G_{f/M}(x^{(k-1)}) \parallel_2 \end{align} > $$ from $$\tilde{f}_{k-1} = \tilde{g}_{k-1} + h$$ $$mI \preceq \nabla^2 g \preceq MI$$is. and, , as follows: $$\eta_k$$ inexact proximal newton local superlinear rate proof. > >$$ \begin{align} > \eta_k \le \min \left\{ \frac{m}{2}, \frac{\parallel G_{\tilde{f}_{k-2}/M}(x^{(k-1)}) - G_{f/M}(x^{(k-1)}) \parallel_2}{\parallel G_{f/M}(x^{(k-2)}) \parallel_2} \right\} > \end{align} > $$