--- layout: post title: 19-01-01 Reminder - proximal gradient descent chapter: '19' order: '3' owner: Hooncheol Shin categories: - - chapter19 lang: en --- Before examining the **Proximal newton method** that we will learn in this chapter, let's first review **Proximal gradient descent**. For detailed information, see [09 Proximal Gradient Descent and Acceleration]({% multilang_post_url contents/chapter09/20-01-08-09_proximal_gradient_descent_and_acceleration %}) see. # # Proximal gradient descent **Proximal gradient descent** works on the following problem. >$$ f(x) = g(x) + h(x)$$ * $$g$$ is convex and differentiable. (**dom**$$(g) = \mathbb{R}^n$$) * $$h$$ is convex and non-differentiable and "simple". # # # Algorithm Proximal gradient descent start $$x^{(0)}$$at, startsolutionfrom next process iteration. >$$ x^{(k)} = \text{prox}_{t_k}(x^{(k-1)} - t_k \nabla g(x^{(k-1)}) ), k=1, 2, 3,. . $$ from $$\text{prox}_{t}(\cdot)$$ $$h$$and, association, proximal operator is. > \begin{align} \text{prox}_{t}(x) = \underset{z}{\text{argmin}} \frac{1}{2t} \parallel x - z \parallel_2^2 + h(z) \end{align} Update generalized gradient $$G_{t}$$ using, from table to, table exists. > \begin{align} > x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k}(x^{(k-1)}), \space \space \text{where} \space G_{t}(x) = \frac{x-\text{prox}_{t} (x - t \nabla g(x))}{t} \\\\ > \end{align} # # # Performance * **Proximal gradient descent**performance $$h$$according to, exists. if, , $$h$$ complex function particularly, closed form minimize when, computation a lot solution to, performance very exists. * also, , $$g$$of function convergence rateand, convergence speed. , iteration when, prox operator execution because of, prox computation efficiency case, to, useful. # # Motivation **Proximal gradient descent**at, derivative possible function $$g$$ Taylor 2differencewith, approximation to, derivative function $$h$$ functionto, definition iterationwith, minimization. therefore, , as follows: 2difference with, theoremsolution exists. to, process [09-01 Proximal gradient descent]({% multilang_post_url contents/chapter09/20-01-08-09_01_proximal_gradient_descent %}) reference. > >$$ \begin{align} x^+ & = \underset{z}{\text{argmin}} \, \frac{1}{2t} \parallel x - t \nabla g(x) - z \parallel_2 ^2 + h(z) \\\\ > & = \underset{z}{\text{argmin}} \ \nabla g(x)^T (z - x) + \frac{1}{2t} \parallel z - x \parallel_2 ^2 + h(z) \\\\ > \end{align} > $$ second 1and, 2 of $$g$$ Taylor 2difference approximationwith, from, that can be derived, first, constant $$g(x)$$ (gradient descentat, and, to, ) Hessian $$\nabla^2 g(x)$$ $$\frac{1}{t} I$$(spherical curvature)to, solutionfrom exists. next figureat, proximal gradient of descent update stepat, $$g$$ 2difference approximationwith, minimization process showing, exists. <figure class="image" style="align: center; " > <p align="center" > <img src="{{ site. baseurl }}/img/chapter_img/chapter19/09. 01_01_proximal_gradient_descent. png" alt="[Fig 1] Proximal gradient descent updates [3]" width="70%" > <figcaption style="text-align: center; ">[Fig 1] Proximal gradient descent updates [3]</figcaption > </p > </figure > Gradient descentand, newton's of method difference2difference approximation when, of function local hessian $$\nabla^2 g(x)$$ use is. that, the above at, $$\frac{1}{t} I$$ instead, $$\nabla^2 g(x)$$ use What will it be? to, next at, explanation **proximal newton method** background, is. 