--- layout: post title: 19-01-02 Proximal Newton method chapter: '19' order: '4' owner: Hooncheol Shin categories: - - chapter19 lang: en --- In the previous section, we explained that the **proximal newton method** is a method that wants to use the local hessian $$\nabla^2 g(x)$$ instead of the spherical curvature $$\frac{1}{t} I$$ in the **proximal gradient descent** formula. The proximal newton method is an old idea that is being studied in is stattics under the term local score. Now let's look at how the **proximal newton method** can be formulated. # # Algorithm The Proximal gradient descent algorithm is consists of the process of finding the direction $$v$$ of the next step and then optimizing the step size $$t_k$$. * Step 1: Starting from the starting point $$x^{(0)}$$, iterate the following process. ($$k=1, 2, 3,. . $$) * Step 2: Find the direction $$v$$ of the next step. > \begin{align} v^{(k)} & = \underset{v}{\text{argmin}} \ \nabla g(x^{(k-1)})^T v + \frac{1}{2} v^T H^{(k-1)} v + h(x^{(k-1)} + v) \end{align} from $$H^{(k-1)} = \nabla^2 g(x^{(k-1)})$$ $$x^{(k-1)}$$at, is Hessian. * 3step: $$v^{(k)}$$ directionwith, step for, step size optimization. > \begin{align} x^{(k)} & =x^{(k-1)} + t_k v^{(k)} \end{align} $$t_k$$ step sizeto, $$t_k=1$$ pure proximal Newton is method. Backtracking line search through, step size optimizationprocess exists proximal gradient descent methodand, different is. # # # Next position view the above direction $$v$$ next position of $$z$$ at, table as follows: . > >$$ \begin{align} > z^{(k)} & = \underset{z}{\text{argmin}} \ \nabla g(x^{(k-1)})^T (z - x^{(k-1)})^T + \frac{1}{2} (z - x^{(k-1)})^T H^{(k-1)} (z - x^{(k-1)}) + h(z) \\\\ > x^{(k)} & =x^{(k-1)} + t_k (z^{(k)} - x^{(k-1)} ) > \end{align} > $$ with, first stepat, function minimization surrogate point $$z$$. that next, $$x^{(k-1)}$$at, of $$z$$ directionwith, but, always, $$z$$to, is not. 