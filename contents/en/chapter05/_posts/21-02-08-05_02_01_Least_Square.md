---
layout: post
title: 05-02-01 Linear Least-Squares Problems
chapter: '05'
order: 8
owner: Hooncheol Shin
categories:
- chapter05
lang: en
lesson_type: required
---

## Linear Least-Squares Problems

A linear least-squares problem is an optimization problem without constraints where we minimize the sum of squared errors:

$$\text{minimize}_{x} \quad f_0(x) = \|Ax - b\|_2^2 = \sum_{i=1}^{k} (a_i^T x - b_i)^2$$

**where:**
- $$A \in \mathbb{R}^{k \times n}$$ is a matrix with $$k \geq n$$
- $$a_i^T$$ are the rows of $$A$$
- $$x \in \mathbb{R}^n$$ is the variable we want to find
- $$b \in \mathbb{R}^k$$ is the target vector

**Goal:** Find $$x$$ to minimize the sum of squared residuals.

### Example: Linear Regression

Finding the best-fit line $$y = mx + c$$ through data points. We minimize the sum of squared vertical distances from points to the line.

**Goal:** Find $$m, c$$.

<div id="linear-regression-demo" style="border: 2px solid #ddd; padding: 20px; margin: 20px 0; border-radius: 10px; background-color: #f9f9f9;">
    <h4 style="text-align: center; color: #333;">Interactive Linear Regression Demonstration</h4>
    
    <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
        <!-- Canvas for visualization -->
        <div style="flex: 1; min-width: 400px;">
            <canvas id="regressionCanvas" width="400" height="300" style="border: 1px solid #ccc; background: white; cursor: crosshair;"></canvas>
            <p style="font-size: 12px; color: #666; margin-top: 5px;">
                <strong>Instructions:</strong> Click on the canvas to add data points. The red line shows the best-fit line.
            </p>
        </div>
        
        <!-- Controls and information -->
        <div style="flex: 1; min-width: 250px;">
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h5 style="margin-top: 0; color: #444;">Regression Parameters</h5>
                <div id="regression-params" style="font-family: monospace; font-size: 14px; line-height: 1.6;">
                    <div><strong>Slope (m):</strong> <span id="slope-value">0.000</span></div>
                    <div><strong>Intercept (c):</strong> <span id="intercept-value">0.000</span></div>
                    <div><strong>R² Score:</strong> <span id="r2-value">N/A</span></div>
                    <div><strong>MSE:</strong> <span id="mse-value">N/A</span></div>
                </div>
                
                <div style="margin-top: 15px;">
                    <h5 style="color: #444;">Equation</h5>
                    <div id="equation" style="font-family: monospace; font-size: 16px; background: #f0f0f0; padding: 8px; border-radius: 4px;">
                        y = 0.000x + 0.000
                    </div>
                </div>
                
                <div style="margin-top: 15px;">
                    <button onclick="clearPoints()" style="background: #ff6b6b; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; margin-right: 10px;">Clear Points</button>
                    <button onclick="addRandomPoints()" style="background: #4ecdc4; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Add Random Points</button>
                </div>
            </div>
            
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-top: 15px;">
                <h5 style="margin-top: 0; color: #444;">Mathematical Formulation</h5>
                <div style="font-size: 13px; line-height: 1.5;">
                    <p><strong>Objective:</strong> Minimize sum of squared residuals</p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace;">
                        S(m,c) = Σ(yᵢ - mxᵢ - c)²
                    </div>
                    <p style="margin-top: 10px;"><strong>Solution:</strong></p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace; font-size: 11px;">
                        m = Σ(xᵢ-x̄)(yᵢ-ȳ) / Σ(xᵢ-x̄)²<br>
                        c = ȳ - mx̄
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
// Linear Regression Interactive Demo
class LinearRegressionDemo {
    constructor() {
        this.canvas = document.getElementById('regressionCanvas');
        this.ctx = this.canvas.getContext('2d');
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        
        // Set up canvas
        this.canvas.addEventListener('click', (e) => this.addPoint(e));
        
        // Initialize with some sample points
        this.addRandomPoints();
        this.draw();
    }
    
    addPoint(event) {
        const rect = this.canvas.getBoundingClientRect();
        const x = event.clientX - rect.left;
        const y = event.clientY - rect.top;
        
        // Convert canvas coordinates to data coordinates
        const dataX = (x / this.canvas.width) * 10;
        const dataY = ((this.canvas.height - y) / this.canvas.height) * 10;
        
        this.points.push({x: dataX, y: dataY});
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    addRandomPoints() {
        // Add some sample points with a trend
        const baseSlope = 0.8;
        const baseIntercept = 2;
        
        for (let i = 0; i < 8; i++) {
            const x = Math.random() * 8 + 1;
            const y = baseSlope * x + baseIntercept + (Math.random() - 0.5) * 2;
            this.points.push({x: x, y: Math.max(0, Math.min(10, y))});
        }
        
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    clearPoints() {
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        this.draw();
        this.updateDisplay();
    }
    
    calculateRegression() {
        if (this.points.length < 2) {
            this.slope = 0;
            this.intercept = 0;
            return;
        }
        
        const n = this.points.length;
        const sumX = this.points.reduce((sum, p) => sum + p.x, 0);
        const sumY = this.points.reduce((sum, p) => sum + p.y, 0);
        const sumXY = this.points.reduce((sum, p) => sum + p.x * p.y, 0);
        const sumXX = this.points.reduce((sum, p) => sum + p.x * p.x, 0);
        
        const meanX = sumX / n;
        const meanY = sumY / n;
        
        const numerator = sumXY - n * meanX * meanY;
        const denominator = sumXX - n * meanX * meanX;
        
        if (Math.abs(denominator) < 1e-10) {
            this.slope = 0;
            this.intercept = meanY;
        } else {
            this.slope = numerator / denominator;
            this.intercept = meanY - this.slope * meanX;
        }
    }
    
    calculateR2() {
        if (this.points.length < 2) return 0;
        
        const meanY = this.points.reduce((sum, p) => sum + p.y, 0) / this.points.length;
        let ssRes = 0;
        let ssTot = 0;
        
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            ssRes += Math.pow(point.y - predicted, 2);
            ssTot += Math.pow(point.y - meanY, 2);
        }
        
        return ssTot === 0 ? 1 : 1 - (ssRes / ssTot);
    }
    
    calculateMSE() {
        if (this.points.length === 0) return 0;
        
        let mse = 0;
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            mse += Math.pow(point.y - predicted, 2);
        }
        
        return mse / this.points.length;
    }
    
    draw() {
        // Clear canvas
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw grid
        this.drawGrid();
        
        // Draw regression line
        if (this.points.length >= 2) {
            this.drawRegressionLine();
        }
        
        // Draw points and residuals
        this.drawPoints();
        
        // Draw axes labels
        this.drawLabels();
    }
    
    drawGrid() {
        this.ctx.strokeStyle = '#f0f0f0';
        this.ctx.lineWidth = 1;
        
        // Vertical lines
        for (let i = 0; i <= 10; i++) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.beginPath();
            this.ctx.moveTo(x, 0);
            this.ctx.lineTo(x, this.canvas.height);
            this.ctx.stroke();
        }
        
        // Horizontal lines
        for (let i = 0; i <= 10; i++) {
            const y = (i / 10) * this.canvas.height;
            this.ctx.beginPath();
            this.ctx.moveTo(0, y);
            this.ctx.lineTo(this.canvas.width, y);
            this.ctx.stroke();
        }
    }
    
    drawRegressionLine() {
        this.ctx.strokeStyle = '#ff4757';
        this.ctx.lineWidth = 2;
        
        const x1 = 0;
        const y1 = this.intercept;
        const x2 = 10;
        const y2 = this.slope * x2 + this.intercept;
        
        const canvasX1 = (x1 / 10) * this.canvas.width;
        const canvasY1 = this.canvas.height - (y1 / 10) * this.canvas.height;
        const canvasX2 = (x2 / 10) * this.canvas.width;
        const canvasY2 = this.canvas.height - (y2 / 10) * this.canvas.height;
        
        this.ctx.beginPath();
        this.ctx.moveTo(canvasX1, canvasY1);
        this.ctx.lineTo(canvasX2, canvasY2);
        this.ctx.stroke();
    }
    
    drawPoints() {
        for (const point of this.points) {
            const canvasX = (point.x / 10) * this.canvas.width;
            const canvasY = this.canvas.height - (point.y / 10) * this.canvas.height;
            
            // Draw residual line (vertical distance to regression line)
            if (this.points.length >= 2) {
                const predictedY = this.slope * point.x + this.intercept;
                const predictedCanvasY = this.canvas.height - (predictedY / 10) * this.canvas.height;
                
                this.ctx.strokeStyle = '#ff6b6b';
                this.ctx.lineWidth = 1;
                this.ctx.setLineDash([2, 2]);
                this.ctx.beginPath();
                this.ctx.moveTo(canvasX, canvasY);
                this.ctx.lineTo(canvasX, predictedCanvasY);
                this.ctx.stroke();
                this.ctx.setLineDash([]);
            }
            
            // Draw point
            this.ctx.fillStyle = '#2f3542';
            this.ctx.beginPath();
            this.ctx.arc(canvasX, canvasY, 4, 0, 2 * Math.PI);
            this.ctx.fill();
        }
    }
    
    drawLabels() {
        this.ctx.fillStyle = '#666';
        this.ctx.font = '12px Arial';
        
        // X-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.fillText(i.toString(), x - 5, this.canvas.height - 5);
        }
        
        // Y-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const y = this.canvas.height - (i / 10) * this.canvas.height;
            this.ctx.fillText(i.toString(), 5, y + 3);
        }
    }
    
    updateDisplay() {
        document.getElementById('slope-value').textContent = this.slope.toFixed(3);
        document.getElementById('intercept-value').textContent = this.intercept.toFixed(3);
        document.getElementById('equation').textContent = `y = ${this.slope.toFixed(3)}x + ${this.intercept.toFixed(3)}`;
        
        if (this.points.length >= 2) {
            document.getElementById('r2-value').textContent = this.calculateR2().toFixed(3);
            document.getElementById('mse-value').textContent = this.calculateMSE().toFixed(3);
        } else {
            document.getElementById('r2-value').textContent = 'N/A';
            document.getElementById('mse-value').textContent = 'N/A';
        }
    }
}

// Global functions for buttons
function clearPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
    }
}

function addRandomPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
        window.regressionDemo.addRandomPoints();
    }
}

// Initialize when DOM is loaded
document.addEventListener('DOMContentLoaded', function() {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
});

// Initialize immediately if DOM is already loaded
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
        if (document.getElementById('regressionCanvas')) {
            window.regressionDemo = new LinearRegressionDemo();
        }
    });
} else {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
}
</script>

**Problem:** Given $$n$$ data points $$(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$$, find the line $$y = mx + c$$ that minimizes the sum of squared vertical distances from the points to the line.

**Objective Function:** We want to minimize
$$S(m,c) = \sum_{i=1}^{n} (y_i - mx_i - c)^2$$

**Solution:** To find the minimum, we take partial derivatives and set them equal to zero.

Taking the partial derivative with respect to $$c$$:
> $$\frac{\partial S}{\partial c} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-1) = -2\sum_{i=1}^{n} (y_i - mx_i - c) = 0$$

This gives us:
$$\sum_{i=1}^{n} y_i = m\sum_{i=1}^{n} x_i + nc$$

Therefore:

> $$c = \frac{1}{n}\sum_{i=1}^{n} y_i - m\frac{1}{n}\sum_{i=1}^{n} x_i = \bar{y} - m\bar{x}$$

where $$\bar{x}$$ and $$\bar{y}$$ are the means of $$x$$ and $$y$$ values.

Taking the partial derivative with respect to $$m$$:

> $$\frac{\partial S}{\partial m} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-x_i) = -2\sum_{i=1}^{n} x_i(y_i - mx_i - c) = 0$$

Substituting $$c = \bar{y} - m\bar{x}$$:
$$\sum_{i=1}^{n} x_i(y_i - mx_i - \bar{y} + m\bar{x}) = 0$$

Rearranging:
$$\sum_{i=1}^{n} x_iy_i - m\sum_{i=1}^{n} x_i^2 - \bar{y}\sum_{i=1}^{n} x_i + m\bar{x}\sum_{i=1}^{n} x_i = 0$$

Since $$\sum_{i=1}^{n} x_i = n\bar{x}$$ and $$\sum_{i=1}^{n} x_iy_i - \bar{y}\sum_{i=1}^{n} x_i = \sum_{i=1}^{n} x_i(y_i - \bar{y})$$:

> $$\sum_{i=1}^{n} x_i(y_i - \bar{y}) = m\left(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2\right)$$

Note that $$\sum_{i=1}^{n} x_i^2 - n\bar{x}^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2$$

Therefore:

> $$m = \frac{\sum_{i=1}^{n} x_i(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

**Final Result:** The best-fit line has parameters:

> $$\boxed{m = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \quad \text{and} \quad c = \bar{y} - m\bar{x}}$$

This is the classical least-squares solution for linear regression, also known as the **Normal Equations**.

---

## The Optimal Solution of Linear Regression

**Problem Statement:**

In Linear Regression, we aim to find a vector of coefficients $$\mathbf{w}$$ that best fits a linear model to a given dataset. We have $$n$$ data points, each with $$p$$ features.

Let $$X$$ be the design matrix of size $$n \times p$$, where each row represents a data point and each column represents a feature.

Let $$\mathbf{y}$$ be the vector of target values of size $$n \times 1$$.

Our linear model predicts the target values $$\hat{\mathbf{y}}$$ as:
$$\hat{\mathbf{y}} = X\mathbf{w}$$
where $$\mathbf{w}$$ is the vector of unknown coefficients of size $$p \times 1$$.

**Objective Function (Cost Function):**

The goal is to minimize the sum of squared errors (residuals) between the actual target values $$\mathbf{y}$$ and the predicted values $$\hat{\mathbf{y}}$$. This is known as the Ordinary Least Squares (OLS) objective function:
$$J(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \| \mathbf{y} - X\mathbf{w} \|^2$$

We can express this in matrix form by expanding the squared Euclidean norm:
$$J(\mathbf{w}) = (\mathbf{y} - X\mathbf{w})^{\text{T}}(\mathbf{y} - X\mathbf{w})$$

Expanding this expression:
$$J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - \mathbf{y}^{\text{T}}X\mathbf{w} - (X\mathbf{w})^{\text{T}}\mathbf{y} + (X\mathbf{w})^{\text{T}}X\mathbf{w}$$

Using the property that $$(AB)^{\text{T}} = B^{\text{T}}A^{\text{T}}$$, we have $$(X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}$$.

Also, since $$\mathbf{y}^{\text{T}}X\mathbf{w}$$ is a scalar, its transpose is itself: $$(\mathbf{y}^{\text{T}}X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y}$$.

Thus, the two middle terms are identical:
$$J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - 2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y} + \mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w}$$

**Minimization using Calculus:**

To find the optimal $$\mathbf{w}$$ that minimizes $$J(\mathbf{w})$$, we take the derivative of $$J(\mathbf{w})$$ with respect to $$\mathbf{w}$$ and set it to zero. We use the following matrix calculus rules:

1. $$\frac{\partial (\mathbf{a}^{\text{T}}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{a}$$
2. $$\frac{\partial (\mathbf{x}^{\text{T}}A\mathbf{x})}{\partial \mathbf{x}} = (A + A^{\text{T}})\mathbf{x}$$ (If $$A$$ is symmetric, this simplifies to $$2A\mathbf{x}$$)

Applying these rules to $$J(\mathbf{w})$$:
$$\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = \frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} - \frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} + \frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}}$$

Let's evaluate each term:
- $$\frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 0$$ (since $$\mathbf{y}^{\text{T}}\mathbf{y}$$ is a scalar constant with respect to $$\mathbf{w}$$)
- $$\frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 2X^{\text{T}}\mathbf{y}$$ (using rule 1, with $$\mathbf{a} = X^{\text{T}}\mathbf{y}$$)
- For the third term, let $$A = X^{\text{T}}X$$. Note that $$A$$ is a symmetric matrix because $$(X^{\text{T}}X)^{\text{T}} = X^{\text{T}}(X^{\text{T}})^{\text{T}} = X^{\text{T}}X$$.
  
  So, $$\frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}} = 2X^{\text{T}}X\mathbf{w}$$ (using rule 2 for a symmetric matrix $$A$$)

Combining these, the derivative is:

> $$\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = 0 - 2X^{\text{T}}\mathbf{y} + 2X^{\text{T}}X\mathbf{w}$$

**Performance:**
- Time complexity: roughly $$n^2k$$ operations
- A standard computer solves problems with hundreds of variables and thousands of terms in seconds
- Sparse matrices (many zero entries) can be solved much faster

**Example:** A sparse matrix for image processing might have only 5 non-zero entries per row in a 10,000 × 10,000 matrix.