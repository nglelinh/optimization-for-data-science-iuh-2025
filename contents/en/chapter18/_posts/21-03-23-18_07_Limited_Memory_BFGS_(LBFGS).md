--- layout: post title: 18-07 Limited Memory BFGS (LBFGS) chapter: '18' order: '8' owner: Hooncheol Shin categories: - - chapter18 lang: en --- # # Introduction LBFGS is an example of Limited-memory quasi-Newton methods, and is useful when the cost of computing or storing the Hessian matrix is not reasonable. is Th method estimates (approximates) the Hessian matrix by maintaining only a few $$n$$-dimensional vectors instead of storing a dense $$n \times n$$ Hessian matrix. The LBFGS algorithm is based on BFGS, as its name suggests. The main idea is to use curvature information from the most recent iterations to estimate the Hessian. On the other hand, curvature information from older iterations is not used to save storage space, as it may be somewhat is dtant from the behavior shown by the Hessian of the current iteration. As a side note, limited-memory versions of other quasi-Newton algorithms (e. g. , SR1) can also be derived using the same technique [14]. # # LBFGS LBFGS with, explanationto, frontfrom BFGS methodabout, let's look at. stepat, BFGS as follows: $$x$$. >$$ >x^+ = x - t H \nabla f, \\\\ >\text{where } t \text{ thise step length and } H \text{ is updated at every iteration by means of the formula, }\\\\ >\text{ }\\\\ >H^+ = \big( I - \frac{sy^T}{y^Ts} \big) H \big( I - \frac{ys^T}{y^Ts} \big) + \frac{ss^T}{y^Ts}. \\\\ >$$ $$H$$to, about, use $$H^+q, q \in \mathbb{R}^n$$ scalar $$\alpha, \beta \in \mathbb{R}$$and, vector $$p, s \in \mathbb{R}^n$$ using, table exists. >$$ > \begin{align} >H^+q &= \big( I - \frac{sy^T}{y^Ts} \big) H \big( I - \frac{ys^T}{y^Ts} \big)q + \frac{ss^Tq}{y^Ts}\\\\ > &= \big( I - \frac{sy^T}{y^Ts} \big) \underbrace{H \\big( q - \frac{s^T q}{y^Ts} y \big)}_{p} + \underbrace{\frac{s^Tq}{y^Ts}}_{\alpha} s\\\\ > &= \big( I - \frac{sy^T}{y^Ts} \big) p + \alpha s\\\\ > &= p - \underbrace{\frac{y^Tp}{y^Ts}}_{\beta}s + \alpha s \\\\ > &= p + (\alpha - \beta) s, \\\\ >& \text{where } \alpha = \frac{s^Tq}{y^Ts}, q^+ = q - \alpha y, p = Hq, \beta = \frac{y^Tp}{y^Ts}. > \end{align} \\\\ >$$ $$H$$ kBFGS update through, when, , $$Hq= -H\nabla f(x)$$ length of k iteration 2to, computation exists (below algorithm reference). , of memory efficiency use for, $m$ iterationsat, curvature only information use. ($$k \ge m$$) # # Algorithm <figure class="image" style="align: center; " > <p align="center" > <img src="{{ site. baseurl }}/img/chapter_img/chapter18/algorithm_quasi-newton. png" alt="[Fig1] The algorithm of LBFGS [3]" width="90%" > <figcaption style="text-align: center; ">[Fig1] The algorithm of LBFGS [3]</figcaption > </p > </figure > usually, inverse Hessian approximation $$H_k$$ and dense, of variable case, operation very becomes. LBFGS $$H_k \nabla f_k$$ continuous vectorsumand, vectorproductwith, obtaining, with, $$H_of k$$ computation problem exists. computationto, useinitial Hessian approximation $$H^{0, k}$$ usually, (at, very effect, with, verification) identity matrixto, some constant product ($$H^{0, k} = \gamma_k I$$) because of, computationto, that large ([14]7. 2). > >$$ H^{0, k} = \gamma_k I, \\\\ > \text{where } \: \gamma_k = \frac{s^T_{k-1}y_{k-1}}{y^T_{k-1}y_{k-1}}. > $$ * **Note: ** $$H^{0, k}$$ iteration exists. 