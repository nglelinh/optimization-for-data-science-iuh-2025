--- layout: post title: 20-01-01 Convergence is Analys chapter: '20' order: '3' owner: Hooncheol Shin categories: - - chapter20 lang: en --- # # Lipschitz gradients and strong convexity Let's assume $$f$$ is a closed convex function. Then the following equivalence relationship holds. >\begin{equation} \text{$$f$$ is strongly convex with parameter $$d$$ $$\Longleftrightarrow \nabla f^{\ast}$$ Lipschitz with parameter $$1/d$$. } \end{equation} # # # Proof # # # if, $$g$$ strongly convex $$x$$at, becomes minimize next relationship,. let us >\begin{equation} g(y) \geq g(x) + \frac{d}{2}\lVert y-x \rVert_2^2, \text{ for all } y \end{equation}, $$g(x) = f(x) − u^T x$$ minimization$$x_u = \nabla f^{\ast}(u)$$and, $$g(x) = f(x) − v^T x$$ minimization$$x_v = \nabla f^{\ast}(v)$$ exists. that, above from, next exists. > >$$ \begin{alignat}{1} > f(x_v) - u^Tx_v \geq f(x_u) - u^T x_u + \frac{d}{2} \lVert x_u - x_v \rVert_2^2 \\ > f(x_u) - v^Tx_u \geq f(x_v) - v^T x_v + \frac{d}{2} \lVert x_u - x_v \rVert_2^2 > \end{alignat} > $$ above nextand, exists. >\begin{equation} f(x_v) - u^Tx_v + f(x_u) - v^Tx_u \geq f(x_u) - u^T x_u + f(x_v) - v^T x_v + d \lVert x_u - x_v \rVert_2^ 2. \end{equation} Cauchy-Schwartz application as follows: becomes theorem. > >$$ \begin{align} > d \lVert x_u - x_v \rVert_2^2 & \leq - u^Tx_v - v^Tx_u + u^T x_u + v^T x_v \\\\ > & = (u-v)^T(x_u - x_v) \\\\ > & \leq \lVert u-v \rVert_2 \lVert x_u - x_v \rVert_2 > \end{align} > $$ therefore, , nextand, relationship, exists. > $$\lVert x_u - x_v \rVert_2 \leq \frac{1}{d} \lVert u-v \rVert_2$$ to, $$\nabla f^{\ast}$$ Lipschitz with parameter $$1/d$$ proof. # # Convergence guarantees above result, and, gradient descent combining, , dual of objective optimal solutionto, convergenceas follows: explanation exists. * if, $$f$$ $$d$$to, strongly convex, step size $$t_k=d (k=1, 2, 3, \dots$$)about, from, dual gradient ascent $$O(1/\epsilon)$$with, converge. * if, $$f$$ $$d$$to, strongly convex, $$\nabla f$$ $$L$$to, Lipschitz, step size $$t_k=2/(1/d + 1/L)$$ ($$k=1, 2, 3, \dots$$)about, from, dual gradient ascent $$O(\log(1/\epsilon))$$with, converge. (linear convergence)