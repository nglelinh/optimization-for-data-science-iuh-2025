--- layout: post title: 20-03 Augmented Lagrangians chapter: '20' order: '7' owner: Hooncheol Shin categories: - - chapter20 lang: en --- Dual of ascent is dadvantage convergence for, condition, necessaryis. (convergence $$f$$ strongly convexsolution. ) is dadvantage **Augmented Lagrangian method** (or, **Method of multipliers**)by, improvement exists. Primal problem belowand, transformation. >\begin{equation} \min_x f(x) + \frac{\rho}{2} \lVert Ax - b \rVert _2^2 \quad \text{ subject to } \quad Ax = b \end{equation} from $$\rho > 0$$is. $$A$$ full column rank strongly convex. problemand, accurate same problem becomes. (Augmented term $$Ax - b$$ 0 because, is. ) # # Augmented Lagrangian Method **Dual gradient ascent**: $$k=1, 2, 3, \dots$$about, next iteration. > >$$ \begin{alignat}{1} > x^{(k)} & \in \arg\min_x f(x) + (u^{(k-1)})^T A x + \frac{\rho}{2} \lVert Ax - b \rVert_2^2 \\ > u^{(k)} & = u^{(k-1)} + \rho (A x^{(k)} - b) > \end{alignat} > $$ above dual algorithmat, $$\rho$$ step size, that is, $$t_k=\rho$$is. next, from that reason, exists. # # # $$\rho$$ step size when, optimality proof $$x^{(k)}$$ $$f(x) + (u^{(k-1)})^T Ax + \frac{\rho}{2} \lVert Ax - b\rVert _2^2$$ minimizationto, , primal problemto, about, stationary condition, according to, , $$x^{(k)}$$at, subgradient belowand, $$0$$ solution. > >$$ \begin{alignat}{1} > 0 & \in \partial f(x^{(k)}) + A^T (u^{(k-1)}) + \rho (A x^{(k)} -b)) \\ > & = \partial f(x^{(k)}) + A^T u^{(k)} > \end{alignat} > $$ aboveat, , $$u^{(k)} = u^{(k-1)} + \rho (A x^{(k)} - b)$$to, , condition, at, $$Ax^{(k)}-b$$ $$0$$with, from feasible solution start, with, KKT condition, , $$x^{(k)}$$and, $$u^{(k)}$$ optimalityto, exists. **Augmented Lagrangian method**advantage good convergence, is dadvantage problem solution decomposability is. 