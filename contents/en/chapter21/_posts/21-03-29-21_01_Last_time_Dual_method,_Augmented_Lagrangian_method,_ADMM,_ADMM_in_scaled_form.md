--- layout: post title: 21-01 Last time - Dual method, Augmented Lagrangian method, ADMM, ADMM in scaled form chapter: '21' order: '2' owner: Hooncheol Shin categories: - - chapter21 lang: en --- before, 20at, Dual methods, ADMMabout,. of ADMM to, from, Dual methodsand, Augmented Lagrangian method, ADMM, ADMM in scaled formabout,. # # Dual method the following problem let's look at. let us >$$ > \begin{align} >&\min_{x} &&f(x) \\\\ >&\text{ subject to } &&Ax = b > \end{align} >$$ from $f$ strictly convex is exts. of problem Lagrangian belowand, equals. >$$ > \begin{align} >L(x, u) = f(x)+u^{T}(Ax-b) > \end{align} >$$ above of problem dual problem belowand, equals. >$$ >\begin{equation} >\max_u -f^{\ast}(-A^T u) - b^T u >\end{equation} >$$ at, u dual is variable. to, about, dual gradient ascent the following iterationwith, computation. ($k=1, 2, 3,. . $) >$$ > \begin{align} >x^{(k)}&=\underset{x}{\operatorname{argmin}} L(x, u^{(k-1)}) \\\\ >u^{(k)}&= u^{(k-1)} +t_{k}(Ax^{(k)}-b) > \end{align} >$$ $$t_{k}$$ k of iteration step is size. dual methodat, , primal variable $$x$$ first before, at, $$u^{(k-1)}$$at, Lagrangian minimization$$x$$with, , dual variable $$u$$ $$Ax-b$$ gradient direction gradient of ascent to, becomes. of method advantage $$f$$ Bproblemto, possible when, (decomposable), $$x$$ also, Bwith, $$( x =(x_{1},. . , x_{B})\in \mathbb{R}^{n}, \text{ where }x_{i}\in \mathbb{R}^{n_{i}})$$, matrix A also, Bsub-matrix with, decompose possiblesolutionfrom$$(A = [A_{1},. . , A_{B}] \text{ where }A_{i} \in \mathbb{R}^{m \times n_{i}})$$, or, possible computation. but, is dadvantage convergence for, to, condition, necessary; of primal feasible for, , $$f$$ strongly convexcondition, necessary. [[20-01-01]]({% multilang_post_url contents/chapter20/21-03-27-is 20_01_01_Convergence_Analys %}) # # Augmented Lagrangian method Method of multipliersAugmented Lagrangian method primal problemto, computation. iteration iterationfrom difference of KKT conditions becomes. Dual methodand, comparing, convergenceto, about, condition, (f strongly convex). instead of, of problem solution(decompose) possiblesolutionis dadvantage exists. Primal of problem definition as follows: . >$$ > \begin{align} >&\min_{x} &&f(x)+\frac{\rho}{2}||Ax-b||_{2}^{2}&\\\\ >&\text{subject to} &&Ax=b > \end{align} >$$ from $$\rho>0$$is. of problem Lagrangian belowand, equals. >$$ > \begin{align} >L_{\rho}(x, u)=f(x)+u^{T}(Ax-b)+\frac{\rho}{2}||Ax-b||_{2}^{2}. > \end{align} >$$ Dual gradient ascent next iteration. ($$k=1, 2, 3,. . $$) >$$ > \begin{align} >x^{(k)}&=\underset{x}{\operatorname{argmin}} L_{\rho}(x, u^{(k-1)}) \\\\ >u^{(k)}&= u^{(k-1)} +\rho(Ax^{(k)}-b) > \end{align} >$$ of method advantage aboveat, , dual methodto, convergence condition,. is dadvantage product to, solutionpossible (decomposability) becomes. # # Alternating direction method of multipliers(ADMM) ADMM dual methodand, augmented Lagrangian of method advantage is method. let us problem the following to, definition is exts. >$$ > \begin{align} >\min_{x} f(x)+g(z) \qquad \text{subject to }Ax+Bz=c > \end{align} >$$ to, $$\rho>0$$ augmented Lagrangian definition exists. >$$ > \begin{align} >&L_{\rho} (x, z, u) = f(x)+g(z)+u^{T}(Ax+Bz-c)+\frac{\rho}{2}||Ax+Bz-c||_{2}^{2}\\\\ > \end{align} >$$ from belowiteration variable. >$$ > \begin{align} >&\text{for k = 1, 2, 3,. . }\\\\ >&x^{(k)}=\underset{x}{\operatorname{argmin}} L_{\rho}(x, z^{(k-1)}, u^{(k-1)})\\\\ >&z^{(k)}=\underset{z}{\operatorname{argmin}} L_{\rho}(x^{(k)}, z, u^{(k-1)})\\\\ >&u^{(k)}=u^{(k-1)}+\rho(Ax^{(k)}+Bz^{(k)}-c) > \end{align} >$$ ADMMat, primal variable $$x, z$$ together, differencewith, each. and, differencewith, when, different variable use. that is, , k iterationat, $$z$$ when, to, before, of iteration $$x^{(k-1)}$$ $$x^{(k)}$$ use, u when, also, current iterationat, obtained primal variable $$x^{(k)}, z^{(k)}$$ to, use. # # Alternating direction method of multipliers(ADMM) ADMM constraint Aand, B full rank, $$f$$and, $$g$$to, about, large constraint (under modeset assumption) all $$\rho > 0$$about, satisfies next. * Residual convergence: $$k$$ $$\infty$$to, when, , $$r^{(k)} = A x^{(k)} - B z^{(k)} - c \to 0$$, that is, primal iteration feasibilityto,. * Objective convergence: $$f(x^{(k)}) + g(x^{(k)}) \to f^{\star} + g^{\star}$$, from $$f^{\star} + g^{\star}$$ of optimal primal objective is. * Dual convergence: $$u^{(k)} \to u^{\star}$$, from $$u^{\star}$$ dual solution is. Convergence rateabout, fromyet generally, informing, , research is exts. Convergenceto, about, reference [21 ]({% multilang_post_url contents/chapter21/21-03-29-21_00_Alternating_Direction_Method_of_Multipliers %})to, is fromexts. # # ADMM in scaled form We can express ADMM in scaled form by changing the dual variable $$u$$ to the scaled variable $$w=u/\rho$$. In summary, the ADMM steps can be represented as follows: >$$ > \begin{align} &x^{(k)} = \underset{x}{\operatorname{argmin}} f(x) + \frac{\rho}{2} ||Ax + Bz^{(k-1)} - c + w^{(k-1)} ||_2^2 \\\\ &z^{(k)} = \arg\min_z g(x) + \frac{\rho}{2} || Ax^{(k)} + Bz - c + w^{(k-1)} ||_2^2 \\\\ &w^{(k)} = w^{(k-1)} + Ax^{(k)} + Bz^{(k)} - c \end{align} >$$ Here, $$w^{(k)}$$ can also be expressed as the sum of residuals up to the $$k$$-th iteration as shown below. >$$ > \begin{align} w^{(k)} = w^{(0)} + \sum_{i=1}^k (Ax^{(i)} + Bz^{(i)} - c) \end{align} >$$