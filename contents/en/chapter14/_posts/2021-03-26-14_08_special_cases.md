---
layout: post
title: 14-08 Special cases
chapter: "14"
order: 14
owner: "Minjoo Lee"
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
    });
</script>

## Sparse, structured problems
If the linear system matrix in the problem is sparse and structured, and the Hessian can be efficiently computed, we can solve the problem more efficiently.

For example, if $$\nabla^{2}f(x)$$ is sparse and structured for all $$x$$, such as a [band matrix](https://en.wikipedia.org/wiki/Band_matrix), Newton's method can achieve $$O(n)$$ performance in memory and computation. (A band matrix is a matrix where nonzero entries are only near the diagonal.)

Let's look at two typical examples of functions with structured Hessians:

* If $$g(\beta) = f(X\beta)$$, then $$\nabla^{2}g(\beta)=X^{T}\nabla^{2}f(X\beta)X$$. If $$X$$ is a structured predictor matrix and $$\nabla^{2}f$$ (the Hessian of $$f$$) is diagonal, then $$\nabla^{2}g$$ is also structured.

* If $$\nabla^{2}f$$ is diagonal and $$g$$ is non-smooth, consider minimizing $$f(\beta)+g(D\beta)$$, where $$D$$ is a structured penalty matrix. The Lagrange dual is $$-f^{*}(-D^{T}u)-g^{*}(-u)$$. In general, $$-D\nabla^{2}f^{*}(-D^{T}u)D^{T}$$ is also structured.

## Equality-constrained Newton's method
Now let's look at optimization problems with equality constraints. Generally, we can approach this problem in three ways:
>$$\begin{align}
>&\min_{x} f(x) & \text{subject to }Ax=b.
>\end{align}$$

1) Reduced-space approach: Restrict the domain to the space satisfying the equality constraint. For the above problem, express $$x$$ as $$x=Fy+x_{0}$$, where $$F$$ spans the null space of $$A$$ and $$Ax_{0}=b$$. Then solve for $$y$$.

2) Equality-constrained Newton's method: Similar to unconstrained Newton's method, but with two differences. First, the initial value must be feasible ($$x \in dom (f)$$ and $$Ax = b$$). Second, the Newton step $$\Delta x_{nt}$$ must satisfy $$A\Delta x_{nt}=0$$. See below for details.

3) Dual approach: The Fenchel dual is $$-f^{*}(-A^{T}v)-b^{T}v$$, and strong duality holds. ([16-03]({% multilang_post_url contents/chapter16/21-03-31-16_03_fenchel_duality %}) covers this in detail.) Use the conjugate function to solve the dual problem. Here, $$f^{*}$$ is the conjugate of $$f$$.
>$$\begin{align}
>g(v) &= -b^{T}v + \min_{x}(f(x)+v^{T}Ax)\\\\
> &= -b^{T}v - \max_{x}\big( (-A^{T}v)^{T}x - f(x) \big)\\\\
> &= -b^{T}v - f^{*}(-A^{T}v),
>\end{align}$$

This leads to the following dual problem:

>$$\begin{align}
>\max -b^{T}v-f^{*}(-A^{T}v). 
>\end{align}$$

Assuming the optimal value exists, this problem is strictly feasible and satisfies Slater's condition. Therefore, as mentioned earlier, strong duality holds, and there exists a $$v^{*}$$ such that $$g(v^{*})=p^{*}$$.[1, p.525]

Now, let's examine the second method.
To derive a feasible Newton step $$\Delta x_{nt}$$, we replace the objective function in the original problem with a quadratic approximation around $$x$$. This can be expressed as:
>$$\begin{align}
>\text{minimize}\quad &\hat{f}(x+v) = f(x) + \nabla f(x)^{T}v + \frac{1}{2}v^{T}\nabla^{2} f(x) v\\\\
>\text{subject to}\quad &A(x+v) = b,
>\end{align}$$

This can also be expressed as:
>$$\begin{align}
>x^{+} = x + tv,\,\, \text{where}\\\\
>v = \underset{A(x+z)=b}{\operatorname{argmin}}\big( f(x)+\nabla f(x)^{T}z+\frac{1}{2}z^{T}\nabla^{2} f(x)z \big)\\\\
>\end{align}$$

Since $$Ax^{+} = Ax+tAv = b$$, the solution $$x$$ remains within the constraint in the subsequent steps of the iteration.

The KKT conditions for this problem can be expressed as follows, and by solving the linear system below, we can obtain the solution. Recall that $$v$$ is the Newton step $$\Delta x_{nt}$$.
>$$\begin{align}
>\begin{bmatrix}
> \nabla^{2} f(x) & A^{T}\\\\
> A & 0
>\end{bmatrix}
>\begin{bmatrix}
>v\\\\
>w
>\end{bmatrix}
>=-
>\begin{bmatrix}
>\nabla f(x)\\\\
>Ax-b
>\end{bmatrix}
>\end{align}$$

Here, $$w$$ is the optimal dual variable for the above quadratic problem.