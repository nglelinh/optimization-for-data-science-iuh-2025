---
layout: post
title: Analysis
chapter: "00"
order: 4
owner: "GitHub Copilot"
---


Continuity and Uniform Continuity

**Continuity** and **Uniform Continuity** are fundamental concepts that describe the behavior of functions, particularly concerning their "smoothness" or "predictability." While closely related, they capture distinct properties, with uniform continuity being a stronger condition than mere continuity.

---
### Definition of Continuity

A function $$f: A \to \mathbb{R}$$ is said to be **continuous at a point** $$c \in A$$ if, for every positive real number $$\varepsilon > 0$$, there exists a positive real number $$\delta > 0$$ such that for all $$x \in A$$, if $$\lvert x - c \rvert < \delta$$, then $$\lvert f(x) - f(c) \rvert < \varepsilon$$. 

Intuitively, it means that for any desired level of precision $$\varepsilon$$ in the output $$f(x)$$, we can find a sufficiently small interval around $$c$$ (of width $$2\delta$$) such that all $$x$$ values within this interval map to $$f(x)$$ values within an $$\varepsilon$$-interval around $$f(c)$$. The crucial aspect here is that the choice of $$\delta$$ generally depends not only on $$\varepsilon$$ but also on the specific point $$c$$.

---

A function is **continuous on a set** $$A$$ if it is continuous at every point $$c \in A$$.

**Examples of continuous functions** include all polynomials (e.g., $$f(x) = x^2 + 3x - 1$$), trigonometric functions like $$\sin(x)$$ and $$\cos(x)$$, and exponential functions $$e^x$$ on their respective domains. 

A classic example of a discontinuous function is the Heaviside step function, $$H(x)$$, which is $$0$$ for $$x < 0$$ and $$1$$ for $$x \ge 0$$, exhibiting a jump discontinuity at $$x=0$$.

---
#### Dirichlet function

$$D(x) = \begin{cases} 1 & \text{if } x \in \mathbb{Q} \\ 0 & \text{if } x \notin \mathbb{Q} \end{cases}$$
where $$\mathbb{Q}$$ is the set of rational numbers.

 **Nowhere Continuous:** The Dirichlet function is discontinuous at every point on the real line. For any point, any neighborhood contains both rational and irrational numbers, causing the function's value to jump between 0 and 1.

---
### Definition of Uniform Continuity

**Uniform Continuity**, on the other hand, imposes a more stringent condition. A function $$f: A \to \mathbb{R}$$ is said to be **uniformly continuous on a set** $$A$$ if, for every positive real number $$\varepsilon > 0$$, there exists a positive real number $$\delta > 0$$ such that for all $$x, y \in A$$, if $$\lvert x - y \rvert < \delta$$, then $$\lvert f(x) - f(y) \rvert < \varepsilon$$.

The **key distinction** from point-wise continuity lies in the order of quantifiers: for **uniform** continuity, the $$\delta$$ depends *only* on $$\varepsilon$$ and is independent of the specific points $$x$$ and $$y$$ in the domain. This means that for a given $$\varepsilon$$, a single $$\delta$$ works uniformly across the entire domain $$A$$. Geometrically, it implies that the "steepness" (độ dốc) of the function is bounded; there are no points where the function becomes arbitrarily steep, which would necessitate an arbitrarily small $$\delta$$ for a fixed $$\varepsilon$$.

---
### Examples and Comparison

Consider the function $$f(x) = x^2$$. This function is continuous on all of $$\mathbb{R}$$. However, it is not uniformly continuous on $$\mathbb{R}$$. To see this, for a given $$\varepsilon$$, as $$x$$ gets larger, the slope $$2x$$ increases, meaning that for a fixed $$\varepsilon$$, the required $$\delta$$ becomes smaller and smaller. For instance, if we choose $$x_n = n$$ and $$y_n = n + \delta_n$$, then $$\lvert f(y_n) - f(x_n) \rvert = \lvert (n+\delta_n)^2 - n^2 \rvert = \lvert 2n\delta_n + \delta_n^2 \rvert$$. For this to be less than $$\varepsilon$$, $$\delta_n$$ must decrease as $$n$$ increases, demonstrating that no single $$\delta$$ works for all $$x$$. In contrast, $$f(x) = x^2$$ *is* uniformly continuous on any compact interval, such as $$[0, 1]$$.

Another example is $$f(x) = \frac{1}{x}$$ on the interval $$(0, 1)$$. It is continuous on this interval, but not uniformly continuous, as the function becomes infinitely steep as $$x \to 0^+$$. However, $$f(x) = \sin(x)$$ is uniformly continuous on $$\mathbb{R}$$.

---
### Relationship and Theorems

The relationship between these concepts is hierarchical: if a function is uniformly continuous on a set, it is necessarily continuous on that set. The converse, as illustrated by $$f(x) = x^2$$ on $$\mathbb{R}$$, is not generally true.

The **Heine-Cantor Theorem**: if a function is continuous on a compact set (a closed and bounded set in $$\mathbb{R}$$), then it is uniformly continuous on that set.

Uniform continuity is particularly important in areas like the theory of integration (e.g., Riemann integrability), the **construction of extensions** of functions, and in numerical analysis, where it guarantees that small errors in input lead to small errors in output, uniformly across the domain.

---
### Derivatives and Rate of Change

The derivative (đạo hàm) of a single variable function represents its instantaneous rate of change.

**Slope(Độ dốc) between two points:**
$$\text{Slope} = \frac{y_2 - y_1}{x_2 - x_1}$$

This formula calculates the average rate of change between two points $$(x_1, y_1)$$ and $$(x_2, y_2)$$.

**Example:** For points $$(1, 3)$$ and $$(4, 9)$$:
$$\text{Slope} = \frac{9 - 3}{4 - 1} = \frac{6}{3} = 2$$

**Derivative (instantaneous rate of change):**
$$f'(x_0) = \lim_{x_1 \to x_0} \frac{f(x_1) - f(x_0)}{x_1 - x_0} = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}$$

The derivative at point $$x_0$$ represents the slope of the tangent line (đường tiếp tuyến) at that exact point. It measures how fast the function is changing at that specific point.

**Example:** For $$f(x) = x^2$$ at $$x_0 = 3$$:
- Average slope from $$x = 3$$ to $$x = 3.1$$: $$\frac{(3.1)^2 - 3^2}{0.1} = \frac{9.61 - 9}{0.1} = 6.1$$
- As we make the interval smaller, this approaches $$f'(3) = 6$$ (the exact derivative)

**Note**: With a single variable function, the derivative at a specific point is just a number (scalar).

---
### Level Curves (Đường cong mức) of Functions

Level curves are a fundamental concept in multivariable calculus used to visualize functions of two variables, typically denoted as $$f(x, y)$$. They provide a way to represent a 3D surface in a 2D plane.

A **level curve** of a function $$f(x, y)$$ is the set of all points $$(x, y)$$ in the domain of $$f$$ where the function takes a constant value.

Mathematically, a level curve for a specific constant value $$c$$ is defined by the equation:
$$f(x, y) = c$$
where $$c$$ is a constant in the range of $$f$$.

Each different value of $$c$$ corresponds to a different level curve. A collection of level curves for various values of $$c$$ is called a **contour map** or **contour plot**.

<!-- Hình ảnh minh họa level curves sẽ được thêm vào sau -->

---
### Purpose and Significance

Level curves are important because they:

1. **Visualize 3D in 2D:** They allow us to understand the shape and behavior of a 3D surface using a 2D plot, which is much easier to draw and analyze on paper or a screen.
2. **Show where the function is constant:** All points on a single level curve have the same function value.
3. **Indicate Steepness:** The spacing between level curves provides information about the rate of change (steepness) of the function.
   - Where level curves are close together, the function is changing rapidly (the surface is steep).
   - Where level curves are far apart, the function is changing slowly (the surface is relatively flat).
4. **Identify Peaks and Valleys:** Peaks (local maxima) and valleys (local minima) on the surface often appear as points or small closed curves around which the level curves cluster.

---
### Examples

1. **Function:** $$f(x, y) = x^2 + y^2$$
   - **Level Curves:** $$x^2 + y^2 = c$$.
   - If $$c > 0$$, these are circles centered at the origin with radius $$\sqrt{c}$$.
   - If $$c = 0$$, it's the single point $$(0, 0)$$.
   - If $$c < 0$$, there are no points, so the level curve is empty.
   - The contour map is a series of concentric circles. This corresponds to the graph $$z = x^2 + y^2$$, which is a paraboloid opening upwards.

2. **Function:** $$f(x, y) = x - y$$
   - **Level Curves:** $$x - y = c$$, which can be rewritten as $$y = x - c$$.
   - These are straight lines with a slope of 1. Different values of $$c$$ give different parallel lines.
   - The graph $$z = x - y$$ is a plane.

3. **Function:** $$f(x, y) = xy$$
   - **Level Curves:** $$xy = c$$.
   - If $$c \neq 0$$, these are hyperbolas.
   - If $$c = 0$$, the equation is $$xy = 0$$, which means $$x=0$$ or $$y=0$$. This corresponds to the union of the x-axis and the y-axis.
   - The graph $$z = xy$$ is a hyperbolic paraboloid (saddle shape).
---
## 2. Calculus (Multivariable)

**Why it's needed:** Calculus provides the tools to analyze the behavior of functions, find derivatives (gradients), and understand optimality conditions.

**Key Concepts:**

- **Partial derivatives** $$\frac{\partial f}{\partial x_1}$$
- **Gradient vector:** $$\nabla f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$$
- **Hessian matrix (matrix of second partial derivatives):** $$\nabla^2 f(\mathbf{x})_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$$. 

$$ \nabla^2 f(\mathbf{x}) = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix} $$

---
#### **Chain rule for multivariable functions**

The chain rule is a fundamental concept in calculus, providing a method for computing the derivative of a composite function. In single-variable calculus, if $$y = f(u)$$ and $$u = g(x)$$, then the derivative of $$y$$ with respect to $$x$$ is given by $$\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}$$. 

**Case 1: Single Independent Variable**

Consider a scenario where a scalar function $$z$$ depends on multiple variables, say $$x$$ and $$y$$, i.e., $$z = f(x, y)$$. Furthermore, these intermediate variables $$x$$ and $$y$$ themselves depend on a single independent variable, $$t$$, such that $$x = g(t)$$ and $$y = h(t)$$. We are interested in finding the rate of change of $$z$$ with respect to $$t$$, denoted as $$\frac{dz}{dt}$$. The multivariable chain rule states that this total derivative is the sum of the contributions from each intermediate path:
$$ \frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} $$
Here, $$\frac{\partial f}{\partial x}$$ and $$\frac{\partial f}{\partial y}$$ represent the partial derivatives of $$f$$ with respect to $$x$$ and $$y$$, respectively, holding the other variable constant. These terms quantify how sensitive $$f$$ is to changes in $$x$$ and $$y$$. The terms $$\frac{dx}{dt}$$ and $$\frac{dy}{dt}$$ are ordinary derivatives, representing the rates of change of $$x$$ and $$y$$ with respect to $$t$$. Intuitively, a small change $$\Delta t$$ in $$t$$ causes changes $$\Delta x = \frac{dx}{dt}\Delta t$$ and $$\Delta y = \frac{dy}{dt}\Delta t$$. These, in turn, contribute to the total change in $$z$$ by approximately $$\frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y$$. Dividing by $$\Delta t$$ and taking the limit as $$\Delta t \to 0$$ yields the chain rule.

**Example 1:**
Let $$z = f(x, y) = x^2 y + \sin(y)$$, where $$x = t^2$$ and $$y = e^t$$. We want to find $$\frac{dz}{dt}$$.
First, compute the necessary partial and ordinary derivatives:
$$ \frac{\partial f}{\partial x} = 2xy $$
$$ \frac{\partial f}{\partial y} = x^2 + \cos(y) $$
$$ \frac{dx}{dt} = 2t $$
$$ \frac{dy}{dt} = e^t $$
Now, apply the chain rule formula:
$$ \frac{dz}{dt} = (2xy)(2t) + (x^2 + \cos(y))(e^t) $$
Substituting $$x = t^2$$ and $$y = e^t$$ back into the expression:
$$ \frac{dz}{dt} = (2(t^2)(e^t))(2t) + ((t^2)^2 + \cos(e^t))(e^t) $$
$$ \frac{dz}{dt} = 4t^3 e^t + (t^4 + \cos(e^t))e^t $$
$$ \frac{dz}{dt} = 4t^3 e^t + t^4 e^t + e^t \cos(e^t) $$

**Case 2: Multiple Independent Variables**

The chain rule becomes even more versatile when the intermediate variables themselves depend on multiple independent variables. Suppose $$z = f(x, y)$$, but now $$x = g(s, t)$$ and $$y = h(s, t)$$. In this scenario, $$z$$ is ultimately a function of $$s$$ and $$t$$, and we need to find its partial derivatives with respect to $$s$$ and $$t$$. The chain rule provides two distinct formulas, one for each independent variable:
$$ \frac{\partial z}{\partial s} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial s} $$
$$ \frac{\partial z}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t} $$
Each formula follows the same logic as the single independent variable case, but now we are considering the rate of change with respect to one independent variable while holding the others constant. For instance, to find $$\frac{\partial z}{\partial s}$$, we consider how $$x$$ and $$y$$ change with $$s$$ (i.e., $$\frac{\partial x}{\partial s}$$ and $$\frac{\partial y}{\partial s}$$) and how these changes propagate through $$f$$.

**Example 2:**
Let $$z = f(x, y) = x \ln(y)$$, where $$x = s^2 + t^2$$ and $$y = e^{st}$$. We want to find $$\frac{\partial z}{\partial s}$$ and $$\frac{\partial z}{\partial t}$$.
First, compute the necessary partial derivatives:
$$ \frac{\partial f}{\partial x} = \ln(y) $$
$$ \frac{\partial f}{\partial y} = \frac{x}{y} $$
$$ \frac{\partial x}{\partial s} = 2s $$
$$ \frac{\partial x}{\partial t} = 2t $$
$$ \frac{\partial y}{\partial s} = t e^{st} $$
$$ \frac{\partial y}{\partial t} = s e^{st} $$
Now, apply the chain rule formulas:
For $$\frac{\partial z}{\partial s}$$:
$$ \frac{\partial z}{\partial s} = (\ln(y))(2s) + \left(\frac{x}{y}\right)(t e^{st}) $$
Substitute $$x = s^2 + t^2$$ and $$y = e^{st}$$:
$$ \frac{\partial z}{\partial s} = (\ln(e^{st}))(2s) + \left(\frac{s^2 + t^2}{e^{st}}\right)(t e^{st}) $$
$$ \frac{\partial z}{\partial s} = (st)(2s) + (s^2 + t^2)t $$
$$ \frac{\partial z}{\partial s} = 2s^2 t + s^2 t + t^3 = 3s^2 t + t^3 $$
For $$\frac{\partial z}{\partial t}$$:
$$ \frac{\partial z}{\partial t} = (\ln(y))(2t) + \left(\frac{x}{y}\right)(s e^{st}) $$
Substitute $$x = s^2 + t^2$$ and $$y = e^{st}$$:
$$ \frac{\partial z}{\partial t} = (\ln(e^{st}))(2t) + \left(\frac{s^2 + t^2}{e^{st}}\right)(s e^{st}) $$
$$ \frac{\partial z}{\partial t} = (st)(2t) + (s^2 + t^2)s $$
$$ \frac{\partial z}{\partial t} = 2st^2 + s^3 + st^2 = s^3 + 3st^2 $$

**Generalization: Vector and Matrix Form (Jacobian Matrix)**

Let $$f: \mathbb{R}^n \to \mathbb{R}^m$$ be a differentiable function and $$g: \mathbb{R}^p \to \mathbb{R}^n$$ be a differentiable function. The composite function is $$h = f \circ g: \mathbb{R}^p \to \mathbb{R}^m$$, defined by $$h(\mathbf{x}) = f(g(\mathbf{x}))$$. The derivative of $$h$$ at a point $$\mathbf{a} \in \mathbb{R}^p$$ is given by the product of the Jacobian matrices:
$$ D(f \circ g)(\mathbf{a}) = Df(g(\mathbf{a})) Dg(\mathbf{a}) $$
Here, $$D f(\mathbf{x})$$ denotes the Jacobian matrix of $$f$$ at $$\mathbf{x}$$, whose $$(i, j)$$-th entry is $$\frac{\partial f_i}{\partial x_j}$$. Specifically, if $$f = (f_1, \dots, f_m)$$ and $$g = (g_1, \dots, g_n)$$, then $$Df$$ is an $$m \times n$$ matrix and $$Dg$$ is an $$n \times p$$ matrix. The product $$Df Dg$$ is an $$m \times p$$ matrix, which is precisely the Jacobian matrix of $$h$$. This compact form encapsulates all the individual partial derivative chain rules. For instance, if $$z = f(x_1, \dots, x_n)$$ is a scalar-valued function ($$m=1$$) and $$x_i = g_i(t_1, \dots, t_p)$$ are functions of $$p$$ variables, then the Jacobian of $$z$$ with respect to $$(t_1, \dots, t_p)$$ is a $$1 \times p$$ row vector, and the formula expands to:
$$ \frac{\partial z}{\partial t_j} = \sum_{i=1}^n \frac{\partial f}{\partial x_i} \frac{\partial x_i}{\partial t_j} $$
This is the most general form of the chain rule for scalar-valued functions of multiple variables.

**Application: Implicit Differentiation**

A powerful application of the multivariable chain rule is in implicit differentiation. Suppose an equation $$F(x, y, z) = 0$$ implicitly defines $$z$$ as a function of $$x$$ and $$y$$, i.e., $$z = \phi(x, y)$$. We can find $$\frac{\partial z}{\partial x}$$ and $$\frac{\partial z}{\partial y}$$ without explicitly solving for $$z$$.
Treat $$F$$ as a composite function where $$F(x, y, \phi(x, y)) = 0$$. Differentiating both sides with respect to $$x$$ using the chain rule:
$$ \frac{\partial F}{\partial x} \frac{\partial x}{\partial x} + \frac{\partial F}{\partial y} \frac{\partial y}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 $$
Since $$\frac{\partial x}{\partial x} = 1$$ and $$\frac{\partial y}{\partial x} = 0$$ (as $$y$$ is held constant when differentiating with respect to $$x$$), this simplifies to:
$$ \frac{\partial F}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 $$
Solving for $$\frac{\partial z}{\partial x}$$:
$$ \frac{\partial z}{\partial x} = - \frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial z}} = - \frac{F_x}{F_z} $$
Similarly, for $$\frac{\partial z}{\partial y}$$:
$$ \frac{\partial z}{\partial y} = - \frac{\frac{\partial F}{\partial y}}{\frac{\partial F}{\partial z}} = - \frac{F_y}{F_z} $$
These formulas are widely used in various fields, including thermodynamics and economics, where variables are often implicitly related.

**Key Concepts and Intuition**

The chain rule can be intuitively understood through "tree diagrams" where nodes represent variables and edges represent dependencies. Each path from the ultimate dependent variable to an independent variable contributes a product of derivatives along that path, and the total derivative is the sum of these products. This visual aid helps in correctly identifying all terms in the sum.
Fundamentally, the chain rule quantifies how a change in an independent variable propagates through a sequence of functional dependencies to affect the ultimate dependent variable. It is a statement about the linearity of differentiation and the local approximation of functions by their differentials. The total differential $$dz = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$$ provides a direct link, as dividing by $$dt$$ (or $$ds$$, $$dt$$) yields the chain rule.

---
**Importance and Applications**

The multivariable chain rule is a cornerstone of advanced calculus and its applications. It is crucial in:
*   **Optimization:** When optimizing functions subject to constraints, or when parameters of a function are themselves functions of other variables.
*   **Physics and Engineering:** Describing rates of change in systems where quantities depend on intermediate variables (e.g., fluid flow, heat transfer, mechanics). For instance, in thermodynamics, state variables like pressure, volume, and temperature are often related, and the chain rule helps in deriving relationships between their partial derivatives.
*   **Machine Learning:** The backpropagation algorithm, which is central to training neural networks, is essentially an application of the multivariable chain rule to compute gradients efficiently. Each layer's weights are updated based on the error propagated backward through the network, using the chain rule to determine how changes in earlier weights affect the final output error.
*   **Coordinate Transformations:** When changing coordinate systems (e.g., from Cartesian to polar, cylindrical, or spherical coordinates), the chain rule is used to transform derivatives from one system to another.**

---
## Gradient Vector of Multiple Variable Functions

The gradient $$\nabla f$$ is a vector composed of the partial derivatives of the function $$f$$ with respect to each of its variables. It indicates the direction of the steepest ascent of the function at a given point.

For instance, if we have a function of two variables, $$f(x, y)$$, its gradient is:
$$ \nabla f = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix} $$

As a specific example, consider the function $$f(x, y) = x^3y$$.
The partial derivative with respect to $$x$$ is $$\frac{\partial f}{\partial x} = 3x^2y$$.
The partial derivative with respect to $$y$$ is $$\frac{\partial f}{\partial y} = x^3$$.
So, the gradient is $$\nabla f = \begin{pmatrix} 3x^2y \\ x^3 \end{pmatrix}$$.

An easy way to think of a gradient is that if we pick a point on some function, it gives us the "direction" the function is heading.

---
### Geometric Description of the Gradient Vector

There is a nice way to describe the gradient geometrically. Consider 

$$z=f(x,y)=4x^2+y^2$$

The level curves are the ellipses $$4x^2+y^2=c$$

**Note**: The gradient vector here exists only in the xy-plane (2D), not in 3D space.

**Example**: For a function $$f(x,y)$$, the gradient is $$\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)$$ - a 2D vector with no z-component.

---
### Theorem

**The direction of greatest decrease of f is the direction opposite to the gradient vector**


---
### Directional Derivatives - Đạo hàm theo hướng

**For a function $$z = f(x,y)$$, the partial derivative $$\frac{\partial f}{\partial x}$$ tells us how fast $$f$$ changes when we move in the $$x$$-direction**. Similarly, $$\frac{\partial f}{\partial y}$$ tells us how fast $$f$$ changes in the $$y$$-direction. But what if we want to know how fast $$f$$ changes when we move in some other direction?

The **directional derivative** measures the rate of change of $$f$$ when we move in any chosen direction $$\mathbf{u}$$. Here $$\mathbf{u}$$ must be a unit vector (length 1).

For a function $$w = f(x,y,z)$$ and unit vector $$\mathbf{u} = \langle u_1, u_2, u_3 \rangle$$:

$$D_{\mathbf{u}}f = \nabla f \cdot \mathbf{u} = \frac{\partial f}{\partial x}u_1 + \frac{\partial f}{\partial y}u_2 + \frac{\partial f}{\partial z}u_3$$

The directional derivative equals the dot product of the gradient $$\nabla f$$ and the direction vector $$\mathbf{u}$$.

**Note**: If $$\mathbf{u} = \langle 1,0,0 \rangle$$ (the $$x$$-direction), then $$D_{\mathbf{u}}f = \frac{\partial f}{\partial x}$$. So partial derivatives are just special cases of directional derivatives.

### Example

Find the directional derivative of $$f(x,y) = 4x^2 + y^2$$ at point $$(1,1)$$ in direction $$\langle 1,2 \rangle$$.

**Step 1:** Find the gradient
$$\nabla f = \langle 8x, 2y \rangle$$
At $$(1,1)$$: $$\nabla f = \langle 8, 2 \rangle$$

**Step 2:** Normalize the direction vector
$$\mathbf{u} = \frac{\langle 1,2 \rangle}{\lvert \langle 1,2 \rangle \rvert} = \frac{\langle 1,2 \rangle}{\sqrt{5}}$$

**Step 3:** Compute the directional derivative
$$D_{\mathbf{u}}f = \nabla f \cdot \mathbf{u} = \langle 8,2 \rangle \cdot \frac{\langle 1,2 \rangle}{\sqrt{5}} = \frac{8 + 4}{\sqrt{5}} = \frac{12}{\sqrt{5}}$$

---
### Maximum and Minimum Rates of Change

The directional derivative can be written as:
$$D_{\mathbf{u}}f = \lvert \nabla f \rvert \cos \theta$$

where $$\theta$$ is the angle between $$\nabla f$$ and $$\mathbf{u}$$.

Since $$-1 \leq \cos \theta \leq 1$$:
- **Maximum increase**: When $$\theta = 0$$ (same direction as $$\nabla f$$), we get $$D_{\mathbf{u}}f = \lvert \nabla f \rvert$$
- **Maximum decrease**: When $$\theta = \pi$$ (opposite to $$\nabla f$$), we get $$D_{\mathbf{u}}f = -\lvert \nabla f \rvert$$

Therefore:
- The gradient $$\nabla f$$ points in the direction of steepest increase
- The direction $$-\nabla f$$ points in the direction of steepest decrease
---
### Relation of Level Curves to the Gradient

At any point $$(x_0, y_0)$$ on a level curve $$f(x, y) = c$$, the gradient vector $$\nabla f(x_0, y_0)$$ is **orthogonal (perpendicular)** to the tangent line of the level curve at that point. The gradient vector points in the direction of the *steepest increase* of the function.

<!-- Hình ảnh minh họa gradient và level curves sẽ được thêm vào sau -->

---
### **Generalization: Level Surfaces**

The concept of level curves extends to functions of three variables, $$f(x, y, z)$$. For such a function, the set of points $$(x, y, z)$$ in the domain where $$f(x, y, z)$$ takes a constant value $$c$$ is called a **level surface**:
$$f(x, y, z) = c$$
These are surfaces in 3D space. For example, the level surfaces of $$f(x, y, z) = x^2 + y^2 + z^2$$ are spheres centered at the origin ($$x^2 + y^2 + z^2 = c$$, for $$c > 0$$).

The most important thing to know about gradients is that they always point in the direction of a function's steepest slope (độ dốc lớn nhất) at a given point.

The gradients of f are always perpendicular to its level curves.
$$\nabla f(x) \perp \text{level curves of } f - \{f=a\}$$

Using Inner Product:
$$
\nabla f(x) \cdot v = 0 \quad \text{for all } v \in T_x
$$

Where:

- **$$\nabla f(x)$$ (Gradient):** A vector showing the direction of the *steepest increase* of the function $$f$$ at point $$x$$.
- **Level Set of $$f$$:** All points where $$f$$ has a constant value (đường đồng mức).
- **$$T_x$$ (Tangent Space):** The set of all vectors tangent to the level set of $$f$$ at point $$x$$. These are directions you can move while staying on the level set.
- **$$v \in T_x$$ (Tangent Vector):** Any vector in the tangent space $$T_x$$.

**Simple Example (2D):**

For $$f(x, y) = x^2 + y^2$$, the level sets are circles.
At point $$(2, 0)$$ on the circle $$x^2+y^2=4$$:
- The gradient is $$\nabla f(2, 0) = \begin{pmatrix} 4 \\ 0 \end{pmatrix}$$. This points radially outward.
- The tangent space $$T_{(2,0)}$$ contains vertical vectors, like $$v = \begin{pmatrix} 0 \\ a \end{pmatrix}$$.
- Their dot product is $$\begin{pmatrix} 4 \\ 0 \end{pmatrix} \cdot \begin{pmatrix} 0 \\ a \end{pmatrix} = 0$$, showing they are orthogonal.

---
**Tính chất:**
Nếu các đạo hàm riêng bậc hai là liên tục, thì theo định lý Schwarz (hoặc định lý Clairaut), thứ tự lấy đạo hàm không quan trọng, tức là $$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$$. Do đó, ma trận Hessian là một ma trận đối xứng.

**Ý nghĩa:**
*   **Xác định tính lồi/lõm:**
    *   Nếu $$H f(\mathbf{x})$$ là ma trận xác định dương (positive definite) tại một điểm, hàm số $$f$$ là **lồi cục bộ** tại điểm đó.
    *   Nếu $$H f(\mathbf{x})$$ là ma trận xác định âm (negative definite) tại một điểm, hàm số $$f$$ là **lõm cục bộ** tại điểm đó.
    *   **Nếu $$H f(\mathbf{x})$$ là ma trận nửa xác định dương (positive semi-definite) trên toàn miền, hàm số $$f$$ là lồi.**
    *   **Nếu $$H f(\mathbf{x})$$ là ma trận nửa xác định âm (negative semi-definite) trên toàn miền, hàm số $$f$$ là lõm.**
*   **Điều kiện bậc hai cho cực trị:** Tại một điểm dừng $$\mathbf{x}^*$$ (nơi $$\nabla f(\mathbf{x}^*) = \mathbf{0}$$):
    *   Nếu $$H f(\mathbf{x}^*)$$ là xác định dương, $$\mathbf{x}^*$$ là một điểm cực tiểu cục bộ.
    *   Nếu $$H f(\mathbf{x}^*)$$ là xác định âm, $$\mathbf{x}^*$$ là một điểm cực đại cục bộ.
    *   Nếu $$H f(\mathbf{x}^*)$$ là không xác định (indefinite), $$\mathbf{x}^*$$ là một điểm yên ngựa.
    *   Nếu $$H f(\mathbf{x}^*)$$ là nửa xác định dương hoặc nửa xác định âm nhưng không xác định, kiểm tra thêm là cần thiết (ví dụ, sử dụng các đạo hàm bậc cao hơn).


---
#### Taylor series expansion (first and second order are particularly useful):

  - First-order: $$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)$$
  - Second-order: $$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \nabla^2 f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)$$The Taylor series expansion helps us approximate a complex function with a simpler polynomial function around a specific point. The first-order expansion uses a straight line (linear approximation) to estimate the function's value, while the second-order expansion uses a curve (quadratic approximation) for a more accurate estimate, also capturing its curvature.

Imagine you're describing a hill. A first-order approximation tells you if it's going up or down at your current spot. A second-order approximation also tells you if the hill is getting steeper or flatter.

This concept is vital for simplifying complex calculations, optimizing functions, and understanding local behavior in various scientific and engineering fields.
