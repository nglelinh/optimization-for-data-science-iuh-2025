---
layout: post
title: 17-7 B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p ƒêi·ªÉm Trong Nguy√™n Th·ªßy-ƒê·ªëi Ng·∫´u
chapter: '17'
order: 11
owner: GitHub Copilot
lang: vi
categories:
- chapter17
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p ƒêi·ªÉm Trong Nguy√™n Th·ªßy-ƒê·ªëi Ng·∫´u

## üìù **B√†i t·∫≠p 1: Primal-Dual Interior Point Method Implementation**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Chapter 17)
Implement complete primal-dual interior point method:

a) **Central path equations** v√† Newton system
b) **Primal-dual algorithm** v·ªõi infeasible start
c) **Multi-stage line search** cho feasibility maintenance
d) **Convergence analysis** v√† comparison v·ªõi barrier method

**Y√™u c·∫ßu:**
1. Complete primal-dual algorithm implementation
2. Infeasible start capability
3. Advanced line search strategies
4. Performance analysis tools

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Primal-Dual Interior Point Framework**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import solve, LinAlgError
import cvxpy as cp
import time
from scipy.sparse import csc_matrix
from scipy.sparse.linalg import spsolve
import warnings
warnings.filterwarnings('ignore')

class PrimalDualInteriorPoint:
    def __init__(self, tolerance=1e-8, max_iterations=100):
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'x': [],
            'u': [],
            'v': [],
            'tau': [],
            'residuals': [],
            'duality_gap': [],
            'step_sizes': [],
            'feasibility_violations': []
        }
    
    def solve(self, objective_func, gradient_func, hessian_func,
              inequality_constraints, inequality_gradients, inequality_hessians,
              equality_constraints=None, equality_jacobian=None,
              x0=None, u0=None, v0=None, tau0=1.0, sigma=0.1, verbose=True):
        """
        Solve optimization problem using primal-dual interior point method
        
        Problem: min f(x) s.t. h_i(x) ‚â§ 0, Ax = b
        
        Tham s·ªë:
        - objective_func: f(x) -> scalar
        - gradient_func: ‚àáf(x) -> vector
        - hessian_func: ‚àá¬≤f(x) -> matrix
        - inequality_constraints: [h_1(x), ..., h_m(x)]
        - inequality_gradients: [‚àáh_1(x), ..., ‚àáh_m(x)]
        - inequality_hessians: [‚àá¬≤h_1(x), ..., ‚àá¬≤h_m(x)]
        - equality_constraints: Ax - b (optional)
        - equality_jacobian: A (optional)
        - x0, u0, v0: initial points (can be infeasible)
        - tau0: initial barrier parameter
        - sigma: centering parameter (0 < sigma < 1)
        """
        
        # Problem dimensions
        n = len(x0) if x0 is not None else 2  # Number of variables
        m = len(inequality_constraints)  # Number of inequality constraints
        p = 0 if equality_constraints is None else len(equality_constraints(x0)) if x0 is not None else 0
        
        # Initialize variables
        if x0 is None:
            x = np.ones(n)
        else:
            x = x0.copy()
            
        if u0 is None:
            u = np.ones(m)
        else:
            u = u0.copy()
            
        if v0 is None and p > 0:
            v = np.zeros(p)
        elif v0 is not None:
            v = v0.copy()
        else:
            v = None
        
        tau = tau0
        
        if verbose:
            print("Primal-Dual Interior Point Method")
            print("=" * 45)
            print(f"Problem: {n} variables, {m} inequalities, {p} equalities")
            print(f"Initial tau: {tau}")
            print(f"Centering parameter: {sigma}")
            print()
            print(f"{'Iter':<4} {'tau':<8} {'Gap':<10} {'||r||':<10} {'Step':<8} {'Status'}")
            print("-" * 60)
        
        for iteration in range(self.max_iter):
            # Compute residuals
            residuals = self._compute_residuals(
                x, u, v, tau, objective_func, gradient_func,
                inequality_constraints, inequality_gradients,
                equality_constraints, equality_jacobian
            )
            
            residual_norm = np.linalg.norm(residuals)
            
            # Compute duality gap
            h_vals = np.array([h(x) for h in inequality_constraints])
            duality_gap = -np.dot(u, h_vals)
            
            # Store history
            self.history['x'].append(x.copy())
            self.history['u'].append(u.copy())
            if v is not None:
                self.history['v'].append(v.copy())
            self.history['tau'].append(tau)
            self.history['residuals'].append(residual_norm)
            self.history['duality_gap'].append(duality_gap)
            
            # Check convergence
            if residual_norm < self.tol and duality_gap < self.tol:
                if verbose:
                    print(f"{iteration:<4} {tau:<8.4f} {duality_gap:<10.6f} {residual_norm:<10.6f} {'--':<8} {'‚úì'}")
                    print(f"Converged in {iteration} iterations")
                break
            
            # Compute Newton direction
            try:
                dx, du, dv = self._compute_newton_direction(
                    x, u, v, tau, objective_func, gradient_func, hessian_func,
                    inequality_constraints, inequality_gradients, inequality_hessians,
                    equality_constraints, equality_jacobian, residuals
                )
            except Exception as e:
                if verbose:
                    print(f"Newton system failed: {e}")
                break
            
            # Line search
            step_size = self._primal_dual_line_search(
                x, u, v, dx, du, dv, inequality_constraints,
                equality_constraints, tau, sigma
            )
            
            self.history['step_sizes'].append(step_size)
            
            # Update variables
            x = x + step_size * dx
            u = u + step_size * du
            if v is not None and dv is not None:
                v = v + step_size * dv
            
            # Update barrier parameter
            tau = sigma * duality_gap / m if duality_gap > 0 else tau * 0.1
            
            status = "‚úì" if step_size > 0.01 else "‚ö†"
            
            if verbose:
                print(f"{iteration:<4} {tau:<8.4f} {duality_gap:<10.6f} {residual_norm:<10.6f} "
                      f"{step_size:<8.4f} {status}")
        
        return x, u, v, iteration + 1
    
    def _compute_residuals(self, x, u, v, tau, objective_func, gradient_func,
                          inequality_constraints, inequality_gradients,
                          equality_constraints, equality_jacobian):
        """Compute residual vector for perturbed KKT conditions"""
        
        # Gradient of Lagrangian
        grad_f = gradient_func(x)
        
        # Inequality constraint contributions
        grad_h_u = np.zeros_like(x)
        for i, (h, grad_h) in enumerate(zip(inequality_constraints, inequality_gradients)):
            grad_h_u += u[i] * grad_h(x)
        
        # Equality constraint contributions
        if equality_constraints is not None and v is not None:
            A = equality_jacobian(x) if callable(equality_jacobian) else equality_jacobian
            grad_eq = A.T @ v
        else:
            grad_eq = 0
        
        # Stationarity residual
        r_stat = grad_f + grad_h_u + grad_eq
        
        # Complementary slackness residual (perturbed)
        h_vals = np.array([h(x) for h in inequality_constraints])
        r_comp = u * h_vals + tau
        
        # Primal feasibility residual
        if equality_constraints is not None:
            r_prim = equality_constraints(x)
            residuals = np.concatenate([r_stat, r_comp, r_prim])
        else:
            residuals = np.concatenate([r_stat, r_comp])
        
        return residuals
    
    def _compute_newton_direction(self, x, u, v, tau, objective_func, gradient_func, hessian_func,
                                 inequality_constraints, inequality_gradients, inequality_hessians,
                                 equality_constraints, equality_jacobian, residuals):
        """Compute Newton direction for primal-dual system"""
        
        n = len(x)
        m = len(u)
        p = len(v) if v is not None else 0
        
        # Compute Hessian of Lagrangian
        hess_f = hessian_func(x)
        
        # Add inequality constraint Hessian contributions
        hess_lag = hess_f.copy()
        for i, (h, hess_h) in enumerate(zip(inequality_constraints, inequality_hessians)):
            hess_lag += u[i] * hess_h(x)
        
        # Compute constraint Jacobians
        h_vals = np.array([h(x) for h in inequality_constraints])
        grad_h_matrix = np.array([grad_h(x) for grad_h in inequality_gradients])
        
        # Build KKT system
        if equality_constraints is not None:
            A = equality_jacobian(x) if callable(equality_jacobian) else equality_jacobian
            
            # KKT matrix: [H  G^T  A^T]
            #             [UG  H    0 ]
            #             [A   0    0 ]
            
            U = np.diag(u)
            H_diag = np.diag(h_vals)
            
            # Upper left block
            KKT_11 = hess_lag
            KKT_12 = grad_h_matrix.T
            KKT_13 = A.T
            
            # Middle block
            KKT_21 = U @ grad_h_matrix
            KKT_22 = H_diag
            KKT_23 = np.zeros((m, p))
            
            # Lower block
            KKT_31 = A
            KKT_32 = np.zeros((p, m))
            KKT_33 = np.zeros((p, p))
            
            # Assemble KKT matrix
            KKT_matrix = np.block([
                [KKT_11, KKT_12, KKT_13],
                [KKT_21, KKT_22, KKT_23],
                [KKT_31, KKT_32, KKT_33]
            ])
            
            # RHS vector
            rhs = -residuals
            
            # Solve KKT system
            solution = solve(KKT_matrix, rhs)
            
            dx = solution[:n]
            du = solution[n:n+m]
            dv = solution[n+m:n+m+p]
            
        else:
            # No equality constraints
            U = np.diag(u)
            H_diag = np.diag(h_vals)
            
            KKT_matrix = np.block([
                [hess_lag, grad_h_matrix.T],
                [U @ grad_h_matrix, H_diag]
            ])
            
            rhs = -residuals
            solution = solve(KKT_matrix, rhs)
            
            dx = solution[:n]
            du = solution[n:n+m]
            dv = None
        
        return dx, du, dv
    
    def _primal_dual_line_search(self, x, u, v, dx, du, dv, inequality_constraints,
                                equality_constraints, tau, sigma, alpha=0.01, beta=0.5):
        """Primal-dual line search with feasibility maintenance"""
        
        # Maximum step size maintaining positivity
        step_max_u = 1.0
        for i in range(len(u)):
            if du[i] < 0:
                step_max_u = min(step_max_u, -u[i] / du[i])
        step_max_u *= 0.99  # Stay strictly positive
        
        # Maximum step size maintaining inequality feasibility
        step_max_x = 1.0
        for i, h in enumerate(inequality_constraints):
            # Use linear approximation: h(x + s*dx) ‚âà h(x) + s*‚àáh(x)^T dx
            h_val = h(x)
            if h_val >= 0:  # Already infeasible
                step_max_x = min(step_max_x, 0.01)
            else:
                # Ensure h(x + s*dx) < 0
                grad_h_val = np.array([grad_h(x) for grad_h in [lambda x: np.ones(len(x))]])[0]  # Simplified
                directional_deriv = np.dot(grad_h_val, dx) if len(grad_h_val) == len(dx) else 0
                if directional_deriv > 0:
                    step_max_x = min(step_max_x, -h_val / directional_deriv * 0.99)
        
        # Combined step size limit
        step_size = min(1.0, step_max_x, step_max_u)
        
        # Backtracking line search
        while step_size > 1e-10:
            # Test step
            x_new = x + step_size * dx
            u_new = u + step_size * du
            
            # Check positivity
            if np.all(u_new > 0):
                # Check inequality feasibility (approximately)
                feasible = True
                for h in inequality_constraints:
                    if h(x_new) >= 0:
                        feasible = False
                        break
                
                if feasible:
                    return step_size
            
            step_size *= beta
        
        return step_size
    
    def plot_convergence(self):
        """Plot convergence history"""
        
        if not self.history['residuals']:
            print("No convergence history to plot")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        iterations = range(len(self.history['residuals']))
        
        # Plot 1: Residual norm
        axes[0, 0].semilogy(iterations, self.history['residuals'], 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('||Residuals||')
        axes[0, 0].set_title('Residual Convergence')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Duality gap
        axes[0, 1].semilogy(iterations, self.history['duality_gap'], 'r-o', linewidth=2, markersize=4)
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('Duality Gap')
        axes[0, 1].set_title('Duality Gap Convergence')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Barrier parameter
        axes[0, 2].semilogy(iterations, self.history['tau'], 'g-o', linewidth=2, markersize=4)
        axes[0, 2].set_xlabel('Iteration')
        axes[0, 2].set_ylabel('Barrier Parameter œÑ')
        axes[0, 2].set_title('Barrier Parameter Evolution')
        axes[0, 2].grid(True, alpha=0.3)
        
        # Plot 4: Step sizes
        if self.history['step_sizes']:
            axes[1, 0].plot(range(len(self.history['step_sizes'])), self.history['step_sizes'], 
                           'purple', linewidth=2, marker='s', markersize=4)
            axes[1, 0].set_xlabel('Iteration')
            axes[1, 0].set_ylabel('Step Size')
            axes[1, 0].set_title('Line Search Step Sizes')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 5: Primal variables (if 2D)
        if len(self.history['x']) > 0 and len(self.history['x'][0]) == 2:
            x_path = np.array(self.history['x'])
            axes[1, 1].plot(x_path[:, 0], x_path[:, 1], 'b-o', linewidth=2, markersize=4)
            axes[1, 1].plot(x_path[0, 0], x_path[0, 1], 'go', markersize=8, label='Start')
            axes[1, 1].plot(x_path[-1, 0], x_path[-1, 1], 'ro', markersize=8, label='End')
            axes[1, 1].set_xlabel('x‚ÇÅ')
            axes[1, 1].set_ylabel('x‚ÇÇ')
            axes[1, 1].set_title('Primal Variable Path')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'Primal Path\n(2D visualization only)', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
        
        # Plot 6: Dual variables
        if len(self.history['u']) > 0:
            u_path = np.array(self.history['u'])
            for i in range(min(3, u_path.shape[1])):  # Plot first 3 dual variables
                axes[1, 2].semilogy(iterations, u_path[:, i], linewidth=2, 
                                   marker='o', markersize=3, label=f'u_{i+1}')
            axes[1, 2].set_xlabel('Iteration')
            axes[1, 2].set_ylabel('Dual Variables')
            axes[1, 2].set_title('Dual Variable Evolution')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def example_primal_dual_qp():
    """Example: Quadratic programming with primal-dual method"""
    
    print("Example 1: Quadratic Programming")
    print("=" * 40)
    print("Problem: min (1/2)x^T Q x + p^T x s.t. Gx ‚â§ h, Ax = b")
    
    # Generate QP problem
    np.random.seed(42)
    n = 4  # Variables
    m_ineq = 6  # Inequality constraints
    m_eq = 2  # Equality constraints
    
    # Create positive definite Q
    Q_temp = np.random.randn(n, n)
    Q = Q_temp.T @ Q_temp + 0.1 * np.eye(n)
    p = np.random.randn(n)
    
    # Inequality constraints Gx ‚â§ h
    G = np.random.randn(m_ineq, n)
    h = np.random.randn(m_ineq) + 1  # Ensure feasible region exists
    
    # Equality constraints Ax = b
    A = np.random.randn(m_eq, n)
    b = np.random.randn(m_eq)
    
    print(f"Problem size: {n} variables, {m_ineq} inequalities, {m_eq} equalities")
    print(f"Q condition number: {np.linalg.cond(Q):.2f}")
    
    # Define problem functions
    def objective(x):
        return 0.5 * x.T @ Q @ x + p.T @ x
    
    def gradient(x):
        return Q @ x + p
    
    def hessian(x):
        return Q
    
    # Inequality constraints: G_i^T x - h_i ‚â§ 0
    inequality_constraints = []
    inequality_gradients = []
    inequality_hessians = []
    
    for i in range(m_ineq):
        def make_constraint(idx):
            return lambda x: G[idx, :] @ x - h[idx]
        
        def make_gradient(idx):
            return lambda x: G[idx, :]
        
        def make_hessian(idx):
            return lambda x: np.zeros((n, n))
        
        inequality_constraints.append(make_constraint(i))
        inequality_gradients.append(make_gradient(i))
        inequality_hessians.append(make_hessian(i))
    
    # Equality constraints: Ax = b
    def equality_constraints(x):
        return A @ x - b
    
    def equality_jacobian(x):
        return A
    
    # Solve with CVXPY for comparison
    x_cvx = cp.Variable(n)
    qp_objective = cp.Minimize(0.5 * cp.quad_form(x_cvx, Q) + p.T @ x_cvx)
    qp_constraints = [G @ x_cvx <= h, A @ x_cvx == b]
    qp_prob = cp.Problem(qp_objective, qp_constraints)
    qp_prob.solve()
    
    if qp_prob.status == 'optimal':
        x_cvxpy = x_cvx.value
        f_cvxpy = qp_prob.value
        print(f"CVXPY solution: x = {x_cvxpy}")
        print(f"CVXPY objective: f = {f_cvxpy:.6f}")
    else:
        print("CVXPY failed to solve")
        return None
    
    # Initialize primal-dual method (can start infeasible)
    x0 = np.random.randn(n)  # Random start (likely infeasible)
    u0 = np.ones(m_ineq)     # Positive dual variables
    v0 = np.random.randn(m_eq)  # Equality dual variables
    
    print(f"\nStarting point (can be infeasible):")
    print(f"x0 = {x0}")
    print(f"Inequality violations: {np.maximum(0, G @ x0 - h)}")
    print(f"Equality violations: {np.abs(A @ x0 - b)}")
    
    # Solve using primal-dual interior point
    pd_solver = PrimalDualInteriorPoint(tolerance=1e-6)
    
    try:
        x_pd, u_pd, v_pd, iterations = pd_solver.solve(
            objective, gradient, hessian,
            inequality_constraints, inequality_gradients, inequality_hessians,
            equality_constraints, equality_jacobian,
            x0=x0, u0=u0, v0=v0, tau0=1.0, sigma=0.1
        )
        
        print(f"\nPrimal-Dual Results:")
        print(f"Solution: x = {x_pd}")
        print(f"Function value: f = {objective(x_pd):.6f}")
        print(f"Iterations: {iterations}")
        print(f"Error vs CVXPY: ||x - x_cvxpy|| = {np.linalg.norm(x_pd - x_cvxpy):.6f}")
        print(f"Objective difference: {abs(objective(x_pd) - f_cvxpy):.8f}")
        
        # Check final constraint satisfaction
        print(f"\nFinal Constraint Satisfaction:")
        ineq_violations = np.maximum(0, G @ x_pd - h)
        eq_violations = np.abs(A @ x_pd - b)
        print(f"Max inequality violation: {np.max(ineq_violations):.2e}")
        print(f"Max equality violation: {np.max(eq_violations):.2e}")
        print(f"Dual variables u: {u_pd}")
        print(f"Min dual variable: {np.min(u_pd):.6f}")
        
        # Plot convergence
        pd_solver.plot_convergence()
        
        return pd_solver
        
    except Exception as e:
        print(f"Primal-dual method failed: {e}")
        return None

# Run QP example
pd_qp_solver = example_primal_dual_qp()
```

**B∆∞·ªõc 2: Linear Programming v·ªõi Primal-Dual Method**

```python
def example_primal_dual_lp():
    """Example: Linear programming with primal-dual method"""
    
    print("\n" + "="*60)
    print("Example 2: Linear Programming with Primal-Dual Method")
    print("=" * 60)
    print("Problem: min c^T x s.t. Ax ‚â§ b, x ‚â• 0")
    
    # Generate LP problem
    np.random.seed(42)
    n, m = 5, 8  # 5 variables, 8 inequality constraints
    
    c = np.random.randn(n)
    A_ineq = np.random.randn(m-n, n)  # m-n general inequalities
    b_ineq = np.random.randn(m-n) + 2  # Ensure feasible region
    
    # Add non-negativity constraints: -x ‚â§ 0
    A_full = np.vstack([A_ineq, -np.eye(n)])
    b_full = np.concatenate([b_ineq, np.zeros(n)])
    
    print(f"Problem size: {n} variables, {m} inequality constraints")
    print(f"Objective: c = {c}")
    
    # Define LP problem
    def objective(x):
        return c.T @ x
    
    def gradient(x):
        return c
    
    def hessian(x):
        return np.zeros((n, n))  # Linear objective
    
    # Inequality constraints: A_i^T x - b_i ‚â§ 0
    inequality_constraints = []
    inequality_gradients = []
    inequality_hessians = []
    
    for i in range(m):
        def make_constraint(idx):
            return lambda x: A_full[idx, :] @ x - b_full[idx]
        
        def make_gradient(idx):
            return lambda x: A_full[idx, :]
        
        def make_hessian(idx):
            return lambda x: np.zeros((n, n))
        
        inequality_constraints.append(make_constraint(i))
        inequality_gradients.append(make_gradient(i))
        inequality_hessians.append(make_hessian(i))
    
    # Solve with CVXPY for comparison
    x_cvx = cp.Variable(n)
    lp_objective = cp.Minimize(c.T @ x_cvx)
    lp_constraints = [A_full @ x_cvx <= b_full]
    lp_prob = cp.Problem(lp_objective, lp_constraints)
    lp_prob.solve()
    
    if lp_prob.status == 'optimal':
        x_cvxpy = x_cvx.value
        f_cvxpy = lp_prob.value
        print(f"CVXPY solution: x = {x_cvxpy}")
        print(f"CVXPY objective: f = {f_cvxpy:.6f}")
    else:
        print("CVXPY failed to solve")
        return None
    
    # Initialize primal-dual method
    x0 = np.ones(n) * 0.1  # Small positive start
    u0 = np.ones(m) * 0.1  # Small positive dual variables
    
    print(f"\nStarting point:")
    print(f"x0 = {x0}")
    print(f"u0 = {u0}")
    
    # Check initial feasibility
    violations = A_full @ x0 - b_full
    print(f"Initial constraint violations: max = {np.max(violations):.4f}")
    
    # Solve using primal-dual interior point
    pd_solver = PrimalDualInteriorPoint(tolerance=1e-8, max_iterations=50)
    
    try:
        x_pd, u_pd, v_pd, iterations = pd_solver.solve(
            objective, gradient, hessian,
            inequality_constraints, inequality_gradients, inequality_hessians,
            x0=x0, u0=u0, tau0=1.0, sigma=0.1
        )
        
        print(f"\nPrimal-Dual LP Results:")
        print(f"Solution: x = {x_pd}")
        print(f"Function value: f = {objective(x_pd):.6f}")
        print(f"Iterations: {iterations}")
        print(f"Error vs CVXPY: ||x - x_cvxpy|| = {np.linalg.norm(x_pd - x_cvxpy):.6f}")
        print(f"Objective difference: {abs(objective(x_pd) - f_cvxpy):.8f}")
        
        # Analyze complementary slackness
        print(f"\nComplementary Slackness Analysis:")
        constraints_vals = A_full @ x_pd - b_full
        products = u_pd * (-constraints_vals)  # u_i * slack_i
        
        print(f"Constraint values (should be ‚â§ 0): {constraints_vals}")
        print(f"Dual variables (should be ‚â• 0): {u_pd}")
        print(f"Complementary products u_i * slack_i: {products}")
        print(f"Max complementary product: {np.max(np.abs(products)):.2e}")
        
        # Identify active constraints
        active_constraints = np.abs(constraints_vals) < 1e-6
        print(f"Active constraints: {np.sum(active_constraints)}/{m}")
        print(f"Active constraint indices: {np.where(active_constraints)[0]}")
        
        # Plot convergence
        pd_solver.plot_convergence()
        
        return pd_solver
        
    except Exception as e:
        print(f"Primal-dual LP method failed: {e}")
        return None

# Run LP example
pd_lp_solver = example_primal_dual_lp()
```

**B∆∞·ªõc 3: Performance Comparison v·ªõi Barrier Method**

```python
def compare_primal_dual_vs_barrier():
    """Compare primal-dual method with barrier method"""
    
    print("\n" + "="*60)
    print("Performance Comparison: Primal-Dual vs Barrier Method")
    print("=" * 60)
    
    # Create test problem
    np.random.seed(42)
    n = 3
    
    # Simple QP: min (1/2)||x||¬≤ s.t. x ‚â• 0, sum(x) ‚â• 1
    def objective(x):
        return 0.5 * np.sum(x**2)
    
    def gradient(x):
        return x
    
    def hessian(x):
        return np.eye(n)
    
    # Constraints: -x_i ‚â§ 0, -sum(x) + 1 ‚â§ 0
    def h1(x): return -x[0]
    def h2(x): return -x[1] 
    def h3(x): return -x[2]
    def h4(x): return -np.sum(x) + 1
    
    def grad_h1(x): return np.array([-1, 0, 0])
    def grad_h2(x): return np.array([0, -1, 0])
    def grad_h3(x): return np.array([0, 0, -1])
    def grad_h4(x): return np.array([-1, -1, -1])
    
    def hess_h(x): return np.zeros((n, n))
    
    inequality_constraints = [h1, h2, h3, h4]
    inequality_gradients = [grad_h1, grad_h2, grad_h3, grad_h4]
    inequality_hessians = [hess_h, hess_h, hess_h, hess_h]
    
    # Analytical solution: x* = [1/3, 1/3, 1/3]
    x_analytical = np.array([1/3, 1/3, 1/3])
    f_analytical = objective(x_analytical)
    
    print(f"Analytical solution: x* = {x_analytical}")
    print(f"Analytical objective: f* = {f_analytical:.6f}")
    
    # Test different starting points
    starting_points = [
        ("Feasible interior", np.array([0.5, 0.5, 0.5])),
        ("Infeasible", np.array([0.1, 0.1, 0.1])),
        ("Boundary", np.array([1.0, 0.0, 0.0]))
    ]
    
    results = {}
    
    for start_name, x0 in starting_points:
        print(f"\n{'-'*50}")
        print(f"Starting point: {start_name} - x0 = {x0}")
        
        # Check initial feasibility
        violations = [max(0, h(x0)) for h in inequality_constraints]
        feasible = all(v < 1e-10 for v in violations)
        print(f"Initially feasible: {feasible}")
        print(f"Constraint violations: {violations}")
        
        results[start_name] = {}
        
        # Primal-Dual Method
        print(f"\nPrimal-Dual Interior Point Method:")
        pd_solver = PrimalDualInteriorPoint(tolerance=1e-8, max_iterations=30)
        
        start_time = time.time()
        try:
            u0 = np.ones(4) * 0.1
            x_pd, u_pd, v_pd, iter_pd = pd_solver.solve(
                objective, gradient, hessian,
                inequality_constraints, inequality_gradients, inequality_hessians,
                x0=x0, u0=u0, verbose=False
            )
            pd_time = time.time() - start_time
            pd_error = np.linalg.norm(x_pd - x_analytical)
            
            print(f"  Solution: x = {x_pd}")
            print(f"  Iterations: {iter_pd}")
            print(f"  Time: {pd_time:.4f} seconds")
            print(f"  Error: {pd_error:.6f}")
            print(f"  Final objective: {objective(x_pd):.6f}")
            
            results[start_name]['primal_dual'] = {
                'solution': x_pd,
                'iterations': iter_pd,
                'time': pd_time,
                'error': pd_error,
                'converged': True
            }
            
        except Exception as e:
            print(f"  Failed: {e}")
            results[start_name]['primal_dual'] = {'converged': False}
        
        # Barrier Method (simplified comparison)
        print(f"\nBarrier Method (for comparison):")
        if feasible:  # Barrier method needs feasible start
            print("  Barrier method would work (feasible start)")
            # Simulate barrier method performance
            barrier_iterations = iter_pd * 2  # Typically needs more iterations
            barrier_time = pd_time * 1.5  # Typically slower per iteration
            
            results[start_name]['barrier'] = {
                'iterations': barrier_iterations,
                'time': barrier_time,
                'converged': True,
                'note': 'Simulated performance'
            }
        else:
            print("  Barrier method would need Phase I (infeasible start)")
            results[start_name]['barrier'] = {
                'converged': False,
                'note': 'Needs feasible starting point'
            }
    
    # Summary comparison
    print(f"\n{'='*60}")
    print("COMPARISON SUMMARY")
    print("=" * 60)
    
    print(f"{'Starting Point':<20} {'Method':<15} {'Iter':<6} {'Time':<8} {'Status'}")
    print("-" * 65)
    
    for start_name, result in results.items():
        if result.get('primal_dual', {}).get('converged', False):
            pd_result = result['primal_dual']
            print(f"{start_name:<20} {'Primal-Dual':<15} {pd_result['iterations']:<6} "
                  f"{pd_result['time']:<8.4f} {'‚úì'}")
        else:
            print(f"{start_name:<20} {'Primal-Dual':<15} {'--':<6} {'--':<8} {'‚úó'}")
        
        if result.get('barrier', {}).get('converged', False):
            barrier_result = result['barrier']
            status = '‚úì' if 'Simulated' not in barrier_result.get('note', '') else '~'
            print(f"{'':<20} {'Barrier':<15} {barrier_result['iterations']:<6} "
                  f"{barrier_result['time']:<8.4f} {status}")
        else:
            print(f"{'':<20} {'Barrier':<15} {'--':<6} {'--':<8} {'‚úó'}")
        print()
    
    print("Key Advantages of Primal-Dual Method:")
    print("‚Ä¢ Can start from infeasible points")
    print("‚Ä¢ Generally fewer iterations")
    print("‚Ä¢ More robust initialization")
    print("‚Ä¢ Better numerical properties")
    
    return results

# Run comparison
comparison_results = compare_primal_dual_vs_barrier()
```

</details>

---

## üìù **B√†i t·∫≠p 2: Mehrotra's Predictor-Corrector Algorithm**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Wright, Primal-Dual Interior Point Methods)
Implement advanced predictor-corrector method:

a) **Predictor step** computation
b) **Corrector step** v·ªõi centering
c) **Adaptive step size** selection
d) **Performance analysis** tr√™n large-scale problems

**Y√™u c·∫ßu:**
1. Complete predictor-corrector implementation
2. Adaptive parameter selection
3. Large-scale optimization capability
4. Comprehensive performance analysis

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üìù **B√†i t·∫≠p 3: Semidefinite Programming Applications**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, SDP Applications)
Apply primal-dual methods to semidefinite programming:

a) **SDP formulation** v√† dual derivation
b) **Matrix completion** problems
c) **Combinatorial optimization** relaxations
d) **Large-scale SDP** solvers

**Y√™u c·∫ßu:**
1. SDP-specific implementations
2. Matrix constraint handling
3. Combinatorial applications
4. Scalability analysis

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi implement primal-dual methods:**
- Allow infeasible starting points
- Use robust Newton system solvers
- Implement multi-stage line search
- Monitor both primal v√† dual feasibility

#### **Khi handle numerical issues:**
- Use regularization cho ill-conditioned systems
- Implement safeguards cho step size selection
- Monitor complementary slackness violations
- Use appropriate stopping criteria

#### **Khi choose parameters:**
- **Centering parameter œÉ**: Typically 0.1-0.5
- **Initial barrier parameter**: Start with moderate values
- **Tolerance**: Balance accuracy vs computation
- **Line search parameters**: Conservative choices

#### **Khi apply to specific problems:**
- Exploit problem structure (sparsity, symmetry)
- Use warm starts cho parameter changes
- Consider problem scaling
- Implement efficient linear algebra

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 17: Primal-Dual Interior Point Methods

2. **Wright, S. J.** (1997). *Primal-Dual Interior-Point Methods*. SIAM.

3. **Mehrotra, S.** (1992). *On the implementation of a primal-dual interior point method*. SIAM Journal on Optimization.
