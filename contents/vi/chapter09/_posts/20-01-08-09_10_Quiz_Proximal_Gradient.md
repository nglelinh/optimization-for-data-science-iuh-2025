---
layout: post
title: 09-10 B√†i t·∫≠p tr·∫Øc nghi·ªám - Ph∆∞∆°ng Ph√°p Proximal Gradient
chapter: '09'
order: 11
owner: GitHub Copilot
lang: vi
categories:
- chapter09
lesson_type: quiz
---

## üìö √în t·∫≠p l√Ω thuy·∫øt

Tr∆∞·ªõc khi l√†m b√†i t·∫≠p, h√£y √¥n l·∫°i c√°c kh√°i ni·ªám ch√≠nh trong ch∆∞∆°ng n√†y:

### ‚ö° **Ph∆∞∆°ng Ph√°p Gradient G·∫ßn K·ªÅ (Proximal Gradient)**

#### **B√†i to√°n composite**
X√©t h√†m m·ª•c ti√™u c√≥ th·ªÉ ph√¢n t√°ch:
$$f(x) = g(x) + h(x)$$

v·ªõi:
- $$g(x)$$: h√†m l·ªìi, kh·∫£ vi, $$\text{dom}(g) = \mathbb{R}^n$$
- $$h(x)$$: h√†m l·ªìi, c√≥ th·ªÉ kh√¥ng kh·∫£ vi

#### **√ù t∆∞·ªüng ch√≠nh**
- **X·∫•p x·ªâ $$g$$:** S·ª≠ d·ª•ng Taylor b·∫≠c hai v·ªõi Hessian $$\frac{1}{t}I$$
- **Gi·ªØ nguy√™n $$h$$:** Kh√¥ng x·∫•p x·ªâ ph·∫ßn kh√¥ng kh·∫£ vi
- **K·∫øt h·ª£p t·ªëi ∆∞u:** T√¨m ƒëi·ªÉm t·ªëi ∆∞u c·ªßa h√†m x·∫•p x·ªâ + $$h$$

#### **Thu·∫≠t to√°n c∆° b·∫£n**
$$x^{(k)} = \text{prox}_{t_k}(x^{(k-1)} - t_k \nabla g(x^{(k-1)}))$$

v·ªõi **√°nh x·∫° g·∫ßn k·ªÅ (proximal mapping):**
$$\text{prox}_t(x) = \underset{z}{\arg\min} \left\{ \frac{1}{2t} \lVert x - z \rVert_2^2 + h(z) \right\}$$

#### **Gi·∫£i th√≠ch tr·ª±c quan**
1. **B∆∞·ªõc gradient:** $$x - t\nabla g(x)$$ (gradient descent cho $$g$$)
2. **ƒêi·ªÅu ch·ªânh proximal:** T√¨m ƒëi·ªÉm g·∫ßn v·ªõi b∆∞·ªõc gradient nh∆∞ng l√†m nh·ªè $$h$$
3. **Trade-off:** C√¢n b·∫±ng gi·ªØa "g·∫ßn v·ªõi gradient step" v√† "nh·ªè $$h$$"

### üìè **√Ånh X·∫° G·∫ßn K·ªÅ (Proximal Mapping)**

#### **ƒê·ªãnh nghƒ©a**
$$\text{prox}_t(x) = \underset{z}{\arg\min} \left\{ \frac{1}{2t} \lVert x - z \rVert_2^2 + h(z) \right\}$$

#### **T√≠nh ch·∫•t quan tr·ªçng**
- **Ch·ªâ ph·ª• thu·ªôc v√†o $$h$$:** Kh√¥ng ph·ª• thu·ªôc v√†o $$g$$
- **C√≥ th·ªÉ t√≠nh gi·∫£i t√≠ch:** Cho nhi·ªÅu h√†m $$h$$ quan tr·ªçng
- **ƒê∆°n tr·ªã:** Nghi·ªám duy nh·∫•t (do t√≠nh l·ªìi ch·∫∑t)
- **Non-expansive:** $$\lVert \text{prox}_t(x) - \text{prox}_t(y) \rVert \le \lVert x - y \rVert$$

#### **V√≠ d·ª• quan tr·ªçng**

**1. L1 norm (LASSO):**
$$h(x) = \lambda \lVert x \rVert_1 \Rightarrow \text{prox}_t(x) = S_{\lambda t}(x)$$

**Soft-thresholding operator:**
$$[S_{\lambda t}(x)]_i = \begin{cases}
x_i - \lambda t & \text{if } x_i > \lambda t \\
0 & \text{if } |x_i| \le \lambda t \\
x_i + \lambda t & \text{if } x_i < -\lambda t
\end{cases}$$

**2. Indicator function (Projection):**
$$h(x) = I_C(x) \Rightarrow \text{prox}_t(x) = P_C(x)$$

**3. L2 norm:**
$$h(x) = \lambda \lVert x \rVert_2 \Rightarrow \text{prox}_t(x) = \max\left\{0, 1 - \frac{\lambda t}{\lVert x \rVert_2}\right\} x$$

### üìä **Ph√¢n T√≠ch H·ªôi T·ª•**

#### **Gi·∫£ ƒë·ªãnh**
- $$g$$ l·ªìi, kh·∫£ vi v·ªõi $$\nabla g$$ Lipschitz li√™n t·ª•c (h·∫±ng s·ªë $$L$$)
- $$h$$ l·ªìi, $$\text{prox}_t$$ t√≠nh ƒë∆∞·ª£c

#### **ƒê·ªãnh l√Ω h·ªôi t·ª•**
V·ªõi step size $$t \le 1/L$$:
$$f(x^{(k)}) - f^* \le \frac{\lVert x^{(0)} - x^* \rVert_2^2}{2tk}$$

**√ù nghƒ©a:**
- **T·ªëc ƒë·ªô h·ªôi t·ª•:** $$O(1/k)$$ (gi·ªëng gradient descent)
- **ƒêi·ªÅu ki·ªán step size:** $$t \le 1/L$$ (gi·ªëng gradient descent)
- **Hi·ªáu su·∫•t:** T∆∞∆°ng ƒë∆∞∆°ng gradient descent v·ªÅ s·ªë iterations

#### **Backtracking line search**
**ƒêi·ªÅu ki·ªán:** 
$$g(x - tG_t(x)) > g(x) - t\nabla g(x)^T G_t(x) + \frac{t}{2} \lVert G_t(x) \rVert_2^2$$

v·ªõi $$G_t(x) = \frac{x - \text{prox}_t(x - t\nabla g(x))}{t}$$

**L∆∞u √Ω:** Ch·ªâ ki·ªÉm tra ƒëi·ªÅu ki·ªán tr√™n $$g$$, kh√¥ng ph·∫£i $$f$$

### üéØ **V√≠ D·ª•: ISTA (Iterative Soft-Thresholding Algorithm)**

#### **B√†i to√°n LASSO**
$$\min_\beta \frac{1}{2} \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$

**Ph√¢n t√°ch:**
- $$g(\beta) = \frac{1}{2} \lVert y - X\beta \rVert_2^2$$ (kh·∫£ vi)
- $$h(\beta) = \lambda \lVert \beta \rVert_1$$ (kh√¥ng kh·∫£ vi)

#### **Gradient c·ªßa $$g$$**
$$\nabla g(\beta) = -X^T(y - X\beta) = X^T(X\beta - y)$$

#### **ISTA update**
$$\beta^{(k)} = S_{\lambda t}(\beta^{(k-1)} + tX^T(y - X\beta^{(k-1)}))$$

**∆Øu ƒëi·ªÉm:** ƒê∆°n gi·∫£n, hi·ªáu qu·∫£ h∆°n subgradient method nhi·ªÅu

### üéØ **V√≠ D·ª•: Matrix Completion**

#### **B√†i to√°n**
$$\min_B \frac{1}{2} \sum_{(i,j) \in \Omega} (Y_{ij} - B_{ij})^2 + \lambda \lVert B \rVert_{tr}$$

**Ph√¢n t√°ch:**
- $$g(B) = \frac{1}{2} \lVert P_\Omega(Y) - P_\Omega(B) \rVert_F^2$$ (kh·∫£ vi)
- $$h(B) = \lambda \lVert B \rVert_{tr}$$ (trace norm, kh√¥ng kh·∫£ vi)

#### **To√°n t·ª≠ chi·∫øu**
$$[P_\Omega(B)]_{ij} = \begin{cases}
B_{ij} & \text{if } (i,j) \in \Omega \\
0 & \text{otherwise}
\end{cases}$$

#### **Gradient c·ªßa $$g$$**
$$\nabla g(B) = -(P_\Omega(Y) - P_\Omega(B))$$

#### **Proximal mapping cho trace norm**
$$\text{prox}_t(B) = S_{\lambda t}(B)$$ (matrix soft-thresholding)

N·∫øu $$B = U\Sigma V^T$$ (SVD), th√¨:
$$S_{\lambda t}(B) = U\Sigma_{\lambda t} V^T$$

v·ªõi $$(\Sigma_{\lambda t})_{ii} = \max\{\Sigma_{ii} - \lambda t, 0\}$$

#### **Soft-Impute Algorithm**
$$B^{(k)} = S_{\lambda}(P_\Omega(Y) + P_\Omega^\perp(B^{(k-1)}))$$

### üîÑ **C√°c Tr∆∞·ªùng H·ª£p ƒê·∫∑c Bi·ªát**

Proximal gradient l√† **ph∆∞∆°ng ph√°p t·ªïng qu√°t** bao g·ªìm:

#### **1. Gradient Descent**
$$h = 0 \Rightarrow \text{prox}_t(x) = x$$
$$x^{(k)} = x^{(k-1)} - t\nabla g(x^{(k-1)})$$

#### **2. Projected Gradient Descent**
$$h = I_C \Rightarrow \text{prox}_t(x) = P_C(x)$$
$$x^{(k)} = P_C(x^{(k-1)} - t\nabla g(x^{(k-1)}))$$

#### **3. Proximal Minimization Algorithm (PMA)**
$$g = 0 \Rightarrow x^{(k)} = \text{prox}_t(x^{(k-1)})$$

### ‚ö° **Accelerated Proximal Gradient (FISTA)**

#### **√ù t∆∞·ªüng**
Th√™m **momentum** ƒë·ªÉ tƒÉng t·ªëc h·ªôi t·ª• t·ª´ $$O(1/k)$$ l√™n $$O(1/k^2)$$

#### **Thu·∫≠t to√°n**
$$v = x^{(k-1)} + \frac{k-2}{k+1}(x^{(k-1)} - x^{(k-2)})$$
$$x^{(k)} = \text{prox}_{t_k}(v - t_k \nabla g(v))$$

#### **Momentum weight**
$$\frac{k-2}{k+1} \to 1$$ khi $$k \to \infty$$

#### **ƒê·∫∑c ƒëi·ªÉm**
- **T·ªëc ƒë·ªô h·ªôi t·ª•:** $$O(1/k^2)$$ - t·ªëi ∆∞u cho first-order methods
- **Kh√¥ng monotonic:** C√≥ th·ªÉ tƒÉng t·∫°m th·ªùi ("Nesterov ripples")
- **Hi·ªáu qu·∫£ cao:** ƒê·∫∑c bi·ªát t·ªët cho c√°c b√†i to√°n c√≥ condition number l·ªõn

#### **FISTA cho LASSO**
C·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi ISTA v√† subgradient method

### üîÑ **So S√°nh C√°c Ph∆∞∆°ng Ph√°p**

| Ph∆∞∆°ng ph√°p | ƒêi·ªÅu ki·ªán | T·ªëc ƒë·ªô h·ªôi t·ª• | ·ª®ng d·ª•ng |
|-------------|-----------|---------------|----------|
| **Gradient Descent** | $$f$$ kh·∫£ vi | $$O(1/k)$$ | Smooth optimization |
| **Subgradient** | $$f$$ l·ªìi | $$O(1/\sqrt{k})$$ | Non-smooth |
| **Proximal Gradient** | $$f = g + h$$, $$g$$ smooth | $$O(1/k)$$ | Composite problems |
| **Accelerated Proximal** | $$f = g + h$$, $$g$$ smooth | $$O(1/k^2)$$ | Fast composite |

### üõ†Ô∏è **K·ªπ Thu·∫≠t Th·ª±c H√†nh**

#### **Khi n√†o s·ª≠ d·ª•ng Proximal Gradient:**
- **Composite functions:** $$f = g + h$$ v·ªõi $$g$$ smooth, $$h$$ non-smooth
- **Regularization problems:** LASSO, elastic net, group LASSO
- **Constrained optimization:** Projected gradient descent
- **Matrix problems:** Matrix completion, robust PCA
- **Sparse optimization:** Feature selection, compressed sensing

#### **Y√™u c·∫ßu implementation:**
1. **T√≠nh $$\nabla g$$:** Ph·∫£i t√≠nh ƒë∆∞·ª£c gradient c·ªßa ph·∫ßn smooth
2. **T√≠nh $$\text{prox}_t$$:** Ph·∫£i c√≥ c√¥ng th·ª©c gi·∫£i t√≠ch ho·∫∑c solver hi·ªáu qu·∫£
3. **Lipschitz constant $$L$$:** ƒê·ªÉ ch·ªçn step size ph√π h·ª£p

#### **Debugging v√† tuning:**
- **Ki·ªÉm tra proximal mapping:** Verify t√≠nh ƒë√∫ng ƒë·∫Øn
- **Step size selection:** Fixed ($$t = 1/L$$) ho·∫∑c backtracking
- **Convergence monitoring:** Theo d√µi $$f(x^{(k)})$$
- **Acceleration:** Th·ª≠ FISTA n·∫øu c·∫ßn tƒÉng t·ªëc

### üìà **·ª®ng D·ª•ng Th·ª±c T·∫ø**

#### **1. Machine Learning**
- **LASSO Regression:** Feature selection
- **Elastic Net:** K·∫øt h·ª£p L1 v√† L2 regularization
- **Group LASSO:** Structured sparsity
- **Sparse Logistic Regression:** Classification v·ªõi feature selection

#### **2. Signal Processing**
- **Compressed Sensing:** Kh√¥i ph·ª•c t√≠n hi·ªáu sparse
- **Image Denoising:** Total variation regularization
- **Matrix Completion:** Collaborative filtering, recommender systems

#### **3. Computer Vision**
- **Image Segmentation:** Total variation methods
- **Optical Flow:** Regularized motion estimation
- **Structure from Motion:** Bundle adjustment v·ªõi sparsity

#### **4. Statistics**
- **Variable Selection:** High-dimensional regression
- **Robust Statistics:** Huber loss, quantile regression
- **Graphical Models:** Sparse precision matrix estimation

### üí° **V√≠ D·ª• Minh H·ªça**

#### **B√†i to√°n:** $$\min_x \frac{1}{2}(x-2)^2 + |x|$$

**Ph√¢n t√°ch:**
- $$g(x) = \frac{1}{2}(x-2)^2$$, $$\nabla g(x) = x-2$$
- $$h(x) = |x|$$

**Proximal mapping:**
$$\text{prox}_t(x) = S_t(x) = \begin{cases}
x - t & \text{if } x > t \\
0 & \text{if } |x| \le t \\
x + t & \text{if } x < -t
\end{cases}$$

**Proximal gradient step:**
1. Gradient step: $$x - t(x-2) = (1-t)x + 2t$$
2. Soft-thresholding: $$\text{prox}_t((1-t)x + 2t)$$

### üéØ **K·∫øt N·ªëi V·ªõi C√°c Ch∆∞∆°ng Kh√°c**

#### **T·ª´ ch∆∞∆°ng tr∆∞·ªõc:**
- **Ch∆∞∆°ng 08:** Subgradient method cho non-smooth functions
- **Ch∆∞∆°ng 07:** Subgradient v√† subdifferential
- **Ch∆∞∆°ng 06:** Gradient descent cho smooth functions

#### **∆Øu ƒëi·ªÉm c·ªßa Proximal Gradient:**
- **Nhanh h∆°n subgradient:** $$O(1/k)$$ vs $$O(1/\sqrt{k})$$
- **T·ªïng qu√°t h∆°n gradient descent:** X·ª≠ l√Ω ƒë∆∞·ª£c non-smooth components
- **C√≥ th·ªÉ tƒÉng t·ªëc:** FISTA ƒë·∫°t $$O(1/k^2)$$

#### **H∆∞·ªõng ph√°t tri·ªÉn:**
- **ADMM:** Alternating Direction Method of Multipliers
- **Primal-dual methods:** X·ª≠ l√Ω constrained problems
- **Stochastic variants:** Mini-batch v√† online versions

### üåü **√ù Nghƒ©a L√Ω Thuy·∫øt**

#### **Proximal operator nh∆∞ "generalized gradient":**
- **Gradient:** $$\nabla f(x)$$ cho smooth functions
- **Subgradient:** $$\partial f(x)$$ cho non-smooth functions  
- **Proximal operator:** $$\text{prox}_t$$ cho composite functions

#### **Moreau envelope:**
$$f_t(x) = \min_z \left\{ f(z) + \frac{1}{2t}\lVert x - z \rVert^2 \right\}$$

**T√≠nh ch·∫•t:** $$f_t$$ smooth v·ªõi $$\nabla f_t(x) = \frac{x - \text{prox}_t(x)}{t}$$

#### **K·∫øt n·ªëi v·ªõi variational analysis:**
- Proximal gradient ‚Üî Forward-backward splitting
- Acceleration ‚Üî Inertial methods
- Convergence rates ‚Üî Complexity theory

### üí° **M·∫πo Th·ª±c H√†nh**

#### **Ch·ªçn ph∆∞∆°ng ph√°p:**
1. **Smooth function:** Gradient descent
2. **Non-smooth function:** Subgradient method
3. **Composite function v·ªõi prox t√≠nh ƒë∆∞·ª£c:** Proximal gradient
4. **C·∫ßn tƒÉng t·ªëc:** FISTA
5. **Constrained:** Projected gradient descent

#### **Implementation tips:**
- **Vectorization:** T·∫≠n d·ª•ng parallel computation
- **Warm start:** S·ª≠ d·ª•ng nghi·ªám tr∆∞·ªõc l√†m kh·ªüi t·∫°o
- **Adaptive step size:** Backtracking line search
- **Early stopping:** Monitoring convergence criteria

#### **Common pitfalls:**
- **Sai proximal mapping:** Ki·ªÉm tra c√¥ng th·ª©c c·∫©n th·∫≠n
- **Step size qu√° l·ªõn:** C√≥ th·ªÉ kh√¥ng h·ªôi t·ª•
- **Kh√¥ng ki·ªÉm tra Lipschitz constant:** ·∫¢nh h∆∞·ªüng convergence
- **Qu√™n normalize data:** ·∫¢nh h∆∞·ªüng numerical stability

---

üí° **M·∫πo:** Proximal gradient method l√† c·∫ßu n·ªëi ho√†n h·∫£o gi·ªØa smooth v√† non-smooth optimization, ƒë·∫∑c bi·ªát m·∫°nh m·∫Ω cho machine learning v·ªõi regularization!

---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    ignoreHtmlClass: ".*|",
    processHtmlClass: "arithmatex"
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
/* S·ª≠a l·ªói alignment cho MathJax inline math */
.MathJax {
    vertical-align: baseline !important;
}

mjx-container[jax="CHTML"][display="false"] {
    vertical-align: baseline !important;
    display: inline !important;
}

/* ƒê·∫£m b·∫£o c√°c label trong quiz c√≥ alignment t·ªët */
.options label {
    display: flex;
    align-items: center;
    margin: 8px 0;
    padding: 8px;
    border-radius: 4px;
    transition: background-color 0.2s;
}

.options label:hover {
    background-color: #f0f0f0;
}

.options input[type="radio"] {
    margin-right: 8px;
    flex-shrink: 0;
}

/* Styling cho quiz container */
#quiz-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.question {
    background: #f8f9fa;
    border-left: 4px solid #007bff;
    padding: 20px;
    margin: 20px 0;
    border-radius: 8px;
}

.question h3 {
    color: #2c3e50;
    margin-bottom: 15px;
}

.explanation {
    background: #e8f5e8;
    border: 1px solid #28a745;
    padding: 15px;
    margin-top: 15px;
    border-radius: 6px;
}

.quiz-controls {
    text-align: center;
    margin: 30px 0;
}

.quiz-controls button {
    background: #007bff;
    color: white;
    border: none;
    padding: 12px 24px;
    margin: 0 10px;
    border-radius: 6px;
    cursor: pointer;
    font-size: 16px;
    transition: background-color 0.3s;
}

.quiz-controls button:hover {
    background: #0056b3;
}

.quiz-controls button:disabled {
    background: #6c757d;
    cursor: not-allowed;
}

.progress-bar {
    width: 100%;
    height: 8px;
    background: #e9ecef;
    border-radius: 4px;
    margin: 20px 0;
    overflow: hidden;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #007bff, #28a745);
    transition: width 0.3s ease;
}

.score-display {
    text-align: center;
    font-size: 18px;
    font-weight: bold;
    color: #2c3e50;
    margin: 20px 0;
}

.options label.selected {
    background-color: #e3f2fd;
    border: 2px solid #2196f3;
}

.final-score {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 30px;
    border-radius: 15px;
    text-align: center;
    margin: 30px 0;
}
</style>

B√†i t·∫≠p tr·∫Øc nghi·ªám n√†y ki·ªÉm tra hi·ªÉu bi·∫øt c·ªßa b·∫°n v·ªÅ ph∆∞∆°ng ph√°p gradient g·∫ßn k·ªÅ (proximal gradient method), proximal operator, composite optimization v√† acceleration methods nh∆∞ FISTA.

---

<div id="quiz-container">
    <div id="quiz-header">
        <h2>üéØ Quiz: Ph∆∞∆°ng Ph√°p Proximal Gradient</h2>
        <div class="progress-bar">
            <div class="progress-fill" id="progress-fill" style="width: 5%"></div>
        </div>
        <div class="score-display" id="score-display">C√¢u h·ªèi: <span id="current-q">1</span> / <span id="total-q">20</span></div>
    </div>

    <div id="quiz-content">
        <!-- C√¢u h·ªèi 1: Composite function form -->
        <div class="question" id="q1" style="display: block;">
            <h3>C√¢u 1: Proximal gradient method √°p d·ª•ng cho b√†i to√°n c√≥ d·∫°ng:</h3>
            <div class="options">
                <label><input type="radio" name="q1" value="a"> A) \(\min f(x)\) v·ªõi \(f\) kh·∫£ vi</label>
                <label><input type="radio" name="q1" value="b"> B) \(\min f(x) = g(x) + h(x)\) v·ªõi \(g\) kh·∫£ vi, \(h\) l·ªìi</label>
                <label><input type="radio" name="q1" value="c"> C) \(\min f(x)\) v·ªõi \(f\) kh√¥ng l·ªìi</label>
                <label><input type="radio" name="q1" value="d"> D) \(\min f(x)\) v·ªõi r√†ng bu·ªôc tuy·∫øn t√≠nh</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\min f(x) = g(x) + h(x)$$ v·ªõi $$g$$ kh·∫£ vi, $$h$$ l·ªìi</strong><br>
                Proximal gradient method ƒë∆∞·ª£c thi·∫øt k·∫ø cho composite optimization v·ªõi $$g$$ kh·∫£ vi v√† $$h$$ c√≥ th·ªÉ kh√¥ng kh·∫£ vi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 2: Proximal operator definition -->
        <div class="question" id="q2" style="display: none;">
            <h3>C√¢u 2: Proximal operator c·ªßa h√†m \(h\) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q2" value="a"> A) \(\text{prox}_h(v) = \arg\min_x \{h(x) + \frac{1}{2}\|x - v\|^2\}\)</label>
                <label><input type="radio" name="q2" value="b"> B) \(\text{prox}_h(v) = \arg\min_x \{h(x) - \frac{1}{2}\|x - v\|^2\}\)</label>
                <label><input type="radio" name="q2" value="c"> C) \(\text{prox}_h(v) = \arg\min_x \{h(x) + \|x - v\|\}\)</label>
                <label><input type="radio" name="q2" value="d"> D) \(\text{prox}_h(v) = v - \nabla h(v)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\text{prox}_h(v) = \arg\min_x \{h(x) + \frac{1}{2}\|x - v\|^2\}$$</strong><br>
                Proximal operator c√¢n b·∫±ng gi·ªØa vi·ªác gi·∫£m thi·ªÉu $$h(x)$$ v√† gi·ªØ $$x$$ g·∫ßn v·ªõi $$v$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 3: Proximal gradient update -->
        <div class="question" id="q3" style="display: none;">
            <h3>C√¢u 3: C√¥ng th·ª©c c·∫≠p nh·∫≠t c·ªßa proximal gradient method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q3" value="a"> A) \(x^{(k+1)} = x^{(k)} - t \nabla g(x^{(k)})\)</label>
                <label><input type="radio" name="q3" value="b"> B) \(x^{(k+1)} = \text{prox}_{th}(x^{(k)} - t \nabla g(x^{(k)}))\)</label>
                <label><input type="radio" name="q3" value="c"> C) \(x^{(k+1)} = \text{prox}_{tg}(x^{(k)} - t \nabla h(x^{(k)}))\)</label>
                <label><input type="radio" name="q3" value="d"> D) \(x^{(k+1)} = x^{(k)} - t(\nabla g(x^{(k)}) + \nabla h(x^{(k)}))\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$x^{(k+1)} = \text{prox}_{th}(x^{(k)} - t \nabla g(x^{(k)}))$$</strong><br>
                Th·ª±c hi·ªán gradient step cho $$g$$ r·ªìi √°p d·ª•ng proximal operator cho $$h$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 4: Proximal operator of L1 norm -->
        <div class="question" id="q4" style="display: none;">
            <h3>C√¢u 4: Proximal operator c·ªßa \(h(x) = \lambda\|x\|_1\) l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q4" value="a"> A) \(\text{prox}_{\lambda\|\cdot\|_1}(v) = v - \lambda\)</label>
                <label><input type="radio" name="q4" value="b"> B) \(\text{prox}_{\lambda\|\cdot\|_1}(v) = \text{sign}(v) \max(|v| - \lambda, 0)\)</label>
                <label><input type="radio" name="q4" value="c"> C) \(\text{prox}_{\lambda\|\cdot\|_1}(v) = \frac{v}{1 + \lambda}\)</label>
                <label><input type="radio" name="q4" value="d"> D) \(\text{prox}_{\lambda\|\cdot\|_1}(v) = v\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\text{prox}_{\lambda\|\cdot\|_1}(v) = \text{sign}(v) \max(|v| - \lambda, 0)$$</strong><br>
                ƒê√¢y l√† soft thresholding operator, co c√°c th√†nh ph·∫ßn v·ªÅ ph√≠a 0 m·ªôt l∆∞·ª£ng $$\lambda$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 5: Proximal operator of L2 norm squared -->
        <div class="question" id="q5" style="display: none;">
            <h3>C√¢u 5: Proximal operator c·ªßa \(h(x) = \frac{\lambda}{2}\|x\|_2^2\) l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q5" value="a"> A) \(\text{prox}_{\frac{\lambda}{2}\|\cdot\|_2^2}(v) = \frac{v}{1 + \lambda}\)</label>
                <label><input type="radio" name="q5" value="b"> B) \(\text{prox}_{\frac{\lambda}{2}\|\cdot\|_2^2}(v) = v - \lambda v\)</label>
                <label><input type="radio" name="q5" value="c"> C) \(\text{prox}_{\frac{\lambda}{2}\|\cdot\|_2^2}(v) = v\)</label>
                <label><input type="radio" name="q5" value="d"> D) \(\text{prox}_{\frac{\lambda}{2}\|\cdot\|_2^2}(v) = 0\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\text{prox}_{\frac{\lambda}{2}\|\cdot\|_2^2}(v) = \frac{v}{1 + \lambda}$$</strong><br>
                Proximal operator c·ªßa quadratic function c√≥ d·∫°ng shrinkage v·ªõi h·ªá s·ªë $$\frac{1}{1 + \lambda}$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 6: Convergence rate -->
        <div class="question" id="q6" style="display: none;">
            <h3>C√¢u 6: T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa proximal gradient method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q6" value="a"> A) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q6" value="b"> B) \(O(1/k)\)</label>
                <label><input type="radio" name="q6" value="c"> C) \(O(1/\sqrt{k})\)</label>
                <label><input type="radio" name="q6" value="d"> D) \(O(e^{-k})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$O(1/k)$$</strong><br>
                Proximal gradient method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª• $$O(1/k)$$ gi·ªëng nh∆∞ gradient descent cho h√†m l·ªìi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 7: When to use proximal gradient -->
        <div class="question" id="q7" style="display: none;">
            <h3>C√¢u 7: Proximal gradient method th√≠ch h·ª£p khi:</h3>
            <div class="options">
                <label><input type="radio" name="q7" value="a"> A) C·∫£ \(g\) v√† \(h\) ƒë·ªÅu kh·∫£ vi</label>
                <label><input type="radio" name="q7" value="b"> B) \(g\) kh·∫£ vi v√† proximal operator c·ªßa \(h\) d·ªÖ t√≠nh</label>
                <label><input type="radio" name="q7" value="c"> C) C·∫£ \(g\) v√† \(h\) ƒë·ªÅu kh√¥ng kh·∫£ vi</label>
                <label><input type="radio" name="q7" value="d"> D) H√†m m·ª•c ti√™u kh√¥ng l·ªìi</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$g$$ kh·∫£ vi v√† proximal operator c·ªßa $$h$$ d·ªÖ t√≠nh</strong><br>
                Ph∆∞∆°ng ph√°p n√†y hi·ªáu qu·∫£ khi c√≥ th·ªÉ t√≠nh gradient c·ªßa $$g$$ v√† proximal operator c·ªßa $$h$$ m·ªôt c√°ch d·ªÖ d√†ng.
            </div>
        </div>

        <!-- C√¢u h·ªèi 8: FISTA algorithm -->
        <div class="question" id="q8" style="display: none;">
            <h3>C√¢u 8: FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q8" value="a"> A) Phi√™n b·∫£n ch·∫≠m c·ªßa proximal gradient</label>
                <label><input type="radio" name="q8" value="b"> B) Phi√™n b·∫£n tƒÉng t·ªëc c·ªßa proximal gradient v·ªõi momentum</label>
                <label><input type="radio" name="q8" value="c"> C) Thu·∫≠t to√°n cho b√†i to√°n kh√¥ng l·ªìi</label>
                <label><input type="radio" name="q8" value="d"> D) Thu·∫≠t to√°n ch·ªâ cho L1 regularization</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Phi√™n b·∫£n tƒÉng t·ªëc c·ªßa proximal gradient v·ªõi momentum</strong><br>
                FISTA s·ª≠ d·ª•ng momentum ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô h·ªôi t·ª• $$O(1/k^2)$$ thay v√¨ $$O(1/k)$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 9: FISTA convergence rate -->
        <div class="question" id="q9" style="display: none;">
            <h3>C√¢u 9: T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa FISTA l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q9" value="a"> A) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q9" value="b"> B) \(O(1/k)\)</label>
                <label><input type="radio" name="q9" value="c"> C) \(O(1/\sqrt{k})\)</label>
                <label><input type="radio" name="q9" value="d"> D) \(O(e^{-k})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$O(1/k^2)$$</strong><br>
                FISTA ƒë·∫°t t·ªëc ƒë·ªô h·ªôi t·ª• optimal $$O(1/k^2)$$ cho first-order methods tr√™n h√†m l·ªìi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 10: FISTA momentum parameter -->
        <div class="question" id="q10" style="display: none;">
            <h3>C√¢u 10: Trong FISTA, tham s·ªë momentum \(t_k\) ƒë∆∞·ª£c c·∫≠p nh·∫≠t theo c√¥ng th·ª©c:</h3>
            <div class="options">
                <label><input type="radio" name="q10" value="a"> A) \(t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}\)</label>
                <label><input type="radio" name="q10" value="b"> B) \(t_{k+1} = t_k + 1\)</label>
                <label><input type="radio" name="q10" value="c"> C) \(t_{k+1} = 0.9 t_k\)</label>
                <label><input type="radio" name="q10" value="d"> D) \(t_{k+1} = 1\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}$$</strong><br>
                ƒê√¢y l√† c√¥ng th·ª©c ƒë·∫∑c bi·ªát trong FISTA ƒë·ªÉ ƒë·∫£m b·∫£o t·ªëc ƒë·ªô h·ªôi t·ª• optimal.
            </div>
        </div>

        <!-- C√¢u h·ªèi 11: Proximal operator properties -->
        <div class="question" id="q11" style="display: none;">
            <h3>C√¢u 11: Proximal operator c√≥ t√≠nh ch·∫•t:</h3>
            <div class="options">
                <label><input type="radio" name="q11" value="a"> A) Kh√¥ng gi·∫£m kho·∫£ng c√°ch (non-expansive)</label>
                <label><input type="radio" name="q11" value="b"> B) TƒÉng kho·∫£ng c√°ch</label>
                <label><input type="radio" name="q11" value="c"> C) Kh√¥ng li√™n t·ª•c</label>
                <label><input type="radio" name="q11" value="d"> D) Ch·ªâ √°p d·ª•ng cho h√†m kh·∫£ vi</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Kh√¥ng gi·∫£m kho·∫£ng c√°ch (non-expansive)</strong><br>
                $$\|\text{prox}_h(u) - \text{prox}_h(v)\| \leq \|u - v\|$$ v·ªõi m·ªçi $$u, v$$, ƒë√¢y l√† t√≠nh ch·∫•t quan tr·ªçng cho ph√¢n t√≠ch h·ªôi t·ª•.
            </div>
        </div>

        <!-- C√¢u h·ªèi 12: Lasso problem -->
        <div class="question" id="q12" style="display: none;">
            <h3>C√¢u 12: B√†i to√°n Lasso \(\min \frac{1}{2}\|Ax - b\|_2^2 + \lambda\|x\|_1\) c√≥ th·ªÉ gi·∫£i b·∫±ng:</h3>
            <div class="options">
                <label><input type="radio" name="q12" value="a"> A) Gradient descent</label>
                <label><input type="radio" name="q12" value="b"> B) Proximal gradient v·ªõi \(g(x) = \frac{1}{2}\|Ax - b\|_2^2\), \(h(x) = \lambda\|x\|_1\)</label>
                <label><input type="radio" name="q12" value="c"> C) Newton method</label>
                <label><input type="radio" name="q12" value="d"> D) Kh√¥ng th·ªÉ gi·∫£i ƒë∆∞·ª£c</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Proximal gradient v·ªõi $$g(x) = \frac{1}{2}\|Ax - b\|_2^2$$, $$h(x) = \lambda\|x\|_1$$</strong><br>
                Lasso l√† v√≠ d·ª• ƒëi·ªÉn h√¨nh c·ªßa composite optimization v·ªõi $$g$$ kh·∫£ vi v√† $$h$$ kh√¥ng kh·∫£ vi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 13: Matrix completion -->
        <div class="question" id="q13" style="display: none;">
            <h3>C√¢u 13: Trong matrix completion, proximal operator c·ªßa nuclear norm th·ª±c hi·ªán:</h3>
            <div class="options">
                <label><input type="radio" name="q13" value="a"> A) Hard thresholding tr√™n singular values</label>
                <label><input type="radio" name="q13" value="b"> B) Soft thresholding tr√™n singular values</label>
                <label><input type="radio" name="q13" value="c"> C) Kh√¥ng thay ƒë·ªïi ma tr·∫≠n</label>
                <label><input type="radio" name="q13" value="d"> D) ƒê·∫∑t t·∫•t c·∫£ ph·∫ßn t·ª≠ b·∫±ng 0</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Soft thresholding tr√™n singular values</strong><br>
                Proximal operator c·ªßa nuclear norm $$\|X\|_*$$ th·ª±c hi·ªán SVD v√† soft thresholding tr√™n c√°c singular values.
            </div>
        </div>

        <!-- C√¢u h·ªèi 14: Backtracking line search -->
        <div class="question" id="q14" style="display: none;">
            <h3>C√¢u 14: Trong proximal gradient method, c√≥ th·ªÉ s·ª≠ d·ª•ng backtracking line search kh√¥ng?</h3>
            <div class="options">
                <label><input type="radio" name="q14" value="a"> A) Kh√¥ng, v√¨ h√†m kh√¥ng kh·∫£ vi</label>
                <label><input type="radio" name="q14" value="b"> B) C√≥, d·ª±a tr√™n ƒëi·ªÅu ki·ªán gi·∫£m ƒë·ªß cho composite function</label>
                <label><input type="radio" name="q14" value="c"> C) Ch·ªâ khi \(h = 0\)</label>
                <label><input type="radio" name="q14" value="d"> D) Ch·ªâ cho FISTA</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) C√≥, d·ª±a tr√™n ƒëi·ªÅu ki·ªán gi·∫£m ƒë·ªß cho composite function</strong><br>
                C√≥ th·ªÉ s·ª≠ d·ª•ng backtracking v·ªõi ƒëi·ªÅu ki·ªán $$f(x^+) \leq Q_t(x^+, x)$$ trong ƒë√≥ $$Q_t$$ l√† quadratic upper bound.
            </div>
        </div>

        <!-- C√¢u h·ªèi 15: Proximal operator of indicator function -->
        <div class="question" id="q15" style="display: none;">
            <h3>C√¢u 15: Proximal operator c·ªßa h√†m ch·ªâ th·ªã \(I_C(x)\) c·ªßa t·∫≠p l·ªìi \(C\) l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q15" value="a"> A) \(\text{prox}_{I_C}(v) = v\)</label>
                <label><input type="radio" name="q15" value="b"> B) \(\text{prox}_{I_C}(v) = P_C(v)\) (projection onto \(C\))</label>
                <label><input type="radio" name="q15" value="c"> C) \(\text{prox}_{I_C}(v) = 0\)</label>
                <label><input type="radio" name="q15" value="d"> D) Kh√¥ng t·ªìn t·∫°i</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\text{prox}_{I_C}(v) = P_C(v)$$ (projection onto $$C$$)</strong><br>
                Proximal operator c·ªßa h√†m ch·ªâ th·ªã ch√≠nh l√† ph√©p chi·∫øu l√™n t·∫≠p r√†ng bu·ªôc.
            </div>
        </div>

        <!-- C√¢u h·ªèi 16: Composite gradient mapping -->
        <div class="question" id="q16" style="display: none;">
            <h3>C√¢u 16: Composite gradient mapping \(G_t(x)\) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q16" value="a"> A) \(G_t(x) = \nabla g(x)\)</label>
                <label><input type="radio" name="q16" value="b"> B) \(G_t(x) = \frac{1}{t}(x - \text{prox}_{th}(x - t\nabla g(x)))\)</label>
                <label><input type="radio" name="q16" value="c"> C) \(G_t(x) = \text{prox}_{th}(x)\)</label>
                <label><input type="radio" name="q16" value="d"> D) \(G_t(x) = x - t\nabla g(x)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$G_t(x) = \frac{1}{t}(x - \text{prox}_{th}(x - t\nabla g(x)))$$</strong><br>
                Composite gradient mapping t·ªïng qu√°t h√≥a gradient cho composite functions v√† ƒë∆∞·ª£c d√πng trong ƒëi·ªÅu ki·ªán d·ª´ng.
            </div>
        </div>

        <!-- C√¢u h·ªèi 17: Acceleration always beneficial -->
        <div class="question" id="q17" style="display: none;">
            <h3>C√¢u 17: Acceleration (nh∆∞ FISTA) c√≥ lu√¥n c√≥ l·ª£i kh√¥ng?</h3>
            <div class="options">
                <label><input type="radio" name="q17" value="a"> A) Lu√¥n c√≥ l·ª£i</label>
                <label><input type="radio" name="q17" value="b"> B) Kh√¥ng, c√≥ th·ªÉ l√†m ch·∫≠m h·ªôi t·ª• trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p</label>
                <label><input type="radio" name="q17" value="c"> C) Ch·ªâ c√≥ l·ª£i cho h√†m quadratic</label>
                <label><input type="radio" name="q17" value="d"> D) Ch·ªâ c√≥ l·ª£i cho L1 regularization</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Kh√¥ng, c√≥ th·ªÉ l√†m ch·∫≠m h·ªôi t·ª• trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p</strong><br>
                Acceleration c√≥ th·ªÉ g√¢y dao ƒë·ªông v√† ch·∫≠m h·ªôi t·ª•, ƒë·∫∑c bi·ªát khi h√†m c√≥ conditioning k√©m ho·∫∑c trong giai ƒëo·∫°n ƒë·∫ßu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 18: Proximal gradient vs subgradient -->
        <div class="question" id="q18" style="display: none;">
            <h3>C√¢u 18: So v·ªõi subgradient method, proximal gradient method:</h3>
            <div class="options">
                <label><input type="radio" name="q18" value="a"> A) Ch·∫≠m h∆°n</label>
                <label><input type="radio" name="q18" value="b"> B) Nhanh h∆°n v√† ·ªïn ƒë·ªãnh h∆°n</label>
                <label><input type="radio" name="q18" value="c"> C) Ch·ªâ √°p d·ª•ng cho h√†m kh·∫£ vi</label>
                <label><input type="radio" name="q18" value="d"> D) C√≥ c√πng t·ªëc ƒë·ªô h·ªôi t·ª•</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Nhanh h∆°n v√† ·ªïn ƒë·ªãnh h∆°n</strong><br>
                Proximal gradient c√≥ t·ªëc ƒë·ªô $$O(1/k)$$ so v·ªõi $$O(1/\sqrt{k})$$ c·ªßa subgradient, v√† √≠t dao ƒë·ªông h∆°n.
            </div>
        </div>

        <!-- C√¢u h·ªèi 19: Strongly convex case -->
        <div class="question" id="q19" style="display: none;">
            <h3>C√¢u 19: Khi \(g\) l√† strongly convex, proximal gradient method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª•:</h3>
            <div class="options">
                <label><input type="radio" name="q19" value="a"> A) \(O(1/k)\)</label>
                <label><input type="radio" name="q19" value="b"> B) \(O(\rho^k)\) v·ªõi \(\rho < 1\) (linear)</label>
                <label><input type="radio" name="q19" value="c"> C) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q19" value="d"> D) \(O(1/\sqrt{k})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$O(\rho^k)$$ v·ªõi $$\rho < 1$$ (linear)</strong><br>
                Strong convexity c·ªßa $$g$$ ƒë·∫£m b·∫£o t·ªëc ƒë·ªô h·ªôi t·ª• linear (geometric) cho proximal gradient method.
            </div>
        </div>

        <!-- C√¢u h·ªèi 20: Practical considerations -->
        <div class="question" id="q20" style="display: none;">
            <h3>C√¢u 20: Trong th·ª±c t·∫ø, ƒë·ªÉ √°p d·ª•ng proximal gradient method hi·ªáu qu·∫£, c·∫ßn:</h3>
            <div class="options">
                <label><input type="radio" name="q20" value="a"> A) Proximal operator c·ªßa \(h\) ph·∫£i c√≥ closed-form solution</label>
                <label><input type="radio" name="q20" value="b"> B) \(g\) ph·∫£i c√≥ Lipschitz gradient</label>
                <label><input type="radio" name="q20" value="c"> C) Bi·∫øt step size th√≠ch h·ª£p</label>
                <label><input type="radio" name="q20" value="d"> D) T·∫•t c·∫£ c√°c ƒëi·ªÅu ki·ªán tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ƒëi·ªÅu ki·ªán tr√™n</strong><br>
                ƒê·ªÉ hi·ªáu qu·∫£, c·∫ßn proximal operator d·ªÖ t√≠nh, Lipschitz gradient cho $$g$$, v√† step size ph√π h·ª£p.
            </div>
        </div>
        <!-- C√¢u h·ªèi 21: Th·ª±c h√†nh -->
        <div class="question" id="q21" style="display: none;">
            <h3>C√¢u 21: Proximal operator c·ªßa h√†m $$f(x) = \lambda|x|$$ l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q21" value="a"> A) $$\text{prox}_f(x) = \text{sign}(x)\max\{|x|-\lambda, 0\}$$</label>
                <label><input type="radio" name="q21" value="b"> B) $$\text{prox}_f(x) = x - \lambda$$</label>
                <label><input type="radio" name="q21" value="c"> C) $$\text{prox}_f(x) = \lambda x$$</label>
                <label><input type="radio" name="q21" value="d"> D) $$\text{prox}_f(x) = |x|$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\text{prox}_f(x) = \text{sign}(x)\max\{|x|-\lambda, 0\}$$</strong><br>
                ƒê√¢y l√† soft thresholding operator, m·ªôt trong nh·ªØng proximal operator quan tr·ªçng nh·∫•t.
            </div>
        </div>

        <!-- C√¢u h·ªèi 22: Th·ª±c h√†nh -->
        <div class="question" id="q22" style="display: none;">
            <h3>C√¢u 22: Proximal gradient method k·∫øt h·ª£p:</h3>
            <div class="options">
                <label><input type="radio" name="q22" value="a"> A) Gradient descent + proximal operator</label>
                <label><input type="radio" name="q22" value="b"> B) Newton method + gradient</label>
                <label><input type="radio" name="q22" value="c"> C) SGD + momentum</label>
                <label><input type="radio" name="q22" value="d"> D) Line search + Hessian</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Gradient descent + proximal operator</strong><br>
                Proximal gradient method: $$x^{(k+1)} = \text{prox}_{tg}(x^{(k)} - t\nabla f(x^{(k)}))$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 23: Th·ª±c h√†nh -->
        <div class="question" id="q23" style="display: none;">
            <h3>C√¢u 23: FISTA l√† thu·∫≠t to√°n:</h3>
            <div class="options">
                <label><input type="radio" name="q23" value="a"> A) TƒÉng t·ªëc proximal gradient</label>
                <label><input type="radio" name="q23" value="b"> B) Newton method c·∫£i ti·∫øn</label>
                <label><input type="radio" name="q23" value="c"> C) Stochastic gradient descent</label>
                <label><input type="radio" name="q23" value="d"> D) Interior point method</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) TƒÉng t·ªëc proximal gradient</strong><br>
                FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) l√† phi√™n b·∫£n tƒÉng t·ªëc c·ªßa proximal gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 24: Th·ª±c h√†nh -->
        <div class="question" id="q24" style="display: none;">
            <h3>C√¢u 24: B√†i to√°n LASSO c√≥ d·∫°ng:</h3>
            <div class="options">
                <label><input type="radio" name="q24" value="a"> A) $$\min \frac{1}{2}\|Ax-b\|^2 + \lambda\|x\|_1$$</label>
                <label><input type="radio" name="q24" value="b"> B) $$\min \frac{1}{2}\|Ax-b\|^2 + \lambda\|x\|_2$$</label>
                <label><input type="radio" name="q24" value="c"> C) $$\min \|Ax-b\|_1 + \lambda\|x\|_2$$</label>
                <label><input type="radio" name="q24" value="d"> D) $$\min \|Ax-b\|_\infty + \lambda\|x\|_1$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\min \frac{1}{2}\|Ax-b\|^2 + \lambda\|x\|_1$$</strong><br>
                LASSO s·ª≠ d·ª•ng regularization L1: $$\min \frac{1}{2}\|Ax-b\|^2 + \lambda\|x\|_1$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 25: Th·ª±c h√†nh -->
        <div class="question" id="q25" style="display: none;">
            <h3>C√¢u 25: V·ªõi $$\text{prox}_{\lambda|\cdot|}(2)$$ v√† $$\lambda = 0.5$$, k·∫øt qu·∫£ l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q25" value="a"> A) $$1.5$$</label>
                <label><input type="radio" name="q25" value="b"> B) $$2$$</label>
                <label><input type="radio" name="q25" value="c"> C) $$1$$</label>
                <label><input type="radio" name="q25" value="d"> D) $$0.5$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$1.5$$</strong><br>
                $$\text{prox}_{0.5|\cdot|}(2) = \text{sign}(2)\max\{|2|-0.5, 0\} = 1 \cdot 1.5 = 1.5$$.
            </div>
        </div>

    </div>

    <div class="quiz-controls">
        <button onclick="previousQuestion()" id="prev-btn" disabled>‚Üê C√¢u tr∆∞·ªõc</button>
        <button onclick="nextQuestion()" id="next-btn">C√¢u ti·∫øp ‚Üí</button>
        <button onclick="submitQuiz()" id="submit-btn" style="display: none;">N·ªôp b√†i</button>
        <button onclick="resetQuiz()" id="reset-btn" style="display: none;">L√†m l·∫°i</button>
    </div>

    <div id="final-result" style="display: none;" class="final-score">
        <h2>üéâ Ho√†n th√†nh Quiz!</h2>
        <p>ƒêi·ªÉm s·ªë c·ªßa b·∫°n: <span id="final-score-text"></span></p>
        <p id="performance-message"></p>
    </div>
</div>

<script>
let currentQuestion = 1;
const totalQuestions = 25;
let userAnswers = {};
let quizSubmitted = false;

// ƒê√°p √°n ƒë√∫ng
const correctAnswers = {
    q1: 'b', q2: 'a', q3: 'b', q4: 'b', q5: 'a',
    q6: 'b', q7: 'b', q8: 'b', q9: 'a', q10: 'a',
    q11: 'a', q12: 'b', q13: 'b', q14: 'b', q15: 'b',
    q16: 'b', q17: 'b', q18: 'b', q19: 'b', q20: 'd'
,
    q21: 'a', q22: 'a', q23: 'a', q24: 'a', q25: 'a'};

function showQuestion(n) {
    // ·∫®n t·∫•t c·∫£ c√¢u h·ªèi
    for (let i = 1; i <= totalQuestions; i++) {
        document.getElementById(`q${i}`).style.display = 'none';
    }
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi hi·ªán t·∫°i
    document.getElementById(`q${n}`).style.display = 'block';
    
    // C·∫≠p nh·∫≠t progress bar
    const progress = (n / totalQuestions) * 100;
    document.getElementById('progress-fill').style.width = progress + '%';
    
    // C·∫≠p nh·∫≠t s·ªë c√¢u h·ªèi
    document.getElementById('current-q').textContent = n;
    
    // C·∫≠p nh·∫≠t n√∫t ƒëi·ªÅu h∆∞·ªõng
    document.getElementById('prev-btn').disabled = (n === 1);
    document.getElementById('next-btn').style.display = (n === totalQuestions) ? 'none' : 'inline-block';
    document.getElementById('submit-btn').style.display = (n === totalQuestions) ? 'inline-block' : 'none';
    
    currentQuestion = n;
}

function nextQuestion() {
    if (currentQuestion < totalQuestions) {
        showQuestion(currentQuestion + 1);
    }
}

function previousQuestion() {
    if (currentQuestion > 1) {
        showQuestion(currentQuestion - 1);
    }
}

function saveAnswer(questionId, answer) {
    userAnswers[questionId] = answer;
    
    // Highlight selected option
    const questionDiv = document.getElementById(questionId);
    const labels = questionDiv.querySelectorAll('.options label');
    labels.forEach(label => {
        label.classList.remove('selected');
        const input = label.querySelector('input[type="radio"]');
        if (input.checked) {
            label.classList.add('selected');
        }
    });
}

function calculateScore() {
    let correct = 0;
    for (let q in correctAnswers) {
        if (userAnswers[q] === correctAnswers[q]) {
            correct++;
        }
    }
    return correct;
}

function submitQuiz() {
    quizSubmitted = true;
    const score = calculateScore();
    const percentage = Math.round((score / totalQuestions) * 100);
    
    // Hi·ªÉn th·ªã t·∫•t c·∫£ gi·∫£i th√≠ch
    for (let i = 1; i <= totalQuestions; i++) {
        const explanation = document.querySelector(`#q${i} .explanation`);
        explanation.style.display = 'block';
        
        // Highlight ƒë√°p √°n ƒë√∫ng v√† sai
        const questionDiv = document.getElementById(`q${i}`);
        const labels = questionDiv.querySelectorAll('.options label');
        labels.forEach(label => {
            const input = label.querySelector('input[type="radio"]');
            const value = input.value;
            
            if (value === correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#d4edda';
                label.style.border = '2px solid #28a745';
            } else if (value === userAnswers[`q${i}`] && value !== correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#f8d7da';
                label.style.border = '2px solid #dc3545';
            }
        });
    }
    
    // Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi
    document.getElementById('final-result').style.display = 'block';
    document.getElementById('final-score-text').textContent = `${score}/${totalQuestions} (${percentage}%)`;
    
    let message = '';
    if (percentage >= 90) {
        message = 'üåü Xu·∫•t s·∫Øc! B·∫°n ƒë√£ th√†nh th·∫°o proximal gradient method!';
    } else if (percentage >= 80) {
        message = 'üëç R·∫•t t·ªët! B·∫°n hi·ªÉu t·ªët v·ªÅ proximal operators v√† FISTA.';
    } else if (percentage >= 70) {
        message = 'üìö Kh√° ·ªïn! H√£y xem l·∫°i proximal operators v√† acceleration.';
    } else if (percentage >= 60) {
        message = 'üí™ C·∫ßn c·∫£i thi·ªán! √în l·∫°i composite optimization v√† proximal operators.';
    } else {
        message = 'üìñ H√£y h·ªçc l·∫°i t·ª´ ƒë·∫ßu v·ªÅ proximal gradient method nh√©!';
    }
    
    document.getElementById('performance-message').textContent = message;
    
    // ·∫®n n√∫t submit, hi·ªán n√∫t reset
    document.getElementById('submit-btn').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'inline-block';
    
    // Cu·ªôn ƒë·∫øn k·∫øt qu·∫£
    document.getElementById('final-result').scrollIntoView({ behavior: 'smooth' });
}

function resetQuiz() {
    // Reset tr·∫°ng th√°i
    currentQuestion = 1;
    userAnswers = {};
    quizSubmitted = false;
    
    // ·∫®n t·∫•t c·∫£ gi·∫£i th√≠ch
    document.querySelectorAll('.explanation').forEach(exp => {
        exp.style.display = 'none';
    });
    
    // Reset style c·ªßa labels
    document.querySelectorAll('.options label').forEach(label => {
        label.style.backgroundColor = '';
        label.style.border = '';
        label.classList.remove('selected');
    });
    
    // Uncheck t·∫•t c·∫£ radio buttons
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.checked = false;
    });
    
    document.querySelectorAll('.options label').forEach(label => {
        label.classList.remove('selected');
    });
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi ƒë·∫ßu ti√™n
    showQuestion(1);
    
    // Cu·ªôn l√™n ƒë·∫ßu
    document.getElementById('quiz-header').scrollIntoView({ behavior: 'smooth' });
    
    // ·∫®n k·∫øt qu·∫£ cu·ªëi v√† reset button
    document.getElementById('final-result').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'none';
}

// Kh·ªüi t·∫°o b√†i t·∫≠p khi trang ƒë∆∞·ª£c t·∫£i
document.addEventListener('DOMContentLoaded', function() {
    showQuestion(1);
    
    // Th√™m event listener cho t·∫•t c·∫£ radio button
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.addEventListener('change', function() {
            const questionId = this.name;
            const answer = this.value;
            saveAnswer(questionId, answer);
        });
    });
    
    // Render MathJax sau khi DOM ƒë∆∞·ª£c t·∫£i
    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});

// X·ª≠ l√Ω ph√≠m t·∫Øt
document.addEventListener('keydown', function(event) {
    if (quizSubmitted) return;
    
    switch(event.key) {
        case 'ArrowLeft':
            if (currentQuestion > 1) previousQuestion();
            break;
        case 'ArrowRight':
            if (currentQuestion < totalQuestions) nextQuestion();
            break;
        case 'Enter':
            if (currentQuestion === totalQuestions) submitQuiz();
            break;
    }
});
</script>
