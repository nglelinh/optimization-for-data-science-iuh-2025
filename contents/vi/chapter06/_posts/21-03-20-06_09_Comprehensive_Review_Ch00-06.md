---
layout: post
title: 06-09 Ã”n táº­p tá»•ng há»£p - Ná»n táº£ng Tá»‘i Æ°u hÃ³a Lá»“i (ChÆ°Æ¡ng 00-06)
chapter: '06'
order: 17
owner: GitHub Copilot
lang: vi
categories:
- chapter06
lesson_type: review
---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    ignoreHtmlClass: ".*|",
    processHtmlClass: "arithmatex"
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

# ğŸ“š Ã”n táº­p tá»•ng há»£p: Ná»n táº£ng Tá»‘i Æ°u hÃ³a Lá»“i

ÄÃ¢y lÃ  bÃ i Ã´n táº­p tá»•ng há»£p cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi tá»« chÆ°Æ¡ng 00 Ä‘áº¿n chÆ°Æ¡ng 06, Ä‘Æ°á»£c sáº¯p xáº¿p theo logic xÃ¢y dá»±ng kiáº¿n thá»©c tá»« ná»n táº£ng toÃ¡n há»c Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n thá»±c táº¿.

---

## ğŸ—ï¸ **PHáº¦N I: Ná»€N Táº¢NG TOÃN Há»ŒC (ChÆ°Æ¡ng 00)**

### ğŸ”¢ **1. Giáº£i tÃ­ch cÆ¡ báº£n**

#### **Äáº¡o hÃ m vÃ  Gradient**
- **Äáº¡o hÃ m:** $$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$
- **Äáº¡o hÃ m riÃªng:** $$\frac{\partial f}{\partial x_i}$$ - Ä‘áº¡o hÃ m theo $$x_i$$ khi cÃ¡c biáº¿n khÃ¡c cá»‘ Ä‘á»‹nh
- **Gradient:** $$\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$$ - vector chá»‰ hÆ°á»›ng tÄƒng dá»‘c nháº¥t

#### **Äáº¡o hÃ m cÃ³ hÆ°á»›ng vÃ  Chuá»—i Taylor**
- **Äáº¡o hÃ m cÃ³ hÆ°á»›ng:** $$D_{\mathbf{u}}f = \nabla f \cdot \mathbf{u} = \|\nabla f\| \cos \theta$$
- **Chuá»—i Taylor báº­c 1:** $$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)$$
- **Chuá»—i Taylor báº­c 2:** $$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \nabla^2 f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)$$

### ğŸ“ **2. Äáº¡i sá»‘ tuyáº¿n tÃ­nh**

#### **Vector vÃ  PhÃ©p toÃ¡n**
- **TÃ­ch vÃ´ hÆ°á»›ng:** $$\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^n u_i v_i = \|\mathbf{u}\| \|\mathbf{v}\| \cos \theta$$
- **Chuáº©n vector:**
  - L2 (Euclid): $$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$$
  - L1 (Manhattan): $$\|\mathbf{x}\|_1 = \sum_{i=1}^n \lvert x_i \rvert$$
  - Lâˆ (Max): $$\|\mathbf{x}\|_\infty = \max_i \lvert x_i \rvert$$

#### **Ma tráº­n vÃ  Eigenvalue**
- **PhÃ©p nhÃ¢n ma tráº­n:** $$(AB)_{ij} = \sum_{k} A_{ik} B_{kj}$$
- **Eigenvalue/eigenvector:** $$A\mathbf{v} = \lambda \mathbf{v}$$ vá»›i $$\mathbf{v} \neq \mathbf{0}$$
- **Ma tráº­n xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng:** $$A \succ 0 \Leftrightarrow \mathbf{x}^T A \mathbf{x} > 0, \forall \mathbf{x} \neq 0$$

### ğŸ“Š **3. LÃ½ thuyáº¿t táº­p há»£p**

#### **Táº­p há»£p cÆ¡ báº£n**
- **Táº­p má»Ÿ:** Má»i Ä‘iá»ƒm Ä‘á»u cÃ³ lÃ¢n cáº­n náº±m trong táº­p
- **Táº­p Ä‘Ã³ng:** Chá»©a táº¥t cáº£ Ä‘iá»ƒm biÃªn
- **Táº­p compact:** ÄÃ³ng vÃ  bá»‹ cháº·n trong $$\mathbb{R}^n$$

#### **TÃ­nh liÃªn tá»¥c**
- **LiÃªn tá»¥c:** $$\lim_{x \to x_0} f(x) = f(x_0)$$
- **LiÃªn tá»¥c Ä‘á»u:** $$\forall \epsilon > 0, \exists \delta > 0$$ sao cho $$\lvert x-y \rvert < \delta \Rightarrow \lvert f(x)-f(y) \rvert < \epsilon$$

---

## ğŸ¯ **PHáº¦N II: KHÃI NIá»†M Tá»I Æ¯U HÃ“A (ChÆ°Æ¡ng 01)**

### ğŸ” **4. BÃ i toÃ¡n tá»‘i Æ°u hÃ³a tá»•ng quÃ¡t**

#### **Dáº¡ng chuáº©n**
$$\begin{align} 
&\min_{x\in D}\ &&f(x) \\
&\text{vá»›i Ä‘iá»u kiá»‡n} && g_i(x) \le 0,\ i = 1, ..., m \\
&&& h_j(x) = 0,\ j = 1,\ ..., r
\end{align}$$

**CÃ¡c thÃ nh pháº§n:**
- $$x \in \mathbb{R}^n$$: biáº¿n tá»‘i Æ°u hÃ³a
- $$f(x)$$: hÃ m má»¥c tiÃªu
- $$g_i(x) \le 0$$: rÃ ng buá»™c báº¥t Ä‘áº³ng thá»©c
- $$h_j(x) = 0$$: rÃ ng buá»™c Ä‘áº³ng thá»©c
- $$x^*$$: nghiá»‡m tá»‘i Æ°u

### ğŸŒŸ **5. Tá»‘i Æ°u hÃ³a lá»“i - TÃ­nh cháº¥t vÃ ng**

#### **Äá»‹nh nghÄ©a bÃ i toÃ¡n lá»“i**
BÃ i toÃ¡n tá»‘i Æ°u hÃ³a lÃ  lá»“i khi:
- HÃ m má»¥c tiÃªu $$f$$ lÃ  lá»“i
- CÃ¡c hÃ m rÃ ng buá»™c báº¥t Ä‘áº³ng thá»©c $$g_i$$ lÃ  lá»“i
- CÃ¡c hÃ m rÃ ng buá»™c Ä‘áº³ng thá»©c $$h_j$$ lÃ  affine

#### **TÃ­nh cháº¥t quan trá»ng**
> **Má»i cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng cá»§a bÃ i toÃ¡n tá»‘i Æ°u hÃ³a lá»“i Ä‘á»u lÃ  cá»±c tiá»ƒu toÃ n cá»¥c**

Äiá»u nÃ y lÃ m cho tá»‘i Æ°u hÃ³a lá»“i trá»Ÿ nÃªn Ä‘Ã¡ng tin cáº­y vÃ  hiá»‡u quáº£!

---

## ğŸ”µ **PHáº¦N III: Táº¬P Lá»’I (ChÆ°Æ¡ng 02)**

### ğŸ“ **6. Tá»« Affine Ä‘áº¿n Lá»“i**

#### **Táº­p Affine**
- **Äá»‹nh nghÄ©a:** $$\theta x_1 + (1-\theta)x_2 \in C$$ vá»›i má»i $$\theta \in \mathbb{R}$$
- **Tá»• há»£p affine:** $$\sum_{i=1}^k \theta_i x_i$$ vá»›i $$\sum_{i=1}^k \theta_i = 1$$
- **Ã nghÄ©a:** ÄÆ°á»ng tháº³ng qua hai Ä‘iá»ƒm

#### **Táº­p Lá»“i**
- **Äá»‹nh nghÄ©a:** $$\theta x_1 + (1-\theta)x_2 \in C$$ vá»›i má»i $$0 \le \theta \le 1$$
- **Tá»• há»£p lá»“i:** $$\sum_{i=1}^k \theta_i x_i$$ vá»›i $$\sum_{i=1}^k \theta_i = 1, \theta_i \ge 0$$
- **Ã nghÄ©a:** Äoáº¡n tháº³ng ná»‘i hai Ä‘iá»ƒm

#### **HÃ¬nh NÃ³n Lá»“i**
- **Äá»‹nh nghÄ©a:** Vá»«a lÃ  nÃ³n vá»«a lÃ  lá»“i
- **Tá»• há»£p nÃ³n:** $$\sum_{i=1}^k \theta_i x_i$$ vá»›i $$\theta_i \ge 0$$
- **Ã nghÄ©a:** Tia tá»« gá»‘c qua Ä‘iá»ƒm

### ğŸ—ï¸ **7. CÃ¡c táº­p lá»“i quan trá»ng**

#### **Táº­p há»£p cÆ¡ báº£n**
- **SiÃªu pháº³ng:** $$\{x \mid a^T x = b\}$$ - affine vÃ  lá»“i
- **Ná»­a khÃ´ng gian:** $$\{x \mid a^T x \le b\}$$ - lá»“i
- **HÃ¬nh cáº§u Euclidean:** $$\{x \mid \lVert x - x_c \rVert_2 \le r\}$$ - lá»“i
- **Ellipsoid:** $$\{x \mid (x-x_c)^T P^{-1} (x-x_c) \le 1\}$$ vá»›i $$P \succ 0$$ - lá»“i

#### **PhÃ©p toÃ¡n báº£o toÃ n tÃ­nh lá»“i**
- **Giao:** Giao cá»§a cÃ¡c táº­p lá»“i lÃ  lá»“i
- **Tá»• há»£p affine:** $$\alpha C + \beta D$$ lá»“i náº¿u $$C, D$$ lá»“i
- **áº¢nh affine:** $$f(C) = \{Ax + b \mid x \in C\}$$ lá»“i náº¿u $$C$$ lá»“i

---

## ğŸ“ˆ **PHáº¦N IV: HÃ€M Lá»’I (ChÆ°Æ¡ng 03)**

### ğŸ” **8. Äá»‹nh nghÄ©a vÃ  Ä‘áº·c trÆ°ng**

#### **HÃ m lá»“i**
$$f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$$
vá»›i má»i $$x, y \in \text{dom}(f), 0 \le \theta \le 1$$

#### **CÃ¡c loáº¡i hÃ m lá»“i**
- **Lá»“i cháº·t:** Báº¥t Ä‘áº³ng thá»©c nghiÃªm ngáº·t vá»›i $$x \neq y, 0 < \theta < 1$$
- **Lá»“i máº¡nh:** $$f - \frac{m}{2}\lVert x \rVert_2^2$$ lá»“i vá»›i $$m > 0$$
- **HÃ m lÃµm:** $$f$$ lÃµm âŸº $$-f$$ lá»“i

### ğŸ¯ **9. Äáº·c trÆ°ng hÃ m lá»“i**

#### **Äáº·c trÆ°ng epigraph**
$$f$$ lá»“i âŸº $$\text{epi}(f) = \{(x,t) \mid f(x) \le t\}$$ lÃ  táº­p lá»“i

#### **Äáº·c trÆ°ng báº­c nháº¥t (hÃ m kháº£ vi)**
$$f$$ lá»“i âŸº $$f(y) \ge f(x) + \nabla f(x)^T (y-x)$$ vá»›i má»i $$x,y$$

#### **Äáº·c trÆ°ng báº­c hai (hÃ m kháº£ vi hai láº§n)**
$$f$$ lá»“i âŸº $$\nabla^2 f(x) \succeq 0$$ vá»›i má»i $$x$$

### â­ **10. HÃ m liÃªn há»£p**

#### **Äá»‹nh nghÄ©a**
$$f^*(y) = \sup_{x \in \text{dom}(f)} \{y^T x - f(x)\}$$

#### **TÃ­nh cháº¥t quan trá»ng**
- $$f^*$$ luÃ´n lá»“i (dÃ¹ $$f$$ cÃ³ lá»“i hay khÃ´ng)
- **Báº¥t Ä‘áº³ng thá»©c Fenchel:** $$f(x) + f^*(y) \ge x^T y$$
- **Fenchel biconjugate:** Náº¿u $$f$$ Ä‘Ã³ng vÃ  lá»“i: $$f^{**} = f$$

---

## ğŸ† **PHáº¦N V: CÆ  Báº¢N Tá»I Æ¯U HÃ“A Lá»’I (ChÆ°Æ¡ng 04)**

### âœ… **11. Äiá»u kiá»‡n tá»‘i Æ°u**

#### **Äiá»u kiá»‡n báº­c nháº¥t (khÃ´ng rÃ ng buá»™c)**
$$\nabla f(x^*) = 0$$

#### **Äiá»u kiá»‡n báº­c nháº¥t (cÃ³ rÃ ng buá»™c)**
Vá»›i bÃ i toÃ¡n $$\min_x f(x)$$ s.t. $$x \in C$$:
$$\nabla f(x^*)^T (y - x^*) \ge 0 \quad \forall y \in C$$

#### **Äiá»u kiá»‡n báº­c hai**
- **Äiá»u kiá»‡n cáº§n:** $$\nabla f(x^*) = 0$$ vÃ  $$\nabla^2 f(x^*) \succeq 0$$
- **Äiá»u kiá»‡n Ä‘á»§:** $$\nabla f(x^*) = 0$$ vÃ  $$\nabla^2 f(x^*) \succ 0$$

### ğŸ”„ **12. Ká»¹ thuáº­t biáº¿n Ä‘á»•i bÃ i toÃ¡n**

#### **Biáº¿n phá»¥ (Slack Variables)**
$$g_i(x) \le 0 \quad \Rightarrow \quad g_i(x) + s_i = 0, \quad s_i \ge 0$$

#### **LÃ m giáº£n (Relaxation)**
Thay tháº¿ táº­p kháº£ thi $$C$$ báº±ng siÃªu táº­p $$\tilde{C} \supseteq C$$

#### **Loáº¡i bá» rÃ ng buá»™c Ä‘áº³ng thá»©c**
Náº¿u $$Ax = b$$ cÃ³ nghiá»‡m: $$x = x_0 + Fz$$ vá»›i $$Ax_0 = b$$ vÃ  $$AF = 0$$

---

## ğŸ“Š **PHáº¦N VI: CÃC BÃ€I TOÃN KINH ÄIá»‚N (ChÆ°Æ¡ng 05)**

### ğŸ¯ **13. PhÃ¢n cáº¥p bÃ i toÃ¡n tá»‘i Æ°u**

```
LP âŠ† QP âŠ† QCQP âŠ† SDP
    âŠ† SOCP âŠ†
```

#### **Linear Programming (LP)**
$$\begin{align}
&\min_{x} &&c^T x \\
&\text{s.t.} &&Ax = b \\
&&&x \succeq 0
\end{align}$$

**Äáº·c Ä‘iá»ƒm:**
- HÃ m má»¥c tiÃªu vÃ  rÃ ng buá»™c Ä‘á»u affine
- Nghiá»‡m tá»‘i Æ°u táº¡i Ä‘á»‰nh cá»§a Ä‘a diá»‡n
- Giáº£i báº±ng thuáº­t toÃ¡n Simplex

#### **Quadratic Programming (QP)**
$$\begin{align}
&\min_{x} &&\frac{1}{2}x^T P x + q^T x \\
&\text{s.t.} &&Gx \preceq h \\
&&&Ax = b
\end{align}$$

**Äiá»u kiá»‡n lá»“i:** $$P \succeq 0$$

### ğŸ¯ **14. BÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu**

#### **BÃ i toÃ¡n cÆ¡ báº£n**
$$\min_{x} \lVert Ax - b \rVert_2^2$$

#### **Nghiá»‡m dáº¡ng Ä‘Ã³ng (Normal Equation)**
$$x^* = (A^T A)^{-1} A^T b$$

#### **Há»“i quy tuyáº¿n tÃ­nh**
- **Má»™t biáº¿n:** $$m = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$, $$c = \bar{y} - m\bar{x}$$
- **Nhiá»u biáº¿n:** $$\beta^* = (X^T X)^{-1} X^T y$$

---

## âš¡ **PHáº¦N VII: GRADIENT DESCENT (ChÆ°Æ¡ng 06)**

### ğŸ¯ **15. Thuáº­t toÃ¡n cÆ¡ báº£n**

#### **Quy táº¯c cáº­p nháº­t**
$$x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)})$$

#### **Giáº£i thÃ­ch**
- Gradient $$\nabla f(x)$$ chá»‰ hÆ°á»›ng tÄƒng dá»‘c nháº¥t
- Gradient Ã¢m $$-\nabla f(x)$$ chá»‰ hÆ°á»›ng giáº£m dá»‘c nháº¥t
- Táº¡i cá»±c tiá»ƒu: $$\nabla f(x^*) = 0$$

### ğŸ“ **16. Lá»±a chá»n kÃ­ch thÆ°á»›c bÆ°á»›c**

#### **KÃ­ch thÆ°á»›c bÆ°á»›c cá»‘ Ä‘á»‹nh**
$$t = \text{const} > 0$$

#### **Backtracking line search**
**Äiá»u kiá»‡n Armijo:** $$f(x + t\Delta x) \le f(x) + \alpha t \nabla f(x)^T \Delta x$$

#### **Exact line search**
$$t^* = \arg\min_{t > 0} f(x + t\Delta x)$$

### ğŸ“Š **17. PhÃ¢n tÃ­ch há»™i tá»¥**

#### **Äiá»u kiá»‡n Lipschitz**
$$\lVert \nabla f(x) - \nabla f(y) \rVert_2 \le L \lVert x - y \rVert_2$$

#### **Há»™i tá»¥ cho hÃ m lá»“i**
$$f(x^{(k)}) - f^* \le \frac{\lVert x^{(0)} - x^* \rVert_2^2}{2tk}$$
**Tá»‘c Ä‘á»™:** $$O(1/k)$$ - sublinear

#### **Há»™i tá»¥ cho hÃ m lá»“i máº¡nh**
$$f(x^{(k)}) - f^* \le \left(1 - \frac{2m}{m + L}\right)^k \left(f(x^{(0)}) - f^*\right)$$
**Tá»‘c Ä‘á»™:** $$O(\rho^k)$$ vá»›i $$\rho < 1$$ - linear

### ğŸ² **18. Stochastic Gradient Descent**

#### **BÃ i toÃ¡n tá»•ng**
$$\min_x f(x) = \min_x \sum_{i=1}^m f_i(x)$$

#### **Batch vs SGD**
- **Batch GD:** $$x^{(k)} = x^{(k-1)} - t_k \sum_{i=1}^m \nabla f_i(x^{(k-1)})$$ - Chi phÃ­ $$O(m)$$
- **SGD:** $$x^{(k)} = x^{(k-1)} - t_k \nabla f_{i_k}(x^{(k-1)})$$ - Chi phÃ­ $$O(1)$$

#### **Mini-batch SGD**
$$x^{(k)} = x^{(k-1)} - t_k \frac{1}{b} \sum_{i \in \mathcal{B}_k} \nabla f_i(x^{(k-1)})$$

### âš¡ **19. Momentum vÃ  cÃ¡c ká»¹ thuáº­t nÃ¢ng cao**

#### **Classical Momentum**
$$\begin{align}
v^{(k)} &= \beta v^{(k-1)} + \nabla f(x^{(k-1)}) \\
x^{(k)} &= x^{(k-1)} - t v^{(k)}
\end{align}$$

#### **Adaptive Methods**
- **AdaGrad:** Äiá»u chá»‰nh learning rate theo lá»‹ch sá»­ gradient
- **RMSprop:** Exponential moving average
- **Adam:** Káº¿t há»£p momentum vÃ  adaptive learning rate

---

## ğŸ”— **PHáº¦N VIII: Káº¾T Ná»I VÃ€ Tá»”NG Há»¢P**

### ğŸŒŸ **20. Má»‘i liÃªn há»‡ giá»¯a cÃ¡c khÃ¡i niá»‡m**

#### **Chuá»—i logic xÃ¢y dá»±ng**
```
Ná»n táº£ng toÃ¡n há»c (Ch 00)
    â†“
KhÃ¡i niá»‡m tá»‘i Æ°u hÃ³a (Ch 01)
    â†“
Táº­p lá»“i (Ch 02) â†’ HÃ m lá»“i (Ch 03)
    â†“
Äiá»u kiá»‡n tá»‘i Æ°u (Ch 04)
    â†“
BÃ i toÃ¡n cá»¥ thá»ƒ (Ch 05) â†’ Thuáº­t toÃ¡n (Ch 06)
```

#### **Káº¿t ná»‘i quan trá»ng**
- **Gradient** (Ch 00) â†’ **Äiá»u kiá»‡n tá»‘i Æ°u báº­c nháº¥t** (Ch 04) â†’ **Gradient descent** (Ch 06)
- **Táº­p lá»“i** (Ch 02) + **HÃ m lá»“i** (Ch 03) â†’ **BÃ i toÃ¡n lá»“i** (Ch 01,04,05)
- **Chuá»—i Taylor** (Ch 00) â†’ **Giáº£i thÃ­ch gradient descent** (Ch 06)
- **Eigenvalue** (Ch 00) â†’ **Ma tráº­n xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng** (Ch 03) â†’ **Hessian** (Ch 04)

### ğŸ¯ **21. á»¨ng dá»¥ng thá»‘ng nháº¥t**

#### **Machine Learning Pipeline**
1. **Formulation:** Chuyá»ƒn bÃ i toÃ¡n ML thÃ nh tá»‘i Æ°u hÃ³a (Ch 01, 05)
2. **Analysis:** Kiá»ƒm tra tÃ­nh lá»“i (Ch 02, 03)
3. **Optimality:** Ãp dá»¥ng Ä‘iá»u kiá»‡n tá»‘i Æ°u (Ch 04)
4. **Algorithm:** Sá»­ dá»¥ng gradient descent (Ch 06)

#### **VÃ­ dá»¥: Linear Regression**
- **BÃ i toÃ¡n:** $$\min_w \lVert Xw - y \rVert_2^2$$ (Ch 05 - Least Squares)
- **TÃ­nh lá»“i:** HÃ m má»¥c tiÃªu lÃ  lá»“i (Ch 03)
- **Äiá»u kiá»‡n tá»‘i Æ°u:** $$\nabla f(w^*) = 2X^T(Xw^* - y) = 0$$ (Ch 04)
- **Nghiá»‡m:** $$w^* = (X^TX)^{-1}X^Ty$$ (Ch 05)
- **Thuáº­t toÃ¡n:** Gradient descent vá»›i $$\nabla f(w) = 2X^T(Xw - y)$$ (Ch 06)

### ğŸ”„ **22. So sÃ¡nh vÃ  lá»±a chá»n phÆ°Æ¡ng phÃ¡p**

| BÃ i toÃ¡n | PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|----------|-------------|---------|------------|
| **LP** | Simplex | ChÃ­nh xÃ¡c, hiá»‡u quáº£ | Chá»‰ cho LP |
| **QP** | Active Set | ChÃ­nh xÃ¡c | Phá»©c táº¡p |
| **Least Squares** | Normal Equation | Nghiá»‡m Ä‘Ã³ng | $$O(n^3)$$ |
| **General Convex** | Gradient Descent | ÄÆ¡n giáº£n, tá»•ng quÃ¡t | Cháº­m |
| **Large Scale** | SGD | Nhanh, scalable | Nhiá»…u |

### ğŸ’¡ **23. NguyÃªn táº¯c thiáº¿t káº¿ thuáº­t toÃ¡n**

#### **Tá»« lÃ½ thuyáº¿t Ä‘áº¿n thá»±c hÃ nh**
1. **PhÃ¢n tÃ­ch bÃ i toÃ¡n:** TÃ­nh lá»“i, Ä‘iá»u kiá»‡n tá»‘i Æ°u
2. **Chá»n thuáº­t toÃ¡n:** Dá»±a trÃªn cáº¥u trÃºc bÃ i toÃ¡n
3. **Tá»‘i Æ°u tham sá»‘:** Learning rate, stopping criteria
4. **ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t:** Convergence rate, computational cost

#### **Trade-offs quan trá»ng**
- **Accuracy vs Speed:** Exact methods vs Gradient methods
- **Memory vs Computation:** Batch vs SGD
- **Simplicity vs Performance:** Basic GD vs Advanced optimizers

---

## ğŸ“ **PHáº¦N IX: Tá»”NG Káº¾T VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N**

### ğŸŒŸ **24. Nhá»¯ng Ä‘iá»ƒm máº¥u chá»‘t**

#### **Táº¡i sao tá»‘i Æ°u hÃ³a lá»“i quan trá»ng?**
1. **Äáº£m báº£o tá»‘i Æ°u toÃ n cá»¥c:** KhÃ´ng cÃ³ local minima "giáº£"
2. **Thuáº­t toÃ¡n hiá»‡u quáº£:** Polynomial time algorithms
3. **LÃ½ thuyáº¿t cháº·t cháº½:** Convergence guarantees
4. **á»¨ng dá»¥ng rá»™ng rÃ£i:** ML, signal processing, finance, engineering

#### **Gradient descent - TrÃ¡i tim cá»§a ML**
- **Backpropagation** = Gradient descent + Chain rule
- **Deep learning** = SGD + Momentum + Adaptive methods
- **Modern optimizers** = Sophisticated variants of gradient descent

### ğŸš€ **25. HÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo**

#### **ChÆ°Æ¡ng 07+: Má»Ÿ rá»™ng lÃ½ thuyáº¿t**
- **Subgradient:** Non-smooth optimization
- **Proximal methods:** Composite optimization
- **Duality:** Lagrangian vÃ  KKT conditions
- **Interior point methods:** Polynomial-time algorithms

#### **á»¨ng dá»¥ng hiá»‡n Ä‘áº¡i**
- **Deep learning:** Neural networks, transformers
- **Reinforcement learning:** Policy optimization
- **Distributed optimization:** Federated learning
- **Quantum optimization:** Quantum algorithms

### ğŸ¯ **26. Lá»i khuyÃªn há»c táº­p**

#### **CÃ¡ch tiáº¿p cáº­n hiá»‡u quáº£**
1. **Náº¯m vá»¯ng ná»n táº£ng:** ToÃ¡n há»c cÆ¡ báº£n lÃ  chÃ¬a khÃ³a
2. **Káº¿t ná»‘i khÃ¡i niá»‡m:** Hiá»ƒu má»‘i liÃªn há»‡ giá»¯a cÃ¡c chÆ°Æ¡ng
3. **Thá»±c hÃ nh tÃ­nh toÃ¡n:** LÃ m bÃ i táº­p vÃ  implement algorithms
4. **á»¨ng dá»¥ng thá»±c táº¿:** Ãp dá»¥ng vÃ o bÃ i toÃ¡n ML/AI

#### **Debugging vÃ  troubleshooting**
- **Kiá»ƒm tra tÃ­nh lá»“i:** Äáº£m báº£o bÃ i toÃ¡n well-posed
- **Verify gradients:** Numerical vs analytical gradients
- **Monitor convergence:** Loss curves, gradient norms
- **Tune hyperparameters:** Learning rate, batch size

---

## ğŸ’¡ **Káº¾T LUáº¬N**

HÃ nh trÃ¬nh tá»« chÆ°Æ¡ng 00 Ä‘áº¿n 06 Ä‘Ã£ Ä‘Æ°a chÃºng ta qua:

ğŸ”¢ **Ná»n táº£ng toÃ¡n há»c** â†’ ğŸ¯ **KhÃ¡i niá»‡m tá»‘i Æ°u hÃ³a** â†’ ğŸ”µ **Táº­p lá»“i** â†’ ğŸ“ˆ **HÃ m lá»“i** â†’ ğŸ† **Äiá»u kiá»‡n tá»‘i Æ°u** â†’ ğŸ“Š **BÃ i toÃ¡n cá»¥ thá»ƒ** â†’ âš¡ **Thuáº­t toÃ¡n thá»±c táº¿**

ÄÃ¢y khÃ´ng chá»‰ lÃ  kiáº¿n thá»©c lÃ½ thuyáº¿t mÃ  lÃ  **ná»n táº£ng cá»§a AI vÃ  machine learning hiá»‡n Ä‘áº¡i**. Má»—i khÃ¡i niá»‡m Ä‘á»u cÃ³ vai trÃ² quan trá»ng trong viá»‡c hiá»ƒu vÃ  phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a tiÃªn tiáº¿n.

**Tá»‘i Æ°u hÃ³a lá»“i** lÃ  cáº§u ná»‘i giá»¯a toÃ¡n há»c thuáº§n tÃºy vÃ  á»©ng dá»¥ng thá»±c táº¿, giÃºp chÃºng ta giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phá»©c táº¡p má»™t cÃ¡ch hiá»‡u quáº£ vÃ  Ä‘Ã¡ng tin cáº­y.

---

*"Optimization is the engine of machine learning, and convex optimization is its most reliable and well-understood component."*

<style>
/* Custom styling for comprehensive review */
.MathJax {
    vertical-align: baseline !important;
}

h2 {
    color: #2c3e50;
    border-bottom: 2px solid #3498db;
    padding-bottom: 10px;
}

h3 {
    color: #34495e;
    margin-top: 25px;
}

h4 {
    color: #7f8c8d;
    margin-top: 20px;
}

blockquote {
    background-color: #f8f9fa;
    border-left: 4px solid #3498db;
    padding: 15px;
    margin: 20px 0;
    font-style: italic;
}

table {
    width: 100%;
    border-collapse: collapse;
    margin: 15px 0;
}

table th, table td {
    border: 1px solid #ddd;
    padding: 12px;
    text-align: left;
}

table th {
    background-color: #f2f2f2;
    font-weight: bold;
}

.highlight-box {
    background-color: #e8f4fd;
    border: 1px solid #bee5eb;
    border-radius: 5px;
    padding: 15px;
    margin: 15px 0;
}

.formula-box {
    background-color: #f8f9fa;
    border: 1px solid #dee2e6;
    border-radius: 5px;
    padding: 10px;
    margin: 10px 0;
    text-align: center;
}

code {
    background-color: #f1f1f1;
    padding: 2px 4px;
    border-radius: 3px;
    font-family: monospace;
}

pre {
    background-color: #f8f9fa;
    border: 1px solid #e9ecef;
    border-radius: 5px;
    padding: 15px;
    overflow-x: auto;
}
</style>
