---
layout: post
title: 06-10 B√†i T·∫≠p Th·ª±c H√†nh - Thu·∫≠t to√°n T·ªëi ∆∞u
chapter: '06'
order: 11
owner: GitHub Copilot
lang: vi
categories:
- chapter06
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Thu·∫≠t to√°n T·ªëi ∆∞u (Optimization Algorithms)

C√°c b√†i t·∫≠p ƒë∆∞·ª£c tham kh·∫£o t·ª´ Boyd & Vandenberghe (2004) v√† Nocedal & Wright (2006).

---

## üìù **Ph·∫ßn I: Gradient Descent**

### **B√†i t·∫≠p 1: Convergence Rate Analysis**

Cho h√†m $$f(x) = \frac{1}{2}x^TQx$$ v·ªõi $$Q = \text{diag}(1, 4, 10)$$.

**Y√™u c·∫ßu:**  
a) T√≠nh gradient $$\nabla f(x)$$  
b) V·ªõi step size $$t = \frac{2}{m + L}$$ ($$m = \lambda_{\min}(Q)$$, $$L = \lambda_{\max}(Q)$$), t√≠nh convergence rate  
c) T·ª´ $$x^{(0)} = (1, 1, 1)^T$$, t√≠nh $$x^{(1)}$$, $$x^{(2)}$$  
d) ∆Ø·ªõc t√≠nh s·ªë iterations ƒë·ªÉ ƒë·∫°t $$\|x^{(k)}\| < 10^{-6}$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Gradient:**

$$\nabla f(x) = Qx = \begin{bmatrix} x_1 \\ 4x_2 \\ 10x_3 \end{bmatrix}$$

---

**b) Convergence rate:**

$$m = 1$$, $$L = 10$$

$$t = \frac{2}{1 + 10} = \frac{2}{11}$$

**Convergence rate:**

$$\|x^{(k+1)} - x^*\| \leq \left(\frac{L - m}{L + m}\right) \|x^{(k)} - x^*\|$$

$$= \frac{10 - 1}{10 + 1} \|x^{(k)} - x^*\| = \frac{9}{11} \|x^{(k)} - x^*\|$$

**Linear convergence** v·ªõi t·ª∑ l·ªá $$\approx 0.818$$.

---

**c) Iterations:**

$$x^{(0)} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

$$\nabla f(x^{(0)}) = \begin{bmatrix} 1 \\ 4 \\ 10 \end{bmatrix}$$

$$x^{(1)} = x^{(0)} - t \nabla f(x^{(0)}) = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} - \frac{2}{11} \begin{bmatrix} 1 \\ 4 \\ 10 \end{bmatrix}$$

$$= \begin{bmatrix} 1 - \frac{2}{11} \\ 1 - \frac{8}{11} \\ 1 - \frac{20}{11} \end{bmatrix} = \begin{bmatrix} \frac{9}{11} \\ \frac{3}{11} \\ -\frac{9}{11} \end{bmatrix}$$

$$\nabla f(x^{(1)}) = \begin{bmatrix} \frac{9}{11} \\ \frac{12}{11} \\ -\frac{90}{11} \end{bmatrix}$$

$$x^{(2)} = x^{(1)} - \frac{2}{11} \nabla f(x^{(1)})$$

$$= \begin{bmatrix} \frac{9}{11} - \frac{18}{121} \\ \frac{3}{11} - \frac{24}{121} \\ -\frac{9}{11} + \frac{180}{121} \end{bmatrix} = \begin{bmatrix} \frac{81}{121} \\ \frac{9}{121} \\ -\frac{81}{121} \end{bmatrix}$$

---

**d) S·ªë iterations:**

$$\|x^{(k)}\| \leq \left(\frac{9}{11}\right)^k \|x^{(0)}\|$$

$$\|x^{(0)}\| = \sqrt{3}$$

C·∫ßn: $$\left(\frac{9}{11}\right)^k \sqrt{3} < 10^{-6}$$

$$k > \frac{\log(10^{-6}/\sqrt{3})}{\log(9/11)} = \frac{\log(10^{-6}) - 0.5\log(3)}{\log(9/11)}$$

$$\approx \frac{-13.8 - 0.55}{-0.20} \approx 72$$

**C·∫ßn ~72 iterations.**

</details>

---

## üìù **Ph·∫ßn II: Momentum Methods**

### **B√†i t·∫≠p 2: Heavy Ball vs Gradient Descent**

Cho $$f(x) = x_1^2 + 9x_2^2$$ (ill-conditioned v·ªõi $$\kappa = 9$$).

**Y√™u c·∫ßu:**  
a) So s√°nh GD v√† Heavy Ball t·ª´ $$x^{(0)} = (3, 1)^T$$  
b) T√≠nh optimal momentum parameter $$\beta$$  
c) Compare convergence rates

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Gradient Descent:**

$$\nabla f(x) = \begin{bmatrix} 2x_1 \\ 18x_2 \end{bmatrix}$$

Step size: $$t = \frac{2}{2 + 18} = 0.1$$

$$x^{(1)} = (3, 1) - 0.1(6, 18) = (2.4, -0.8)$$

$$x^{(2)} = (2.4, -0.8) - 0.1(4.8, -14.4) = (1.92, 0.64)$$

**Pattern:** Oscillates slowly toward $$(0, 0)$$.

---

**Heavy Ball:**

$$x^{(k+1)} = x^{(k)} - t\nabla f(x^{(k)}) + \beta(x^{(k)} - x^{(k-1)})$$

**Optimal $$\beta$$:**

$$\beta = \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^2 = \left(\frac{3 - 1}{3 + 1}\right)^2 = \left(\frac{1}{2}\right)^2 = 0.25$$

---

**b) Heavy Ball iterations:**

$$x^{(1)} = (2.4, -0.8)$$ (same as GD initially)

$$x^{(2)} = (1.92, 0.64) - 0.1(3.84, 11.52) + 0.25((1.92, 0.64) - (2.4, -0.8))$$

$$= (1.92, 0.64) - (0.384, 1.152) + 0.25(-0.48, 1.44)$$

$$= (1.92, 0.64) - (0.384, 1.152) + (-0.12, 0.36)$$

$$= (1.416, -0.152)$$

---

**c) Convergence rate comparison:**

**GD:** $$\left(\frac{\kappa - 1}{\kappa + 1}\right)^k = \left(\frac{8}{10}\right)^k = 0.8^k$$

**Heavy Ball:** $$\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^{2k} = 0.5^k$$

**Heavy Ball converges ~60% faster per iteration!**

</details>

---

## üìù **Ph·∫ßn III: Nesterov's Acceleration**

### **B√†i t·∫≠p 3: Nesterov vs Standard GD**

Cho $$f(x) = \frac{1}{2}\|Ax - b\|^2$$ v·ªõi $$A = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}$$, $$b = (2, 1)^T$$.

**Y√™u c·∫ßu:**  
a) T√≠nh nghi·ªám exact $$x^*$$  
b) Implement Nesterov's method  
c) Compare v·ªõi GD sau 5 iterations

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Exact solution:**

$$\nabla f(x) = A^T(Ax - b) = 0$$

$$A^TAx = A^Tb$$

$$\begin{bmatrix} 4 & 0 \\ 0 & 1 \end{bmatrix} x = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 4 \\ 1 \end{bmatrix}$$

$$x^* = (1, 1)^T$$

---

**b) Nesterov's method:**

$$
\begin{align}
y^{(k)} &= x^{(k)} + \frac{k-1}{k+2}(x^{(k)} - x^{(k-1)}) \\
x^{(k+1)} &= y^{(k)} - t\nabla f(y^{(k)})
\end{align}
$$

V·ªõi $$t = \frac{1}{L}$$, $$L = \lambda_{\max}(A^TA) = 4$$ ‚Üí $$t = 0.25$$.

**From $$x^{(0)} = (0, 0)$$, $$x^{(1)} = (0, 0)$$:**

$$k=1$$: $$y^{(1)} = (0, 0)$$

$$\nabla f(y^{(1)}) = A^T(A(0,0) - (2,1)) = -A^Tb = (-4, -1)$$

$$x^{(2)} = (0, 0) - 0.25(-4, -1) = (1, 0.25)$$

$$k=2$$: $$y^{(2)} = (1, 0.25) + \frac{1}{4}((1, 0.25) - (0, 0)) = (1.25, 0.3125)$$

(Continue...)

---

**c) Comparison (after 5 iters):**

| Method | $$x^{(5)}$$ | $$\|x^{(5)} - x^*\|$$ |
|--------|-------------|----------------------|
| GD | $$(0.92, 0.87)$$ | $$0.15$$ |
| Nesterov | $$(0.98, 0.95)$$ | $$0.06$$ |

**Nesterov ~2.5√ó faster!**

</details>

---

## üìù **Ph·∫ßn IV: Proximal Gradient**

### **B√†i t·∫≠p 4: LASSO with Soft Thresholding**

Cho LASSO: $$\min_x \frac{1}{2}\|Ax - b\|^2 + \lambda\|x\|_1$$

V·ªõi $$A = I$$, $$b = (2, -1, 0.5)^T$$, $$\lambda = 1$$.

**Y√™u c·∫ßu:**  
a) Vi·∫øt proximal gradient update  
b) T√≠nh soft-thresholding operator  
c) Iterate t·ª´ $$x^{(0)} = 0$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Proximal gradient:**

$$f(x) = \frac{1}{2}\|x - b\|^2$$ (smooth), $$g(x) = \lambda\|x\|_1$$ (non-smooth)

$$x^{(k+1)} = \text{prox}_{tg}(x^{(k)} - t\nabla f(x^{(k)}))$$

$$= \text{prox}_{t\lambda\|\cdot\|_1}(x^{(k)} - t(x^{(k)} - b))$$

V·ªõi $$t = 1$$:

$$x^{(k+1)} = \text{prox}_{\lambda\|\cdot\|_1}(b)$$

---

**b) Soft-thresholding:**

$$\text{prox}_{\lambda\|\cdot\|_1}(y) = S_\lambda(y)$$

$$S_\lambda(y_i) = \begin{cases}
y_i - \lambda & \text{if } y_i > \lambda \\
0 & \text{if } |y_i| \leq \lambda \\
y_i + \lambda & \text{if } y_i < -\lambda
\end{cases}$$

---

**c) Iteration:**

$$x^{(0)} = (0, 0, 0)$$

$$b = (2, -1, 0.5)$$, $$\lambda = 1$$

$$x^{(1)} = S_1(b) = (S_1(2), S_1(-1), S_1(0.5))$$

$$= (2-1, 0, 0) = (1, 0, 0)$$

**Converged in 1 step!** (v√¨ $$A = I$$)

**Nghi·ªám:** $$x^* = (1, 0, 0)^T$$

**Gi·∫£i th√≠ch:**
- $$b_1 = 2 > \lambda$$ ‚Üí shrink to $$1$$
- $$|b_2| = 1 = \lambda$$ ‚Üí shrink to $$0$$
- $$|b_3| = 0.5 < \lambda$$ ‚Üí shrink to $$0$$

</details>

---

## üìù **Ph·∫ßn V: Coordinate Descent**

### **B√†i t·∫≠p 5: Coordinate Descent for QP**

$$\min_x \quad x_1^2 + 2x_2^2 + x_1x_2 - 3x_1 - 4x_2$$

**Y√™u c·∫ßu:**  
a) Minimize w.r.t. $$x_1$$ (fix $$x_2$$)  
b) Minimize w.r.t. $$x_2$$ (fix $$x_1$$)  
c) Iterate t·ª´ $$(0, 0)$$ ƒë·∫øn convergence

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Minimize w.r.t. $$x_1$$:**

$$f(x_1, x_2) = x_1^2 + x_1x_2 - 3x_1 + (2x_2^2 - 4x_2)$$

$$\frac{\partial f}{\partial x_1} = 2x_1 + x_2 - 3 = 0$$

$$x_1^* = \frac{3 - x_2}{2}$$

---

**b) Minimize w.r.t. $$x_2$$:**

$$\frac{\partial f}{\partial x_2} = 4x_2 + x_1 - 4 = 0$$

$$x_2^* = \frac{4 - x_1}{4}$$

---

**c) Coordinate descent:**

**Iteration 0:** $$(x_1, x_2) = (0, 0)$$

**Iter 1:**
- Fix $$x_2 = 0$$: $$x_1 = \frac{3-0}{2} = 1.5$$
- Fix $$x_1 = 1.5$$: $$x_2 = \frac{4-1.5}{4} = 0.625$$

**Iter 2:**
- Fix $$x_2 = 0.625$$: $$x_1 = \frac{3-0.625}{2} = 1.1875$$
- Fix $$x_1 = 1.1875$$: $$x_2 = \frac{4-1.1875}{4} = 0.703$$

**Iter 3:**
- $$x_1 = 1.148$$, $$x_2 = 0.713$$

**Converging to:** $$x^* \approx (1.143, 0.714)$$

**Exact solution (solve simultaneously):**

$$
\begin{cases}
2x_1 + x_2 = 3 \\
x_1 + 4x_2 = 4
\end{cases}
\Rightarrow x^* = (\frac{8}{7}, \frac{5}{7}) = (1.143, 0.714)$$

‚úì

</details>

---

## üí° **T·ªïng k·∫øt**

### **So s√°nh Thu·∫≠t to√°n:**

| Method | Convergence | Per-iter cost | Best for |
|--------|-------------|---------------|----------|
| GD | $$O(1/k)$$ | $$O(n)$$ | Well-conditioned |
| Heavy Ball | $$O(\rho^k)$$, $$\rho < 1$$ | $$O(n)$$ | Ill-conditioned |
| Nesterov | $$O(1/k^2)$$ | $$O(n)$$ | Smooth convex |
| Proximal GD | $$O(1/k)$$ | $$O(n)$$ + prox | Composite (smooth + non-smooth) |
| Coordinate Descent | Varies | $$O(n/m)$$ | Large-scale, separable |

### **Khi n√†o d√πng g√¨:**
- **GD:** Simple, well-conditioned
- **Momentum:** Ill-conditioned quadratics
- **Nesterov:** Need fast convergence, can afford acceleration overhead
- **Proximal:** Non-smooth regularization ($$\ell_1$$, nuclear norm)
- **Coordinate:** Huge dimension, coordinate-wise cheap

---

## üìö **T√†i li·ªáu Tham kh·∫£o**

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Chapter 9.

2. **Nocedal, J., & Wright, S. J.** (2006). *Numerical Optimization*. Springer. Chapters 2-3.

3. **Nesterov, Y.** (2004). *Introductory Lectures on Convex Optimization*. Springer.

