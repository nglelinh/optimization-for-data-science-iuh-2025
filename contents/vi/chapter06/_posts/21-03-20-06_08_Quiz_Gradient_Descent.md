---
layout: post
title: 06-08 B√†i t·∫≠p tr·∫Øc nghi·ªám - Gradient Descent
chapter: '06'
order: 16
owner: GitHub Copilot
lang: vi
categories:
- chapter06
lesson_type: quiz
---

## üìö √în t·∫≠p l√Ω thuy·∫øt

Tr∆∞·ªõc khi l√†m b√†i t·∫≠p, h√£y √¥n l·∫°i c√°c kh√°i ni·ªám ch√≠nh trong ch∆∞∆°ng n√†y:

### üéØ **Gradient Descent C∆° B·∫£n**

**B√†i to√°n t·ªëi ∆∞u:**
$$\min_x f(x), \quad f \text{ kh·∫£ vi v√† } \text{dom}(f) = \mathbb{R}^n$$

**Quy t·∫Øc c·∫≠p nh·∫≠t:**
$$x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)}), \quad k = 1, 2, 3, \ldots$$

**M√£ gi·∫£:**
1. **Cho ƒëi·ªÉm b·∫Øt ƒë·∫ßu** $$x \in \text{dom}(f)$$
2. **L·∫∑p l·∫°i:**
   - X√°c ƒë·ªãnh h∆∞·ªõng gi·∫£m: $$\Delta x = -\nabla f(x)$$
   - T√¨m ki·∫øm ƒë∆∞·ªùng: ch·ªçn k√≠ch th∆∞·ªõc b∆∞·ªõc $$t > 0$$
   - C·∫≠p nh·∫≠t: $$x = x + t \Delta x$$
3. **Cho ƒë·∫øn khi** ti√™u chu·∫©n d·ª´ng ƒë∆∞·ª£c th·ªèa m√£n

**Gi·∫£i th√≠ch h√¨nh h·ªçc:**
- Gradient $$\nabla f(x)$$ ch·ªâ h∆∞·ªõng tƒÉng d·ªëc nh·∫•t
- Gradient √¢m $$-\nabla f(x)$$ ch·ªâ h∆∞·ªõng gi·∫£m d·ªëc nh·∫•t
- T·∫°i c·ª±c ti·ªÉu: $$\nabla f(x^*) = 0$$

### üìè **C√°ch Ch·ªçn K√≠ch Th∆∞·ªõc B∆∞·ªõc**

#### **1. K√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh**
$$t = \text{const} > 0$$

**∆Øu ƒëi·ªÉm:** ƒê∆°n gi·∫£n, t√≠nh to√°n nhanh
**Nh∆∞·ª£c ƒëi·ªÉm:** Kh√≥ ch·ªçn gi√° tr·ªã ph√π h·ª£p

#### **2. T√¨m ki·∫øm ƒë∆∞·ªùng l√πi (Backtracking)**
**ƒêi·ªÅu ki·ªán Armijo:**
$$f(x + t\Delta x) \le f(x) + \alpha t \nabla f(x)^T \Delta x$$

**Thu·∫≠t to√°n:**
- B·∫Øt ƒë·∫ßu v·ªõi $$t = t_0$$
- N·∫øu ƒëi·ªÅu ki·ªán Armijo kh√¥ng th·ªèa m√£n: $$t := \beta t$$ (v·ªõi $$0 < \beta < 1$$)
- L·∫∑p l·∫°i cho ƒë·∫øn khi th·ªèa m√£n

**Tham s·ªë th√¥ng d·ª•ng:** $$\alpha = 0.01 \div 0.3$$, $$\beta = 0.1 \div 0.8$$

#### **3. T√¨m ki·∫øm ƒë∆∞·ªùng ch√≠nh x√°c**
$$t^* = \arg\min_{t > 0} f(x + t\Delta x)$$

**∆Øu ƒëi·ªÉm:** T·ªëi ∆∞u v·ªÅ l√Ω thuy·∫øt
**Nh∆∞·ª£c ƒëi·ªÉm:** T·ªën k√©m t√≠nh to√°n

### üìä **Ph√¢n T√≠ch H·ªôi T·ª•**

#### **ƒêi·ªÅu ki·ªán Lipschitz**
Gradient $$\nabla f$$ l√† Lipschitz li√™n t·ª•c v·ªõi h·∫±ng s·ªë $$L > 0$$:
$$\lVert \nabla f(x) - \nabla f(y) \rVert_2 \le L \lVert x - y \rVert_2$$

#### **ƒê·ªãnh l√Ω h·ªôi t·ª• cho h√†m l·ªìi**
V·ªõi k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh $$t \le \frac{1}{L}$$:
$$f(x^{(k)}) - f^* \le \frac{\lVert x^{(0)} - x^* \rVert_2^2}{2tk}$$

**T·ªëc ƒë·ªô h·ªôi t·ª•:** $$O(1/k)$$ - h·ªôi t·ª• sublinear

#### **ƒê·ªãnh l√Ω h·ªôi t·ª• cho h√†m l·ªìi m·∫°nh**
N·∫øu $$f$$ l·ªìi m·∫°nh v·ªõi tham s·ªë $$m > 0$$:
$$f(x^{(k)}) - f^* \le \left(1 - \frac{2m}{m + L}\right)^k \left(f(x^{(0)}) - f^*\right)$$

**T·ªëc ƒë·ªô h·ªôi t·ª•:** $$O(\rho^k)$$ v·ªõi $$\rho < 1$$ - h·ªôi t·ª• linear

#### **S·ªë ƒëi·ªÅu ki·ªán**
$$\kappa = \frac{L}{m}$$ - t·ª∑ l·ªá gi·ªØa Lipschitz constant v√† strong convexity parameter

- $$\kappa$$ nh·ªè: h·ªôi t·ª• nhanh
- $$\kappa$$ l·ªõn: h·ªôi t·ª• ch·∫≠m (ill-conditioned)

### üé≤ **Stochastic Gradient Descent (SGD)**

#### **B√†i to√°n t·ªïng**
$$\min_x f(x) = \min_x \sum_{i=1}^m f_i(x)$$

#### **Batch Gradient Descent**
$$x^{(k)} = x^{(k-1)} - t_k \sum_{i=1}^m \nabla f_i(x^{(k-1)})$$

**Chi ph√≠:** $$O(m)$$ l·∫ßn ƒë√°nh gi√° gradient m·ªói l·∫ßn l·∫∑p

#### **Stochastic Gradient Descent**
$$x^{(k)} = x^{(k-1)} - t_k \nabla f_{i_k}(x^{(k-1)})$$

**Chi ph√≠:** $$O(1)$$ l·∫ßn ƒë√°nh gi√° gradient m·ªói l·∫ßn l·∫∑p

#### **Chi·∫øn l∆∞·ª£c ch·ªçn ch·ªâ s·ªë**
- **Quy t·∫Øc v√≤ng tr√≤n:** $$i_k = 1, 2, \ldots, m, 1, 2, \ldots$$
- **Quy t·∫Øc ng·∫´u nhi√™n:** $$i_k$$ ch·ªçn ng·∫´u nhi√™n t·ª´ $$\{1, 2, \ldots, m\}$$

#### **Mini-batch SGD**
$$x^{(k)} = x^{(k-1)} - t_k \frac{1}{b} \sum_{i \in \mathcal{B}_k} \nabla f_i(x^{(k-1)})$$

V·ªõi $$\mathcal{B}_k$$ l√† mini-batch k√≠ch th∆∞·ªõc $$b$$

### ‚ö° **Gradient Descent v·ªõi Momentum**

#### **Classical Momentum**
$$\begin{align}
v^{(k)} &= \beta v^{(k-1)} + \nabla f(x^{(k-1)}) \\
x^{(k)} &= x^{(k-1)} - t v^{(k)}
\end{align}$$

#### **Nesterov Accelerated Gradient**
$$\begin{align}
v^{(k)} &= \beta v^{(k-1)} + \nabla f(x^{(k-1)} - t\beta v^{(k-1)}) \\
x^{(k)} &= x^{(k-1)} - t v^{(k)}
\end{align}$$

**Tham s·ªë th√¥ng d·ª•ng:** $$\beta = 0.9$$

**∆Øu ƒëi·ªÉm:**
- TƒÉng t·ªëc h·ªôi t·ª•
- V∆∞·ª£t qua local oscillations
- Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa nhi·ªÖu

### üîÑ **So S√°nh C√°c Ph∆∞∆°ng Ph√°p**

| Ph∆∞∆°ng ph√°p | T·ªëc ƒë·ªô/l·∫ßn l·∫∑p | B·ªô nh·ªõ | H·ªôi t·ª• | ·ª®ng d·ª•ng |
|-------------|----------------|--------|--------|----------|
| **Batch GD** | Ch·∫≠m | Cao | ·ªîn ƒë·ªãnh | T·∫≠p d·ªØ li·ªáu nh·ªè |
| **SGD** | Nhanh | Th·∫•p | Nhi·ªÖu | T·∫≠p d·ªØ li·ªáu l·ªõn |
| **Mini-batch** | Trung b√¨nh | Trung b√¨nh | C√¢n b·∫±ng | Th·ª±c t·∫ø |
| **Momentum** | Nhanh | Th·∫•p | TƒÉng t·ªëc | Deep learning |

### üõ†Ô∏è **C√°c K·ªπ Thu·∫≠t Th·ª±c T·∫ø**

#### **Learning Rate Scheduling**
- **Step decay:** $$t_k = t_0 \cdot \gamma^{\lfloor k/s \rfloor}$$
- **Exponential decay:** $$t_k = t_0 \cdot e^{-\lambda k}$$
- **Polynomial decay:** $$t_k = t_0 \cdot (1 + \lambda k)^{-p}$$

#### **Adaptive Methods**
- **AdaGrad:** ƒêi·ªÅu ch·ªânh learning rate theo l·ªãch s·ª≠ gradient
- **RMSprop:** C·∫£i ti·∫øn AdaGrad v·ªõi exponential moving average
- **Adam:** K·∫øt h·ª£p momentum v√† adaptive learning rate

#### **Regularization**
- **L1 regularization:** $$f(x) + \lambda \lVert x \rVert_1$$
- **L2 regularization:** $$f(x) + \frac{\lambda}{2} \lVert x \rVert_2^2$$
- **Dropout:** Ng·∫´u nhi√™n h√≥a trong neural networks

### üìà **·ª®ng D·ª•ng Th·ª±c T·∫ø**

#### **Machine Learning**
- **Linear regression:** $$\min_w \lVert Xw - y \rVert_2^2$$
- **Logistic regression:** $$\min_w \sum_i \log(1 + e^{-y_i w^T x_i})$$
- **Neural networks:** Backpropagation + SGD

#### **Deep Learning**
- **Convolutional Neural Networks (CNNs)**
- **Recurrent Neural Networks (RNNs)**
- **Transformer models**

#### **Optimization Problems**
- **Portfolio optimization**
- **Signal processing**
- **Computer vision**

### üîç **Ti√™u Chu·∫©n D·ª´ng**

#### **Gradient-based**
$$\lVert \nabla f(x^{(k)}) \rVert_2 < \epsilon$$

#### **Function value-based**
$$|f(x^{(k)}) - f(x^{(k-1)})| < \epsilon$$

#### **Parameter-based**
$$\lVert x^{(k)} - x^{(k-1)} \rVert_2 < \epsilon$$

#### **Maximum iterations**
$$k > k_{\max}$$

### ‚ö†Ô∏è **Th√°ch Th·ª©c v√† Gi·∫£i Ph√°p**

#### **V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p:**
- **Vanishing/exploding gradients:** Gradient qu√° nh·ªè ho·∫∑c qu√° l·ªõn
- **Saddle points:** ƒêi·ªÉm y√™n ng·ª±a trong kh√¥ng gian nhi·ªÅu chi·ªÅu
- **Local minima:** C·ª±c ti·ªÉu ƒë·ªãa ph∆∞∆°ng (v·ªõi h√†m kh√¥ng l·ªìi)
- **Ill-conditioning:** S·ªë ƒëi·ªÅu ki·ªán l·ªõn

#### **Gi·∫£i ph√°p:**
- **Gradient clipping:** Gi·ªõi h·∫°n ƒë·ªô l·ªõn gradient
- **Batch normalization:** Chu·∫©n h√≥a input c·ªßa t·ª´ng layer
- **Residual connections:** Skip connections trong deep networks
- **Preconditioning:** Bi·∫øn ƒë·ªïi kh√¥ng gian t·ªëi ∆∞u

### üí° **M·∫πo Th·ª±c H√†nh**

#### **Ch·ªçn learning rate:**
1. B·∫Øt ƒë·∫ßu v·ªõi $$t = 0.01$$
2. TƒÉng/gi·∫£m theo powers of 10
3. Quan s√°t loss curve
4. S·ª≠ d·ª•ng learning rate scheduling

#### **Debugging:**
- Ki·ªÉm tra gradient implementation
- Visualize loss curve
- Monitor gradient norms
- Use validation set

#### **TƒÉng t·ªëc:**
- S·ª≠ d·ª•ng mini-batch thay v√¨ full batch
- Parallel/distributed training
- GPU acceleration
- Mixed precision training

---

üí° **M·∫πo:** Gradient descent l√† n·ªÅn t·∫£ng c·ªßa machine learning hi·ªán ƒë·∫°i. Hi·ªÉu r√µ c√°c bi·∫øn th·ªÉ v√† k·ªπ thu·∫≠t t·ªëi ∆∞u s·∫Ω gi√∫p b·∫°n hu·∫•n luy·ªán m√¥ h√¨nh hi·ªáu qu·∫£ h∆°n!

---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    ignoreHtmlClass: ".*|",
    processHtmlClass: "arithmatex"
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
/* S·ª≠a l·ªói alignment cho MathJax inline math */
.MathJax {
    vertical-align: baseline !important;
}

mjx-container[jax="CHTML"][display="false"] {
    vertical-align: baseline !important;
    display: inline !important;
}

/* ƒê·∫£m b·∫£o c√°c label trong quiz c√≥ alignment t·ªët */
.options label {
    display: flex;
    align-items: center;
    margin: 8px 0;
    padding: 8px;
    border-radius: 4px;
    transition: background-color 0.2s;
}

.options label:hover {
    background-color: #f0f0f0;
}

.options input[type="radio"] {
    margin-right: 8px;
    flex-shrink: 0;
}
</style>

B√†i t·∫≠p tr·∫Øc nghi·ªám n√†y ki·ªÉm tra hi·ªÉu bi·∫øt c·ªßa b·∫°n v·ªÅ thu·∫≠t to√°n gradient descent v√† c√°c bi·∫øn th·ªÉ, bao g·ªìm l·ª±a ch·ªçn k√≠ch th∆∞·ªõc b∆∞·ªõc, ph√¢n t√≠ch h·ªôi t·ª•, SGD v√† momentum.

---

<div id="quiz-container">
    <div id="quiz-header">
        <h2>B√†i t·∫≠p tr·∫Øc nghi·ªám: Gradient Descent</h2>
        <p>Ch·ªçn ƒë√°p √°n ƒë√∫ng nh·∫•t cho m·ªói c√¢u h·ªèi. B√†i t·∫≠p t·∫≠p trung v√†o c√°c kh√°i ni·ªám c∆° b·∫£n v√† ·ª©ng d·ª•ng th·ª±c t·∫ø c·ªßa gradient descent.</p>
        <div id="progress-bar">
            <div id="progress-fill"></div>
        </div>
        <p id="progress-text">C√¢u h·ªèi 1/25</p>
    </div>

    <div id="quiz-questions">
        <!-- C√¢u h·ªèi 1: Quy t·∫Øc c·∫≠p nh·∫≠t gradient descent -->
        <div class="question" id="q1">
            <h3>C√¢u 1: Quy t·∫Øc c·∫≠p nh·∫≠t c∆° b·∫£n c·ªßa gradient descent l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q1" value="a"> A) \(x^{(k)} = x^{(k-1)} + t \nabla f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q1" value="b"> B) \(x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q1" value="c"> C) \(x^{(k)} = x^{(k-1)} - t \nabla^2 f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q1" value="d"> D) \(x^{(k)} = x^{(k-1)} + t f(x^{(k-1)})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)})$$</strong><br>
                Gradient descent di chuy·ªÉn theo h∆∞·ªõng ng∆∞·ª£c v·ªõi gradient ƒë·ªÉ gi·∫£m gi√° tr·ªã h√†m m·ª•c ti√™u.
            </div>
        </div>

        <!-- C√¢u h·ªèi 2: H∆∞·ªõng gi·∫£m -->
        <div class="question" id="q2" style="display: none;">
            <h3>C√¢u 2: H∆∞·ªõng gi·∫£m trong gradient descent l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q2" value="a"> A) \(\Delta x = \nabla f(x)\)</label>
                <label><input type="radio" name="q2" value="b"> B) \(\Delta x = -\nabla f(x)\)</label>
                <label><input type="radio" name="q2" value="c"> C) \(\Delta x = \nabla^2 f(x)\)</label>
                <label><input type="radio" name="q2" value="d"> D) \(\Delta x = f(x)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\Delta x = -\nabla f(x)$$</strong><br>
                H∆∞·ªõng gi·∫£m l√† √¢m c·ªßa gradient, ch·ªâ h∆∞·ªõng gi·∫£m nhanh nh·∫•t c·ªßa h√†m s·ªë.
            </div>
        </div>

        <!-- C√¢u h·ªèi 3: T√°c ƒë·ªông c·ªßa k√≠ch th∆∞·ªõc b∆∞·ªõc -->
        <div class="question" id="q3" style="display: none;">
            <h3>C√¢u 3: N·∫øu k√≠ch th∆∞·ªõc b∆∞·ªõc $$t$$ qu√° l·ªõn, gradient descent c√≥ th·ªÉ:</h3>
            <div class="options">
                <label><input type="radio" name="q3" value="a"> A) H·ªôi t·ª• nhanh h∆°n</label>
                <label><input type="radio" name="q3" value="b"> B) V∆∞·ª£t qu√° c·ª±c ti·ªÉu v√† ph√¢n k·ª≥</label>
                <label><input type="radio" name="q3" value="c"> C) D·ª´ng l·∫°i ngay l·∫≠p t·ª©c</label>
                <label><input type="radio" name="q3" value="d"> D) T√¨m ƒë∆∞·ª£c nghi·ªám ch√≠nh x√°c</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) V∆∞·ª£t qu√° c·ª±c ti·ªÉu v√† ph√¢n k·ª≥</strong><br>
                K√≠ch th∆∞·ªõc b∆∞·ªõc qu√° l·ªõn c√≥ th·ªÉ l√†m thu·∫≠t to√°n dao ƒë·ªông ho·∫∑c ph√¢n k·ª≥ kh·ªèi nghi·ªám t·ªëi ∆∞u.
            </div>
        </div>

        <!-- C√¢u h·ªèi 4: K√≠ch th∆∞·ªõc b∆∞·ªõc qu√° nh·ªè -->
        <div class="question" id="q4" style="display: none;">
            <h3>C√¢u 4: N·∫øu k√≠ch th∆∞·ªõc b∆∞·ªõc $$t$$ qu√° nh·ªè, gradient descent s·∫Ω:</h3>
            <div class="options">
                <label><input type="radio" name="q4" value="a"> A) H·ªôi t·ª• r·∫•t ch·∫≠m</label>
                <label><input type="radio" name="q4" value="b"> B) Ph√¢n k·ª≥</label>
                <label><input type="radio" name="q4" value="c"> C) Dao ƒë·ªông</label>
                <label><input type="radio" name="q4" value="d"> D) D·ª´ng ngay l·∫≠p t·ª©c</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) H·ªôi t·ª• r·∫•t ch·∫≠m</strong><br>
                K√≠ch th∆∞·ªõc b∆∞·ªõc qu√° nh·ªè l√†m thu·∫≠t to√°n c·∫≠p nh·∫≠t r·∫•t ch·∫≠m, c·∫ßn nhi·ªÅu l·∫ßn l·∫∑p ƒë·ªÉ h·ªôi t·ª•.
            </div>
        </div>

        <!-- C√¢u h·ªèi 5: T√¨m ki·∫øm ƒë∆∞·ªùng l√πi -->
        <div class="question" id="q5" style="display: none;">
            <h3>C√¢u 5: T√¨m ki·∫øm ƒë∆∞·ªùng l√πi (backtracking line search) ho·∫°t ƒë·ªông b·∫±ng c√°ch:</h3>
            <div class="options">
                <label><input type="radio" name="q5" value="a"> A) TƒÉng d·∫ßn k√≠ch th∆∞·ªõc b∆∞·ªõc</label>
                <label><input type="radio" name="q5" value="b"> B) B·∫Øt ƒë·∫ßu v·ªõi k√≠ch th∆∞·ªõc b∆∞·ªõc l·ªõn v√† gi·∫£m d·∫ßn</label>
                <label><input type="radio" name="q5" value="c"> C) S·ª≠ d·ª•ng k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh</label>
                <label><input type="radio" name="q5" value="d"> D) T√≠nh to√°n ƒë·∫°o h√†m b·∫≠c hai</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) B·∫Øt ƒë·∫ßu v·ªõi k√≠ch th∆∞·ªõc b∆∞·ªõc l·ªõn v√† gi·∫£m d·∫ßn</strong><br>
                Backtracking b·∫Øt ƒë·∫ßu v·ªõi b∆∞·ªõc l·ªõn v√† gi·∫£m d·∫ßn cho ƒë·∫øn khi ƒë·∫°t ƒë∆∞·ª£c s·ª± gi·∫£m ƒë·ªß (Armijo condition).
            </div>
        </div>

        <!-- C√¢u h·ªèi 6: ƒêi·ªÅu ki·ªán Armijo -->
        <div class="question" id="q6" style="display: none;">
            <h3>C√¢u 6: ƒêi·ªÅu ki·ªán Armijo trong backtracking y√™u c·∫ßu:</h3>
            <div class="options">
                <label><input type="radio" name="q6" value="a"> A) \(f(x + t\Delta x) \leq f(x)\)</label>
                <label><input type="radio" name="q6" value="b"> B) \(f(x + t\Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x\)</label>
                <label><input type="radio" name="q6" value="c"> C) \(f(x + t\Delta x) = f(x)\)</label>
                <label><input type="radio" name="q6" value="d"> D) \(\nabla f(x + t\Delta x) = 0\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$f(x + t\Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x$$</strong><br>
                ƒêi·ªÅu ki·ªán Armijo ƒë·∫£m b·∫£o s·ª± gi·∫£m ƒë·ªß v·ªõi tham s·ªë $$\alpha \in (0, 0.5)$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 7: T√¨m ki·∫øm ƒë∆∞·ªùng ch√≠nh x√°c -->
        <div class="question" id="q7" style="display: none;">
            <h3>C√¢u 7: T√¨m ki·∫øm ƒë∆∞·ªùng ch√≠nh x√°c (exact line search) t√¨m k√≠ch th∆∞·ªõc b∆∞·ªõc b·∫±ng c√°ch:</h3>
            <div class="options">
                <label><input type="radio" name="q7" value="a"> A) S·ª≠ d·ª•ng c√¥ng th·ª©c c·ªë ƒë·ªãnh</label>
                <label><input type="radio" name="q7" value="b"> B) T·ªëi thi·ªÉu h√≥a \(f(x - t \nabla f(x))\) theo \(t\)</label>
                <label><input type="radio" name="q7" value="c"> C) Ch·ªçn ng·∫´u nhi√™n</label>
                <label><input type="radio" name="q7" value="d"> D) S·ª≠ d·ª•ng ƒë·∫°o h√†m b·∫≠c hai</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) T·ªëi thi·ªÉu h√≥a $$f(x - t \nabla f(x))$$ theo $$t$$</strong><br>
                Exact line search t√¨m k√≠ch th∆∞·ªõc b∆∞·ªõc t·ªëi ∆∞u b·∫±ng c√°ch gi·∫£i b√†i to√°n t·ªëi ∆∞u h√≥a m·ªôt chi·ªÅu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 8: ƒêi·ªÅu ki·ªán Lipschitz -->
        <div class="question" id="q8" style="display: none;">
            <h3>C√¢u 8: ƒêi·ªÅu ki·ªán Lipschitz cho gradient c√≥ nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q8" value="a"> A) \(\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|\)</label>
                <label><input type="radio" name="q8" value="b"> B) \(\|f(x) - f(y)\| \leq L \|x - y\|\)</label>
                <label><input type="radio" name="q8" value="c"> C) \(\nabla f(x) = L\)</label>
                <label><input type="radio" name="q8" value="d"> D) \(f(x) \leq L\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|$$</strong><br>
                ƒêi·ªÅu ki·ªán Lipschitz ƒë·∫£m b·∫£o gradient kh√¥ng thay ƒë·ªïi qu√° nhanh, quan tr·ªçng cho ph√¢n t√≠ch h·ªôi t·ª•.
            </div>
        </div>

        <!-- C√¢u h·ªèi 9: T·ªëc ƒë·ªô h·ªôi t·ª• gradient descent -->
        <div class="question" id="q9" style="display: none;">
            <h3>C√¢u 9: V·ªõi h√†m l·ªìi v√† gradient Lipschitz, gradient descent c√≥ t·ªëc ƒë·ªô h·ªôi t·ª•:</h3>
            <div class="options">
                <label><input type="radio" name="q9" value="a"> A) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q9" value="b"> B) \(O(1/k)\)</label>
                <label><input type="radio" name="q9" value="c"> C) \(O(\log k)\)</label>
                <label><input type="radio" name="q9" value="d"> D) \(O(e^{-k})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$O(1/k)$$</strong><br>
                Gradient descent c√≥ t·ªëc ƒë·ªô h·ªôi t·ª• sublinear $$O(1/k)$$ cho h√†m l·ªìi v·ªõi gradient Lipschitz.
            </div>
        </div>

        <!-- C√¢u h·ªèi 10: SGD vs Batch GD -->
        <div class="question" id="q10" style="display: none;">
            <h3>C√¢u 10: Stochastic Gradient Descent (SGD) kh√°c v·ªõi Batch GD ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q10" value="a"> A) SGD s·ª≠ d·ª•ng t·∫•t c·∫£ d·ªØ li·ªáu m·ªói l·∫ßn c·∫≠p nh·∫≠t</label>
                <label><input type="radio" name="q10" value="b"> B) SGD ch·ªâ s·ª≠ d·ª•ng m·ªôt m·∫´u d·ªØ li·ªáu m·ªói l·∫ßn c·∫≠p nh·∫≠t</label>
                <label><input type="radio" name="q10" value="c"> C) SGD kh√¥ng s·ª≠ d·ª•ng gradient</label>
                <label><input type="radio" name="q10" value="d"> D) SGD lu√¥n h·ªôi t·ª• nhanh h∆°n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) SGD ch·ªâ s·ª≠ d·ª•ng m·ªôt m·∫´u d·ªØ li·ªáu m·ªói l·∫ßn c·∫≠p nh·∫≠t</strong><br>
                SGD c·∫≠p nh·∫≠t tham s·ªë d·ª±a tr√™n gradient c·ªßa m·ªôt m·∫´u duy nh·∫•t thay v√¨ to√†n b·ªô dataset.
            </div>
        </div>

        <!-- C√¢u h·ªèi 11: ∆Øu ƒëi·ªÉm c·ªßa SGD -->
        <div class="question" id="q11" style="display: none;">
            <h3>C√¢u 11: ∆Øu ƒëi·ªÉm ch√≠nh c·ªßa SGD so v·ªõi Batch GD l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q11" value="a"> A) ƒê·ªô ch√≠nh x√°c cao h∆°n</label>
                <label><input type="radio" name="q11" value="b"> B) Chi ph√≠ t√≠nh to√°n th·∫•p h∆°n m·ªói l·∫ßn c·∫≠p nh·∫≠t</label>
                <label><input type="radio" name="q11" value="c"> C) Lu√¥n h·ªôi t·ª• ƒë·∫øn nghi·ªám t·ªëi ∆∞u</label>
                <label><input type="radio" name="q11" value="d"> D) Kh√¥ng c·∫ßn ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô h·ªçc</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Chi ph√≠ t√≠nh to√°n th·∫•p h∆°n m·ªói l·∫ßn c·∫≠p nh·∫≠t</strong><br>
                SGD c√≥ ƒë·ªô ph·ª©c t·∫°p O(1) m·ªói l·∫ßn c·∫≠p nh·∫≠t so v·ªõi O(m) c·ªßa Batch GD, r·∫•t hi·ªáu qu·∫£ cho big data.
            </div>
        </div>

        <!-- C√¢u h·ªèi 12: Mini-batch GD -->
        <div class="question" id="q12" style="display: none;">
            <h3>C√¢u 12: Mini-batch Gradient Descent l√† s·ª± k·∫øt h·ª£p gi·ªØa:</h3>
            <div class="options">
                <label><input type="radio" name="q12" value="a"> A) SGD v√† Newton method</label>
                <label><input type="radio" name="q12" value="b"> B) Batch GD v√† SGD</label>
                <label><input type="radio" name="q12" value="c"> C) Gradient descent v√† momentum</label>
                <label><input type="radio" name="q12" value="d"> D) Line search v√† fixed step</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Batch GD v√† SGD</strong><br>
                Mini-batch s·ª≠ d·ª•ng m·ªôt t·∫≠p con nh·ªè d·ªØ li·ªáu (th∆∞·ªùng 32-256 m·∫´u) ƒë·ªÉ c√¢n b·∫±ng gi·ªØa hi·ªáu qu·∫£ v√† ·ªïn ƒë·ªãnh.
            </div>
        </div>

        <!-- C√¢u h·ªèi 13: Quy t·∫Øc c·∫≠p nh·∫≠t momentum -->
        <div class="question" id="q13" style="display: none;">
            <h3>C√¢u 13: Quy t·∫Øc c·∫≠p nh·∫≠t c·ªßa Gradient Descent v·ªõi momentum l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q13" value="a"> A) \(v^{(k)} = \beta v^{(k-1)} + \nabla f(x^{(k-1)})\), \(x^{(k)} = x^{(k-1)} - t v^{(k)}\)</label>
                <label><input type="radio" name="q13" value="b"> B) \(v^{(k)} = \beta v^{(k-1)} + (1-\beta) \nabla f(x^{(k-1)})\), \(x^{(k)} = x^{(k-1)} - t v^{(k)}\)</label>
                <label><input type="radio" name="q13" value="c"> C) \(x^{(k)} = x^{(k-1)} - t \beta \nabla f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q13" value="d"> D) \(x^{(k)} = \beta x^{(k-1)} - t \nabla f(x^{(k-1)})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$v^{(k)} = \beta v^{(k-1)} + (1-\beta) \nabla f(x^{(k-1)})$$, $$x^{(k)} = x^{(k-1)} - t v^{(k)}$$</strong><br>
                Momentum t√≠ch l≈©y v·∫≠n t·ªëc v·ªõi trung b√¨nh ƒë·ªông c√≥ tr·ªçng s·ªë m≈© c·ªßa c√°c gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 14: H·ªá s·ªë momentum -->
        <div class="question" id="q14" style="display: none;">
            <h3>C√¢u 14: H·ªá s·ªë momentum $$\beta$$ th∆∞·ªùng ƒë∆∞·ª£c ch·ªçn trong kho·∫£ng:</h3>
            <div class="options">
                <label><input type="radio" name="q14" value="a"> A) [0.1, 0.3]</label>
                <label><input type="radio" name="q14" value="b"> B) [0.5, 0.7]</label>
                <label><input type="radio" name="q14" value="c"> C) [0.9, 0.99]</label>
                <label><input type="radio" name="q14" value="d"> D) [1.0, 1.5]</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) [0.9, 0.99]</strong><br>
                H·ªá s·ªë momentum th∆∞·ªùng ƒë∆∞·ª£c ch·ªçn cao (0.9 ho·∫∑c 0.99) ƒë·ªÉ duy tr√¨ "b·ªô nh·ªõ" l√¢u d√†i c·ªßa c√°c gradient tr∆∞·ªõc ƒë√≥.
            </div>
        </div>

        <!-- C√¢u h·ªèi 15: ∆Øu ƒëi·ªÉm c·ªßa momentum -->
        <div class="question" id="q15" style="display: none;">
            <h3>C√¢u 15: Momentum gi√∫p gradient descent:</h3>
            <div class="options">
                <label><input type="radio" name="q15" value="a"> A) Gi·∫£m dao ƒë·ªông v√† tƒÉng t·ªëc h·ªôi t·ª•</label>
                <label><input type="radio" name="q15" value="b"> B) TƒÉng ƒë·ªô ch√≠nh x√°c t√≠nh to√°n</label>
                <label><input type="radio" name="q15" value="c"> C) Gi·∫£m b·ªô nh·ªõ s·ª≠ d·ª•ng</label>
                <label><input type="radio" name="q15" value="d"> D) Lo·∫°i b·ªè ho√†n to√†n local minima</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Gi·∫£m dao ƒë·ªông v√† tƒÉng t·ªëc h·ªôi t·ª•</strong><br>
                Momentum l√†m m∆∞·ª£t qu√° tr√¨nh t·ªëi ∆∞u h√≥a, gi·∫£m dao ƒë·ªông v√† gi√∫p tƒÉng t·ªëc trong c√°c h∆∞·ªõng nh·∫•t qu√°n.
            </div>
        </div>

        <!-- C√¢u h·ªèi 16: Nesterov Accelerated Gradient -->
        <div class="question" id="q16" style="display: none;">
            <h3>C√¢u 16: Nesterov Accelerated Gradient (NAG) kh√°c v·ªõi momentum th√¥ng th∆∞·ªùng ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q16" value="a"> A) S·ª≠ d·ª•ng h·ªá s·ªë momentum kh√°c</label>
                <label><input type="radio" name="q16" value="b"> B) T√≠nh gradient t·∫°i v·ªã tr√≠ "nh√¨n tr∆∞·ªõc"</label>
                <label><input type="radio" name="q16" value="c"> C) Kh√¥ng s·ª≠ d·ª•ng momentum</label>
                <label><input type="radio" name="q16" value="d"> D) Ch·ªâ √°p d·ª•ng cho h√†m l·ªìi</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) T√≠nh gradient t·∫°i v·ªã tr√≠ "nh√¨n tr∆∞·ªõc"</strong><br>
                NAG t√≠nh gradient t·∫°i $$x^{(k-1)} - \beta v^{(k-1)}$$ thay v√¨ $$x^{(k-1)}$$, cho ph√©p "nh√¨n tr∆∞·ªõc" h∆∞·ªõng di chuy·ªÉn.
            </div>
        </div>

        <!-- C√¢u h·ªèi 17: ·ª®ng d·ª•ng gradient descent -->
        <div class="question" id="q17" style="display: none;">
            <h3>C√¢u 17: Gradient descent KH√îNG ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp trong:</h3>
            <div class="options">
                <label><input type="radio" name="q17" value="a"> A) Hu·∫•n luy·ªán neural networks</label>
                <label><input type="radio" name="q17" value="b"> B) Linear regression</label>
                <label><input type="radio" name="q17" value="c"> C) Logistic regression</label>
                <label><input type="radio" name="q17" value="d"> D) Integer programming</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) Integer programming</strong><br>
                Integer programming l√† b√†i to√°n t·ªëi ∆∞u r·ªùi r·∫°c, kh√¥ng th·ªÉ s·ª≠ d·ª•ng gradient descent v√¨ kh√¥ng c√≥ gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 18: ƒêi·ªÅu ki·ªán d·ª´ng -->
        <div class="question" id="q18" style="display: none;">
            <h3>C√¢u 18: ƒêi·ªÅu ki·ªán d·ª´ng ph·ªï bi·∫øn cho gradient descent l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q18" value="a"> A) \(\|\nabla f(x)\| < \epsilon\)</label>
                <label><input type="radio" name="q18" value="b"> B) \(f(x) = 0\)</label>
                <label><input type="radio" name="q18" value="c"> C) \(x = 0\)</label>
                <label><input type="radio" name="q18" value="d"> D) \(t = 0\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\|\nabla f(x)\| < \epsilon$$</strong><br>
                Thu·∫≠t to√°n d·ª´ng khi ƒë·ªô l·ªõn gradient nh·ªè h∆°n ng∆∞·ª°ng $$\epsilon$$, cho th·∫•y ƒë√£ g·∫ßn ƒë·∫°t c·ª±c ti·ªÉu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 19: H√†m l·ªìi m·∫°nh -->
        <div class="question" id="q19" style="display: none;">
            <h3>C√¢u 19: V·ªõi h√†m l·ªìi m·∫°nh, gradient descent c√≥ t·ªëc ƒë·ªô h·ªôi t·ª•:</h3>
            <div class="options">
                <label><input type="radio" name="q19" value="a"> A) \(O(1/k)\)</label>
                <label><input type="radio" name="q19" value="b"> B) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q19" value="c"> C) \(O(\rho^k)\) v·ªõi \(\rho < 1\) (linear)</label>
                <label><input type="radio" name="q19" value="d"> D) \(O(\log k)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) $$O(\rho^k)$$ v·ªõi $$\rho < 1$$ (linear)</strong><br>
                H√†m l·ªìi m·∫°nh cho ph√©p gradient descent ƒë·∫°t t·ªëc ƒë·ªô h·ªôi t·ª• tuy·∫øn t√≠nh (geometric), nhanh h∆°n nhi·ªÅu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 20: Backpropagation -->
        <div class="question" id="q20" style="display: none;">
            <h3>C√¢u 20: Backpropagation trong neural networks v·ªÅ b·∫£n ch·∫•t l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q20" value="a"> A) M·ªôt thu·∫≠t to√°n t·ªëi ∆∞u h√≥a m·ªõi</label>
                <label><input type="radio" name="q20" value="b"> B) C√°ch hi·ªáu qu·∫£ ƒë·ªÉ t√≠nh gradient cho gradient descent</label>
                <label><input type="radio" name="q20" value="c"> C) Thay th·∫ø cho gradient descent</label>
                <label><input type="radio" name="q20" value="d"> D) Ch·ªâ √°p d·ª•ng cho h√†m tuy·∫øn t√≠nh</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) C√°ch hi·ªáu qu·∫£ ƒë·ªÉ t√≠nh gradient cho gradient descent</strong><br>
                Backpropagation s·ª≠ d·ª•ng chain rule ƒë·ªÉ t√≠nh gradient hi·ªáu qu·∫£ trong neural networks, sau ƒë√≥ d√πng gradient descent ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë.
            </div>
        </div>
        <!-- C√¢u h·ªèi 21: Th·ª±c h√†nh -->
        <div class="question" id="q21" style="display: none;">
            <h3>C√¢u 21: Trong gradient descent, k√≠ch th∆∞·ªõc b∆∞·ªõc $$t = 0.01$$ c√≥ nghƒ©a l√† g√¨?</h3>
            <div class="options">
                <label><input type="radio" name="q21" value="a"> A) Di chuy·ªÉn 1% theo h∆∞·ªõng gradient</label>
                <label><input type="radio" name="q21" value="b"> B) Di chuy·ªÉn 0.01 ƒë∆°n v·ªã theo h∆∞·ªõng $$-\nabla f$$</label>
                <label><input type="radio" name="q21" value="c"> C) Chia gradient cho 100</label>
                <label><input type="radio" name="q21" value="d"> D) S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Di chuy·ªÉn 0.01 ƒë∆°n v·ªã theo h∆∞·ªõng $$-\nabla f$$</strong><br>
                K√≠ch th∆∞·ªõc b∆∞·ªõc $$t$$ x√°c ƒë·ªãnh ƒë·ªô d√†i di chuy·ªÉn theo h∆∞·ªõng $$-\nabla f$$: $$x^{(k+1)} = x^{(k)} - t\nabla f(x^{(k)})$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 22: Th·ª±c h√†nh -->
        <div class="question" id="q22" style="display: none;">
            <h3>C√¢u 22: ƒêi·ªÅu ki·ªán Armijo trong line search ki·ªÉm tra:</h3>
            <div class="options">
                <label><input type="radio" name="q22" value="a"> A) Gi·∫£m ƒë·ªß c·ªßa h√†m m·ª•c ti√™u</label>
                <label><input type="radio" name="q22" value="b"> B) ƒê·ªô d·ªëc c·ªßa gradient</label>
                <label><input type="radio" name="q22" value="c"> C) T√≠nh l·ªìi c·ªßa h√†m</label>
                <label><input type="radio" name="q22" value="d"> D) H·ªôi t·ª• c·ªßa thu·∫≠t to√°n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Gi·∫£m ƒë·ªß c·ªßa h√†m m·ª•c ti√™u</strong><br>
                ƒêi·ªÅu ki·ªán Armijo ƒë·∫£m b·∫£o $$f(x + t\Delta x) \leq f(x) + c_1 t \nabla f(x)^T \Delta x$$ v·ªõi $$c_1 \in (0,1)$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 23: Th·ª±c h√†nh -->
        <div class="question" id="q23" style="display: none;">
            <h3>C√¢u 23: SGD kh√°c v·ªõi GD ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q23" value="a"> A) S·ª≠ d·ª•ng to√†n b·ªô d·ªØ li·ªáu</label>
                <label><input type="radio" name="q23" value="b"> B) S·ª≠ d·ª•ng m·ªôt m·∫´u ng·∫´u nhi√™n</label>
                <label><input type="radio" name="q23" value="c"> C) C√≥ k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh</label>
                <label><input type="radio" name="q23" value="d"> D) Lu√¥n h·ªôi t·ª• nhanh h∆°n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) S·ª≠ d·ª•ng m·ªôt m·∫´u ng·∫´u nhi√™n</strong><br>
                Stochastic Gradient Descent s·ª≠ d·ª•ng m·ªôt m·∫´u (ho·∫∑c mini-batch) ng·∫´u nhi√™n thay v√¨ to√†n b·ªô dataset.
            </div>
        </div>

        <!-- C√¢u h·ªèi 24: Th·ª±c h√†nh -->
        <div class="question" id="q24" style="display: none;">
            <h3>C√¢u 24: Momentum trong gradient descent gi√∫p:</h3>
            <div class="options">
                <label><input type="radio" name="q24" value="a"> A) TƒÉng t·ªëc ƒë·ªô h·ªôi t·ª•</label>
                <label><input type="radio" name="q24" value="b"> B) V∆∞·ª£t qua local minima</label>
                <label><input type="radio" name="q24" value="c"> C) Gi·∫£m dao ƒë·ªông</label>
                <label><input type="radio" name="q24" value="d"> D) T·∫•t c·∫£ c√°c l√Ω do tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c l√Ω do tr√™n</strong><br>
                Momentum gi√∫p tƒÉng t·ªëc h·ªôi t·ª•, gi·∫£m dao ƒë·ªông v√† c√≥ th·ªÉ v∆∞·ª£t qua m·ªôt s·ªë local minima nh·ªè.
            </div>
        </div>

        <!-- C√¢u h·ªèi 25: Th·ª±c h√†nh -->
        <div class="question" id="q25" style="display: none;">
            <h3>C√¢u 25: V·ªõi h√†m $$f(x) = x^2$$, gradient descent t·ª´ $$x_0 = 4$$ v·ªõi $$t = 0.1$$ cho $$x_1 = ?$$</h3>
            <div class="options">
                <label><input type="radio" name="q25" value="a"> A) $$3.2$$</label>
                <label><input type="radio" name="q25" value="b"> B) $$4.8$$</label>
                <label><input type="radio" name="q25" value="c"> C) $$3.6$$</label>
                <label><input type="radio" name="q25" value="d"> D) $$2.4$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$3.2$$</strong><br>
                $$\nabla f(4) = 2 \cdot 4 = 8$$. $$x_1 = 4 - 0.1 \cdot 8 = 4 - 0.8 = 3.2$$.
            </div>
        </div>

    </div>

    <div id="quiz-navigation">
        <button id="prev-btn" onclick="previousQuestion()" disabled>C√¢u tr∆∞·ªõc</button>
        <button id="next-btn" onclick="nextQuestion()">C√¢u ti·∫øp</button>
        <button id="submit-btn" onclick="submitQuiz()" style="display: none;">N·ªôp b√†i</button>
    </div>

    <div id="quiz-results" style="display: none;">
        <h3>K·∫øt qu·∫£ b√†i t·∫≠p</h3>
        <div id="score-display"></div>
        <div id="detailed-results"></div>
        <button onclick="restartQuiz()">L√†m l·∫°i</button>
    </div>
</div>

<style>
#quiz-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

#quiz-header {
    text-align: center;
    margin-bottom: 30px;
}

#progress-bar {
    width: 100%;
    height: 10px;
    background-color: #e0e0e0;
    border-radius: 5px;
    margin: 20px 0;
    overflow: hidden;
}

#progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #2196F3, #00BCD4);
    width: 5%;
    transition: width 0.3s ease;
}

.question {
    background: #f9f9f9;
    border: 1px solid #ddd;
    border-radius: 8px;
    padding: 25px;
    margin-bottom: 20px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.question h3 {
    color: #333;
    margin-bottom: 20px;
    font-size: 1.1em;
    line-height: 1.4;
}

.options {
    display: flex;
    flex-direction: column;
    gap: 12px;
}

.options label {
    display: flex;
    align-items: flex-start;
    padding: 12px;
    background: white;
    border: 2px solid #e0e0e0;
    border-radius: 6px;
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 1em;
    line-height: 1.4;
}

.options label:hover {
    border-color: #2196F3;
    background-color: #e3f2fd;
}

.options input[type="radio"] {
    margin-right: 12px;
    margin-top: 2px;
    transform: scale(1.2);
    flex-shrink: 0;
}

.options label.selected {
    border-color: #2196F3;
    background-color: #bbdefb;
}

.explanation {
    margin-top: 20px;
    padding: 15px;
    background-color: #e3f2fd;
    border-left: 4px solid #2196F3;
    border-radius: 4px;
    font-size: 0.95em;
    line-height: 1.5;
}

.explanation strong {
    color: #1565C0;
}

#quiz-navigation {
    text-align: center;
    margin: 30px 0;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

#quiz-navigation button {
    background: #2196F3;
    color: white;
    border: none;
    padding: 12px 24px;
    border-radius: 6px;
    cursor: pointer;
    font-size: 1em;
    transition: background-color 0.2s ease;
}

#quiz-navigation button:hover:not(:disabled) {
    background: #1976D2;
}

#quiz-navigation button:disabled {
    background: #cccccc;
    cursor: not-allowed;
}

#quiz-results {
    text-align: center;
    padding: 30px;
    background: #e3f2fd;
    border-radius: 8px;
    border: 2px solid #2196F3;
}

#score-display {
    font-size: 1.5em;
    font-weight: bold;
    margin: 20px 0;
    color: #1565C0;
}

#detailed-results {
    text-align: left;
    margin: 20px 0;
    max-height: 400px;
    overflow-y: auto;
}

.result-item {
    padding: 10px;
    margin: 5px 0;
    border-radius: 4px;
    border-left: 4px solid;
}

.result-item.correct {
    background-color: #e8f5e8;
    border-left-color: #4CAF50;
}

.result-item.incorrect {
    background-color: #ffebee;
    border-left-color: #f44336;
}

@media (max-width: 600px) {
    #quiz-container {
        padding: 10px;
    }
    
    .question {
        padding: 15px;
    }
    
    #quiz-navigation {
        flex-direction: column;
        gap: 10px;
    }
    
    #quiz-navigation button {
        width: 100%;
    }
}
</style>

<script>
let currentQuestion = 1;
const totalQuestions = 25;
let userAnswers = {};
let quizSubmitted = false;

// ƒê√°p √°n ƒë√∫ng cho t·ª´ng c√¢u h·ªèi
const correctAnswers = {
    q1: 'b', q2: 'b', q3: 'b', q4: 'a', q5: 'b',
    q6: 'b', q7: 'b', q8: 'a', q9: 'b', q10: 'b',
    q11: 'b', q12: 'b', q13: 'b', q14: 'c', q15: 'a',
    q16: 'b', q17: 'd', q18: 'a', q19: 'c', q20: 'b'
,
    q21: 'b', q22: 'a', q23: 'b', q24: 'd', q25: 'a'};

/**
 * C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh v√† hi·ªÉn th·ªã c√¢u h·ªèi hi·ªán t·∫°i
 */
function updateProgress() {
    const progressFill = document.getElementById('progress-fill');
    const progressText = document.getElementById('progress-text');
    
    const percentage = (currentQuestion / totalQuestions) * 100;
    progressFill.style.width = percentage + '%';
    progressText.textContent = `C√¢u h·ªèi ${currentQuestion}/${totalQuestions}`;
}

/**
 * Hi·ªÉn th·ªã c√¢u h·ªèi theo s·ªë th·ª© t·ª±
 */
function showQuestion(questionNum) {
    // ·∫®n t·∫•t c·∫£ c√¢u h·ªèi
    for (let i = 1; i <= totalQuestions; i++) {
        document.getElementById(`q${i}`).style.display = 'none';
    }
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi hi·ªán t·∫°i
    document.getElementById(`q${questionNum}`).style.display = 'block';
    
    // C·∫≠p nh·∫≠t tr·∫°ng th√°i n√∫t ƒëi·ªÅu h∆∞·ªõng
    document.getElementById('prev-btn').disabled = (questionNum === 1);
    document.getElementById('next-btn').style.display = (questionNum === totalQuestions) ? 'none' : 'inline-block';
    document.getElementById('submit-btn').style.display = (questionNum === totalQuestions) ? 'inline-block' : 'none';
    
    updateProgress();
    
    // Re-render MathJax for the newly displayed question
    if (window.MathJax) {
        MathJax.typesetPromise([document.getElementById(`q${questionNum}`)]);
    }
}

/**
 * Chuy·ªÉn ƒë·∫øn c√¢u h·ªèi ti·∫øp theo
 */
function nextQuestion() {
    if (currentQuestion < totalQuestions) {
        currentQuestion++;
        showQuestion(currentQuestion);
    }
}

/**
 * Quay l·∫°i c√¢u h·ªèi tr∆∞·ªõc
 */
function previousQuestion() {
    if (currentQuestion > 1) {
        currentQuestion--;
        showQuestion(currentQuestion);
    }
}

/**
 * L∆∞u ƒë√°p √°n c·ªßa ng∆∞·ªùi d√πng
 */
function saveAnswer(questionId, answer) {
    userAnswers[questionId] = answer;
    
    // C·∫≠p nh·∫≠t giao di·ªán ƒë·ªÉ hi·ªÉn th·ªã ƒë√°p √°n ƒë√£ ch·ªçn
    const labels = document.querySelectorAll(`#${questionId} .options label`);
    labels.forEach(label => {
        label.classList.remove('selected');
        if (label.querySelector('input').value === answer) {
            label.classList.add('selected');
        }
    });
}

/**
 * N·ªôp b√†i v√† hi·ªÉn th·ªã k·∫øt qu·∫£
 */
function submitQuiz() {
    if (quizSubmitted) return;
    
    quizSubmitted = true;
    let correctCount = 0;
    let detailedResults = '';
    
    // T√≠nh ƒëi·ªÉm v√† t·∫°o b√°o c√°o chi ti·∫øt
    for (let i = 1; i <= totalQuestions; i++) {
        const questionId = `q${i}`;
        const userAnswer = userAnswers[questionId];
        const correctAnswer = correctAnswers[questionId];
        const isCorrect = userAnswer === correctAnswer;
        
        if (isCorrect) correctCount++;
        
        // Hi·ªÉn th·ªã gi·∫£i th√≠ch cho t·∫•t c·∫£ c√¢u h·ªèi
        const explanation = document.querySelector(`#${questionId} .explanation`);
        if (explanation) {
            explanation.style.display = 'block';
        }
        
        // T·∫°o b√°o c√°o chi ti·∫øt
        detailedResults += `
            <div class="result-item ${isCorrect ? 'correct' : 'incorrect'}">
                <strong>C√¢u ${i}:</strong> ${isCorrect ? 'ƒê√∫ng' : 'Sai'}
                ${!isCorrect ? `<br><small>ƒê√°p √°n c·ªßa b·∫°n: ${userAnswer || 'Ch∆∞a tr·∫£ l·ªùi'} | ƒê√°p √°n ƒë√∫ng: ${correctAnswer}</small>` : ''}
            </div>
        `;
    }
    
    // Hi·ªÉn th·ªã k·∫øt qu·∫£
    const scorePercentage = Math.round((correctCount / totalQuestions) * 100);
    document.getElementById('score-display').innerHTML = `
        <div>ƒêi·ªÉm s·ªë: ${correctCount}/${totalQuestions} (${scorePercentage}%)</div>
        <div style="margin-top: 10px; font-size: 0.9em;">
            ${scorePercentage >= 80 ? 'üéâ Xu·∫•t s·∫Øc! B·∫°n ƒë√£ n·∫Øm v·ªØng gradient descent v√† c√°c bi·∫øn th·ªÉ!' : 
              scorePercentage >= 60 ? 'üëç Kh√° t·ªët! H√£y √¥n l·∫°i m·ªôt s·ªë kh√°i ni·ªám v·ªÅ SGD v√† momentum.' : 
              scorePercentage >= 40 ? 'üìö C·∫ßn √¥n t·∫≠p th√™m v·ªÅ thu·∫≠t to√°n gradient descent.' : 'üí™ H√£y ƒë·ªçc l·∫°i l√Ω thuy·∫øt v·ªÅ gradient descent t·ª´ ƒë·∫ßu!'}
        </div>
    `;
    
    document.getElementById('detailed-results').innerHTML = detailedResults;
    document.getElementById('quiz-results').style.display = 'block';
    document.getElementById('quiz-navigation').style.display = 'none';
    
    // Cu·ªôn ƒë·∫øn k·∫øt qu·∫£
    document.getElementById('quiz-results').scrollIntoView({ behavior: 'smooth' });
}

/**
 * Kh·ªüi ƒë·ªông l·∫°i b√†i t·∫≠p
 */
function restartQuiz() {
    currentQuestion = 1;
    userAnswers = {};
    quizSubmitted = false;
    
    // ·∫®n k·∫øt qu·∫£ v√† hi·ªÉn th·ªã l·∫°i ƒëi·ªÅu h∆∞·ªõng
    document.getElementById('quiz-results').style.display = 'none';
    document.getElementById('quiz-navigation').style.display = 'flex';
    
    // ·∫®n t·∫•t c·∫£ gi·∫£i th√≠ch
    document.querySelectorAll('.explanation').forEach(exp => {
        exp.style.display = 'none';
    });
    
    // X√≥a t·∫•t c·∫£ l·ª±a ch·ªçn
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.checked = false;
    });
    
    document.querySelectorAll('.options label').forEach(label => {
        label.classList.remove('selected');
    });
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi ƒë·∫ßu ti√™n
    showQuestion(1);
    
    // Cu·ªôn l√™n ƒë·∫ßu
    document.getElementById('quiz-header').scrollIntoView({ behavior: 'smooth' });
}

// Kh·ªüi t·∫°o b√†i t·∫≠p khi trang ƒë∆∞·ª£c t·∫£i
document.addEventListener('DOMContentLoaded', function() {
    showQuestion(1);
    
    // Th√™m event listener cho t·∫•t c·∫£ radio button
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.addEventListener('change', function() {
            const questionId = this.name;
            const answer = this.value;
            saveAnswer(questionId, answer);
        });
    });
    
    // Render MathJax sau khi DOM ƒë∆∞·ª£c t·∫£i
    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});

// X·ª≠ l√Ω ph√≠m t·∫Øt
document.addEventListener('keydown', function(event) {
    if (quizSubmitted) return;
    
    switch(event.key) {
        case 'ArrowLeft':
            if (currentQuestion > 1) previousQuestion();
            break;
        case 'ArrowRight':
            if (currentQuestion < totalQuestions) nextQuestion();
            break;
        case 'Enter':
            if (currentQuestion === totalQuestions) submitQuiz();
            break;
    }
});
</script>
