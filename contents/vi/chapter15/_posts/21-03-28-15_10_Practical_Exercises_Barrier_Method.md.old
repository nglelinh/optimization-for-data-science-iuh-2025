---
layout: post
title: 15-10 B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p R√†o C·∫£n
chapter: '15'
order: 19
owner: GitHub Copilot
lang: vi
categories:
- chapter15
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p R√†o C·∫£n

## üìù **B√†i t·∫≠p 1: Barrier Method Implementation v√† Central Path**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Chapter 15)
Implement complete barrier method v·ªõi logarithmic barrier function:

a) **Basic barrier method** implementation
b) **Central path** computation v√† visualization
c) **Convergence analysis** v·ªõi duality gap
d) **Comparison** v·ªõi other constrained optimization methods

**Y√™u c·∫ßu:**
1. Complete barrier method implementation
2. Central path tracking
3. Duality gap monitoring
4. Performance analysis

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Barrier Method Framework**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import solve, LinAlgError
import cvxpy as cp
from mpl_toolkits.mplot3d import Axes3D
import time

class BarrierMethod:
    def __init__(self, tolerance=1e-8, max_outer_iterations=50, max_inner_iterations=100):
        self.tol = tolerance
        self.max_outer_iter = max_outer_iterations
        self.max_inner_iter = max_inner_iterations
        self.history = {
            'x': [],
            'f': [],
            'barrier_value': [],
            't_values': [],
            'duality_gap': [],
            'newton_iterations': [],
            'central_path': []
        }
    
    def solve(self, objective_func, gradient_func, hessian_func,
              inequality_constraints, inequality_gradients, inequality_hessians,
              equality_constraints=None, equality_jacobian=None,
              x0=None, t0=1.0, mu=10.0, verbose=True):
        """
        Solve constrained optimization problem using barrier method
        
        Problem: min f(x) s.t. h_i(x) ‚â§ 0, Ax = b
        
        Tham s·ªë:
        - objective_func: f(x) -> scalar
        - gradient_func: ‚àáf(x) -> vector
        - hessian_func: ‚àá¬≤f(x) -> matrix
        - inequality_constraints: [h_1(x), ..., h_m(x)]
        - inequality_gradients: [‚àáh_1(x), ..., ‚àáh_m(x)]
        - inequality_hessians: [‚àá¬≤h_1(x), ..., ‚àá¬≤h_m(x)]
        - equality_constraints: Ax - b (optional)
        - equality_jacobian: A (optional)
        - x0: initial point (must be strictly feasible)
        - t0: initial barrier parameter
        - mu: barrier parameter update factor
        """
        
        if x0 is None:
            raise ValueError("Must provide strictly feasible starting point")
        
        # Check initial feasibility
        if not self._is_strictly_feasible(x0, inequality_constraints):
            raise ValueError("Starting point is not strictly feasible")
        
        x = x0.copy()
        t = t0
        m = len(inequality_constraints)  # Number of inequality constraints
        
        if verbose:
            print("Barrier Method Optimization")
            print("=" * 40)
            print(f"Problem: {len(x)} variables, {m} inequality constraints")
            if equality_constraints is not None:
                print(f"         {len(equality_constraints(x))} equality constraints")
            print(f"Initial barrier parameter: t = {t}")
            print(f"Update factor: Œº = {mu}")
            print()
            print(f"{'Outer':<5} {'t':<8} {'f(x)':<12} {'Gap':<10} {'Newton':<7} {'Status'}")
            print("-" * 65)
        
        for outer_iter in range(self.max_outer_iter):
            # Solve barrier problem: min t*f(x) + œÜ(x) s.t. Ax = b
            x_new, newton_iters, converged = self._solve_barrier_problem(
                x, t, objective_func, gradient_func, hessian_func,
                inequality_constraints, inequality_gradients, inequality_hessians,
                equality_constraints, equality_jacobian
            )
            
            # Compute duality gap
            duality_gap = m / t
            
            # Store history
            f_val = objective_func(x_new)
            barrier_val = self._compute_barrier_value(x_new, inequality_constraints)
            
            self.history['x'].append(x_new.copy())
            self.history['f'].append(f_val)
            self.history['barrier_value'].append(barrier_val)
            self.history['t_values'].append(t)
            self.history['duality_gap'].append(duality_gap)
            self.history['newton_iterations'].append(newton_iters)
            self.history['central_path'].append(x_new.copy())
            
            status = "‚úì" if converged else "‚úó"
            
            if verbose:
                print(f"{outer_iter:<5} {t:<8.2f} {f_val:<12.6f} {duality_gap:<10.6f} "
                      f"{newton_iters:<7} {status}")
            
            # Check convergence
            if duality_gap < self.tol:
                if verbose:
                    print(f"\nConverged: duality gap {duality_gap:.2e} < {self.tol}")
                break
            
            # Update barrier parameter
            t *= mu
            x = x_new
        
        return x, f_val, outer_iter + 1
    
    def _is_strictly_feasible(self, x, inequality_constraints):
        """Check if point is strictly feasible"""
        for h in inequality_constraints:
            if h(x) >= 0:
                return False
        return True
    
    def _compute_barrier_value(self, x, inequality_constraints):
        """Compute logarithmic barrier function value"""
        barrier_val = 0.0
        for h in inequality_constraints:
            h_val = h(x)
            if h_val >= 0:
                return np.inf
            barrier_val -= np.log(-h_val)
        return barrier_val
    
    def _compute_barrier_gradient(self, x, inequality_constraints, inequality_gradients):
        """Compute barrier function gradient"""
        grad = np.zeros_like(x)
        for h, grad_h in zip(inequality_constraints, inequality_gradients):
            h_val = h(x)
            if h_val >= 0:
                return np.full_like(x, np.inf)
            grad -= grad_h(x) / h_val
        return grad
    
    def _compute_barrier_hessian(self, x, inequality_constraints, inequality_gradients, inequality_hessians):
        """Compute barrier function Hessian"""
        n = len(x)
        hess = np.zeros((n, n))
        
        for h, grad_h, hess_h in zip(inequality_constraints, inequality_gradients, inequality_hessians):
            h_val = h(x)
            if h_val >= 0:
                return np.full((n, n), np.inf)
            
            grad_h_val = grad_h(x)
            hess_h_val = hess_h(x)
            
            # Barrier Hessian: ‚àá¬≤œÜ = Œ£ (1/h_i¬≤)‚àáh_i‚àáh_i^T - Œ£ (1/h_i)‚àá¬≤h_i
            hess += np.outer(grad_h_val, grad_h_val) / (h_val**2) - hess_h_val / h_val
        
        return hess
    
    def _solve_barrier_problem(self, x0, t, objective_func, gradient_func, hessian_func,
                              inequality_constraints, inequality_gradients, inequality_hessians,
                              equality_constraints, equality_jacobian):
        """Solve single barrier problem using Newton method"""
        
        x = x0.copy()
        
        for newton_iter in range(self.max_inner_iter):
            # Compute barrier objective: t*f(x) + œÜ(x)
            f_val = objective_func(x)
            barrier_val = self._compute_barrier_value(x, inequality_constraints)
            
            if np.isinf(barrier_val):
                return x, newton_iter, False
            
            # Compute gradients
            grad_f = gradient_func(x)
            grad_barrier = self._compute_barrier_gradient(x, inequality_constraints, inequality_gradients)
            
            if np.any(np.isinf(grad_barrier)):
                return x, newton_iter, False
            
            total_grad = t * grad_f + grad_barrier
            
            # Handle equality constraints
            if equality_constraints is not None:
                A = equality_jacobian(x) if callable(equality_jacobian) else equality_jacobian
                eq_residual = equality_constraints(x)
                
                # KKT system: [H A^T; A 0] [dx; dŒΩ] = [-g; -r]
                hess_f = hessian_func(x)
                hess_barrier = self._compute_barrier_hessian(x, inequality_constraints, 
                                                           inequality_gradients, inequality_hessians)
                H = t * hess_f + hess_barrier
                
                # Solve KKT system
                n = len(x)
                m_eq = len(eq_residual)
                
                KKT_matrix = np.block([[H, A.T], [A, np.zeros((m_eq, m_eq))]])
                KKT_rhs = np.concatenate([-total_grad, -eq_residual])
                
                try:
                    KKT_solution = solve(KKT_matrix, KKT_rhs)
                    dx = KKT_solution[:n]
                    dnu = KKT_solution[n:]
                except LinAlgError:
                    return x, newton_iter, False
            else:
                # Unconstrained barrier problem
                hess_f = hessian_func(x)
                hess_barrier = self._compute_barrier_hessian(x, inequality_constraints,
                                                           inequality_gradients, inequality_hessians)
                H = t * hess_f + hess_barrier
                
                try:
                    dx = solve(H, -total_grad)
                except LinAlgError:
                    return x, newton_iter, False
            
            # Check Newton decrement
            newton_decrement_squared = -total_grad.T @ dx
            if newton_decrement_squared / 2 < self.tol:
                return x, newton_iter + 1, True
            
            # Line search to maintain feasibility
            step_size = self._feasible_line_search(x, dx, inequality_constraints, t, 
                                                 objective_func, total_grad)
            
            # Update
            x = x + step_size * dx
        
        return x, self.max_inner_iter, False
    
    def _feasible_line_search(self, x, dx, inequality_constraints, t, objective_func, grad, 
                             alpha=0.01, beta=0.5):
        """Backtracking line search maintaining strict feasibility"""
        
        # Find maximum step size maintaining feasibility
        step_max = 1.0
        for h in inequality_constraints:
            if np.dot(grad, dx) >= 0:  # Not a descent direction
                continue
                
            # Find step size such that h(x + s*dx) < 0
            for s in np.linspace(0.01, 1.0, 100):
                if h(x + s * dx) >= 0:
                    step_max = min(step_max, s * 0.99)
                    break
        
        # Backtracking line search
        step_size = min(1.0, step_max)
        
        # Compute current barrier objective
        f_current = objective_func(x)
        barrier_current = self._compute_barrier_value(x, inequality_constraints)
        current_obj = t * f_current + barrier_current
        
        while step_size > 1e-10:
            x_new = x + step_size * dx
            
            # Check feasibility
            if self._is_strictly_feasible(x_new, inequality_constraints):
                f_new = objective_func(x_new)
                barrier_new = self._compute_barrier_value(x_new, inequality_constraints)
                new_obj = t * f_new + barrier_new
                
                # Armijo condition
                if new_obj <= current_obj + alpha * step_size * grad.T @ dx:
                    break
            
            step_size *= beta
        
        return step_size
    
    def plot_convergence(self):
        """Plot convergence history"""
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        outer_iterations = range(len(self.history['f']))
        
        # Plot 1: Objective function
        axes[0, 0].semilogy(outer_iterations, self.history['f'], 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_xlabel('Outer Iteration')
        axes[0, 0].set_ylabel('f(x)')
        axes[0, 0].set_title('Objective Function')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Duality gap
        axes[0, 1].semilogy(outer_iterations, self.history['duality_gap'], 'r-o', linewidth=2, markersize=4)
        axes[0, 1].set_xlabel('Outer Iteration')
        axes[0, 1].set_ylabel('Duality Gap (m/t)')
        axes[0, 1].set_title('Duality Gap')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Barrier parameter
        axes[0, 2].semilogy(outer_iterations, self.history['t_values'], 'g-o', linewidth=2, markersize=4)
        axes[0, 2].set_xlabel('Outer Iteration')
        axes[0, 2].set_ylabel('Barrier Parameter t')
        axes[0, 2].set_title('Barrier Parameter Growth')
        axes[0, 2].grid(True, alpha=0.3)
        
        # Plot 4: Barrier function value
        axes[1, 0].semilogy(outer_iterations, self.history['barrier_value'], 'purple', 
                           linewidth=2, marker='s', markersize=4)
        axes[1, 0].set_xlabel('Outer Iteration')
        axes[1, 0].set_ylabel('œÜ(x)')
        axes[1, 0].set_title('Barrier Function Value')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 5: Newton iterations per outer iteration
        axes[1, 1].bar(outer_iterations, self.history['newton_iterations'], alpha=0.7, color='orange')
        axes[1, 1].set_xlabel('Outer Iteration')
        axes[1, 1].set_ylabel('Newton Iterations')
        axes[1, 1].set_title('Newton Iterations per Outer Step')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Plot 6: Central path (for 2D problems)
        if len(self.history['central_path']) > 0 and len(self.history['central_path'][0]) == 2:
            path = np.array(self.history['central_path'])
            axes[1, 2].plot(path[:, 0], path[:, 1], 'b-o', linewidth=2, markersize=6, 
                           markerfacecolor='red', markeredgecolor='blue')
            axes[1, 2].plot(path[0, 0], path[0, 1], 'go', markersize=10, label='Start')
            axes[1, 2].plot(path[-1, 0], path[-1, 1], 'ro', markersize=10, label='End')
            axes[1, 2].set_xlabel('x‚ÇÅ')
            axes[1, 2].set_ylabel('x‚ÇÇ')
            axes[1, 2].set_title('Central Path')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        else:
            axes[1, 2].text(0.5, 0.5, 'Central Path\n(2D visualization only)', 
                           ha='center', va='center', transform=axes[1, 2].transAxes)
        
        plt.tight_layout()
        plt.show()

# Example 1: Simple inequality constrained problem
def example_simple_barrier():
    """Example: Simple barrier method on inequality constrained problem"""
    
    print("Example 1: Simple Inequality Constrained Problem")
    print("=" * 55)
    print("Problem: min x‚ÇÅ¬≤ + x‚ÇÇ¬≤ s.t. -x‚ÇÅ - x‚ÇÇ + 1 ‚â§ 0, -x‚ÇÅ ‚â§ 0, -x‚ÇÇ ‚â§ 0")
    print("Equivalent: min x‚ÇÅ¬≤ + x‚ÇÇ¬≤ s.t. x‚ÇÅ + x‚ÇÇ ‚â• 1, x‚ÇÅ ‚â• 0, x‚ÇÇ ‚â• 0")
    
    # Define problem
    def objective(x):
        return x[0]**2 + x[1]**2
    
    def gradient(x):
        return np.array([2*x[0], 2*x[1]])
    
    def hessian(x):
        return np.array([[2, 0], [0, 2]])
    
    # Inequality constraints: h_i(x) ‚â§ 0
    def h1(x):  # -x‚ÇÅ - x‚ÇÇ + 1 ‚â§ 0  =>  x‚ÇÅ + x‚ÇÇ ‚â• 1
        return -x[0] - x[1] + 1
    
    def h2(x):  # -x‚ÇÅ ‚â§ 0  =>  x‚ÇÅ ‚â• 0
        return -x[0]
    
    def h3(x):  # -x‚ÇÇ ‚â§ 0  =>  x‚ÇÇ ‚â• 0
        return -x[1]
    
    # Gradients of constraints
    def grad_h1(x):
        return np.array([-1, -1])
    
    def grad_h2(x):
        return np.array([-1, 0])
    
    def grad_h3(x):
        return np.array([0, -1])
    
    # Hessians of constraints (all zero for linear constraints)
    def hess_h1(x):
        return np.zeros((2, 2))
    
    def hess_h2(x):
        return np.zeros((2, 2))
    
    def hess_h3(x):
        return np.zeros((2, 2))
    
    # Analytical solution: minimize x‚ÇÅ¬≤ + x‚ÇÇ¬≤ s.t. x‚ÇÅ + x‚ÇÇ = 1, x‚ÇÅ,x‚ÇÇ ‚â• 0
    # Solution: x‚ÇÅ = x‚ÇÇ = 0.5, f* = 0.5
    x_analytical = np.array([0.5, 0.5])
    f_analytical = objective(x_analytical)
    
    print(f"Analytical solution: x* = {x_analytical}, f* = {f_analytical}")
    
    # Solve using barrier method
    x0 = np.array([0.6, 0.6])  # Strictly feasible starting point
    
    barrier = BarrierMethod(tolerance=1e-6)
    x_barrier, f_barrier, iterations = barrier.solve(
        objective, gradient, hessian,
        [h1, h2, h3], [grad_h1, grad_h2, grad_h3], [hess_h1, hess_h2, hess_h3],
        x0=x0, t0=1.0, mu=10.0
    )
    
    print(f"\nBarrier Method Results:")
    print(f"Solution: x = {x_barrier}")
    print(f"Function value: f = {f_barrier:.8f}")
    print(f"Outer iterations: {iterations}")
    print(f"Error: ||x - x*|| = {np.linalg.norm(x_barrier - x_analytical):.6f}")
    
    # Visualize problem and solution
    plt.figure(figsize=(15, 5))
    
    # Plot 1: Feasible region and contours
    plt.subplot(1, 3, 1)
    
    x1 = np.linspace(-0.5, 2, 100)
    x2 = np.linspace(-0.5, 2, 100)
    X1, X2 = np.meshgrid(x1, x2)
    Z = X1**2 + X2**2
    
    # Plot objective contours
    plt.contour(X1, X2, Z, levels=20, alpha=0.6)
    
    # Plot constraints
    plt.fill_between(x1, np.maximum(0, 1 - x1), 2, alpha=0.3, color='lightblue', label='Feasible region')
    plt.plot(x1, 1 - x1, 'r-', linewidth=2, label='x‚ÇÅ + x‚ÇÇ = 1')
    plt.axhline(y=0, color='black', linewidth=1)
    plt.axvline(x=0, color='black', linewidth=1)
    
    # Plot solutions
    plt.plot(x_analytical[0], x_analytical[1], 'ro', markersize=10, label='Analytical')
    plt.plot(x_barrier[0], x_barrier[1], 'bo', markersize=10, label='Barrier method')
    plt.plot(x0[0], x0[1], 'go', markersize=8, label='Starting point')
    
    plt.xlim(-0.2, 1.5)
    plt.ylim(-0.2, 1.5)
    plt.xlabel('x‚ÇÅ')
    plt.ylabel('x‚ÇÇ')
    plt.title('Problem Visualization')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Central path
    plt.subplot(1, 3, 2)
    if len(barrier.history['central_path']) > 0:
        path = np.array(barrier.history['central_path'])
        plt.plot(path[:, 0], path[:, 1], 'b-o', linewidth=2, markersize=4)
        plt.plot(path[0, 0], path[0, 1], 'go', markersize=8, label='Start')
        plt.plot(path[-1, 0], path[-1, 1], 'ro', markersize=8, label='End')
        
        # Add constraint boundaries
        plt.plot(x1, 1 - x1, 'r--', alpha=0.5, label='x‚ÇÅ + x‚ÇÇ = 1')
        plt.axhline(y=0, color='black', alpha=0.5)
        plt.axvline(x=0, color='black', alpha=0.5)
        
        plt.xlabel('x‚ÇÅ')
        plt.ylabel('x‚ÇÇ')
        plt.title('Central Path')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # Plot 3: Convergence
    plt.subplot(1, 3, 3)
    outer_iters = range(len(barrier.history['duality_gap']))
    plt.semilogy(outer_iters, barrier.history['duality_gap'], 'r-o', linewidth=2, markersize=4)
    plt.xlabel('Outer Iteration')
    plt.ylabel('Duality Gap')
    plt.title('Convergence')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Plot detailed convergence
    barrier.plot_convergence()
    
    return barrier

# Run simple barrier example
barrier_simple = example_simple_barrier()
```

**B∆∞·ªõc 2: Linear Programming v·ªõi Barrier Method**

```python
def example_linear_programming_barrier():
    """Example: Linear programming with barrier method"""
    
    print("\n" + "="*60)
    print("Example 2: Linear Programming with Barrier Method")
    print("=" * 60)
    print("Problem: min c^T x s.t. Ax ‚â§ b, x ‚â• 0")
    print("Standard form: min c^T x s.t. Ax = b, x ‚â• 0")
    
    # Generate random LP problem
    np.random.seed(42)
    n, m = 5, 3  # 5 variables, 3 equality constraints
    
    A = np.random.randn(m, n)
    b = np.random.randn(m)
    c = np.random.randn(n)
    
    # Ensure feasible starting point exists
    x_temp = np.random.rand(n) + 0.1  # Ensure x > 0
    b = A @ x_temp  # Adjust b to ensure feasibility
    
    print(f"Problem size: {n} variables, {m} equality constraints")
    print(f"Objective: c = {c}")
    
    # Define LP problem
    def objective(x):
        return c.T @ x
    
    def gradient(x):
        return c
    
    def hessian(x):
        return np.zeros((n, n))
    
    # Inequality constraints: -x_i ‚â§ 0 (i.e., x_i ‚â• 0)
    inequality_constraints = []
    inequality_gradients = []
    inequality_hessians = []
    
    for i in range(n):
        def make_constraint(idx):
            return lambda x: -x[idx]
        
        def make_gradient(idx):
            grad = np.zeros(n)
            grad[idx] = -1
            return lambda x: grad
        
        def make_hessian(idx):
            return lambda x: np.zeros((n, n))
        
        inequality_constraints.append(make_constraint(i))
        inequality_gradients.append(make_gradient(i))
        inequality_hessians.append(make_hessian(i))
    
    # Equality constraints: Ax = b
    def equality_constraints(x):
        return A @ x - b
    
    def equality_jacobian(x):
        return A
    
    # Solve with CVXPY for comparison
    x_cvx = cp.Variable(n)
    prob = cp.Problem(cp.Minimize(c.T @ x_cvx), [A @ x_cvx == b, x_cvx >= 0])
    prob.solve()
    
    if prob.status == 'optimal':
        x_cvxpy = x_cvx.value
        f_cvxpy = prob.value
        print(f"CVXPY solution: x = {x_cvxpy}")
        print(f"CVXPY objective: f = {f_cvxpy:.6f}")
    else:
        print("CVXPY failed to solve")
        x_cvxpy = None
        f_cvxpy = None
    
    # Find strictly feasible starting point
    # Solve: find x s.t. Ax = b, x > Œµ
    epsilon = 0.1
    x_start = cp.Variable(n)
    feas_prob = cp.Problem(cp.Minimize(0), [A @ x_start == b, x_start >= epsilon])
    feas_prob.solve()
    
    if feas_prob.status == 'optimal':
        x0 = x_start.value
        print(f"Starting point: x0 = {x0}")
    else:
        print("Could not find strictly feasible starting point")
        return None
    
    # Solve using barrier method
    barrier = BarrierMethod(tolerance=1e-6, max_outer_iterations=30)
    
    try:
        x_barrier, f_barrier, iterations = barrier.solve(
            objective, gradient, hessian,
            inequality_constraints, inequality_gradients, inequality_hessians,
            equality_constraints, equality_jacobian,
            x0=x0, t0=1.0, mu=5.0
        )
        
        print(f"\nBarrier Method Results:")
        print(f"Solution: x = {x_barrier}")
        print(f"Function value: f = {f_barrier:.6f}")
        print(f"Outer iterations: {iterations}")
        
        if x_cvxpy is not None:
            print(f"Error vs CVXPY: ||x - x_cvxpy|| = {np.linalg.norm(x_barrier - x_cvxpy):.6f}")
            print(f"Objective difference: {abs(f_barrier - f_cvxpy):.8f}")
        
        # Check constraints
        print(f"\nConstraint satisfaction:")
        print(f"Equality: ||Ax - b|| = {np.linalg.norm(A @ x_barrier - b):.2e}")
        print(f"Inequality: min(x) = {np.min(x_barrier):.6f}")
        
        # Plot convergence
        barrier.plot_convergence()
        
        return barrier
        
    except Exception as e:
        print(f"Barrier method failed: {e}")
        return None

# Run LP barrier example
barrier_lp = example_linear_programming_barrier()
```

**B∆∞·ªõc 3: Quadratic Programming v·ªõi Barrier Method**

```python
def example_quadratic_programming_barrier():
    """Example: Quadratic programming with barrier method"""
    
    print("\n" + "="*60)
    print("Example 3: Quadratic Programming with Barrier Method")
    print("=" * 60)
    print("Problem: min (1/2)x^T Q x + p^T x s.t. Gx ‚â§ h, Ax = b")
    
    # Generate QP problem
    np.random.seed(42)
    n = 4  # Variables
    m_ineq = 6  # Inequality constraints
    m_eq = 2  # Equality constraints
    
    # Create positive definite Q
    Q_temp = np.random.randn(n, n)
    Q = Q_temp.T @ Q_temp + 0.1 * np.eye(n)
    p = np.random.randn(n)
    
    # Inequality constraints Gx ‚â§ h
    G = np.random.randn(m_ineq, n)
    
    # Equality constraints Ax = b
    A = np.random.randn(m_eq, n)
    
    # Generate feasible point and adjust h, b
    x_feas = np.random.randn(n)
    h = G @ x_feas + 0.5  # Ensure Gx < h
    b = A @ x_feas
    
    print(f"Problem size: {n} variables, {m_ineq} inequalities, {m_eq} equalities")
    print(f"Q condition number: {np.linalg.cond(Q):.2f}")
    
    # Define QP problem
    def objective(x):
        return 0.5 * x.T @ Q @ x + p.T @ x
    
    def gradient(x):
        return Q @ x + p
    
    def hessian(x):
        return Q
    
    # Inequality constraints: G_i^T x - h_i ‚â§ 0
    inequality_constraints = []
    inequality_gradients = []
    inequality_hessians = []
    
    for i in range(m_ineq):
        def make_constraint(idx):
            return lambda x: G[idx, :] @ x - h[idx]
        
        def make_gradient(idx):
            return lambda x: G[idx, :]
        
        def make_hessian(idx):
            return lambda x: np.zeros((n, n))
        
        inequality_constraints.append(make_constraint(i))
        inequality_gradients.append(make_gradient(i))
        inequality_hessians.append(make_hessian(i))
    
    # Equality constraints: Ax = b
    def equality_constraints(x):
        return A @ x - b
    
    def equality_jacobian(x):
        return A
    
    # Solve with CVXPY for comparison
    x_cvx = cp.Variable(n)
    qp_objective = cp.Minimize(0.5 * cp.quad_form(x_cvx, Q) + p.T @ x_cvx)
    qp_constraints = [G @ x_cvx <= h, A @ x_cvx == b]
    qp_prob = cp.Problem(qp_objective, qp_constraints)
    qp_prob.solve()
    
    if qp_prob.status == 'optimal':
        x_cvxpy = x_cvx.value
        f_cvxpy = qp_prob.value
        print(f"CVXPY solution: x = {x_cvxpy}")
        print(f"CVXPY objective: f = {f_cvxpy:.6f}")
    else:
        print("CVXPY failed to solve")
        return None
    
    # Find strictly feasible starting point
    x_start = cp.Variable(n)
    slack = 0.1
    feas_objective = cp.Minimize(cp.sum_squares(x_start))
    feas_constraints = [G @ x_start <= h - slack, A @ x_start == b]
    feas_prob = cp.Problem(feas_objective, feas_constraints)
    feas_prob.solve()
    
    if feas_prob.status == 'optimal':
        x0 = x_start.value
        print(f"Starting point found: x0 = {x0}")
        
        # Verify feasibility
        print(f"Inequality margins: min(h - Gx0) = {np.min(h - G @ x0):.4f}")
        print(f"Equality residual: ||Ax0 - b|| = {np.linalg.norm(A @ x0 - b):.2e}")
    else:
        print("Could not find strictly feasible starting point")
        return None
    
    # Solve using barrier method
    barrier = BarrierMethod(tolerance=1e-6, max_outer_iterations=25)
    
    try:
        x_barrier, f_barrier, iterations = barrier.solve(
            objective, gradient, hessian,
            inequality_constraints, inequality_gradients, inequality_hessians,
            equality_constraints, equality_jacobian,
            x0=x0, t0=1.0, mu=8.0
        )
        
        print(f"\nBarrier Method Results:")
        print(f"Solution: x = {x_barrier}")
        print(f"Function value: f = {f_barrier:.6f}")
        print(f"Outer iterations: {iterations}")
        print(f"Error vs CVXPY: ||x - x_cvxpy|| = {np.linalg.norm(x_barrier - x_cvxpy):.6f}")
        print(f"Objective difference: {abs(f_barrier - f_cvxpy):.8f}")
        
        # Check constraints
        print(f"\nConstraint satisfaction:")
        print(f"Inequality margins: min(h - Gx) = {np.min(h - G @ x_barrier):.6f}")
        print(f"Equality residual: ||Ax - b|| = {np.linalg.norm(A @ x_barrier - b):.2e}")
        
        # Analyze central path properties
        print(f"\nCentral Path Analysis:")
        if len(barrier.history['central_path']) > 1:
            path = np.array(barrier.history['central_path'])
            path_lengths = [np.linalg.norm(path[i+1] - path[i]) for i in range(len(path)-1)]
            print(f"Path length: {np.sum(path_lengths):.4f}")
            print(f"Average step size: {np.mean(path_lengths):.4f}")
        
        # Plot convergence
        barrier.plot_convergence()
        
        return barrier
        
    except Exception as e:
        print(f"Barrier method failed: {e}")
        return None

# Run QP barrier example
barrier_qp = example_quadratic_programming_barrier()
```

</details>

---

## üìù **B√†i t·∫≠p 2: Primal-Dual Interior Point Methods**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Advanced Topics)
Implement primal-dual interior point method:

a) **Primal-dual formulation** v√† KKT system
b) **Predictor-corrector** algorithm (Mehrotra)
c) **Homogeneous methods** cho infeasible problems
d) **Performance comparison** v·ªõi barrier method

**Y√™u c·∫ßu:**
1. Primal-dual algorithm implementation
2. Advanced predictor-corrector methods
3. Infeasibility handling
4. Comprehensive performance analysis

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üìù **B√†i t·∫≠p 3: Barrier Method Applications**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Applications)
Apply barrier method to practical optimization problems:

a) **Portfolio optimization** v·ªõi risk constraints
b) **Support Vector Machine** training
c) **Robust optimization** problems
d) **Engineering design** optimization

**Y√™u c·∫ßu:**
1. Problem-specific formulations
2. Constraint handling strategies
3. Scalability analysis
4. Real-world applications

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi implement barrier method:**
- Always start v·ªõi strictly feasible point
- Use appropriate barrier parameter update (Œº = 5-20)
- Monitor duality gap cho convergence
- Implement feasible line search

#### **Khi handle numerical issues:**
- Check constraint violations carefully
- Use regularization cho ill-conditioned Hessians
- Monitor barrier function values
- Implement safeguards cho boundary approach

#### **Khi choose parameters:**
- **Initial t**: Start small (t‚ÇÄ = 1-10)
- **Update factor Œº**: Moderate values (5-20)
- **Tolerance**: Balance accuracy vs computation
- **Max iterations**: Allow sufficient outer iterations

#### **Khi apply to specific problems:**
- Exploit problem structure (sparsity, separability)
- Use warm starts cho parameter changes
- Consider problem scaling
- Validate feasibility carefully

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 15: Barrier Method

2. **Wright, S. J.** (1997). *Primal-Dual Interior-Point Methods*. SIAM.

3. **Ye, Y.** (1997). *Interior Point Algorithms: Theory and Analysis*. Wiley.
