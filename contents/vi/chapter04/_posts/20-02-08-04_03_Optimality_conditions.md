---
layout: post
title: 04-03 Äiá»u kiá»‡n tá»‘i Æ°u
chapter: '04'
order: 4
owner: YoungJae Choung
categories:
- chapter04
lang: vi
lesson_type: required
---

## Äiá»u kiá»‡n tá»‘i Æ°u báº­c nháº¥t

Äá»ƒ tÃ¬m hiá»ƒu thÃªm vá» hÃ m lá»“i, xem [ChÆ°Æ¡ng 3: CÃ¡c TÃ­nh cháº¥t ChÃ­nh cá»§a HÃ m Lá»“i]({% multilang_post_url contents/chapter03/20-02-08-03_01_key_properties_of_convex_functions %}).

>$$
\begin{aligned}
&\min_x &&f(x) \\
&\text{subject to} &&x \in C
\end{aligned}
$$

Äá»‘i vá»›i má»™t bÃ i toÃ¡n lá»“i mÃ  hÃ m má»¥c tiÃªu $$f$$ **kháº£ vi**, Ä‘iá»u kiá»‡n sau lÃ  cáº§n vÃ  Ä‘á»§ cho má»™t Ä‘iá»ƒm tá»‘i Æ°u $$x$$:

> $$\nabla f(x)^{T}(y-x) \geq 0 \\
> \text{ vá»›i má»i } y \in C$$

ÄÃ¢y Ä‘Æ°á»£c gá»i lÃ  *Ä‘iá»u kiá»‡n báº­c nháº¥t cho tÃ­nh tá»‘i Æ°u*. 

$$\nabla f(x)^{T}(y-x) = 0$$ Ä‘á»‹nh nghÄ©a má»™t siÃªu pháº³ng Ä‘i qua $$x$$ trong táº­p $$C$$, vÃ  $$- \nabla f(x)$$ chá»‰ hÆ°á»›ng di chuyá»ƒn vá» phÃ­a Ä‘iá»ƒm tá»‘i Æ°u $$x$$. <br><br>

Náº¿u Ä‘iá»u kiá»‡n trÃªn Ä‘Æ°á»£c thá»a mÃ£n, 
táº­p $$C$$ náº±m trong ná»­a khÃ´ng gian Ä‘á»‘i diá»‡n vá»›i $$- \nabla f(x)$$, 
nÃªn $$x$$ lÃ  má»™t Ä‘iá»ƒm tá»‘i Æ°u.

<figure class="image" style="align: center;">
<p align="center">
  <img src="{{ site.baseurl }}/img/chapter_img/chapter04/first-order-condition.png" alt="[Fig1] geometric interpretation of first-order condition for optimality [3]">
  <figcaption style="text-align: center;">[HÃ¬nh 1] Giáº£i thÃ­ch hÃ¬nh há»c vá» Ä‘iá»u kiá»‡n báº­c nháº¥t cho tÃ­nh tá»‘i Æ°u [3]</figcaption>
</p>
</figure>
<br>

### TrÆ°á»ng há»£p Ä‘áº·c biá»‡t quan trá»ng
Khi $$C = \mathbb{R}^n$$ (tá»‘i Æ°u hÃ³a khÃ´ng rÃ ng buá»™c),
Ä‘iá»u kiá»‡n tá»‘i Æ°u lÃ :
> $$\nabla f(x) = 0$$

Trong trÆ°á»ng há»£p nÃ y, $$-\nabla f(x)$$ chá»‰ vá» phÃ­a Ä‘iá»ƒm tá»‘i Æ°u $$x$$, vÃ  $$\nabla f(x) = 0$$ cÃ³ nghÄ©a lÃ  khÃ´ng cÃ²n hÆ°á»›ng nÃ o Ä‘á»ƒ di chuyá»ƒn Ä‘á»ƒ giáº£m $$f$$ táº¡i $$x$$.

---


## Ná»n táº£ng ToÃ¡n há»c

Trong khi cÃ¡c Ä‘iá»u kiá»‡n báº­c nháº¥t sá» dá»¥ng gradient $$\nabla f(x)$$, cÃ¡c Ä‘iá»u kiá»‡n báº­c hai sá»­ dá»¥ng **ma tráº­n Hessian**:

$$H_f(x) = \nabla^2 f(x) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}$$

Ma tráº­n Hessian náº¯m báº¯t **Ä‘á»™ cong** cá»§a hÃ m táº¡i Ä‘iá»ƒm $$x$$, cung cáº¥p thÃ´ng tin vá» hÃ¬nh dáº¡ng cá»¥c bá»™ cá»§a hÃ m má»¥c tiÃªu.

## Äiá»u kiá»‡n Tá»‘i Æ°u Báº­c Hai

Xem xÃ©t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a khÃ´ng rÃ ng buá»™c:
$$\min_x f(x)$$

trong Ä‘Ã³ $$f: \mathbb{R}^n \to \mathbb{R}$$ kháº£ vi liÃªn tá»¥c báº­c hai.

### Äiá»u kiá»‡n Cáº§n (Báº­c Hai)

Náº¿u $$x^*$$ lÃ  cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng cá»§a $$f$$, thÃ¬:

1. **Äiá»u kiá»‡n cáº§n báº­c nháº¥t**: $$\nabla f(x^*) = 0$$
2. **Äiá»u kiá»‡n cáº§n báº­c hai**: $$\nabla^2 f(x^*) \succeq 0$$ (ná»­a xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng)

### Äiá»u kiá»‡n Äá»§ (Báº­c Hai)

Náº¿u táº¡i Ä‘iá»ƒm $$x^*$$:

1. $$\nabla f(x^*) = 0$$ (Ä‘iá»u kiá»‡n báº­c nháº¥t)
2. $$\nabla^2 f(x^*) \succ 0$$ (xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng)

ThÃ¬ $$x^*$$ lÃ  **cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng cháº·t** cá»§a $$f$$.

## Hiá»ƒu vá» TÃ­nh XÃ¡c Äá»‹nh DÆ°Æ¡ng

Má»™t ma tráº­n Ä‘á»‘i xá»©ng $$A$$ lÃ :
- **XÃ¡c Ä‘á»‹nh dÆ°Æ¡ng** ($$A \succ 0$$) náº¿u $$v^T A v > 0$$ vá»›i má»i $$v \neq 0$$
- **Ná»­a xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng** ($$A \succeq 0$$) náº¿u $$v^T A v \geq 0$$ vá»›i má»i $$v$$

**CÃ¡c kiá»ƒm tra thá»±c táº¿ cho tÃ­nh xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng:**
1. **Kiá»ƒm tra giÃ¡ trá»‹ riÃªng**: Táº¥t cáº£ giÃ¡ trá»‹ riÃªng Ä‘á»u dÆ°Æ¡ng
2. **Thá»© Ä‘á»‹nh thá»©c chÃ­nh**: Táº¥t cáº£ thá»© Ä‘á»‹nh thá»©c chÃ­nh Ä‘á»u dÆ°Æ¡ng
3. **PhÃ¢n tÃ­ch Cholesky**: $$A = L L^T$$ tá»“n táº¡i vá»›i $$L$$ tam giÃ¡c dÆ°á»›i

## Giáº£i thÃ­ch HÃ¬nh há»c

CÃ¡c Ä‘iá»u kiá»‡n báº­c hai cung cáº¥p thÃ´ng tin vá» **Ä‘á»™ cong** táº¡i Ä‘iá»ƒm tá»›i háº¡n:

- **$$\nabla^2 f(x^*) \succ 0$$**: HÃ m cong lÃªn trÃªn theo má»i hÆ°á»›ng â†’ **cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng cháº·t**
- **$$\nabla^2 f(x^*) \prec 0$$**: HÃ m cong xuá»‘ng dÆ°á»›i theo má»i hÆ°á»›ng â†’ **cá»±c Ä‘áº¡i Ä‘á»‹a phÆ°Æ¡ng cháº·t**  
- **$$\nabla^2 f(x^*) \succeq 0$$**: Äá»™ cong khÃ´ng Ã¢m â†’ **cÃ³ thá»ƒ lÃ  cá»±c tiá»ƒu**
- **Hessian báº¥t Ä‘á»‹nh**: Äá»™ cong há»—n há»£p â†’ **Ä‘iá»ƒm yÃªn ngá»±a**

<figure class="image" style="align: center;">
<p align="center">
 <img src="{{ site.baseurl }}/img/chapter_img/chapter04/second-order-conditions.png" alt="Second-order optimality conditions visualization" width="80%" height="80%">
 <figcaption style="text-align: center;">[HÃ¬nh 1] Giáº£i thÃ­ch hÃ¬nh há»c vá» cÃ¡c Ä‘iá»u kiá»‡n báº­c hai</figcaption>
</p>
</figure>

## CÃ¡c VÃ­ dá»¥ Chi tiáº¿t

### VÃ­ dá»¥ 1: HÃ m Báº­c Hai
Xem xÃ©t $$f(x_1, x_2) = x_1^2 + 2x_2^2 + x_1 x_2$$

**BÆ°á»›c 1: TÃ¬m Ä‘iá»ƒm tá»›i háº¡n**
$$\nabla f(x) = \begin{bmatrix} 2x_1 + x_2 \\ 4x_2 + x_1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Giáº£i: $$x^* = (0, 0)$$

**BÆ°á»›c 2: TÃ­nh Hessian**
$$\nabla^2 f(x) = \begin{bmatrix} 2 & 1 \\ 1 & 4 \end{bmatrix}$$

**BÆ°á»›c 3: Kiá»ƒm tra tÃ­nh xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng**
- GiÃ¡ trá»‹ riÃªng: $$\lambda_1 = \frac{6 + \sqrt{12}}{2} > 0$$, $$\lambda_2 = \frac{6 - \sqrt{12}}{2} > 0$$
- Thá»© Ä‘á»‹nh thá»©c chÃ­nh: $$2 > 0$$, $$\det = 8 - 1 = 7 > 0$$

**Káº¿t luáº­n**: $$\nabla^2 f(0,0) \succ 0$$ â†’ $$(0,0)$$ lÃ  cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng cháº·t.

### Example 2: Non-Convex Function
Consider $$f(x_1, x_2) = x_1^4 + x_2^4 - 2x_1^2 - 2x_2^2$$

**Step 1: Find critical points**
$$\nabla f(x) = \begin{bmatrix} 4x_1^3 - 4x_1 \\ 4x_2^3 - 4x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Critical points: $$(0,0)$$, $$(\pm 1, 0)$$, $$(0, \pm 1)$$, $$(\pm 1, \pm 1)$$

**Step 2: Analyze $$(0,0)$$**
$$\nabla^2 f(0,0) = \begin{bmatrix} -4 & 0 \\ 0 & -4 \end{bmatrix} \prec 0$$

**Conclusion**: $$(0,0)$$ is a strict local maximum.

**Step 3: Analyze $$(1,1)$$**
$$\nabla^2 f(1,1) = \begin{bmatrix} 8 & 0 \\ 0 & 8 \end{bmatrix} \succ 0$$

**Conclusion**: $$(1,1)$$ is a strict local minimum.

### Example 3: Saddle Point
Consider $$f(x_1, x_2) = x_1^2 - x_2^2$$

**Analysis at $$(0,0)$$:**
- $$\nabla f(0,0) = (0,0)$$ âœ“
- $$\nabla^2 f(0,0) = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}$$ (indefinite)

**Conclusion**: $$(0,0)$$ is a saddle point (neither minimum nor maximum).

## Constrained Optimization: Second-Order Conditions

For constrained problems:
$$\min_x f(x) \text{ subject to } h_i(x) = 0, \, i = 1,\ldots,m$$

The **bordered Hessian** of the Lagrangian is used:
$$\mathcal{L}(x,\lambda) = f(x) + \sum_{i=1}^m \lambda_i h_i(x)$$

**Second-order sufficient condition**: The bordered Hessian has the correct inertia (number of negative eigenvalues equals the number of constraints).

## Comparison: First vs Second-Order Conditions

| Aspect | First-Order | Second-Order |
|--------|-------------|--------------|
| **Information** | Gradient (slope) | Hessian (curvature) |
| **Necessary condition** | $$\nabla f(x^*) = 0$$ | $$\nabla f(x^*) = 0$$ and $$\nabla^2 f(x^*) \succeq 0$$ |
| **Sufficient condition** | Not available for unconstrained | $$\nabla f(x^*) = 0$$ and $$\nabla^2 f(x^*) \succ 0$$ |
| **Strength** | Weaker | Stronger |
| **Computational cost** | $$O(n)$$ | $$O(n^2)$$ |
| **Distinguishes** | Critical points | Minima, maxima, saddle points |

## Interactive Visualization

Explore how second-order conditions work in practice:

<div style="text-align: center; margin: 20px 0;">
    <a href="../second_order_conditions_interactive.html" target="_blank" 
       style="display: inline-block; padding: 12px 24px; background-color: #e74c3c; color: white; text-decoration: none; border-radius: 6px; font-weight: bold; box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
        ğŸ¯ Khá»Ÿi Ä‘á»™ng CÃ´ng cá»¥ KhÃ¡m phÃ¡ Äiá»u kiá»‡n Báº­c Hai
    </a>
</div>

CÃ´ng cá»¥ tÆ°Æ¡ng tÃ¡c minh há»a:
- **PhÃ¢n tÃ­ch giÃ¡ trá»‹ riÃªng Hessian** cho cÃ¡c loáº¡i hÃ m khÃ¡c nhau
- **PhÃ¢n loáº¡i trá»±c quan** cÃ¡c Ä‘iá»ƒm tá»›i háº¡n (cá»±c tiá»ƒu, cá»±c Ä‘áº¡i, yÃªn ngá»±a)
- **Äá»“ thá»‹ Ä‘Æ°á»ng Ä‘áº³ng má»©c** hiá»ƒn thá»‹ hÃ nh vi Ä‘á»™ cong cá»¥c bá»™
- **TÃ­nh toÃ¡n tá»«ng bÆ°á»›c** cho cÃ¡c kiá»ƒm tra báº­c hai

## TÃ³m táº¯t vÃ  CÃ¡c Äiá»ƒm ChÃ­nh

CÃ¡c Ä‘iá»u kiá»‡n tá»‘i Æ°u báº­c hai cung cáº¥p **Ä‘áº·c trÆ°ng máº¡nh hÆ¡n** cho cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u:

### **Káº¿t quáº£ ChÃ­nh:**
1. **Äiá»u kiá»‡n cáº§n**: $$\nabla f(x^*) = 0$$ vÃ  $$\nabla^2 f(x^*) \succeq 0$$
2. **Äiá»u kiá»‡n Ä‘á»§**: $$\nabla f(x^*) = 0$$ vÃ  $$\nabla^2 f(x^*) \succ 0$$
3. **Kháº£ nÄƒng phÃ¢n loáº¡i**: CÃ³ thá»ƒ phÃ¢n biá»‡t giá»¯a cá»±c tiá»ƒu, cá»±c Ä‘áº¡i vÃ  Ä‘iá»ƒm yÃªn ngá»±a

### **Táº§m quan trá»ng Thá»±c táº¿:**
- **Thiáº¿t káº¿ thuáº­t toÃ¡n**: Ná»n táº£ng cho cÃ¡c phÆ°Æ¡ng phÃ¡p kiá»ƒu Newton
- **PhÃ¢n tÃ­ch tÃ­nh lá»“i**: Thiáº¿t yáº¿u cho viá»‡c xÃ¡c minh hÃ m lá»“i
- **TÃ­nh bá»n vá»¯ng**: Äáº£m báº£o máº¡nh hÆ¡n so vá»›i chá»‰ cÃ¡c Ä‘iá»u kiá»‡n báº­c nháº¥t
- **LÃ½ thuyáº¿t tá»‘i Æ°u hÃ³a**: Cáº§u ná»‘i giá»¯a tÃ­nh tá»‘i Æ°u Ä‘á»‹a phÆ°Æ¡ng vÃ  toÃ n cá»¥c

### **CÃ¢n nháº¯c TÃ­nh toÃ¡n:**
- **Chi phÃ­**: LÆ°u trá»¯ vÃ  tÃ­nh toÃ¡n $$O(n^2)$$ cho Hessian
- **Xáº¥p xá»‰**: CÃ¡c phÆ°Æ¡ng phÃ¡p Quasi-Newton giáº£m gÃ¡nh náº·ng tÃ­nh toÃ¡n
- **á»”n Ä‘á»‹nh sá»‘**: TÃ­nh toÃ¡n giÃ¡ trá»‹ riÃªng yÃªu cáº§u triá»ƒn khai cáº©n tháº­n

Hiá»ƒu vá» cÃ¡c Ä‘iá»u kiá»‡n báº­c hai lÃ  thiáº¿t yáº¿u cho lÃ½ thuyáº¿t tá»‘i Æ°u hÃ³a nÃ¢ng cao vÃ  phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n sá»‘ hiá»‡u quáº£. ChÃºng cung cáº¥p ná»n táº£ng toÃ¡n há»c cho nhiá»u phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a hiá»‡n Ä‘áº¡i vÃ  Ä‘Æ°a ra cÃ¡c hiá»ƒu biáº¿t sÃ¢u sáº¯c hÆ¡n vá» cáº¥u trÃºc cá»§a cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a.
