---
layout: post
title: 1-6 B√†i T·∫≠p Th·ª±c H√†nh - Gi·ªõi Thi·ªáu T·ªëi ∆Øu H√≥a
chapter: '1'
order: 7
owner: GitHub Copilot
lang: vi
categories:
- chapter01
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Gi·ªõi Thi·ªáu T·ªëi ∆Øu H√≥a

## üìù **B√†i t·∫≠p 1: Ph√¢n lo·∫°i b√†i to√°n t·ªëi ∆∞u h√≥a c∆° b·∫£n**

**ƒê·ªÅ b√†i:** (B√†i 1.1 t·ª´ Boyd & Vandenberghe)
Ph√¢n lo·∫°i c√°c b√†i to√°n sau ƒë√¢y th√†nh: LP (Linear Programming), QP (Quadratic Programming), QCQP (Quadratically Constrained QP), SOCP (Second-Order Cone Programming), SDP (Semidefinite Programming), hay GP (Geometric Programming):

a) $$\min_{x \in \mathbb{R}^n} c^T x \quad \text{s.t.} \quad Ax \leq b$$

b) $$\min_{x \in \mathbb{R}^n} x^T P x + q^T x \quad \text{s.t.} \quad Ax = b, \quad x \geq 0$$

c) $$\min_{x \in \mathbb{R}^n} \lVert Ax - b \rVert_2 \quad \text{s.t.} \quad \lVert x \rVert_2 \leq 1$$

**Y√™u c·∫ßu:**
1. X√°c ƒë·ªãnh lo·∫°i b√†i to√°n cho m·ªói tr∆∞·ªùng h·ª£p
2. Gi·∫£i th√≠ch l√Ω do ph√¢n lo·∫°i
3. Ch·ªâ ra ƒëi·ªÅu ki·ªán ƒë·ªÉ b√†i to√°n l√† convex
4. ƒê·ªÅ xu·∫•t ph∆∞∆°ng ph√°p gi·∫£i ph√π h·ª£p

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Ph√¢n lo·∫°i**

**a) Quy ho·∫°ch tuy·∫øn t√≠nh (LP)**
- H√†m m·ª•c ti√™u: tuy·∫øn t√≠nh ($$c^T x$$)
- R√†ng bu·ªôc: b·∫•t ƒë·∫≥ng th·ª©c tuy·∫øn t√≠nh ($$Ax \leq b$$)
- ƒê√¢y l√† d·∫°ng chu·∫©n c·ªßa LP

**b) Quy ho·∫°ch b·∫≠c hai (QP)**
- H√†m m·ª•c ti√™u: b·∫≠c hai ($$x^T P x + q^T x$$)
- R√†ng bu·ªôc: tuy·∫øn t√≠nh ($$Ax = b, x \geq 0$$)
- L·ªìi khi $$P \succeq 0$$

**c) Quy ho·∫°ch n√≥n b·∫≠c hai (SOCP)**
- H√†m m·ª•c ti√™u: chu·∫©n ($$\lVert Ax - b \rVert_2$$)
- R√†ng bu·ªôc: r√†ng bu·ªôc chu·∫©n ($$\lVert x \rVert_2 \leq 1$$)
- C√≥ th·ªÉ bi·∫øn ƒë·ªïi th√†nh d·∫°ng chu·∫©n SOCP

**B∆∞·ªõc 2: ƒêi·ªÅu ki·ªán t√≠nh l·ªìi**
- **a)** Lu√¥n l·ªìi (h√†m tuy·∫øn t√≠nh)
- **b)** L·ªìi khi $$P \succeq 0$$ (n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng)
- **c)** Lu√¥n l·ªìi (h√†m chu·∫©n v√† r√†ng bu·ªôc chu·∫©n)

**B∆∞·ªõc 3: Ph∆∞∆°ng ph√°p gi·∫£i**
- **a)** Ph∆∞∆°ng ph√°p ƒë∆°n h√¨nh (Simplex), ph∆∞∆°ng ph√°p ƒëi·ªÉm trong
- **b)** Ph∆∞∆°ng ph√°p t·∫≠p ho·∫°t ƒë·ªông (Active set), ph∆∞∆°ng ph√°p ƒëi·ªÉm trong
- **c)** Ph∆∞∆°ng ph√°p ƒëi·ªÉm trong cho SOCP

</details>

---

## üìù **B√†i t·∫≠p 2: Least squares v√† variants**

**ƒê·ªÅ b√†i:** (B√†i 1.2 t·ª´ Boyd & Vandenberghe)
X√©t c√°c b√†i to√°n least squares sau:

a) **Ordinary least squares:** $$\min_x \lVert Ax - b \rVert_2^2$$

b) **Weighted least squares:** $$\min_x (Ax - b)^T W (Ax - b)$$ v·ªõi $$W \succ 0$$

c) **Regularized least squares:** $$\min_x \lVert Ax - b \rVert_2^2 + \lambda \lVert x \rVert_2^2$$

d) **Robust least squares:** $$\min_x \sum_{i=1}^m \phi((Ax - b)_i)$$ v·ªõi $$\phi(u) = \begin{cases} u^2 & |u| \leq M \\ 2M|u| - M^2 & |u| > M \end{cases}$$

**Y√™u c·∫ßu:**
1. T√¨m nghi·ªám analytical cho c√°c tr∆∞·ªùng h·ª£p a, b, c
2. Ch·ª©ng minh t√≠nh convex c·ªßa t·∫•t c·∫£ c√°c b√†i to√°n
3. Ph√¢n t√≠ch √Ω nghƒ©a geometric c·ªßa regularization
4. So s√°nh robustness c·ªßa c√°c methods

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Nghi·ªám analytical**

**a) Ordinary least squares:**
$$x^* = (A^T A)^{-1} A^T b$$ (n·∫øu $$A^T A$$ invertible)

**b) Weighted least squares:**
$$x^* = (A^T W A)^{-1} A^T W b$$

**c) Regularized least squares (Ridge regression):**
$$x^* = (A^T A + \lambda I)^{-1} A^T b$$

**d) Robust least squares:**
Kh√¥ng c√≥ closed form, c·∫ßn numerical methods.

**B∆∞·ªõc 2: Ch·ª©ng minh convexity**

**a-c)** Quadratic functions v·ªõi positive definite Hessian
**d)** $$\phi(u)$$ l√† convex function (piecewise quadratic v·ªõi non-decreasing derivative)

**B∆∞·ªõc 3: √ù nghƒ©a h√¨nh h·ªçc**
- **Regularization:** Co l·∫°i v·ªÅ g·ªëc t·ªça ƒë·ªô, ngƒÉn ng·ª´a overfitting
- **$$\lambda$$** ki·ªÉm so√°t s·ª± c√¢n b·∫±ng gi·ªØa ƒë·ªô ch·ªách v√† ph∆∞∆°ng sai
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi ti√™n nghi·ªám Bayesian $$x \sim \mathcal{N}(0, \sigma^2/\lambda I)$$

**B∆∞·ªõc 4: So s√°nh t√≠nh b·ªÅn v·ªØng**
- **OLS:** Nh·∫°y c·∫£m v·ªõi ngo·∫°i lai (h√†m ph·∫°t b·∫≠c hai)
- **C√≥ tr·ªçng s·ªë:** C√≥ th·ªÉ gi·∫£m tr·ªçng s·ªë cho ngo·∫°i lai n·∫øu ch·ªçn $$W$$ ph√π h·ª£p  
- **C√≥ regularization:** Gi·∫£m overfitting, c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a
- **B·ªÅn v·ªØng:** H√†m m·∫•t m√°t Huber gi·∫£m ƒë·ªô nh·∫°y v·ªõi ngo·∫°i lai

</details>

---

## üìù **B√†i t·∫≠p 3: Linear programming trong standard form**

**ƒê·ªÅ b√†i:** (B√†i 1.3 t·ª´ Boyd & Vandenberghe)
Chuy·ªÉn ƒë·ªïi c√°c b√†i to√°n sau v·ªÅ LP standard form:

a) $$\min_{x,y} 2x + 3y \quad \text{s.t.} \quad x + 2y \geq 5, \quad x - y = 2, \quad x \geq 0, \quad y \text{ free}$$

b) $$\max_x c^T x \quad \text{s.t.} \quad Ax \leq b, \quad x \geq 0$$

c) $$\min_x \lVert c^T x - d \rVert_\infty \quad \text{s.t.} \quad Ax \leq b$$

**Y√™u c·∫ßu:**
1. Chuy·ªÉn v·ªÅ d·∫°ng $$\min c^T z \text{ s.t. } Az = b, z \geq 0$$
2. X√°c ƒë·ªãnh ma tr·∫≠n $$A$$, vector $$b, c$$ m·ªõi
3. Gi·∫£i th√≠ch c√°c ph√©p bi·∫øn ƒë·ªïi s·ª≠ d·ª•ng
4. Verify t∆∞∆°ng ƒë∆∞∆°ng c·ªßa b√†i to√°n g·ªëc v√† transformed

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Chuy·ªÉn ƒë·ªïi b√†i to√°n a)**

**X·ª≠ l√Ω bi·∫øn t·ª± do:** $$y = y^+ - y^-$$ v·ªõi $$y^+, y^- \geq 0$$

**X·ª≠ l√Ω b·∫•t ƒë·∫≥ng th·ª©c:** $$x + 2y \geq 5 \Rightarrow x + 2y - s = 5$$ v·ªõi $$s \geq 0$$

**D·∫°ng chu·∫©n:**
$$\min_{x,y^+,y^-,s} 2x + 3y^+ - 3y^- \quad \text{s.t.} \quad \begin{bmatrix} 1 & 2 & -2 & -1 \\ 1 & -1 & 1 & 0 \end{bmatrix} \begin{bmatrix} x \\ y^+ \\ y^- \\ s \end{bmatrix} = \begin{bmatrix} 5 \\ 2 \end{bmatrix}$$

v·ªõi $$x, y^+, y^-, s \geq 0$$.

**B∆∞·ªõc 2: Chuy·ªÉn ƒë·ªïi b√†i to√°n b)**

**Chuy·ªÉn max th√†nh min:** $$\max c^T x = -\min (-c^T x)$$

**Chuy·ªÉn b·∫•t ƒë·∫≥ng th·ª©c th√†nh ƒë·∫≥ng th·ª©c:** $$Ax \leq b \Rightarrow Ax + s = b$$ v·ªõi $$s \geq 0$$

**D·∫°ng chu·∫©n:**
$$\min_{x,s} (-c^T x + 0^T s) \quad \text{s.t.} \quad [A \quad I] \begin{bmatrix} x \\ s \end{bmatrix} = b, \quad x,s \geq 0$$

**B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi b√†i to√°n c)**

**Bi·∫øn ƒë·ªïi chu·∫©n $$\ell_\infty$$:** $$\min t \text{ s.t. } -t \leq c^T x - d \leq t$$

**D·∫°ng chu·∫©n:**
$$\min_{x,t,s_1,s_2} t \quad \text{s.t.} \quad \begin{bmatrix} A & 0 & I & 0 \\ c^T & -1 & 0 & I \\ -c^T & -1 & 0 & 0 \end{bmatrix} \begin{bmatrix} x \\ t \\ s_1 \\ s_2 \end{bmatrix} = \begin{bmatrix} b \\ d \\ -d \end{bmatrix}$$

</details>

---

## üìù **B√†i t·∫≠p 4: Quadratic programming applications**

**ƒê·ªÅ b√†i:** (B√†i 1.4 t·ª´ Boyd & Vandenberghe)
X√©t b√†i to√°n portfolio optimization:

$$\min_x \frac{1}{2} x^T \Sigma x \quad \text{s.t.} \quad \mu^T x \geq r, \quad \mathbf{1}^T x = 1, \quad x \geq 0$$

v·ªõi $$\Sigma \succ 0$$ (covariance matrix), $$\mu$$ (expected returns), $$r$$ (target return).

**Y√™u c·∫ßu:**
1. Ch·ª©ng minh ƒë√¢y l√† convex QP
2. Vi·∫øt ƒëi·ªÅu ki·ªán KKT
3. Ph√¢n t√≠ch tr∆∞·ªùng h·ª£p kh√¥ng c√≥ constraint $$x \geq 0$$
4. T√¨m efficient frontier
5. Implement numerical solution

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Ch·ª©ng minh convexity**
- Objective: $$f(x) = \frac{1}{2} x^T \Sigma x$$ v·ªõi $$\nabla^2 f = \Sigma \succ 0$$
- Constraints: t·∫•t c·∫£ ƒë·ªÅu linear
- $$\Rightarrow$$ Convex QP

**B∆∞·ªõc 2: KKT conditions**
Lagrangian: $$L(x,\lambda,\nu,\mu) = \frac{1}{2} x^T \Sigma x - \lambda(\mu^T x - r) + \nu(\mathbf{1}^T x - 1) - \mu^T x$$

KKT conditions:
- Stationarity: $$\Sigma x - \lambda \mu + \nu \mathbf{1} - \mu = 0$$
- Primal feasibility: $$\mu^T x \geq r, \mathbf{1}^T x = 1, x \geq 0$$
- Dual feasibility: $$\lambda \geq 0, \mu \geq 0$$
- Complementary slackness: $$\lambda(\mu^T x - r) = 0, \mu_i x_i = 0$$

**B∆∞·ªõc 3: Tr∆∞·ªùng h·ª£p unconstrained ($$x \geq 0$$)**
$$x^* = \Sigma^{-1}(\lambda^* \mu - \nu^* \mathbf{1})$$

T·ª´ constraints:
- $$\mathbf{1}^T x^* = 1 \Rightarrow \mathbf{1}^T \Sigma^{-1}(\lambda^* \mu - \nu^* \mathbf{1}) = 1$$
- $$\mu^T x^* = r \Rightarrow \mu^T \Sigma^{-1}(\lambda^* \mu - \nu^* \mathbf{1}) = r$$

Gi·∫£i h·ªá:
$$\begin{bmatrix} A & B \\ B & C \end{bmatrix} \begin{bmatrix} \lambda^* \\ -\nu^* \end{bmatrix} = \begin{bmatrix} r \\ 1 \end{bmatrix}$$

v·ªõi $$A = \mu^T \Sigma^{-1} \mu, B = \mu^T \Sigma^{-1} \mathbf{1}, C = \mathbf{1}^T \Sigma^{-1} \mathbf{1}$$

**B∆∞·ªõc 4: Bi√™n hi·ªáu qu·∫£**
$$\sigma^2(r) = \frac{Ar^2 - 2Br + C}{AC - B^2}$$

ƒê√¢y l√† ƒë∆∞·ªùng hyperbola trong kh√¥ng gian $$(r, \sigma)$$.

</details>

---

## üìù **B√†i t·∫≠p 5: Geometric programming**

**ƒê·ªÅ b√†i:** (B√†i 1.5 t·ª´ Boyd & Vandenberghe)
X√©t b√†i to√°n thi·∫øt k·∫ø m·∫°ch ƒëi·ªán:

$$\min_{x,y,z} \frac{1}{xyz} \quad \text{s.t.} \quad \frac{x}{y} + \frac{y}{z} \leq 1, \quad x,y,z > 0$$

**Y√™u c·∫ßu:**
1. Ch·ª©ng minh ƒë√¢y l√† geometric program
2. Chuy·ªÉn v·ªÅ convex form b·∫±ng log transformation
3. Solve b√†i to√°n convex
4. Interpret nghi·ªám trong context thi·∫øt k·∫ø m·∫°ch

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Verify geometric program**
- Objective: $$f_0(x,y,z) = x^{-1} y^{-1} z^{-1}$$ (monomial)
- Constraint: $$g_1(x,y,z) = x^1 y^{-1} z^0 + x^0 y^1 z^{-1}$$ (posynomial)
- Domain: $$x,y,z > 0$$

$$\Rightarrow$$ ƒê√¢y l√† GP trong standard form.

**B∆∞·ªõc 2: Bi·∫øn ƒë·ªïi logarit**
ƒê·∫∑t $$u = \log x, v = \log y, w = \log z$$:

$$\min_{u,v,w} -u - v - w \quad \text{s.t.} \quad \log(e^{u-v} + e^{v-w}) \leq 0$$

T∆∞∆°ng ƒë∆∞∆°ng:
$$\min_{u,v,w} -u - v - w \quad \text{s.t.} \quad e^{u-v} + e^{v-w} \leq 1$$

**B∆∞·ªõc 3: C√¥ng th·ª©c l·ªìi**
$$\min_{u,v,w,t} -u - v - w \quad \text{s.t.} \quad e^{u-v} + e^{v-w} \leq t, \quad t \leq 1$$

ƒê√¢y l√† b√†i to√°n l·ªìi (r√†ng bu·ªôc log-sum-exp).

**B∆∞·ªõc 4: Ph∆∞∆°ng ph√°p gi·∫£i**
S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ƒëi·ªÉm trong ho·∫∑c c√°c b·ªô gi·∫£i GP chuy√™n d·ª•ng.

Nghi·ªám t·ªëi ∆∞u s·∫Ω c√≥ d·∫°ng:
- $$x^* = e^{u^*}, y^* = e^{v^*}, z^* = e^{w^*}$$
- R√†ng bu·ªôc th∆∞·ªùng ho·∫°t ƒë·ªông: $$\frac{x^*}{y^*} + \frac{y^*}{z^*} = 1$$

</details>

---

## üìù **B√†i t·∫≠p 6: Semidefinite programming basics**

**ƒê·ªÅ b√†i:** (B√†i 1.6 t·ª´ Boyd & Vandenberghe)
X√©t b√†i to√°n:

$$\min_X \text{tr}(CX) \quad \text{s.t.} \quad \text{tr}(A_i X) = b_i, \quad i = 1,\ldots,m, \quad X \succeq 0$$

**Y√™u c·∫ßu:**
1. Ch·ª©ng minh ƒë√¢y l√† convex optimization problem
2. Vi·∫øt dual problem
3. Ph√¢n t√≠ch ƒëi·ªÅu ki·ªán strong duality
4. V√≠ d·ª• c·ª• th·ªÉ: Maximum cut relaxation

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Ch·ª©ng minh convexity**
- Objective: $$\text{tr}(CX)$$ linear trong $$X$$
- Constraints: $$\text{tr}(A_i X) = b_i$$ linear
- Constraint: $$X \succeq 0$$ convex (positive semidefinite cone)

$$\Rightarrow$$ Convex SDP

**B∆∞·ªõc 2: Dual problem**
Lagrangian: $$L(X,y,Z) = \text{tr}(CX) + \sum_{i=1}^m y_i (b_i - \text{tr}(A_i X)) - \text{tr}(ZX)$$

Dual function: $$g(y,Z) = \inf_X L(X,y,Z)$$

ƒêi·ªÅu ki·ªán: $$C - \sum_{i=1}^m y_i A_i - Z = 0$$

**Dual problem:**
$$\max_{y,Z} \sum_{i=1}^m b_i y_i \quad \text{s.t.} \quad \sum_{i=1}^m y_i A_i + Z = C, \quad Z \succeq 0$$

**B∆∞·ªõc 3: Strong duality**
Strong duality holds n·∫øu Slater condition th·ªèa m√£n:
$$\exists X \succ 0 \text{ s.t. } \text{tr}(A_i X) = b_i, \forall i$$

**B∆∞·ªõc 4: Maximum cut example**
Cho graph $$G = (V,E)$$ v·ªõi weights $$w_{ij}$$:

$$\max_x \sum_{(i,j) \in E} w_{ij} \frac{1 - x_i x_j}{2} \quad \text{s.t.} \quad x_i \in \{-1,1\}$$

**SDP relaxation:**
$$\max_X \sum_{(i,j) \in E} w_{ij} \frac{1 - X_{ij}}{2} \quad \text{s.t.} \quad X_{ii} = 1, \quad X \succeq 0$$

</details>

---

## üìù **B√†i t·∫≠p 7: Robust optimization introduction**

**ƒê·ªÅ b√†i:** (B√†i 1.7 t·ª´ Boyd & Vandenberghe)
X√©t robust linear program:

$$\min_x c^T x \quad \text{s.t.} \quad a_i^T x \leq b_i \quad \forall a_i \in \mathcal{U}_i, \quad i = 1,\ldots,m$$

v·ªõi uncertainty sets:
a) Box: $$\mathcal{U}_i = \{a_i^{nom} + u : \lVert u \rVert_\infty \leq \rho_i\}$$
b) Ellipsoidal: $$\mathcal{U}_i = \{a_i^{nom} + P_i u : \lVert u \rVert_2 \leq 1\}$$

**Y√™u c·∫ßu:**
1. Reformulate th√†nh deterministic convex programs
2. So s√°nh computational complexity
3. Ph√¢n t√≠ch conservatism level
4. Implement v√† compare solutions

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Box uncertainty**
$$\max_{a_i \in \mathcal{U}_i} a_i^T x = (a_i^{nom})^T x + \rho_i \lVert x \rVert_1$$

**Deterministic equivalent:**
$$\min_x c^T x \quad \text{s.t.} \quad (a_i^{nom})^T x + \rho_i \lVert x \rVert_1 \leq b_i$$

**LP reformulation:**
$$\min_{x,t_i} c^T x \quad \text{s.t.} \quad (a_i^{nom})^T x + \rho_i \sum_j t_{ij} \leq b_i, \quad -t_{ij} \leq x_j \leq t_{ij}$$

**B∆∞·ªõc 2: Ellipsoidal uncertainty**
$$\max_{a_i \in \mathcal{U}_i} a_i^T x = (a_i^{nom})^T x + \lVert P_i^T x \rVert_2$$

**Deterministic equivalent:**
$$\min_x c^T x \quad \text{s.t.} \quad (a_i^{nom})^T x + \lVert P_i^T x \rVert_2 \leq b_i$$

ƒê√¢y l√† Second-Order Cone Program (SOCP).

**B∆∞·ªõc 3: So s√°nh ƒë·ªô ph·ª©c t·∫°p**
- **Box:** LP v·ªõi $$O(mn)$$ bi·∫øn v√† r√†ng bu·ªôc
- **Ellipsoidal:** SOCP v·ªõi $$n + m$$ bi·∫øn v√† $$m$$ r√†ng bu·ªôc SOC
- **Nominal:** LP v·ªõi $$n$$ bi·∫øn v√† $$m$$ r√†ng bu·ªôc

**B∆∞·ªõc 4: Ph√¢n t√≠ch t√≠nh b·∫£o th·ªß**
- **Box:** B·∫£o th·ªß nh·∫•t (tr∆∞·ªùng h·ª£p x·∫•u nh·∫•t theo m·ªçi h∆∞·ªõng)
- **Ellipsoidal:** B·∫£o th·ªß v·ª´a ph·∫£i (b·∫•t ƒë·ªãnh theo h∆∞·ªõng)
- **Nominal:** √çt b·∫£o th·ªß nh·∫•t (kh√¥ng c√≥ b·∫•t ƒë·ªãnh)

Gi√° c·ªßa t√≠nh b·ªÅn v·ªØng = $$\frac{\text{Gi√° tr·ªã t·ªëi ∆∞u b·ªÅn v·ªØng}}{\text{Gi√° tr·ªã t·ªëi ∆∞u danh nghƒ©a}} - 1$$

</details>

---

## üìù **B√†i t·∫≠p 8: Convexity verification**

**ƒê·ªÅ b√†i:** (B√†i 1.8 t·ª´ Boyd & Vandenberghe)
X√°c ƒë·ªãnh t√≠nh convex c·ªßa c√°c functions sau:

a) $$f(x) = x \log x$$ tr√™n $$\mathbb{R}_{++}$$
b) $$f(x) = -\log(\det X)$$ tr√™n $$\mathbb{S}_{++}^n$$
c) $$f(x) = \lVert x \rVert_p$$ v·ªõi $$p \geq 1$$
d) $$f(X) = \lambda_{\max}(X)$$ tr√™n $$\mathbb{S}^n$$

**Y√™u c·∫ßu:**
1. Ch·ª©ng minh/ph·∫£n ch·ª©ng t√≠nh convex
2. T√≠nh second-order conditions khi applicable
3. Verify b·∫±ng definition ho·∫∑c composition rules
4. T√¨m domain convex c·ªßa m·ªói function

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: $$f(x) = x \log x$$**
**Domain:** $$\mathbb{R}_{++}$$ (convex)

**Second derivative:** $$f''(x) = \frac{1}{x} > 0$$ for $$x > 0$$

$$\Rightarrow$$ Convex tr√™n $$\mathbb{R}_{++}$$

**B∆∞·ªõc 2: $$f(X) = -\log(\det X)$$**
**Domain:** $$\mathbb{S}_{++}^n$$ (convex)

**Hessian:** C√≥ th·ªÉ ch·ª©ng minh $$\nabla^2 f \succ 0$$

Alternative: $$-\log(\det X) = -\log(\prod_i \lambda_i) = -\sum_i \log \lambda_i$$

V√¨ $$-\log$$ convex v√† eigenvalues l√† concave functions c·ªßa $$X$$, c·∫ßn careful analysis.

**Result:** Convex tr√™n $$\mathbb{S}_{++}^n$$

**B∆∞·ªõc 3: $$f(x) = \lVert x \rVert_p$$**
**Domain:** $$\mathbb{R}^n$$ (convex)

**For $$p \geq 1$$:** Triangle inequality $$\Rightarrow$$ convex
**For $$0 < p < 1$$:** Not convex (subadditive but not convex)

**Verification:** Minkowski inequality for $$p \geq 1$$

**B∆∞·ªõc 4: $$f(X) = \lambda_{\max}(X)$$**
**Domain:** $$\mathbb{S}^n$$ (convex)

**Convexity:** $$\lambda_{\max}(X) = \max_{\lVert v \rVert = 1} v^T X v$$

Maximum c·ªßa linear functions $$\Rightarrow$$ convex

**Alternative:** Spectral norm property

</details>

---

## üìù **B√†i t·∫≠p 9: Optimization modeling**

**ƒê·ªÅ b√†i:** (B√†i 1.9 t·ª´ Boyd & Vandenberghe)
Formulate c√°c b√†i to√°n th·ª±c t·∫ø sau th√†nh convex optimization:

a) **Antenna array design:** Minimize sidelobe level
b) **Filter design:** Minimize passband ripple v√† stopband energy  
c) **Truss design:** Minimize weight subject to stiffness constraints
d) **Data fitting:** Robust regression v·ªõi outliers

**Y√™u c·∫ßu:**
1. Mathematical formulation cho m·ªói problem
2. Identify variables, objective, constraints
3. Verify convexity
4. Suggest solution approaches

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Antenna array design**
**Variables:** $$w \in \mathbb{C}^n$$ (array weights)

**Objective:** Minimize maximum sidelobe level
$$\min_{w,t} t \quad \text{s.t.} \quad |a(\theta)^H w| \leq t, \quad \forall \theta \in \Theta_{sidelobe}$$

v·ªõi $$a(\theta)$$ l√† steering vector, $$\Theta_{sidelobe}$$ l√† sidelobe region.

**Main beam constraint:** $$a(\theta_0)^H w = 1$$ (normalization)

**Convex:** SOCP (second-order cone constraints)

**B∆∞·ªõc 2: Filter design**
**Variables:** $$h \in \mathbb{R}^n$$ (filter coefficients)

**Passband ripple:** $$|H(\omega) - 1| \leq \delta_p, \quad \omega \in \Omega_p$$
**Stopband attenuation:** $$|H(\omega)| \leq \delta_s, \quad \omega \in \Omega_s$$

v·ªõi $$H(\omega) = \sum_{k=0}^{n-1} h_k e^{-j\omega k}$$

**Convex formulation:**
$$\min_{h,\delta_p,\delta_s} \alpha \delta_p + \beta \delta_s$$
subject to frequency response constraints.

**B∆∞·ªõc 3: Truss design**
**Variables:** $$A \in \mathbb{R}^m$$ (cross-sectional areas)

**Objective:** $$\min \sum_{i=1}^m \rho_i l_i A_i$$ (total weight)

**Stiffness constraint:** $$u^T K(A) u \geq u_{min}^T K_0 u_{min}$$

v·ªõi $$K(A) = \sum_{i=1}^m A_i K_i$$ (stiffness matrix)

**Convex:** Linear objective, linear constraint trong $$A$$

**B∆∞·ªõc 4: Robust regression**
**Variables:** $$\beta \in \mathbb{R}^p$$ (regression coefficients)

**Huber loss:** $$\min_\beta \sum_{i=1}^n \phi(y_i - x_i^T \beta)$$

v·ªõi $$\phi(u) = \begin{cases} \frac{1}{2}u^2 & |u| \leq M \\ M|u| - \frac{1}{2}M^2 & |u| > M \end{cases}$$

**Convex:** Huber function l√† convex

</details>

---

## üìù **B√†i t·∫≠p 10: Duality introduction**

**ƒê·ªÅ b√†i:** (B√†i 1.10 t·ª´ Boyd & Vandenberghe)
Cho b√†i to√°n LP:
$$\min_x c^T x \quad \text{s.t.} \quad Ax = b, \quad x \geq 0$$

**Y√™u c·∫ßu:**
1. Derive dual problem b·∫±ng Lagrangian approach
2. Ch·ª©ng minh weak duality
3. T√¨m ƒëi·ªÅu ki·ªán strong duality
4. Interpret dual variables economically
5. Verify complementary slackness

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Lagrangian v√† dual**
$$L(x,\nu,\lambda) = c^T x + \nu^T(Ax - b) - \lambda^T x$$

**Dual function:**
$$g(\nu,\lambda) = \inf_x L(x,\nu,\lambda) = \inf_x (c + A^T \nu - \lambda)^T x - \nu^T b$$

**ƒêi·ªÅu ki·ªán finite:** $$c + A^T \nu - \lambda = 0$$, i.e., $$\lambda = A^T \nu + c$$

**Dual problem:**
$$\max_{\nu,\lambda} g(\nu,\lambda) = \max_\nu -\nu^T b \quad \text{s.t.} \quad A^T \nu + c \geq 0$$

T∆∞∆°ng ƒë∆∞∆°ng:
$$\max_\nu b^T \nu \quad \text{s.t.} \quad A^T \nu \leq c$$

**B∆∞·ªõc 2: Weak duality**
V·ªõi m·ªçi primal feasible $$x$$ v√† dual feasible $$\nu$$:
$$c^T x \geq b^T \nu$$

**Proof:** 
$$c^T x \geq (A^T \nu)^T x = \nu^T Ax = \nu^T b = b^T \nu$$

**B∆∞·ªõc 3: Strong duality**
Strong duality holds cho LP khi c·∫£ primal v√† dual ƒë·ªÅu feasible (fundamental theorem of LP).

**B∆∞·ªõc 4: Economic interpretation**
- $$\nu_i$$: shadow price c·ªßa resource $$i$$
- $$\nu_i$$ = marginal value c·ªßa constraint $$i$$
- N·∫øu $$b_i$$ tƒÉng 1 unit, optimal value tƒÉng $$\nu_i$$

**B∆∞·ªõc 5: Complementary slackness**
- N·∫øu $$x_j^* > 0$$, th√¨ $$(A^T \nu^*)_j = c_j$$ (reduced cost = 0)
- N·∫øu $$(A^T \nu^*)_j < c_j$$, th√¨ $$x_j^* = 0$$

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi ph√¢n lo·∫°i b√†i to√°n t·ªëi ∆∞u h√≥a:**
- Ki·ªÉm tra d·∫°ng h√†m m·ª•c ti√™u (tuy·∫øn t√≠nh, b·∫≠c hai, t·ªïng qu√°t)
- X√°c ƒë·ªãnh lo·∫°i r√†ng bu·ªôc (tuy·∫øn t√≠nh, b·∫≠c hai, n√≥n)
- X√°c minh t√≠nh l·ªìi c·ªßa t·ª´ng th√†nh ph·∫ßn

#### **Khi chuy·ªÉn ƒë·ªïi v·ªÅ d·∫°ng chu·∫©n:**
- X·ª≠ l√Ω bi·∫øn t·ª± do: $$x = x^+ - x^-$$
- Chuy·ªÉn b·∫•t ƒë·∫≥ng th·ª©c: th√™m bi·∫øn slack
- Chuy·ªÉn max th√†nh min: ƒë·ªïi d·∫•u h√†m m·ª•c ti√™u

#### **Khi x√°c minh t√≠nh l·ªìi:**
- S·ª≠ d·ª•ng ƒëi·ªÅu ki·ªán b·∫≠c hai khi c√≥ th·ªÉ
- √Åp d·ª•ng quy t·∫Øc k·∫øt h·ª£p v√† c√°c ph√©p to√°n b·∫£o to√†n t√≠nh l·ªìi
- Ki·ªÉm tra ƒë·ªãnh nghƒ©a tr·ª±c ti·∫øp n·∫øu c·∫ßn

---

## üí° **T·ªïng k·∫øt**

### **Ph√¢n lo·∫°i B√†i to√°n T·ªëi ∆∞u:**

| Lo·∫°i | H√†m m·ª•c ti√™u | R√†ng bu·ªôc | V√≠ d·ª• |
|------|--------------|-----------|-------|
| **LP** | Tuy·∫øn t√≠nh | Tuy·∫øn t√≠nh | $$\min c^Tx$$, $$Ax \leq b$$ |
| **QP** | B·∫≠c hai | Tuy·∫øn t√≠nh | $$\min \frac{1}{2}x^TQx + c^Tx$$ |
| **QCQP** | B·∫≠c hai | B·∫≠c hai | Portfolio v·ªõi risk constraints |
| **SOCP** | Tuy·∫øn t√≠nh | Chu·∫©n $$\ell_2$$ | $$\|\|Ax-b\|\|_2 \leq c^Tx$$ |
| **SDP** | Tuy·∫øn t√≠nh | Ma tr·∫≠n PSD | $$\min \text{tr}(CX)$$, $$X \succeq 0$$ |

### **Least Squares Variants:**

| Method | Objective | ƒê·∫∑c ƒëi·ªÉm |
|--------|-----------|----------|
| Ordinary LS | $$\|\|Ax-b\|\|_2^2$$ | Nghi·ªám closed-form |
| Weighted LS | $$(Ax-b)^TW(Ax-b)$$ | Weights cho observations |
| Regularized LS | $$\|\|Ax-b\|\|_2^2 + \lambda\|\|x\|\|_2^2$$ | Ridge regression |
| Robust LS | $$\sum \phi((Ax-b)_i)$$ | Resistant to outliers |
| LASSO | $$\|\|Ax-b\|\|_2^2 + \lambda\|\|x\|\|_1$$ | Sparse solutions |

### **Duality Concepts:**

- **Weak duality:** $$d^* \leq p^*$$ (lu√¥n ƒë√∫ng)
- **Strong duality:** $$d^* = p^*$$ (c·∫ßn ƒëi·ªÅu ki·ªán, e.g., Slater)
- **Shadow price:** $$\nu_i^*$$ = marginal value c·ªßa constraint $$i$$
- **Complementary slackness:** $$\lambda_i^* (g_i(x^*)) = 0$$

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 1: Introduction
   - Exercises 1.1-1.15

2. **Nocedal, J., & Wright, S. J.** (2006). *Numerical Optimization*. Springer.

3. **Bertsekas, D. P.** (1999). *Nonlinear Programming*. Athena Scientific.
