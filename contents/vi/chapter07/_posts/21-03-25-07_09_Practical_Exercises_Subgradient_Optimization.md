---
layout: post
title: 07-9 B√†i T·∫≠p Th·ª±c H√†nh - T·ªëi ∆∞u Subgradient
chapter: '07'
order: 10
owner: GitHub Copilot
lang: vi
categories:
- chapter07
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - T·ªëi ∆∞u Subgradient (Subgradient Optimization)

C√°c b√†i t·∫≠p ƒë∆∞·ª£c tham kh·∫£o t·ª´ Boyd & Vandenberghe (2004) v√† Shor (1985).

---

## üìù **Ph·∫ßn I: T√≠nh Subgradient**

### **B√†i t·∫≠p 1: Subgradient c·ªßa H√†m Kh√¥ng Kh·∫£ Vi**

T√≠nh subgradient c·ªßa c√°c h√†m sau t·∫°i ƒëi·ªÉm cho tr∆∞·ªõc:

**a)** $$f(x) = |x|$$ t·∫°i $$x = 0$$

**b)** $$f(x) = \max\{x_1, x_2\}$$ t·∫°i $$x = (1, 1)^T$$

**c)** $$f(x) = \|x\|_1$$ t·∫°i $$x = (0, 0)^T$$

**d)** $$f(x) = \max\{x_1 + x_2, 2x_1 - x_2\}$$ t·∫°i $$x = (1, 0)^T$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) $$f(x) = |x|$$ t·∫°i $$x = 0$$:**

**ƒê·ªãnh nghƒ©a:** $$g \in \partial f(0)$$ n·∫øu 

$$f(y) \geq f(0) + g(y - 0)$$ $$\forall y$$

$$|y| \geq 0 + gy \quad \forall y$$

- V·ªõi $$y > 0$$: $$y \geq gy$$ ‚Üí $$g \leq 1$$
- V·ªõi $$y < 0$$: $$-y \geq gy$$ ‚Üí $$g \geq -1$$

$$\partial f(0) = [-1, 1]$$

**Gi·∫£i th√≠ch h√¨nh h·ªçc:** T·∫≠p h·ª£p t·∫•t c·∫£ slopes c·ªßa c√°c ƒë∆∞·ªùng th·∫≥ng support t·∫°i $$x = 0$$.

---

**b) $$f(x) = \max\{x_1, x_2\}$$ t·∫°i $$x = (1, 1)$$:**

$$f(1, 1) = 1$$ (c·∫£ hai active)

**Active set:** $$I = \{1, 2\}$$

$$\partial f(x) = \text{conv}\{\nabla f_i(x) : i \in I\}$$

$$= \text{conv}\{(1, 0), (0, 1)\}$$

$$= \{(\lambda, 1-\lambda) : 0 \leq \lambda \leq 1\}$$

**H√¨nh h·ªçc:** ƒêo·∫°n th·∫≥ng n·ªëi $$(1, 0)$$ v√† $$(0, 1)$$.

---

**c) $$f(x) = \|x\|_1 = |x_1| + |x_2|$$ t·∫°i $$x = (0, 0)$$:**

$$\partial f(0) = \partial |x_1| \times \partial |x_2|$$

$$= [-1, 1] \times [-1, 1]$$

$$= \{(g_1, g_2) : |g_1| \leq 1, |g_2| \leq 1\}$$

**H√¨nh h·ªçc:** H√¨nh vu√¥ng $$[-1, 1]^2$$.

---

**d) $$f(x) = \max\{x_1 + x_2, 2x_1 - x_2\}$$ t·∫°i $$x = (1, 0)$$:**

$$f_1(1, 0) = 1 + 0 = 1$$

$$f_2(1, 0) = 2 - 0 = 2$$

$$\Rightarrow f(1, 0) = 2$$ (ch·ªâ $$f_2$$ active)

$$\nabla f_2 = (2, -1)$$

$$\partial f(1, 0) = \{(2, -1)\}$$ (singleton v√¨ ch·ªâ 1 active)

</details>

---

## üìù **Ph·∫ßn II: Subgradient Method**

### **B√†i t·∫≠p 2: Convergence Analysis**

Cho $$f(x) = |x - 2| + |x + 1|$$.

**Y√™u c·∫ßu:**  
a) T√¨m $$x^*$$ (minimizer)  
b) V·ªõi step size $$\alpha_k = \frac{1}{\sqrt{k}}$$, iterate t·ª´ $$x^{(0)} = 5$$  
c) T√≠nh $$x^{(1)}, x^{(2)}, x^{(3)}$$  
d) ∆Ø·ªõc t√≠nh convergence rate

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) T√¨m $$x^*$$:**

$$f(x) = |x - 2| + |x + 1|$$

**Case analysis:**
- $$x < -1$$: $$f(x) = -(x-2) - (x+1) = -2x + 1$$ (gi·∫£m)
- $$-1 \leq x \leq 2$$: $$f(x) = -(x-2) + (x+1) = 3$$ (constant!)
- $$x > 2$$: $$f(x) = (x-2) + (x+1) = 2x - 1$$ (tƒÉng)

$$f^* = 3$$, $$x^* \in [-1, 2]$$

---

**b) Subgradient method:**

$$x^{(k+1)} = x^{(k)} - \alpha_k g^{(k)}$$

v·ªõi $$g^{(k)} \in \partial f(x^{(k)})$$.

---

**c) Iterations:**

$$x^{(0)} = 5 > 2$$

$$\partial f(5) = \{\text{sign}(5-2)\} + \{\text{sign}(5+1)\} = \{1\} + \{1\} = \{2\}$$

$$g^{(0)} = 2$$

$$\alpha_0 = \frac{1}{\sqrt{1}} = 1$$

$$x^{(1)} = 5 - 1 \cdot 2 = 3$$

---

$$x^{(1)} = 3 > 2$$

$$g^{(1)} = 2$$

$$\alpha_1 = \frac{1}{\sqrt{2}} \approx 0.707$$

$$x^{(2)} = 3 - 0.707 \cdot 2 = 1.586$$

---

$$x^{(2)} = 1.586 \in [-1, 2]$$ (optimal region!)

$$\partial f(1.586) = [-1, 1]$$ (ch·ªçn $$g^{(2)} = 0$$ ho·∫∑c b·∫•t k·ª≥)

N·∫øu ch·ªçn $$g^{(2)} = 0$$: $$x^{(3)} = 1.586$$ (ƒë√£ h·ªôi t·ª•)

N·∫øu ch·ªçn $$g^{(2)} = -1$$:

$$\alpha_2 = \frac{1}{\sqrt{3}} \approx 0.577$$

$$x^{(3)} = 1.586 + 0.577 = 2.163$$

(Oscillates around optimal set)

---

**d) Convergence rate:**

With $$\alpha_k = \frac{1}{\sqrt{k}}$$:

$$f(x_{\text{best}}^{(k)}) - f^* \leq \frac{R^2 + G^2 \sum_{i=1}^k \alpha_i^2}{2\sum_{i=1}^k \alpha_i}$$

$$\sum_{i=1}^k \frac{1}{\sqrt{i}} \approx 2\sqrt{k}$$

$$\sum_{i=1}^k \frac{1}{i} \approx \log k$$

$$\Rightarrow O\left(\frac{\log k}{\sqrt{k}}\right) = O\left(\frac{1}{\sqrt{k}}\right)$$

**Sublinear convergence** (slower than gradient descent!)

</details>

---

## üìù **Ph·∫ßn III: Step Size Rules**

### **B√†i t·∫≠p 3: So s√°nh Step Size Strategies**

Cho $$f(x) = \max\{x_1, x_2, -x_1 - x_2 + 1\}$$.

**Y√™u c·∫ßu:**  
a) T√¨m $$x^*$$ graphically  
b) Compare 3 step size rules:
   - Constant: $$\alpha_k = 0.1$$
   - Diminishing: $$\alpha_k = \frac{1}{k}$$
   - Square summable: $$\alpha_k = \frac{1}{k^{0.6}}$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Graphical solution:**

$$f(x) = \max\{x_1, x_2, 1 - x_1 - x_2\}$$

**Intersection points:**
- $$x_1 = x_2$$: Line $$x_1 = x_2$$
- $$x_1 = 1 - x_1 - x_2$$: $$2x_1 + x_2 = 1$$
- $$x_2 = 1 - x_1 - x_2$$: $$x_1 + 2x_2 = 1$$

Gi·∫£i h·ªá $$x_1 = x_2$$ v√† $$2x_1 + x_2 = 1$$:

$$3x_1 = 1 \Rightarrow x_1 = \frac{1}{3}$$

$$x^* = \left(\frac{1}{3}, \frac{1}{3}\right)$$, $$f^* = \frac{1}{3}$$

---

**b) Step size comparison:**

**T·ª´ $$x^{(0)} = (1, 1)$$:**

$$f_1 = 1$$, $$f_2 = 1$$, $$f_3 = -1$$ ‚Üí max l√† $$f_1, f_2$$ (tie)

$$\partial f(1, 1) = \text{conv}\{(1, 0), (0, 1)\}$$

Ch·ªçn $$g = (0.5, 0.5)$$ (midpoint)

---

**Constant $$\alpha_k = 0.1$$:**

$$x^{(1)} = (1, 1) - 0.1(0.5, 0.5) = (0.95, 0.95)$$

$$x^{(2)} = (0.90, 0.90)$$

$$x^{(3)} = (0.85, 0.85)$$

...

**H·ªôi t·ª• tuy·∫øn t√≠nh ƒë·∫øn l√¢n c·∫≠n $$x^*$$, nh∆∞ng kh√¥ng ƒë·∫°t ch√≠nh x√°c.**

---

**Diminishing $$\alpha_k = \frac{1}{k}$$:**

$$x^{(1)} = (1, 1) - 1 \cdot (0.5, 0.5) = (0.5, 0.5)$$

$$x^{(2)} = (0.5, 0.5) - 0.5 \cdot g^{(1)} \approx (0.4, 0.4)$$

...

**H·ªôi t·ª• ƒë·∫øn $$x^*$$, nh∆∞ng r·∫•t ch·∫≠m ($$O(1/\sqrt{k})$$).**

---

**Square summable $$\alpha_k = \frac{1}{k^{0.6}}$$:**

$$\sum \alpha_k = \sum \frac{1}{k^{0.6}} = \infty$$ ‚úì

$$\sum \alpha_k^2 = \sum \frac{1}{k^{1.2}} < \infty$$ ‚úì

**H·ªôi t·ª• ƒë·∫øn $$x^*$$ nhanh h∆°n $$1/k$$, ch·∫≠m h∆°n constant.**

**Best balance!**

</details>

---

## üìù **Ph·∫ßn IV: Polyak Step Size**

### **B√†i t·∫≠p 4: Polyak's Step Size for $$\ell_1$$ Minimization**

$$\min_{x} \|x\|_1 \quad \text{s.t.} \quad Ax = b$$

v·ªõi $$A = [1, 2, 3]$$, $$b = 6$$.

**Y√™u c·∫ßu:**  
a) Vi·∫øt Lagrangian  
b) T√¨m dual function  
c) Maximize dual b·∫±ng subgradient v·ªõi Polyak step size

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Lagrangian:**

$$L(x, \nu) = \|x\|_1 + \nu^T(Ax - b)$$

$$= \sum_i (|x_i| + \nu a_i x_i) - \nu b$$

---

**b) Dual function:**

$$g(\nu) = \inf_x L(x, \nu)$$

$$= \inf_x \sum_i (|x_i| + \nu a_i x_i) - \nu b$$

Minimize m·ªói $$x_i$$ ƒë·ªôc l·∫≠p:

$$\inf_{x_i} (|x_i| + \nu a_i x_i) = \begin{cases}
0 & \text{if } |\nu a_i| \leq 1 \\
-\infty & \text{otherwise}
\end{cases}$$

$$g(\nu) = \begin{cases}
-\nu b & \text{if } |\nu a_i| \leq 1 \, \forall i \\
-\infty & \text{otherwise}
\end{cases}$$

V·ªõi $$A = [1, 2, 3]$$:

$$|\nu| \leq 1, \quad |2\nu| \leq 1, \quad |3\nu| \leq 1$$

$$\Rightarrow |\nu| \leq \frac{1}{3}$$

$$g(\nu) = -6\nu$$ for $$|\nu| \leq \frac{1}{3}$$

---

**c) Dual maximization:**

$$\max_{\nu} \quad -6\nu \quad \text{s.t.} \quad -\frac{1}{3} \leq \nu \leq \frac{1}{3}$$

**Obvious:** $$\nu^* = -\frac{1}{3}$$, $$g^* = 2$$

**Subgradient of $$-g(\nu) = 6\nu$$:** $$\partial(-g) = \{6\}$$

**Polyak step size:**

$$\alpha_k = \frac{g(\nu^{(k)}) - g^*}{\|s^{(k)}\|^2}$$

v·ªõi $$s^{(k)} = -6$$ (subgradient of $$-g$$).

T·ª´ $$\nu^{(0)} = 0$$:

$$g(0) = 0$$

$$\alpha_0 = \frac{0 - 2}{36} = -\frac{1}{18}$$

(Negative ‚Üí ƒëi·ªÅu ch·ªânh: use $$|g - g^*|$$)

$$\alpha_0 = \frac{|0 - 2|}{36} = \frac{1}{18}$$

$$\nu^{(1)} = 0 - \frac{1}{18} \cdot (-6) = \frac{1}{3}$$

**Jumped to boundary!** (Nh∆∞ng sai d·∫•u)

**Correction:** Maximize $$g$$, n√™n:

$$\nu^{(1)} = 0 + \frac{1}{18} \cdot (-6) = -\frac{1}{3}$$ ‚úì

**H·ªôi t·ª• trong 1 b∆∞·ªõc!**

</details>

---

## üìù **Ph·∫ßn V: Projected Subgradient**

### **B√†i t·∫≠p 5: Projected Subgradient for Constrained Optimization**

$$\min_{x} \|x - c\|_1 \quad \text{s.t.} \quad \|x\|_2 \leq R$$

v·ªõi $$c = (2, 3)^T$$, $$R = 2$$.

**Y√™u c·∫ßu:**  
a) Vi·∫øt projected subgradient update  
b) Iterate t·ª´ $$x^{(0)} = (0, 0)$$  
c) T√¨m nghi·ªám exact

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Projected subgradient:**

$$x^{(k+1)} = \Pi_C\left(x^{(k)} - \alpha_k g^{(k)}\right)$$

v·ªõi $$C = \{x : \|x\|_2 \leq R\}$$, $$g^{(k)} \in \partial f(x^{(k)})$$.

**Projection onto $$\ell_2$$ ball:**

$$\Pi_C(y) = \begin{cases}
y & \text{if } \|y\| \leq R \\
R \frac{y}{\|y\|} & \text{if } \|y\| > R
\end{cases}$$

---

**b) Iterations:**

$$x^{(0)} = (0, 0)$$

$$\partial f(0, 0) = \partial \|(0, 0) - (2, 3)\|_1 = \partial(|{-2}| + |{-3}|)$$

Subgradient: $$g = \text{sign}(x - c) = (-1, -1)$$

Ch·ªçn $$\alpha_0 = 1$$:

$$\tilde{x}^{(1)} = (0, 0) - 1 \cdot (-1, -1) = (1, 1)$$

$$\|\tilde{x}^{(1)}\| = \sqrt{2} < 2$$ ‚Üí no projection needed

$$x^{(1)} = (1, 1)$$

---

$$g^{(1)} = \text{sign}((1, 1) - (2, 3)) = (-1, -1)$$

$$\tilde{x}^{(2)} = (1, 1) + (1, 1) = (2, 2)$$

$$\|\tilde{x}^{(2)}\| = 2\sqrt{2} > 2$$ ‚Üí project!

$$x^{(2)} = 2 \cdot \frac{(2, 2)}{2\sqrt{2}} = \frac{2}{\sqrt{2}}(1, 1) = (\sqrt{2}, \sqrt{2})$$

---

**c) Exact solution:**

Minimize $$\|x - c\|_1$$ over $$\|x\| \leq 2$$.

**Unconstrained optimum:** $$x = c = (2, 3)$$

$$\|c\| = \sqrt{13} > 2$$ ‚Üí infeasible!

**Constrained optimum:** Project $$c$$ onto ball:

$$x^* = 2 \cdot \frac{c}{\|c\|} = \frac{2}{\sqrt{13}}(2, 3) = \left(\frac{4}{\sqrt{13}}, \frac{6}{\sqrt{13}}\right)$$

$$\approx (1.109, 1.664)$$

**Objective:** $$f(x^*) = |1.109 - 2| + |1.664 - 3| = 0.891 + 1.336 = 2.227$$

</details>

---

## üí° **T·ªïng k·∫øt**

### **Subgradient Method Properties:**

| Aspect | Gradient Descent | Subgradient Method |
|--------|------------------|-------------------|
| **Applicability** | Smooth $$f$$ | Non-smooth $$f$$ |
| **Convergence** | Linear ($$O(\rho^k)$$) | Sublinear ($$O(1/\sqrt{k})$$) |
| **Step size** | Fixed or line search | Diminishing or Polyak |
| **Descent** | Always decreases | May increase! |
| **Usage** | Well-behaved $$f$$ | $$\ell_1$$, max, constraints |

### **Step Size Rules:**

1. **Constant $$\alpha$$:** Converges to neighborhood
2. **Diminishing $$\frac{1}{k}$$:** Converges exactly, slowly
3. **Square summable $$\frac{1}{k^{0.6}}$$:** Best practical balance
4. **Polyak:** Fast if $$f^*$$ known

### **Khi n√†o d√πng Subgradient:**
- Non-differentiable $$f$$ ($$\ell_1$$, max)
- Simple per-iteration cost needed
- Willing to accept slow convergence
- Constraints via projection

---

## üìö **T√†i li·ªáu Tham kh·∫£o**

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Section 3.1.

2. **Shor, N. Z.** (1985). *Minimization Methods for Non-Differentiable Functions*. Springer.

3. **Polyak, B. T.** (1987). *Introduction to Optimization*. Optimization Software.

