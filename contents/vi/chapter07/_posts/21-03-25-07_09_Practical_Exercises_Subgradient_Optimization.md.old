---
layout: post
title: 7-9 B√†i T·∫≠p Th·ª±c H√†nh - Subgradient v√† Thu·∫≠t To√°n T·ªëi ∆Øu
chapter: '7'
order: 17
owner: GitHub Copilot
lang: vi
categories:
- chapter07
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Subgradient v√† Thu·∫≠t To√°n T·ªëi ∆Øu

## üìù **B√†i t·∫≠p 1: T√≠nh to√°n Subgradient c∆° b·∫£n**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Chapter 7)
T√≠nh subgradient $$\partial f(x)$$ c·ªßa c√°c h√†m sau t·∫°i ƒëi·ªÉm $$x$$:

a) $$f(x) = |x - 2|$$ t·∫°i $$x = 2$$, $$x = 1$$, $$x = 3$$

b) $$f(x) = \max\{x_1, x_2\}$$ t·∫°i $$x = (1, 2)$$, $$x = (2, 2)$$, $$x = (3, 1)$$

c) $$f(x) = \|x\|_1 = \sum_{i=1}^n |x_i|$$ t·∫°i $$x = (1, 0, -2)$$

d) $$f(x) = \|x\|_2$$ t·∫°i $$x = (3, 4)$$ v√† $$x = (0, 0)$$

**Y√™u c·∫ßu:**
1. √Åp d·ª•ng ƒë·ªãnh nghƒ©a subgradient
2. S·ª≠ d·ª•ng quy t·∫Øc t√≠nh to√°n subdifferential
3. Ki·ªÉm ch·ª©ng k·∫øt qu·∫£ b·∫±ng gi·∫£i th√≠ch h√¨nh h·ªçc
4. Tr·ª±c quan h√≥a c√°c t·∫≠p subdifferential

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: H√†m gi√° tr·ªã tuy·ªát ƒë·ªëi**
$$f(x) = |x - 2|$$

**T·∫°i $$x = 2$$:** (ƒëi·ªÉm kh√¥ng kh·∫£ vi)
$$\partial f(2) = [-1, 1]$$

**Verification:** $$f(y) \geq f(2) + g(y - 2)$$ v·ªõi $$g \in [-1, 1]$$
- $$|y - 2| \geq 0 + g(y - 2)$$
- ƒê√∫ng v·ªõi m·ªçi $$g \in [-1, 1]$$

**T·∫°i $$x = 1$$:** (kh·∫£ vi)
$$\partial f(1) = \{-1\}$$

**T·∫°i $$x = 3$$:** (kh·∫£ vi)
$$\partial f(3) = \{1\}$$

**B∆∞·ªõc 2: H√†m max**
$$f(x) = \max\{x_1, x_2\}$$

**T·∫°i $$x = (1, 2)$$:** $$x_2 > x_1$$
$$\partial f(1, 2) = \{(0, 1)\}$$

**T·∫°i $$x = (2, 2)$$:** $$x_1 = x_2$$ (kh√¥ng kh·∫£ vi)
$$\partial f(2, 2) = \text{conv}\{(1, 0), (0, 1)\} = \{(\lambda, 1-\lambda) : \lambda \in [0, 1]\}$$

**T·∫°i $$x = (3, 1)$$:** $$x_1 > x_2$$
$$\partial f(3, 1) = \{(1, 0)\}$$

**B∆∞·ªõc 3: Chu·∫©n L1**
$$f(x) = \|x\|_1 = |x_1| + |x_2| + |x_3|$$

**T·∫°i $$x = (1, 0, -2)$$:**
- $$\partial |x_1|(1) = \{1\}$$ (v√¨ $$x_1 = 1 > 0$$)
- $$\partial |x_2|(0) = [-1, 1]$$ (v√¨ $$x_2 = 0$$)
- $$\partial |x_3|(-2) = \{-1\}$$ (v√¨ $$x_3 = -2 < 0$$)

$$\partial f(1, 0, -2) = \{1\} \times [-1, 1] \times \{-1\} = \{(1, g_2, -1) : g_2 \in [-1, 1]\}$$

**B∆∞·ªõc 4: Chu·∫©n L2**
$$f(x) = \|x\|_2$$

**T·∫°i $$x = (3, 4)$$:** (kh·∫£ vi, $$x \neq 0$$)
$$\partial f(3, 4) = \left\{\frac{(3, 4)}{\|(3, 4)\|_2}\right\} = \left\{\frac{(3, 4)}{5}\right\} = \left\{\left(\frac{3}{5}, \frac{4}{5}\right)\right\}$$

**T·∫°i $$x = (0, 0)$$:** (kh√¥ng kh·∫£ vi)
$$\partial f(0, 0) = \{g : \|g\|_2 \leq 1\}$$ (unit ball)

**Verification:** $$\|y\| \geq 0 + g^T y$$ v·ªõi $$\|g\| \leq 1$$
- ƒê√∫ng theo Cauchy-Schwarz: $$g^T y \leq \|g\| \|y\| \leq \|y\|$$

</details>

---

## üìù **B√†i t·∫≠p 2: ƒêi·ªÅu ki·ªán t·ªëi ∆∞u v·ªõi Subgradient**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Section 7.3)
S·ª≠ d·ª•ng ƒëi·ªÅu ki·ªán t·ªëi ∆∞u subgradient ƒë·ªÉ gi·∫£i c√°c b√†i to√°n sau:

a) $$\min_x |x - 3| + |x + 1|$$

b) $$\min_{x \in \mathbb{R}^2} \max\{x_1 + x_2, x_1 - x_2, -x_1\}$$

c) $$\min_{x \in \mathbb{R}^n} \frac{1}{2}\|Ax - b\|_2^2 + \lambda \|x\|_1$$ (LASSO)

d) $$\min_{x \in C} f(x)$$ v·ªõi $$f(x) = |x_1| + |x_2|$$ v√† $$C = \{x : x_1 + x_2 = 1\}$$

**Y√™u c·∫ßu:**
1. √Åp d·ª•ng ƒëi·ªÅu ki·ªán $$0 \in \partial f(x^*)$$
2. Ph√¢n t√≠ch c√°c tr∆∞·ªùng h·ª£p kh√°c nhau
3. Ki·ªÉm ch·ª©ng nghi·ªám b·∫±ng l·∫≠p lu·∫≠n h√¨nh h·ªçc
4. So s√°nh v·ªõi nghi·ªám gi·∫£i t√≠ch

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: B√†i to√°n 1D v·ªõi h√†m tuy·ªát ƒë·ªëi**
$$\min_x |x - 3| + |x + 1|$$

**Ph√¢n t√≠ch subdifferential:**
$$f(x) = |x - 3| + |x + 1|$$

**C√°c kho·∫£ng:**
- $$x < -1$$: $$f(x) = -(x-3) - (x+1) = -2x + 2$$, $$\partial f(x) = \{-2\}$$
- $$-1 < x < 3$$: $$f(x) = -(x-3) + (x+1) = 4$$, $$\partial f(x) = \{0\}$$
- $$x > 3$$: $$f(x) = (x-3) + (x+1) = 2x - 2$$, $$\partial f(x) = \{2\}$$

**T·∫°i $$x = -1$$:**
$$\partial f(-1) = \text{conv}\{-2, 0\} = [-2, 0]$$

**T·∫°i $$x = 3$$:**
$$\partial f(3) = \text{conv}\{0, 2\} = [0, 2]$$

**ƒêi·ªÅu ki·ªán t·ªëi ∆∞u:** $$0 \in \partial f(x^*)$$
- $$0 \in [-2, 0]$$ t·∫°i $$x = -1$$ ‚úì
- $$0 \in \{0\}$$ cho $$x \in (-1, 3)$$ ‚úì
- $$0 \in [0, 2]$$ t·∫°i $$x = 3$$ ‚úì

**Nghi·ªám:** $$x^* \in [-1, 3]$$ v·ªõi gi√° tr·ªã t·ªëi ∆∞u $$f^* = 4$$

**B∆∞·ªõc 2: B√†i to√°n max functions**
$$\min_{x \in \mathbb{R}^2} \max\{x_1 + x_2, x_1 - x_2, -x_1\}$$

**ƒê·∫∑t:** $$f(x) = \max\{f_1(x), f_2(x), f_3(x)\}$$
- $$f_1(x) = x_1 + x_2$$, $$\nabla f_1 = (1, 1)$$
- $$f_2(x) = x_1 - x_2$$, $$\nabla f_2 = (1, -1)$$
- $$f_3(x) = -x_1$$, $$\nabla f_3 = (-1, 0)$$

**Quy t·∫Øc subdifferential:**
$$\partial f(x) = \text{conv}\{\nabla f_i(x) : i \in I(x)\}$$
v·ªõi $$I(x) = \{i : f_i(x) = \max_j f_j(x)\}$$

**Ph√¢n t√≠ch geometric:** T√¨m ƒëi·ªÉm m√† √≠t nh·∫•t 2 functions active.

**Candidate points:**
- $$f_1 = f_2$$: $$x_1 + x_2 = x_1 - x_2 \Rightarrow x_2 = 0$$
- $$f_1 = f_3$$: $$x_1 + x_2 = -x_1 \Rightarrow x_1 = -\frac{x_2}{2}$$
- $$f_2 = f_3$$: $$x_1 - x_2 = -x_1 \Rightarrow x_1 = \frac{x_2}{2}$$

**T·∫°i $$x^* = (0, 0)$$:**
- $$f_1(0, 0) = f_2(0, 0) = f_3(0, 0) = 0$$
- $$\partial f(0, 0) = \text{conv}\{(1, 1), (1, -1), (-1, 0)\}$$
- $$0 \in \partial f(0, 0)$$ (c√≥ th·ªÉ verify b·∫±ng c√°ch t√¨m convex combination)

**Nghi·ªám:** $$x^* = (0, 0)$$ v·ªõi $$f^* = 0$$

**B∆∞·ªõc 3: LASSO problem**
$$\min_{x \in \mathbb{R}^n} \frac{1}{2}\|Ax - b\|_2^2 + \lambda \|x\|_1$$

**Subdifferential:**
$$\partial f(x) = A^T(Ax - b) + \lambda \partial \|x\|_1$$

**ƒêi·ªÅu ki·ªán t·ªëi ∆∞u:** $$0 \in A^T(Ax^* - b) + \lambda \partial \|x^*\|_1$$

**Equivalent:** $$A^T(b - Ax^*) \in \lambda \partial \|x^*\|_1$$

**Component-wise analysis:**
$$[A^T(b - Ax^*)]_i \in \lambda \partial |x^*_i|$$

**Tr∆∞·ªùng h·ª£p:**
- N·∫øu $$x^*_i > 0$$: $$[A^T(b - Ax^*)]_i = \lambda$$
- N·∫øu $$x^*_i < 0$$: $$[A^T(b - Ax^*)]_i = -\lambda$$
- N·∫øu $$x^*_i = 0$$: $$|[A^T(b - Ax^*)]_i| \leq \lambda$$

**B∆∞·ªõc 4: Constrained optimization**
$$\min_{x \in C} f(x)$$ v·ªõi $$f(x) = |x_1| + |x_2|$$, $$C = \{x : x_1 + x_2 = 1\}$$

**Lagrangian approach:**
$$\min_x f(x) + I_C(x)$$

**ƒêi·ªÅu ki·ªán t·ªëi ∆∞u:**
$$0 \in \partial f(x^*) + \partial I_C(x^*)$$
$$0 \in \partial f(x^*) + \mathcal{N}_C(x^*)$$

**Normal cone:** $$\mathcal{N}_C(x) = \{\mu(1, 1) : \mu \in \mathbb{R}\}$$

**Constraint:** $$x_1 + x_2 = 1$$

**Case analysis tr√™n constraint:**
- $$x_1 > 0, x_2 > 0$$: $$\partial f(x) = \{(1, 1)\}$$
- $$x_1 > 0, x_2 = 0$$: $$x_1 = 1$$, $$\partial f(1, 0) = \{1\} \times [-1, 1]$$
- $$x_1 = 0, x_2 > 0$$: $$x_2 = 1$$, $$\partial f(0, 1) = [-1, 1] \times \{1\}$$

**Optimality check:**
- T·∫°i $$(1, 0)$$: $$0 \in \{(1, g_2) : g_2 \in [-1, 1]\} + \{\mu(1, 1)\}$$
  $$\Rightarrow (0, 0) = (1 + \mu, g_2 + \mu)$$
  $$\Rightarrow \mu = -1, g_2 = 1$$ ‚úì
  
**Nghi·ªám:** $$x^* = (1, 0)$$ ho·∫∑c $$x^* = (0, 1)$$ v·ªõi $$f^* = 1$$

</details>

---

## üìù **B√†i t·∫≠p 3: Subgradient Method**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Section 7.4)
L·∫≠p tr√¨nh ph∆∞∆°ng ph√°p subgradient cho c√°c b√†i to√°n sau:

a) $$\min_x |x - 1| + |x - 3|$$ v·ªõi $$x^{(0)} = 0$$

b) $$\min_{x \in \mathbb{R}^2} \max\{x_1, x_2, 1 - x_1 - x_2\}$$

c) $$\min_{x \in \mathbb{R}^n} \|Ax - b\|_1$$ (h·ªìi quy L1)

**Y√™u c·∫ßu:**
1. L·∫≠p tr√¨nh thu·∫≠t to√°n v·ªõi c√°c quy t·∫Øc k√≠ch th∆∞·ªõc b∆∞·ªõc kh√°c nhau
2. Ph√¢n t√≠ch h√†nh vi h·ªôi t·ª•
3. So s√°nh v·ªõi nghi·ªám gi·∫£i t√≠ch
4. Tr·ª±c quan h√≥a ƒë∆∞·ªùng ƒëi t·ªëi ∆∞u

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: 1D problem implementation**
$$\min_x |x - 1| + |x - 3|$$

**Subgradient calculation:**
```python
def subgradient_f(x):
    # f(x) = |x - 1| + |x - 3|
    g1 = 1 if x > 1 else (-1 if x < 1 else 0)  # ‚àÇ|x-1|
    g2 = 1 if x > 3 else (-1 if x < 3 else 0)  # ‚àÇ|x-3|
    
    if x == 1:
        g1 = np.random.uniform(-1, 1)  # Ch·ªçn ng·∫´u nhi√™n t·ª´ [-1,1]
    if x == 3:
        g2 = np.random.uniform(-1, 1)
        
    return g1 + g2

def f_value(x):
    return abs(x - 1) + abs(x - 3)
```

**Tri·ªÉn khai thu·∫≠t to√°n:**
```python
def subgradient_method_1d(x0, max_iter=1000, step_rule='constant'):
    x = x0
    history = [x]
    f_history = [f_value(x)]
    
    for k in range(max_iter):
        g = subgradient_f(x)
        
        # Quy t·∫Øc step size
        if step_rule == 'constant':
            t = 0.1
        elif step_rule == 'diminishing':
            t = 1.0 / (k + 1)
        elif step_rule == 'square_summable':
            t = 1.0 / np.sqrt(k + 1)
            
        x_new = x - t * g
        
        x = x_new
        history.append(x)
        f_history.append(f_value(x))
        
        # Ki·ªÉm tra h·ªôi t·ª•
        if abs(g) < 1e-6 and x >= 1 and x <= 3:
            break
            
    return x, history, f_history
```

**Analytical solution:** $$x^* \in [1, 3]$$, $$f^* = 2$$

**B∆∞·ªõc 2: 2D max problem**
$$\min_{x \in \mathbb{R}^2} \max\{x_1, x_2, 1 - x_1 - x_2\}$$

**Subgradient calculation:**
```python
def subgradient_max_problem(x):
    x1, x2 = x
    f1, f2, f3 = x1, x2, 1 - x1 - x2
    
    max_val = max(f1, f2, f3)
    active_set = []
    
    if abs(f1 - max_val) < 1e-10:
        active_set.append(np.array([1, 0]))
    if abs(f2 - max_val) < 1e-10:
        active_set.append(np.array([0, 1]))
    if abs(f3 - max_val) < 1e-10:
        active_set.append(np.array([-1, -1]))
    
    # T·ªï h·ª£p l·ªìi ng·∫´u nhi√™n c·ªßa c√°c gradient ho·∫°t ƒë·ªông
    if len(active_set) == 1:
        return active_set[0]
    else:
        weights = np.random.dirichlet(np.ones(len(active_set)))
        return sum(w * g for w, g in zip(weights, active_set))

def f_max_value(x):
    x1, x2 = x
    return max(x1, x2, 1 - x1 - x2)
```

**Geometric analysis:** 
- Optimal point: $$x^* = (1/3, 1/3)$$
- Optimal value: $$f^* = 1/3$$
- All three functions active t·∫°i optimal point

**B∆∞·ªõc 3: L1 regression**
$$\min_{x \in \mathbb{R}^n} \|Ax - b\|_1$$

**Subgradient calculation:**
```python
def subgradient_l1_regression(x, A, b):
    residual = A @ x - b
    g = np.zeros_like(x)
    
    for i in range(len(residual)):
        if residual[i] > 0:
            sign_i = 1
        elif residual[i] < 0:
            sign_i = -1
        else:
            sign_i = np.random.uniform(-1, 1)
        
        g += sign_i * A[i, :]
    
    return g

def f_l1_value(x, A, b):
    return np.sum(np.abs(A @ x - b))
```

**Thu·∫≠t to√°n v·ªõi momentum:**
```python
def subgradient_method_l1(A, b, x0, max_iter=5000):
    x = x0.copy()
    best_x = x.copy()
    best_f = f_l1_value(x, A, b)
    
    history = [x.copy()]
    f_history = [best_f]
    
    for k in range(max_iter):
        g = subgradient_l1_regression(x, A, b)
        t = 1.0 / np.sqrt(k + 1)  # Square summable step size
        
        x = x - t * g
        f_val = f_l1_value(x, A, b)
        
        # Keep track of best solution
        if f_val < best_f:
            best_f = f_val
            best_x = x.copy()
        
        history.append(x.copy())
        f_history.append(f_val)
    
    return best_x, history, f_history
```

**Convergence analysis:**
- **Constant step size:** Kh√¥ng h·ªôi t·ª• v·ªÅ ƒëi·ªÉm, nh∆∞ng oscillate quanh optimal
- **Diminishing step size:** H·ªôi t·ª• v·ªÅ optimal value
- **Square summable:** Balance gi·ªØa progress v√† stability

**So s√°nh step size:**
```python
# Compare different step size rules
step_rules = ['constant', 'diminishing', 'square_summable']
results = {}

for rule in step_rules:
    x_opt, hist, f_hist = subgradient_method_1d(0, step_rule=rule)
    results[rule] = {
        'final_x': x_opt,
        'final_f': f_hist[-1],
        'convergence': f_hist
    }
```

</details>

---

## üìù **B√†i t·∫≠p 4: LASSO v√† Soft-thresholding**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Example 7.4)
Ph√¢n t√≠ch b√†i to√°n LASSO v√† to√°n t·ª≠ soft-thresholding:

a) X√¢y d·ª±ng nghi·ªám soft-thresholding cho $$\min_x \frac{1}{2}(x - a)^2 + \lambda |x|$$

b) L·∫≠p tr√¨nh h·∫° t·ªça ƒë·ªô cho LASSO:
   $$\min_{\beta \in \mathbb{R}^p} \frac{1}{2}\|y - X\beta\|_2^2 + \lambda \|\beta\|_1$$

c) So s√°nh v·ªõi nghi·ªám b√¨nh ph∆∞∆°ng t·ªëi thi·ªÉu

d) Ph√¢n t√≠ch ƒë∆∞·ªùng ƒëi·ªÅu chu·∫©n

**Y√™u c·∫ßu:**
1. X√¢y d·ª±ng c√¥ng th·ª©c closed-form c·ªßa soft-thresholding
2. L·∫≠p tr√¨nh h·∫° t·ªça ƒë·ªô hi·ªáu qu·∫£
3. Tr·ª±c quan h√≥a hi·ªáu ·ª©ng ƒëi·ªÅu chu·∫©n
4. So s√°nh t·ªëc ƒë·ªô h·ªôi t·ª•

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Soft-thresholding derivation**
$$\min_x \frac{1}{2}(x - a)^2 + \lambda |x|$$

**Subdifferential approach:**
$$f(x) = \frac{1}{2}(x - a)^2 + \lambda |x|$$

$$\partial f(x) = (x - a) + \lambda \partial |x|$$

**Optimality condition:** $$0 \in \partial f(x^*)$$
$$0 \in (x^* - a) + \lambda \partial |x^*|$$
$$a - x^* \in \lambda \partial |x^*|$$

**Case analysis:**
- **Case 1:** $$x^* > 0$$
  $$a - x^* = \lambda \Rightarrow x^* = a - \lambda$$
  Valid if $$a - \lambda > 0 \Rightarrow a > \lambda$$

- **Case 2:** $$x^* < 0$$
  $$a - x^* = -\lambda \Rightarrow x^* = a + \lambda$$
  Valid if $$a + \lambda < 0 \Rightarrow a < -\lambda$$

- **Case 3:** $$x^* = 0$$
  $$a \in \lambda[-1, 1] = [-\lambda, \lambda]$$
  Valid if $$|a| \leq \lambda$$

**Soft-thresholding operator:**
$$\text{soft}(a, \lambda) = \begin{cases}
a - \lambda & \text{if } a > \lambda \\
0 & \text{if } |a| \leq \lambda \\
a + \lambda & \text{if } a < -\lambda
\end{cases}$$

**Compact form:** $$\text{soft}(a, \lambda) = \text{sign}(a) \max\{|a| - \lambda, 0\}$$

**B∆∞·ªõc 2: LASSO coordinate descent**

**Problem:** $$\min_{\beta} \frac{1}{2}\|y - X\beta\|_2^2 + \lambda \|\beta\|_1$$

**Coordinate-wise optimization:**
Fix $$\beta_j$$ for $$j \neq k$$, optimize over $$\beta_k$$:

$$\min_{\beta_k} \frac{1}{2}\left\|y - \sum_{j \neq k} X_j \beta_j - X_k \beta_k\right\|_2^2 + \lambda |\beta_k|$$

**Residual:** $$r^{(k)} = y - \sum_{j \neq k} X_j \beta_j$$

**Subproblem:** $$\min_{\beta_k} \frac{1}{2}\|r^{(k)} - X_k \beta_k\|_2^2 + \lambda |\beta_k|$$

**Normal equation form:** $$\min_{\beta_k} \frac{1}{2}(\beta_k - \tilde{\beta}_k)^2 \|X_k\|_2^2 + \lambda |\beta_k|$$

v·ªõi $$\tilde{\beta}_k = \frac{X_k^T r^{(k)}}{\|X_k\|_2^2}$$

**Solution:** $$\beta_k^* = \text{soft}\left(\tilde{\beta}_k, \frac{\lambda}{\|X_k\|_2^2}\right)$$

**Tri·ªÉn khai thu·∫≠t to√°n:**
```python
def coordinate_descent_lasso(X, y, lambda_reg, max_iter=1000, tol=1e-6):
    n, p = X.shape
    beta = np.zeros(p)
    
    # Precompute X^T X diagonal and X^T y
    XTX_diag = np.sum(X**2, axis=0)
    XTy = X.T @ y
    
    for iteration in range(max_iter):
        beta_old = beta.copy()
        
        for k in range(p):
            # T√≠nh residual kh√¥ng c√≥ feature th·ª© k
            r_k = XTy[k] - X[:, k] @ (X @ beta - X[:, k] * beta[k])
            
            # Soft-thresholding update
            beta_tilde_k = r_k / XTX_diag[k]
            beta[k] = soft_threshold(beta_tilde_k, lambda_reg / XTX_diag[k])
        
        # Ki·ªÉm tra h·ªôi t·ª•
        if np.linalg.norm(beta - beta_old) < tol:
            break
    
    return beta

def soft_threshold(x, threshold):
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)
```

**B∆∞·ªõc 3: Regularization path**

**Tri·ªÉn khai:**
```python
def lasso_path(X, y, lambda_values, **kwargs):
    solutions = []
    
    for lam in lambda_values:
        beta = coordinate_descent_lasso(X, y, lam, **kwargs)
        solutions.append(beta)
    
    return np.array(solutions)

# Generate regularization path
lambda_max = np.max(np.abs(X.T @ y)) / len(y)
lambda_values = np.logspace(np.log10(lambda_max), np.log10(0.01 * lambda_max), 50)
beta_path = lasso_path(X, y, lambda_values)
```

**Visualization:**
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
for j in range(beta_path.shape[1]):
    plt.plot(lambda_values, beta_path[:, j], label=f'Œ≤_{j+1}')
plt.xscale('log')
plt.xlabel('Œª (regularization parameter)')
plt.ylabel('Coefficient value')
plt.title('LASSO Regularization Path')
plt.legend()
plt.grid(True)
plt.show()
```

**B∆∞·ªõc 4: Comparison v·ªõi least squares**

```python
# Least squares solution
beta_ls = np.linalg.solve(X.T @ X, X.T @ y)

# LASSO solutions for different Œª
lambda_values = [0.1, 1.0, 10.0]
beta_lasso = {}

for lam in lambda_values:
    beta_lasso[lam] = coordinate_descent_lasso(X, y, lam)

# Compare sparsity
print("Least Squares - Non-zero coefficients:", np.sum(np.abs(beta_ls) > 1e-6))
for lam in lambda_values:
    print(f"LASSO (Œª={lam}) - Non-zero coefficients:", 
          np.sum(np.abs(beta_lasso[lam]) > 1e-6))
```

**Analysis:**
- **Sparsity:** LASSO promotes sparse solutions
- **Bias-variance tradeoff:** Higher $$\lambda$$ increases bias, reduces variance
- **Feature selection:** LASSO automatically selects relevant features
- **Convergence:** Coordinate descent converges faster than subgradient method

</details>

---

## üìù **B√†i t·∫≠p 5: Projection onto Convex Sets**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Example 7.5)
Gi·∫£i c√°c b√†i to√°n chi·∫øu s·ª≠ d·ª•ng t√≠nh t·ªëi ∆∞u c·ªßa subgradient:

a) $$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : \|x\|_1 \leq 1\}$$

b) $$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : \|x\|_\infty \leq 1\}$$

c) $$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : Ax = b\}$$

d) L·∫≠p tr√¨nh ph∆∞∆°ng ph√°p subgradient c√≥ chi·∫øu

**Y√™u c·∫ßu:**
1. X√¢y d·ª±ng c√°c to√°n t·ª≠ chi·∫øu
2. Ki·ªÉm ch·ª©ng ƒëi·ªÅu ki·ªán t·ªëi ∆∞u
3. L·∫≠p tr√¨nh c√°c thu·∫≠t to√°n hi·ªáu qu·∫£
4. √Åp d·ª•ng v√†o t·ªëi ∆∞u c√≥ r√†ng bu·ªôc

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Projection onto L1 ball**
$$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : \|x\|_1 \leq 1\}$$

**Lagrangian:** $$L(x, \lambda) = \frac{1}{2}\|x - a\|_2^2 + \lambda(\|x\|_1 - 1)$$

**KKT conditions:**
1. $$0 \in (x^* - a) + \lambda^* \partial \|x^*\|_1$$
2. $$\lambda^* \geq 0$$
3. $$\lambda^*(\|x^*\|_1 - 1) = 0$$
4. $$\|x^*\|_1 \leq 1$$

**Case analysis:**
- **Case 1:** $$\|a\|_1 \leq 1$$ ($$a$$ inside ball)
  $$x^* = a$$, $$\lambda^* = 0$$

- **Case 2:** $$\|a\|_1 > 1$$ ($$a$$ outside ball)
  $$\|x^*\|_1 = 1$$, $$\lambda^* > 0$$

**For Case 2:** Component-wise analysis
$$x^*_i - a_i + \lambda^* \text{sign}(x^*_i) = 0$$ if $$x^*_i \neq 0$$

**Algorithm (Duchi et al.):**
```python
def project_l1_ball(a, radius=1.0):
    if np.sum(np.abs(a)) <= radius:
        return a
    
    # Sort by absolute value (descending)
    u = np.abs(a)
    indices = np.argsort(-u)
    u_sorted = u[indices]
    
    # Find threshold
    cumsum = np.cumsum(u_sorted)
    k = np.max(np.where(u_sorted - (cumsum - radius) / np.arange(1, len(u) + 1) > 0)[0])
    
    theta = (cumsum[k] - radius) / (k + 1)
    
    # Apply soft-thresholding
    x_proj = np.sign(a) * np.maximum(np.abs(a) - theta, 0)
    
    return x_proj
```

**B∆∞·ªõc 2: Projection onto L‚àû ball**
$$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : \|x\|_\infty \leq 1\}$$

**Component-wise projection:**
$$[P_C(a)]_i = \begin{cases}
a_i & \text{if } |a_i| \leq 1 \\
\text{sign}(a_i) & \text{if } |a_i| > 1
\end{cases}$$

**Tri·ªÉn khai:**
```python
def project_linf_ball(a, radius=1.0):
    return np.clip(a, -radius, radius)
```

**Verification:** 
$$x^* = P_C(a) \Leftrightarrow (a - x^*)^T(y - x^*) \leq 0, \forall y \in C$$

**B∆∞·ªõc 3: Projection onto affine subspace**
$$\min_{x \in C} \frac{1}{2}\|x - a\|_2^2$$ v·ªõi $$C = \{x : Ax = b\}$$

**Lagrangian:** $$L(x, \nu) = \frac{1}{2}\|x - a\|_2^2 + \nu^T(Ax - b)$$

**Optimality:** $$x^* - a + A^T\nu^* = 0$$
$$\Rightarrow x^* = a - A^T\nu^*$$

**Constraint:** $$A(a - A^T\nu^*) = b$$
$$\Rightarrow AA^T\nu^* = Aa - b$$
$$\Rightarrow \nu^* = (AA^T)^{-1}(Aa - b)$$

**Projection formula:**
$$P_C(a) = a - A^T(AA^T)^{-1}(Aa - b)$$

**Tri·ªÉn khai:**
```python
def project_affine_subspace(a, A, b):
    # T√≠nh ma tr·∫≠n chi·∫øu
    AAT_inv = np.linalg.inv(A @ A.T)
    P_perp = A.T @ AAT_inv @ A
    
    # Project
    x_proj = a - P_perp @ a + A.T @ AAT_inv @ b
    
    return x_proj
```

**B∆∞·ªõc 4: Projected subgradient method**

**Algorithm:** $$x^{(k+1)} = P_C(x^{(k)} - t_k g^{(k)})$$
v·ªõi $$g^{(k)} \in \partial f(x^{(k)})$$

**Tri·ªÉn khai:**
```python
def projected_subgradient_method(f, subgrad_f, project_C, x0, 
                                max_iter=1000, step_rule='diminishing'):
    x = x0.copy()
    history = [x.copy()]
    f_history = [f(x)]
    
    for k in range(max_iter):
        # T√≠nh subgradient
        g = subgrad_f(x)
        
        # Step size
        if step_rule == 'constant':
            t = 0.01
        elif step_rule == 'diminishing':
            t = 1.0 / (k + 1)
        elif step_rule == 'square_summable':
            t = 1.0 / np.sqrt(k + 1)
        
        # Subgradient step
        x_temp = x - t * g
        
        # Projection step
        x = project_C(x_temp)
        
        history.append(x.copy())
        f_history.append(f(x))
        
        # Ki·ªÉm tra h·ªôi t·ª•
        if np.linalg.norm(x - history[-2]) < 1e-6:
            break
    
    return x, history, f_history
```

**V√≠ d·ª• ·ª©ng d·ª•ng:**
```python
# Minimize |x1| + |x2| subject to x1 + x2 = 1
def f(x):
    return np.sum(np.abs(x))

def subgrad_f(x):
    return np.sign(x)

def project_constraint(x):
    # Project onto {x : x1 + x2 = 1}
    A = np.array([[1, 1]])
    b = np.array([1])
    return project_affine_subspace(x, A, b)

x0 = np.array([0.5, 0.5])
x_opt, hist, f_hist = projected_subgradient_method(
    f, subgrad_f, project_constraint, x0)
```

**Convergence analysis:**
- **Feasibility:** $$x^{(k)} \in C$$ for all $$k$$
- **Convergence rate:** $$O(1/\sqrt{k})$$ for square summable step sizes
- **Comparison:** Faster than penalty methods for simple constraints

</details>

---

## üìù **B√†i t·∫≠p 6: Non-smooth Optimization Applications**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Applications)
√Åp d·ª•ng ph∆∞∆°ng ph√°p subgradient cho c√°c b√†i to√°n th·ª±c t·∫ø:

a) **Kh·ª≠ nhi·ªÖu bi·∫øn ph√¢n to√†n ph·∫ßn:**
   $$\min_x \frac{1}{2}\|x - y\|_2^2 + \lambda \|Dx\|_1$$

b) **M√°y vector h·ªó tr·ª£:**
   $$\min_w \frac{1}{2}\|w\|_2^2 + C \sum_{i=1}^n \max\{0, 1 - y_i w^T x_i\}$$

c) **Ho√†n thi·ªán ma tr·∫≠n:**
   $$\min_X \|X\|_* + \frac{\lambda}{2}\|P_\Omega(X - M)\|_F^2$$

d) **PCA b·ªÅn v·ªØng:**
   $$\min_{L,S} \|L\|_* + \lambda \|S\|_1 \text{ v·ªõi } L + S = M$$

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a th√†nh b√†i to√°n t·ªëi ∆∞u subgradient
2. L·∫≠p tr√¨nh c√°c thu·∫≠t to√°n chuy√™n bi·ªát
3. So s√°nh v·ªõi x·∫•p x·ªâ tr∆°n
4. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Total Variation Denoising**
$$\min_x \frac{1}{2}\|x - y\|_2^2 + \lambda \|Dx\|_1$$

**Problem setup:**
- $$y$$: noisy signal/image
- $$D$$: difference operator (discrete gradient)
- $$\lambda$$: regularization parameter

**1D case:** $$D$$ l√† difference matrix
$$Dx = (x_2 - x_1, x_3 - x_2, \ldots, x_n - x_{n-1})^T$$

**Subgradient:**
$$\partial f(x) = (x - y) + \lambda D^T \partial \|Dx\|_1$$

**Tri·ªÉn khai:**
```python
def tv_denoising_1d(y, lambda_reg, max_iter=1000):
    n = len(y)
    
    # Difference matrix
    D = np.zeros((n-1, n))
    for i in range(n-1):
        D[i, i] = -1
        D[i, i+1] = 1
    
    def f(x):
        return 0.5 * np.sum((x - y)**2) + lambda_reg * np.sum(np.abs(D @ x))
    
    def subgrad_f(x):
        residual = x - y
        Dx = D @ x
        
        # Subgradient of ||Dx||_1
        subgrad_tv = np.zeros_like(Dx)
        for i in range(len(Dx)):
            if Dx[i] > 0:
                subgrad_tv[i] = 1
            elif Dx[i] < 0:
                subgrad_tv[i] = -1
            else:
                subgrad_tv[i] = np.random.uniform(-1, 1)
        
        return residual + lambda_reg * D.T @ subgrad_tv
    
    # Subgradient method
    x = y.copy()  # Initialize with noisy signal
    
    for k in range(max_iter):
        g = subgrad_f(x)
        t = 0.01 / (k + 1)  # Diminishing step size
        x = x - t * g
    
    return x
```

**2D case (images):**
```python
def tv_denoising_2d(Y, lambda_reg, max_iter=500):
    m, n = Y.shape
    X = Y.copy()
    
    for k in range(max_iter):
        # Compute gradients
        grad_x = np.zeros_like(X)
        grad_y = np.zeros_like(X)
        
        # Forward differences
        grad_x[:, :-1] = X[:, 1:] - X[:, :-1]
        grad_y[:-1, :] = X[1:, :] - X[:-1, :]
        
        # TV subgradient
        tv_norm = np.sqrt(grad_x**2 + grad_y**2)
        tv_norm[tv_norm == 0] = 1  # Avoid division by zero
        
        subgrad_x = grad_x / tv_norm
        subgrad_y = grad_y / tv_norm
        
        # Divergence (adjoint of gradient)
        div_x = np.zeros_like(X)
        div_y = np.zeros_like(X)
        
        div_x[:, 1:] = subgrad_x[:, 1:] - subgrad_x[:, :-1]
        div_x[:, 0] = subgrad_x[:, 0]
        
        div_y[1:, :] = subgrad_y[1:, :] - subgrad_y[:-1, :]
        div_y[0, :] = subgrad_y[0, :]
        
        # Subgradient step
        g = (X - Y) + lambda_reg * (div_x + div_y)
        t = 0.1 / np.sqrt(k + 1)
        X = X - t * g
    
    return X
```

**B∆∞·ªõc 2: Support Vector Machine**
$$\min_w \frac{1}{2}\|w\|_2^2 + C \sum_{i=1}^n \max\{0, 1 - y_i w^T x_i\}$$

**Subgradient calculation:**
$$\partial f(w) = w + C \sum_{i=1}^n \partial \max\{0, 1 - y_i w^T x_i\}$$

**Hinge loss subgradient:**
$$\partial \max\{0, 1 - y_i w^T x_i\} = \begin{cases}
\{0\} & \text{if } y_i w^T x_i > 1 \\
\{-y_i x_i\} & \text{if } y_i w^T x_i < 1 \\
[0, -y_i x_i] & \text{if } y_i w^T x_i = 1
\end{cases}$$

**Tri·ªÉn khai:**
```python
def svm_subgradient(X, y, C, max_iter=1000):
    n, d = X.shape
    w = np.zeros(d)
    
    for k in range(max_iter):
        # Compute margins
        margins = y * (X @ w)
        
        # Subgradient
        g = w.copy()
        for i in range(n):
            if margins[i] < 1:
                g -= C * y[i] * X[i]
        
        # Step size
        t = 1.0 / (k + 1)
        w = w - t * g
    
    return w

# Stochastic version
def svm_sgd(X, y, C, max_iter=1000, batch_size=32):
    n, d = X.shape
    w = np.zeros(d)
    
    for k in range(max_iter):
        # Random mini-batch
        idx = np.random.choice(n, batch_size, replace=False)
        X_batch, y_batch = X[idx], y[idx]
        
        # T√≠nh subgradient on batch
        margins = y_batch * (X_batch @ w)
        g = w.copy()
        
        for i in range(batch_size):
            if margins[i] < 1:
                g -= (C / batch_size) * y_batch[i] * X_batch[i]
        
        # Adaptive step size
        t = 1.0 / np.sqrt(k + 1)
        w = w - t * g
    
    return w
```

**B∆∞·ªõc 3: Matrix Completion**
$$\min_X \|X\|_* + \frac{\lambda}{2}\|P_\Omega(X - M)\|_F^2$$

**Nuclear norm subgradient:**
N·∫øu $$X = U\Sigma V^T$$ (SVD), th√¨:
$$\partial \|X\|_* = \{UV^T + W : U^TW = 0, WV = 0, \|W\|_2 \leq 1\}$$

**Proximal gradient approach:**
```python
def matrix_completion_proximal(M, Omega, lambda_reg, max_iter=100):
    """
    M: observed matrix
    Omega: set of observed indices
    """
    X = M.copy()
    
    for k in range(max_iter):
        # Gradient step
        grad = lambda_reg * Omega * (X - M)
        Y = X - (1.0 / lambda_reg) * grad
        
        # Proximal operator (soft-thresholding of singular values)
        U, s, Vt = np.linalg.svd(Y, full_matrices=False)
        s_thresh = np.maximum(s - 1.0 / lambda_reg, 0)
        X = U @ np.diag(s_thresh) @ Vt
        
        # Project onto constraint
        X[Omega] = M[Omega]
    
    return X

def soft_threshold_svd(X, threshold):
    """Soft-thresholding for nuclear norm"""
    U, s, Vt = np.linalg.svd(X, full_matrices=False)
    s_thresh = np.maximum(s - threshold, 0)
    return U @ np.diag(s_thresh) @ Vt
```

**B∆∞·ªõc 4: Robust PCA**
$$\min_{L,S} \|L\|_* + \lambda \|S\|_1 \text{ s.t. } L + S = M$$

**Augmented Lagrangian:**
$$\mathcal{L}(L,S,Y) = \|L\|_* + \lambda \|S\|_1 + \langle Y, M - L - S \rangle + \frac{\mu}{2}\|M - L - S\|_F^2$$

**ADMM algorithm:**
```python
def robust_pca_admm(M, lambda_reg=None, max_iter=100, tol=1e-6):
    if lambda_reg is None:
        lambda_reg = 1.0 / np.sqrt(max(M.shape))
    
    m, n = M.shape
    L = np.zeros_like(M)
    S = np.zeros_like(M)
    Y = np.zeros_like(M)
    mu = 1.25 / np.linalg.norm(M, 2)
    
    for k in range(max_iter):
        # Update L (nuclear norm proximal)
        L = soft_threshold_svd(M - S + Y/mu, 1/mu)
        
        # Update S (L1 proximal)
        S = soft_threshold_matrix(M - L + Y/mu, lambda_reg/mu)
        
        # Update Y (dual variable)
        Y = Y + mu * (M - L - S)
        
        # Ki·ªÉm tra h·ªôi t·ª•
        primal_residual = np.linalg.norm(M - L - S, 'fro')
        if primal_residual < tol:
            break
    
    return L, S

def soft_threshold_matrix(X, threshold):
    """Element-wise soft-thresholding"""
    return np.sign(X) * np.maximum(np.abs(X) - threshold, 0)
```

**Performance comparison:**
```python
# Compare different algorithms
methods = {
    'Subgradient': svm_subgradient,
    'SGD': svm_sgd,
    'Coordinate Descent': coordinate_descent_lasso,
    'Proximal Gradient': matrix_completion_proximal
}

# Benchmark on synthetic data
results = {}
for name, method in methods.items():
    start_time = time.time()
    solution = method(X_train, y_train, **params)
    end_time = time.time()
    
    results[name] = {
        'solution': solution,
        'time': end_time - start_time,
        'objective': compute_objective(solution)
    }
```

**Complexity analysis:**
- **Subgradient method:** $$O(1/\epsilon^2)$$ iterations
- **Proximal gradient:** $$O(1/\epsilon)$$ for smooth + non-smooth
- **ADMM:** Linear convergence for convex problems
- **Coordinate descent:** Often faster in practice for sparse problems

</details>

---

## üìù **B√†i t·∫≠p 7: Convergence Analysis**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Section 7.4)
Ph√¢n t√≠ch t√≠nh ch·∫•t h·ªôi t·ª• c·ªßa c√°c ph∆∞∆°ng ph√°p subgradient:

a) Ch·ª©ng minh h·ªôi t·ª• cho k√≠ch th∆∞·ªõc b∆∞·ªõc h·∫±ng s·ªë
b) Ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª• cho k√≠ch th∆∞·ªõc b∆∞·ªõc gi·∫£m d·∫ßn
c) So s√°nh v·ªõi gradient descent
d) Nghi√™n c·ª©u ·∫£nh h∆∞·ªüng c·ªßa nhi·ªÖu trong subgradient

**Y√™u c·∫ßu:**
1. Ch·ª©ng minh h·ªôi t·ª• ch·∫∑t ch·∫Ω
2. Ph√¢n t√≠ch t·ªëc ƒë·ªô v·ªõi c√°c quy t·∫Øc k√≠ch th∆∞·ªõc b∆∞·ªõc kh√°c nhau
3. Ki·ªÉm ch·ª©ng s·ªë
4. Ph√¢n t√≠ch ƒë·ªô b·ªÅn v·ªØng

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Convergence v·ªõi constant step size**

**Theorem:** Cho convex function $$f$$ v·ªõi $$\|g\| \leq G$$ for all $$g \in \partial f(x)$$. 
V·ªõi constant step size $$t > 0$$:

$$\lim_{k \to \infty} \inf_{i=0,\ldots,k} f(x^{(i)}) \leq f^* + \frac{tG^2}{2}$$

**Proof:**
T·ª´ subgradient inequality:
$$f(x^*) \geq f(x^{(k)}) + g^{(k)T}(x^* - x^{(k)})$$

Rearranging:
$$f(x^{(k)}) - f^* \leq g^{(k)T}(x^{(k)} - x^*)$$

**Key inequality:**
$$\|x^{(k+1)} - x^*\|^2 = \|x^{(k)} - tg^{(k)} - x^*\|^2$$
$$= \|x^{(k)} - x^*\|^2 - 2tg^{(k)T}(x^{(k)} - x^*) + t^2\|g^{(k)}\|^2$$

Substituting subgradient inequality:
$$\|x^{(k+1)} - x^*\|^2 \leq \|x^{(k)} - x^*\|^2 - 2t(f(x^{(k)}) - f^*) + t^2G^2$$

Rearranging:
$$f(x^{(k)}) - f^* \leq \frac{\|x^{(k)} - x^*\|^2 - \|x^{(k+1)} - x^*\|^2 + t^2G^2}{2t}$$

Summing over $$k = 0, \ldots, K-1$$:
$$\sum_{k=0}^{K-1} (f(x^{(k)}) - f^*) \leq \frac{\|x^{(0)} - x^*\|^2 + Kt^2G^2}{2t}$$

Therefore:
$$\min_{k=0,\ldots,K-1} f(x^{(k)}) - f^* \leq \frac{\|x^{(0)} - x^*\|^2 + Kt^2G^2}{2tK}$$

Taking $$K \to \infty$$:
$$\liminf_{k \to \infty} f(x^{(k)}) - f^* \leq \frac{tG^2}{2}$$

**B∆∞·ªõc 2: Diminishing step sizes**

**Theorem:** N·∫øu $$\sum_{k=0}^\infty t_k = \infty$$ v√† $$\sum_{k=0}^\infty t_k^2 < \infty$$, th√¨:
$$\lim_{k \to \infty} \min_{i=0,\ldots,k} f(x^{(i)}) = f^*$$

**Proof sketch:**
T·ª´ key inequality:
$$f(x^{(k)}) - f^* \leq \frac{\|x^{(k)} - x^*\|^2 - \|x^{(k+1)} - x^*\|^2 + t_k^2G^2}{2t_k}$$

Summing:
$$\sum_{k=0}^{K-1} t_k(f(x^{(k)}) - f^*) \leq \frac{\|x^{(0)} - x^*\|^2 + G^2\sum_{k=0}^{K-1} t_k^2}{2}$$

Since $$\sum t_k^2 < \infty$$, right side is bounded.
Since $$\sum t_k = \infty$$, we must have $$\min f(x^{(k)}) \to f^*$$.

**Tri·ªÉn khai:**
```python
def subgradient_convergence_analysis(f, subgrad_f, x_star, x0, 
                                   step_rules, max_iter=1000):
    results = {}
    
    for rule_name, step_func in step_rules.items():
        x = x0.copy()
        f_history = []
        distance_history = []
        
        for k in range(max_iter):
            f_val = f(x)
            f_history.append(f_val)
            distance_history.append(np.linalg.norm(x - x_star))
            
            g = subgrad_f(x)
            t = step_func(k)
            x = x - t * g
        
        results[rule_name] = {
            'f_history': f_history,
            'distance_history': distance_history,
            'final_gap': min(f_history) - f(x_star)
        }
    
    return results

# Different step size rules
step_rules = {
    'Constant': lambda k: 0.01,
    'Diminishing': lambda k: 1.0 / (k + 1),
    'Square Summable': lambda k: 1.0 / np.sqrt(k + 1),
    'Optimal Constant': lambda k: 0.001  # Tuned for specific problem
}
```

**B∆∞·ªõc 3: Comparison v·ªõi gradient descent**

**Gradient Descent:**
- **Convergence rate:** $$O(1/k)$$ for smooth convex functions
- **Strong convexity:** Linear convergence $$O(\rho^k)$$
- **Requires:** Differentiability

**Subgradient Method:**
- **Convergence rate:** $$O(1/\sqrt{k})$$ for diminishing steps
- **No strong convexity benefit:** Still $$O(1/\sqrt{k})$$
- **Advantage:** Works for non-smooth functions

**Numerical comparison:**
```python
def compare_methods(f_smooth, f_nonsmooth, grad_f, subgrad_f, x0):
    # Gradient descent on smooth part
    x_gd = x0.copy()
    gd_history = []
    
    # Subgradient method on non-smooth
    x_sg = x0.copy()
    sg_history = []
    
    for k in range(1000):
        # Gradient descent
        if k > 0:  # Skip first iteration for fair comparison
            g_gd = grad_f(x_gd)
            x_gd = x_gd - 0.01 * g_gd
        gd_history.append(f_smooth(x_gd))
        
        # Subgradient method
        g_sg = subgrad_f(x_sg)
        x_sg = x_sg - (1.0 / np.sqrt(k + 1)) * g_sg
        sg_history.append(f_nonsmooth(x_sg))
    
    return gd_history, sg_history
```

**B∆∞·ªõc 4: Effect c·ªßa noise**

**Noisy subgradient:** $$\tilde{g}^{(k)} = g^{(k)} + \xi^{(k)}$$
v·ªõi $$\mathbb{E}[\xi^{(k)}] = 0$$, $$\mathbb{E}[\|\xi^{(k)}\|^2] \leq \sigma^2$$

**Modified convergence:**
$$\mathbb{E}[f(x^{(k)})] - f^* \leq \frac{\|x^{(0)} - x^*\|^2 + (G^2 + \sigma^2)\sum_{i=0}^{k-1} t_i^2}{2\sum_{i=0}^{k-1} t_i}$$

**Robustness analysis:**
```python
def noisy_subgradient_method(f, subgrad_f, x0, noise_level=0.1, max_iter=1000):
    x = x0.copy()
    f_history = []
    
    for k in range(max_iter):
        f_history.append(f(x))
        
        # Clean subgradient
        g = subgrad_f(x)
        
        # Add noise
        noise = np.random.normal(0, noise_level, size=g.shape)
        g_noisy = g + noise
        
        # Step
        t = 1.0 / np.sqrt(k + 1)
        x = x - t * g_noisy
    
    return x, f_history

# Compare different noise levels
noise_levels = [0.0, 0.1, 0.5, 1.0]
results = {}

for noise in noise_levels:
    x_final, f_hist = noisy_subgradient_method(f, subgrad_f, x0, noise)
    results[f'noise_{noise}'] = {
        'final_gap': min(f_hist) - f_optimal,
        'convergence': f_hist
    }
```

**Practical implications:**
1. **Step size selection:** Critical for performance
2. **Averaging:** $$\bar{x}^{(k)} = \frac{1}{k}\sum_{i=1}^k x^{(i)}$$ often better
3. **Restart strategies:** Periodic restarts can help
4. **Adaptive methods:** Adjust step size based on progress

**M·∫πo tri·ªÉn khai:**
```python
def adaptive_subgradient_method(f, subgrad_f, x0, max_iter=1000):
    x = x0.copy()
    best_x = x.copy()
    best_f = f(x)
    
    # Adaptive step size
    step_size = 1.0
    decrease_factor = 0.8
    patience = 10
    no_improvement = 0
    
    for k in range(max_iter):
        g = subgrad_f(x)
        x_new = x - step_size * g
        f_new = f(x_new)
        
        if f_new < best_f:
            best_f = f_new
            best_x = x_new.copy()
            no_improvement = 0
        else:
            no_improvement += 1
            
        if no_improvement >= patience:
            step_size *= decrease_factor
            no_improvement = 0
        
        x = x_new
    
    return best_x
```

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi t√≠nh subgradient:**
- X√°c ƒë·ªãnh c√°c ƒëi·ªÉm kh√¥ng kh·∫£ vi tr∆∞·ªõc
- S·ª≠ d·ª•ng quy t·∫Øc t·ªï h·ª£p tuy·∫øn t√≠nh v√† max
- Ki·ªÉm ch·ª©ng b·∫±ng ƒë·ªãnh nghƒ©a khi c√≥ th·ªÉ
- Ch√∫ √Ω ƒë·∫øn gi·∫£i th√≠ch h√¨nh h·ªçc

#### **Khi l·∫≠p tr√¨nh ph∆∞∆°ng ph√°p subgradient:**
- Ch·ªçn quy t·∫Øc k√≠ch th∆∞·ªõc b∆∞·ªõc ph√π h·ª£p
- Gi√°m s√°t h·ªôi t·ª• c·ªßa gi√° tr·ªã h√†m m·ª•c ti√™u
- Xem x√©t l·∫•y trung b√¨nh c√°c ƒëi·ªÉm l·∫∑p
- X·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ s·ªë (gradient b·∫±ng 0, v.v.)

#### **Khi gi·∫£i c√°c b√†i to√°n th·ª±c t·∫ø:**
- Khai th√°c c·∫•u tr√∫c b√†i to√°n (t√≠nh t√°ch ƒë∆∞·ª£c, t√≠nh th∆∞a)
- S·ª≠ d·ª•ng c√°c thu·∫≠t to√°n chuy√™n bi·ªát khi c√≥ th·ªÉ
- So s√°nh v·ªõi x·∫•p x·ªâ tr∆°n
- Ki·ªÉm ch·ª©ng k·∫øt qu·∫£ v·ªõi c√°c nghi·ªám ƒë√£ bi·∫øt

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 7: Subgradients and Subdifferentials
   - Exercises 7.1-7.15

2. **Bertsekas, D. P.** (2009). *Convex Optimization Theory*. Athena Scientific.

3. **Shor, N. Z.** (1985). *Minimization Methods for Non-Differentiable Functions*. Springer-Verlag.

4. **Nesterov, Y.** (2004). *Introductory Lectures on Convex Optimization*. Kluwer Academic Publishers.
