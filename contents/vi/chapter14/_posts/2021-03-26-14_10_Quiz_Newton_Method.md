---
layout: post
title: 14-10 B√†i t·∫≠p tr·∫Øc nghi·ªám - Ph∆∞∆°ng Ph√°p Newton
chapter: '14'
order: 17
owner: GitHub Copilot
lang: vi
categories:
- chapter14
lesson_type: quiz
---

## üìö √în t·∫≠p l√Ω thuy·∫øt

Tr∆∞·ªõc khi l√†m b√†i t·∫≠p, h√£y √¥n l·∫°i c√°c kh√°i ni·ªám ch√≠nh trong ch∆∞∆°ng n√†y:

### ‚ö° **Newton Method (Ph∆∞∆°ng Ph√°p Newton)**

#### **T·ª´ First-Order ƒë·∫øn Second-Order**
- **First-order methods:** Gradient Descent - s·ª≠ d·ª•ng $$\nabla f(x)$$
- **Second-order methods:** Newton Method - s·ª≠ d·ª•ng $$\nabla^2 f(x)$$ (Hessian)
- **√ù t∆∞·ªüng:** Thay v√¨ x·∫•p x·ªâ Hessian b·∫±ng $$\frac{1}{t}I$$, t√≠nh ch√≠nh x√°c $$\nabla^2 f(x)$$

#### **B√†i to√°n**
$$\min_x f(x)$$ v·ªõi $$f$$ convex, twice differentiable, $$\text{dom}(f) = \mathbb{R}^n$$

#### **Newton update rule**
$$x^{(k)} = x^{(k-1)} - (\nabla^2 f(x^{(k-1)}))^{-1} \nabla f(x^{(k-1)})$$

**So s√°nh v·ªõi Gradient Descent:**
- **GD:** $$x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)})$$
- **Newton:** $$x^{(k)} = x^{(k-1)} - (\nabla^2 f(x^{(k-1)}))^{-1} \nabla f(x^{(k-1)})$$

### üîç **Gi·∫£i Th√≠ch v√† T√≠nh Ch·∫•t**

#### **Newton step interpretation**
**Newton direction:** $$\Delta x_{nt} = -(\nabla^2 f(x))^{-1} \nabla f(x)$$

**Geometric interpretation:**
- Minimize quadratic approximation: $$f(x) + \nabla f(x)^T (y-x) + \frac{1}{2}(y-x)^T \nabla^2 f(x) (y-x)$$
- Solution: $$y^* = x - (\nabla^2 f(x))^{-1} \nabla f(x)$$

#### **Root finding connection**
Newton method cho optimization ‚â° Newton-Raphson cho root finding c·ªßa $$\nabla f(x) = 0$$

**Newton-Raphson:** $$x^{(k)} = x^{(k-1)} - \frac{g(x^{(k-1)})}{g'(x^{(k-1)})}$$

**For $$g(x) = \nabla f(x)$$:** $$x^{(k)} = x^{(k-1)} - (\nabla^2 f(x^{(k-1)}))^{-1} \nabla f(x^{(k-1)})$$

#### **Affine invariance**
**T√≠nh ch·∫•t quan tr·ªçng:** Newton method kh√¥ng thay ƒë·ªïi d∆∞·ªõi affine transformation

**N·∫øu:** $$g(y) = f(Ay + b)$$ v·ªõi $$A$$ nonsingular
**Th√¨:** Newton steps cho $$g$$ v√† $$f$$ t∆∞∆°ng ƒë∆∞∆°ng

**√ù nghƒ©a:** Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi conditioning nh∆∞ Gradient Descent

### üìê **Newton Decrement**

#### **ƒê·ªãnh nghƒ©a**
$$\lambda(x) = (\nabla f(x)^T (\nabla^2 f(x))^{-1} \nabla f(x))^{1/2}$$

#### **√ù nghƒ©a v·∫≠t l√Ω**
1. **Approximation quality:** $$\frac{1}{2}\lambda^2(x) = f(x) - \min_y \hat{f}(y)$$
   - $$\hat{f}(y)$$: quadratic approximation c·ªßa $$f$$ t·∫°i $$x$$
   - $$\frac{1}{2}\lambda^2(x)$$: upper bound cho $$f(x) - f^*$$

2. **Step length:** $$\lambda(x) = \lVert \Delta x_{nt} \rVert_{\nabla^2 f(x)}$$ (Hessian norm)

3. **Mahalanobis distance:** Distance t·ª´ current point ƒë·∫øn Newton step

#### **Alternative expressions**
- $$\lambda(x) = (\Delta x_{nt}^T \nabla^2 f(x) \Delta x_{nt})^{1/2}$$
- $$\nabla f(x)^T \Delta x_{nt} = -\lambda^2(x)$$

#### **Affine invariance**
Newton decrement c≈©ng affine invariant: $$\lambda_g(y) = \lambda_f(x)$$

### üéØ **Damped Newton Method**

#### **V·∫•n ƒë·ªÅ v·ªõi Pure Newton**
- **Kh√¥ng ƒë·∫£m b·∫£o convergence:** $$f(x^{(k)})$$ c√≥ th·ªÉ tƒÉng
- **Ch·ªâ c√≥ local convergence:** C·∫ßn starting point g·∫ßn optimal

#### **Backtracking line search**
**Algorithm:**
1. Compute Newton direction: $$v = -(\nabla^2 f(x))^{-1} \nabla f(x)$$
2. **Backtracking:** v·ªõi parameters $$0 < \alpha \le 1/2$$, $$0 < \beta < 1$$
   ```
   t = 1
   while f(x + tv) > f(x) + Œ± t ‚àáf(x)^T v:
       t = Œ≤t
   ```
3. **Update:** $$x^+ = x + tv$$

#### **Armijo condition**
$$f(x + tv) \le f(x) + \alpha t \nabla f(x)^T v$$

v·ªõi $$\nabla f(x)^T v = -\lambda^2(x) < 0$$

### üìä **Convergence Analysis**

#### **Assumptions**
1. **$$\nabla f$$ Lipschitz:** $$\lVert \nabla f(x) - \nabla f(y) \rVert_2 \le L \lVert x-y \rVert_2$$
2. **Strong convexity:** $$mI \preceq \nabla^2 f(x) \preceq LI$$
3. **$$\nabla^2 f$$ Lipschitz:** $$\lVert \nabla^2 f(x) - \nabla^2 f(y) \rVert_2 \le M \lVert x-y \rVert_2$$

#### **Two-phase convergence**
**Phase I - Damped phase:** $$\lVert \nabla f(x^{(k)}) \rVert_2 \ge \eta$$
$$f(x^{(k+1)}) - f(x^{(k)}) \le -\gamma$$

**Phase II - Pure phase:** $$\lVert \nabla f(x^{(k)}) \rVert_2 < \eta$$
$$\frac{M}{2m^2} \lVert \nabla f(x^{(k+1)}) \rVert_2 \le \left(\frac{M}{2m^2} \lVert \nabla f(x^{(k)}) \rVert_2\right)^2$$

#### **Convergence rates**
**Phase I:** Linear decrease trong function value
$$f(x^{(k)}) - f^* \le (f(x^{(0)}) - f^*) - \gamma k$$

**Phase II:** Quadratic convergence
$$f(x^{(k)}) - f^* \le \frac{2m^3}{M^2}\left(\frac{1}{2}\right)^{2^{k-k_0+1}}$$

#### **Total complexity**
$$\frac{f(x^{(0)}) - f^*}{\gamma} + \log\log\left(\frac{\epsilon_0}{\epsilon}\right)$$

**Practical estimate:** $$\frac{f(x^{(0)}) - f^*}{\gamma} + 6$$

### üåü **Self-Concordant Functions**

#### **Motivation**
**Problems v·ªõi standard analysis:**
1. **Hard to find constants:** $$L, m, M$$ kh√≥ x√°c ƒë·ªãnh trong th·ª±c t·∫ø
2. **Not affine invariant:** Analysis ph·ª• thu·ªôc coordinate system

#### **Self-concordance definition**
H√†m $$f$$ l√† **self-concordant** n·∫øu:
$$|f'''(x)| \le 2(f''(x))^{3/2}$$ (univariate case)

**Multivariate:** $$|D^3 f(x)[h,h,h]| \le 2(h^T \nabla^2 f(x) h)^{3/2}$$

#### **Key properties**
1. **Affine invariant:** Self-concordance preserved under affine transformation
2. **No constants needed:** Analysis kh√¥ng c·∫ßn $$L, m, M$$
3. **Barrier functions:** Logarithmic barriers are self-concordant

#### **Convergence for self-concordant**
**Simpler analysis:** Ch·ªâ c·∫ßn Newton decrement $$\lambda(x)$$
- **Damped phase:** $$\lambda(x) \ge \eta$$
- **Pure phase:** $$\lambda(x) < \eta$$

### üîÑ **Newton vs Gradient Descent**

#### **Computational comparison**

| Aspect | Newton Method | Gradient Descent |
|--------|---------------|------------------|
| **Memory** | $$O(n^2)$$ (Hessian storage) | $$O(n)$$ (gradient storage) |
| **Computation** | $$O(n^3)$$ (linear system) | $$O(n)$$ (vector operations) |
| **Convergence** | Quadratic (near optimum) | Linear |
| **Conditioning** | Affine invariant | Sensitive to conditioning |
| **Robustness** | Sensitive to numerical errors | More robust |

#### **When to use Newton**
**Advantages:**
- **Fast convergence:** Quadratic near optimum
- **Affine invariant:** Not affected by conditioning
- **Fewer iterations:** Especially for high accuracy

**Disadvantages:**
- **Expensive per iteration:** $$O(n^3)$$ cost
- **Memory intensive:** $$O(n^2)$$ storage
- **Numerical sensitivity:** Hessian inversion issues

#### **Practical considerations**
- **Small to medium $$n$$:** Newton often better
- **Large $$n$$:** Gradient descent or quasi-Newton
- **High accuracy needed:** Newton's quadratic convergence wins
- **Ill-conditioned:** Newton's affine invariance helps

### üõ†Ô∏è **Implementation Details**

#### **Computing Newton step**
1. **Form Hessian:** $$H = \nabla^2 f(x)$$
2. **Solve linear system:** $$H \Delta x = -\nabla f(x)$$
3. **Don't invert explicitly:** Use Cholesky, LU, or CG

#### **Numerical considerations**
- **Hessian positive definite:** Ensure convexity
- **Regularization:** Add $$\epsilon I$$ if needed
- **Condition number:** Monitor $$\kappa(H)$$
- **Line search:** Always use backtracking for global convergence

#### **Stopping criteria**
1. **Newton decrement:** $$\lambda^2(x)/2 < \epsilon$$
2. **Gradient norm:** $$\lVert \nabla f(x) \rVert_2 < \epsilon$$
3. **Function decrease:** $$|f(x^{(k)}) - f(x^{(k-1)})| < \epsilon$$

### üéØ **Special Cases v√† Extensions**

#### **Quasi-Newton methods**
**Motivation:** Avoid computing exact Hessian

**BFGS update:**
$$B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$$

v·ªõi $$s_k = x_{k+1} - x_k$$, $$y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$$

**L-BFGS:** Limited memory version cho large-scale problems

#### **Trust region methods**
**Alternative to line search:**
$$\min_p f(x) + \nabla f(x)^T p + \frac{1}{2}p^T \nabla^2 f(x) p$$
subject to $$\lVert p \rVert \le \Delta$$

### üí° **V√≠ D·ª• Minh H·ªça**

#### **Logistic regression**
**Problem:** $$\min_\beta \sum_{i=1}^n \log(1 + \exp(-y_i x_i^T \beta))$$

**Gradient:** $$\nabla f(\beta) = -\sum_{i=1}^n \frac{y_i x_i}{1 + \exp(y_i x_i^T \beta)}$$

**Hessian:** $$\nabla^2 f(\beta) = \sum_{i=1}^n \frac{\exp(y_i x_i^T \beta)}{(1 + \exp(y_i x_i^T \beta))^2} x_i x_i^T$$

**Newton step:** Solve $$\nabla^2 f(\beta) \Delta \beta = -\nabla f(\beta)$$

#### **Quadratic function**
**Problem:** $$f(x) = \frac{1}{2}x^T Q x + b^T x + c$$ v·ªõi $$Q \succ 0$$

**Newton step:** $$\Delta x = -Q^{-1}(Qx + b) = -(x + Q^{-1}b)$$

**One-step convergence:** $$x^{(1)} = x^{(0)} - (x^{(0)} + Q^{-1}b) = -Q^{-1}b = x^*$$

### üéØ **K·∫øt N·ªëi V·ªõi C√°c Ch∆∞∆°ng**

#### **T·ª´ ch∆∞∆°ng tr∆∞·ªõc:**
- **Ch∆∞∆°ng 06:** Gradient Descent - first-order foundation
- **Ch∆∞∆°ng 13:** Duality uses - computational efficiency
- **Ch∆∞∆°ng 12:** KKT conditions - optimality characterization

#### **T·∫ßm quan tr·ªçng c·ªßa Newton Method:**
- **Second-order information:** S·ª≠ d·ª•ng curvature information
- **Quadratic convergence:** Fastest local convergence rate
- **Affine invariance:** Robust to problem conditioning
- **Interior point foundation:** Basis cho barrier methods

#### **H∆∞·ªõng ph√°t tri·ªÉn:**
- **Ch∆∞∆°ng 15:** Barrier methods - Newton for constrained problems
- **Interior point methods:** Polynomial-time algorithms
- **Quasi-Newton methods:** Practical approximations
- **Trust region methods:** Alternative globalization strategy

### üåü **√ù Nghƒ©a L√Ω Thuy·∫øt**

#### **Newton nh∆∞ optimal second-order method:**
- **Natural step:** Minimizes quadratic approximation
- **Affine invariant:** Coordinate-free algorithm
- **Optimal convergence:** Fastest possible local rate
- **Self-correcting:** Adapts to local curvature

#### **Computational trade-offs:**
- **Per-iteration cost:** $$O(n^3)$$ vs $$O(n)$$ for gradient
- **Total iterations:** Much fewer than first-order
- **Memory requirements:** $$O(n^2)$$ vs $$O(n)$$
- **Numerical stability:** More sensitive than gradient methods

#### **Theoretical foundations:**
- **Quadratic approximation:** Natural extension of linear approximation
- **Self-concordance:** Elegant analysis framework
- **Affine invariance:** Geometric rather than algebraic property
- **Root finding connection:** Unifies optimization v√† equation solving

### üí° **M·∫πo Th·ª±c H√†nh**

#### **When to use Newton:**
1. **Medium-scale problems:** $$n \le 10^3$$ typically
2. **High accuracy needed:** Quadratic convergence valuable
3. **Well-conditioned:** Or when affine invariance helps
4. **Smooth problems:** Hessian computable v√† reliable

#### **Implementation tips:**
- **Never invert Hessian:** Always solve linear system
- **Check positive definiteness:** Ensure Hessian $$\succ 0$$
- **Use regularization:** Add $$\epsilon I$$ if needed
- **Monitor condition number:** Watch for numerical issues
- **Always use line search:** For global convergence

#### **Common pitfalls:**
- **Singular Hessian:** At saddle points or boundaries
- **Expensive Hessian:** When second derivatives costly
- **Poor starting point:** May not reach quadratic phase
- **Numerical errors:** Hessian inversion amplifies errors

### üî¨ **Advanced Topics**

#### **Modified Newton methods:**
- **Regularized Newton:** $$H + \epsilon I$$
- **Levenberg-Marquardt:** Adaptive regularization
- **Cubic regularization:** Third-order methods
- **Inexact Newton:** Approximate linear system solution

#### **Large-scale adaptations:**
- **Quasi-Newton:** BFGS, L-BFGS approximations
- **Gauss-Newton:** For least squares problems
- **Truncated Newton:** CG for linear system
- **Limited memory:** Avoid storing full Hessian

#### **Constrained extensions:**
- **Sequential Quadratic Programming (SQP):** Newton for constrained
- **Interior point methods:** Barrier function approach
- **Augmented Lagrangian:** Penalty method combination
- **Active set methods:** Constraint identification

### üìà **·ª®ng D·ª•ng Th·ª±c T·∫ø**

#### **Machine Learning:**
- **Logistic regression:** Standard Newton application
- **Neural network training:** Second-order optimization
- **Gaussian processes:** Hyperparameter optimization
- **SVM training:** Interior point methods

#### **Statistics:**
- **Maximum likelihood estimation:** Natural Newton application
- **Generalized linear models:** Iteratively reweighted least squares
- **Robust regression:** M-estimators
- **Survival analysis:** Cox regression

#### **Engineering:**
- **Structural optimization:** Design optimization
- **Control systems:** Optimal control problems
- **Signal processing:** Parameter estimation
- **Image processing:** Reconstruction problems

#### **Finance:**
- **Portfolio optimization:** Risk management
- **Option pricing:** Calibration problems
- **Risk modeling:** Parameter estimation
- **Algorithmic trading:** Strategy optimization

### üîÆ **Future Directions**

#### **Emerging developments:**
- **Stochastic Newton:** Online second-order methods
- **Distributed Newton:** Parallel computation
- **Quantum Newton:** Quantum optimization algorithms
- **Adaptive methods:** Problem-specific modifications

#### **Computational advances:**
- **GPU acceleration:** Parallel linear algebra
- **Automatic differentiation:** Efficient Hessian computation
- **Sparse methods:** Structured Hessian exploitation
- **Randomized algorithms:** Sketching techniques

### üíé **Key Insights**

#### **Newton's fundamental contributions:**
1. **Second-order information:** Uses curvature for faster convergence
2. **Affine invariance:** Coordinate-free optimization
3. **Quadratic convergence:** Optimal local convergence rate
4. **Natural algorithm:** Minimizes quadratic approximation

#### **Practical impact:**
- **Interior point revolution:** Enabled polynomial-time algorithms
- **Machine learning:** Foundation for many optimization algorithms
- **Scientific computing:** Standard tool for nonlinear equations
- **Engineering design:** Enables complex optimization problems

### üåü **Legacy c·ªßa ch∆∞∆°ng 14:**

- **Algorithmic breakthrough:** Second-order optimization methods
- **Theoretical foundation:** Convergence analysis framework
- **Practical impact:** Enables solution c·ªßa complex problems
- **Educational value:** Bridges linear algebra v√† optimization
- **Scientific influence:** Foundation cho modern optimization

---

üí° **Th√¥ng ƒëi·ªáp cu·ªëi c√πng:** Newton Method kh√¥ng ch·ªâ l√† faster gradient descent - n√≥ l√† **paradigm shift** t·ª´ first-order sang second-order thinking, enabling quadratic convergence v√† affine invariance cho optimization problems!

---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    ignoreHtmlClass: ".*|",
    processHtmlClass: "arithmatex"
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
/* S·ª≠a l·ªói alignment cho MathJax inline math */
.MathJax {
    vertical-align: baseline !important;
}

mjx-container[jax="CHTML"][display="false"] {
    vertical-align: baseline !important;
    display: inline !important;
}

/* ƒê·∫£m b·∫£o c√°c label trong quiz c√≥ alignment t·ªët */
.options label {
    display: flex;
    align-items: center;
    margin: 8px 0;
    padding: 8px;
    border-radius: 4px;
    transition: background-color 0.2s;
}

.options label:hover {
    background-color: #f0f0f0;
}

.options input[type="radio"] {
    margin-right: 8px;
    flex-shrink: 0;
}

/* Styling cho quiz container */
#quiz-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.question {
    background: #f8f9fa;
    border-left: 4px solid #007bff;
    padding: 20px;
    margin: 20px 0;
    border-radius: 8px;
}

.question h3 {
    color: #2c3e50;
    margin-bottom: 15px;
}

.explanation {
    background: #e8f5e8;
    border: 1px solid #28a745;
    padding: 15px;
    margin-top: 15px;
    border-radius: 6px;
}

.quiz-controls {
    text-align: center;
    margin: 30px 0;
}

.quiz-controls button {
    background: #007bff;
    color: white;
    border: none;
    padding: 12px 24px;
    margin: 0 10px;
    border-radius: 6px;
    cursor: pointer;
    font-size: 16px;
    transition: background-color 0.3s;
}

.quiz-controls button:hover {
    background: #0056b3;
}

.quiz-controls button:disabled {
    background: #6c757d;
    cursor: not-allowed;
}

.progress-bar {
    width: 100%;
    height: 8px;
    background: #e9ecef;
    border-radius: 4px;
    margin: 20px 0;
    overflow: hidden;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #007bff, #28a745);
    transition: width 0.3s ease;
}

.score-display {
    text-align: center;
    font-size: 18px;
    font-weight: bold;
    color: #2c3e50;
    margin: 20px 0;
}

.options label.selected {
    background-color: #e3f2fd;
    border: 2px solid #2196f3;
}

.final-score {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 30px;
    border-radius: 15px;
    text-align: center;
    margin: 30px 0;
}
</style>

B√†i t·∫≠p tr·∫Øc nghi·ªám n√†y ki·ªÉm tra hi·ªÉu bi·∫øt c·ªßa b·∫°n v·ªÅ Newton method, bao g·ªìm second-order optimization, convergence analysis, backtracking line search, self-concordant functions v√† quasi-Newton methods.

---

<div id="quiz-container">
    <div id="quiz-header">
        <h2>üî¢ Quiz: Ph∆∞∆°ng Ph√°p Newton</h2>
        <div class="progress-bar">
            <div class="progress-fill" id="progress-fill" style="width: 5%"></div>
        </div>
        <div class="score-display" id="score-display">C√¢u h·ªèi: <span id="current-q">1</span> / <span id="total-q">20</span></div>
    </div>

    <div id="quiz-content">
        <!-- C√¢u h·ªèi 1: Newton method update -->
        <div class="question" id="q1" style="display: block;">
            <h3>C√¢u 1: C√¥ng th·ª©c c·∫≠p nh·∫≠t c·ªßa Newton method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q1" value="a"> A) \(x^{(k+1)} = x^{(k)} - t_k \nabla f(x^{(k)})\)</label>
                <label><input type="radio" name="q1" value="b"> B) \(x^{(k+1)} = x^{(k)} - t_k [\nabla^2 f(x^{(k)})]^{-1} \nabla f(x^{(k)})\)</label>
                <label><input type="radio" name="q1" value="c"> C) \(x^{(k+1)} = x^{(k)} - [\nabla^2 f(x^{(k)})]^{-1} \nabla f(x^{(k)})\)</label>
                <label><input type="radio" name="q1" value="d"> D) \(x^{(k+1)} = x^{(k)} - \nabla^2 f(x^{(k)}) \nabla f(x^{(k)})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) $$x^{(k+1)} = x^{(k)} - [\nabla^2 f(x^{(k)})]^{-1} \nabla f(x^{(k)})$$</strong><br>
                Pure Newton method s·ª≠ d·ª•ng full Newton step v·ªõi step size = 1, kh√¥ng c·∫ßn line search khi g·∫ßn optimal.
            </div>
        </div>

        <!-- C√¢u h·ªèi 2: Newton direction -->
        <div class="question" id="q2" style="display: none;">
            <h3>C√¢u 2: Newton direction \(\Delta x_{nt}\) ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch gi·∫£i:</h3>
            <div class="options">
                <label><input type="radio" name="q2" value="a"> A) \(\nabla^2 f(x) \Delta x_{nt} = \nabla f(x)\)</label>
                <label><input type="radio" name="q2" value="b"> B) \(\nabla^2 f(x) \Delta x_{nt} = -\nabla f(x)\)</label>
                <label><input type="radio" name="q2" value="c"> C) \(\nabla f(x) \Delta x_{nt} = -\nabla^2 f(x)\)</label>
                <label><input type="radio" name="q2" value="d"> D) \(\Delta x_{nt} = -\nabla f(x)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\nabla^2 f(x) \Delta x_{nt} = -\nabla f(x)$$</strong><br>
                Newton direction ƒë∆∞·ª£c t√¨m b·∫±ng c√°ch gi·∫£i h·ªá tuy·∫øn t√≠nh v·ªõi Hessian matrix v√† negative gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 3: Second-order method -->
        <div class="question" id="q3" style="display: none;">
            <h3>C√¢u 3: Newton method ƒë∆∞·ª£c g·ªçi l√† second-order method v√¨:</h3>
            <div class="options">
                <label><input type="radio" name="q3" value="a"> A) S·ª≠ d·ª•ng ƒë·∫°o h√†m b·∫≠c hai (Hessian)</label>
                <label><input type="radio" name="q3" value="b"> B) H·ªôi t·ª• trong 2 b∆∞·ªõc</label>
                <label><input type="radio" name="q3" value="c"> C) C·∫ßn 2 l·∫ßn t√≠nh gradient</label>
                <label><input type="radio" name="q3" value="d"> D) C√≥ quadratic convergence</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) S·ª≠ d·ª•ng ƒë·∫°o h√†m b·∫≠c hai (Hessian)</strong><br>
                Second-order methods s·ª≠ d·ª•ng th√¥ng tin t·ª´ ƒë·∫°o h√†m b·∫≠c hai, kh√°c v·ªõi first-order methods ch·ªâ d√πng gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 4: Convergence rate -->
        <div class="question" id="q4" style="display: none;">
            <h3>C√¢u 4: T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa Newton method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q4" value="a"> A) Linear</label>
                <label><input type="radio" name="q4" value="b"> B) Superlinear</label>
                <label><input type="radio" name="q4" value="c"> C) Quadratic</label>
                <label><input type="radio" name="q4" value="d"> D) Cubic</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) Quadratic</strong><br>
                Newton method c√≥ quadratic convergence khi g·∫ßn optimal point: $$\lVert x^{(k+1)} - x^* \rVert \leq C \lVert x^{(k)} - x^* \rVert^2$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 5: Newton decrement -->
        <div class="question" id="q5" style="display: none;">
            <h3>C√¢u 5: Newton decrement \(\lambda(x)\) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q5" value="a"> A) \(\lambda(x) = \sqrt{\nabla f(x)^T [\nabla^2 f(x)]^{-1} \nabla f(x)}\)</label>
                <label><input type="radio" name="q5" value="b"> B) \(\lambda(x) = \nabla f(x)^T [\nabla^2 f(x)]^{-1} \nabla f(x)\)</label>
                <label><input type="radio" name="q5" value="c"> C) $$\lambda(x) = \lVert \nabla f(x) \rVert$$</label>
                <label><input type="radio" name="q5" value="d"> D) $$\lambda(x) = \lVert \nabla^2 f(x) \rVert$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\lambda(x) = \sqrt{\nabla f(x)^T [\nabla^2 f(x)]^{-1} \nabla f(x)}$$</strong><br>
                Newton decrement l√† natural stopping criterion cho Newton method, affine invariant.
            </div>
        </div>

        <!-- C√¢u h·ªèi 6: Affine invariance -->
        <div class="question" id="q6" style="display: none;">
            <h3>C√¢u 6: Newton method c√≥ t√≠nh ch·∫•t affine invariant c√≥ nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q6" value="a"> A) Kh√¥ng thay ƒë·ªïi khi transform t·ªça ƒë·ªô affine</label>
                <label><input type="radio" name="q6" value="b"> B) Ch·ªâ ho·∫°t ƒë·ªông v·ªõi affine functions</label>
                <label><input type="radio" name="q6" value="c"> C) C·∫ßn affine constraints</label>
                <label><input type="radio" name="q6" value="d"> D) H·ªôi t·ª• tuy·∫øn t√≠nh</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Kh√¥ng thay ƒë·ªïi khi transform t·ªça ƒë·ªô affine</strong><br>
                Newton method kh√¥ng ph·ª• thu·ªôc v√†o h·ªá t·ªça ƒë·ªô: n·∫øu transform $$x \mapsto Ax + b$$, Newton steps t∆∞∆°ng ·ª©ng c≈©ng transform theo.
            </div>
        </div>

        <!-- C√¢u h·ªèi 7: Backtracking line search -->
        <div class="question" id="q7" style="display: none;">
            <h3>C√¢u 7: Backtracking line search trong Newton method s·ª≠ d·ª•ng:</h3>
            <div class="options">
                <label><input type="radio" name="q7" value="a"> A) Armijo condition v·ªõi Newton direction</label>
                <label><input type="radio" name="q7" value="b"> B) Exact line search</label>
                <label><input type="radio" name="q7" value="c"> C) Fixed step size</label>
                <label><input type="radio" name="q7" value="d"> D) Golden section search</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Armijo condition v·ªõi Newton direction</strong><br>
                Backtracking line search ki·ªÉm tra sufficient decrease condition: $$f(x + t\Delta x_{nt}) \leq f(x) + \alpha t \nabla f(x)^T \Delta x_{nt}$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 8: When Newton method fails -->
        <div class="question" id="q8" style="display: none;">
            <h3>C√¢u 8: Newton method c√≥ th·ªÉ th·∫•t b·∫°i khi:</h3>
            <div class="options">
                <label><input type="radio" name="q8" value="a"> A) Hessian kh√¥ng positive definite</label>
                <label><input type="radio" name="q8" value="b"> B) Starting point xa optimal</label>
                <label><input type="radio" name="q8" value="c"> C) Hessian singular</label>
                <label><input type="radio" name="q8" value="d"> D) T·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p tr√™n</strong><br>
                Newton method c·∫ßn Hessian positive definite, c√≥ th·ªÉ diverge t·ª´ starting point xa, v√† fail khi Hessian singular.
            </div>
        </div>

        <!-- C√¢u h·ªèi 9: Self-concordant functions -->
        <div class="question" id="q9" style="display: none;">
            <h3>C√¢u 9: Self-concordant functions quan tr·ªçng v√¨:</h3>
            <div class="options">
                <label><input type="radio" name="q9" value="a"> A) Convergence analysis kh√¥ng ph·ª• thu·ªôc constants</label>
                <label><input type="radio" name="q9" value="b"> B) Affine invariant analysis</label>
                <label><input type="radio" name="q9" value="c"> C) Barrier functions trong interior point methods</label>
                <label><input type="radio" name="q9" value="d"> D) T·∫•t c·∫£ c√°c l√Ω do tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c l√Ω do tr√™n</strong><br>
                Self-concordant functions gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ constants trong analysis, affine invariant, v√† xu·∫•t hi·ªán t·ª± nhi√™n trong barrier methods.
            </div>
        </div>

        <!-- C√¢u h·ªèi 10: Self-concordance definition -->
        <div class="question" id="q10" style="display: none;">
            <h3>C√¢u 10: H√†m \(f\) l√† self-concordant n·∫øu:</h3>
            <div class="options">
                <label><input type="radio" name="q10" value="a"> A) \(|f'''(x)[h,h,h]| \leq 2(f''(x)[h,h])^{3/2}\)</label>
                <label><input type="radio" name="q10" value="b"> B) \(f''(x) \succ 0\)</label>
                <label><input type="radio" name="q10" value="c"> C) \(f\) l√† convex</label>
                <label><input type="radio" name="q10" value="d"> D) \(f\) l√† smooth</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$|f'''(x)[h,h,h]| \leq 2(f''(x)[h,h])^{3/2}$$</strong><br>
                Self-concordance condition ki·ªÉm so√°t third derivative relative to second derivative.
            </div>
        </div>

        <!-- C√¢u h·ªèi 11: Quasi-Newton methods -->
        <div class="question" id="q11" style="display: none;">
            <h3>C√¢u 11: Quasi-Newton methods kh√°c v·ªõi Newton method ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q11" value="a"> A) Kh√¥ng c·∫ßn t√≠nh Hessian ch√≠nh x√°c</label>
                <label><input type="radio" name="q11" value="b"> B) S·ª≠ d·ª•ng approximation c·ªßa Hessian</label>
                <label><input type="radio" name="q11" value="c"> C) Chi ph√≠ t√≠nh to√°n th·∫•p h∆°n</label>
                <label><input type="radio" name="q11" value="d"> D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</strong><br>
                Quasi-Newton methods approximate Hessian ƒë·ªÉ gi·∫£m computational cost, ph√π h·ª£p khi Hessian expensive to compute.
            </div>
        </div>

        <!-- C√¢u h·ªèi 12: BFGS update -->
        <div class="question" id="q12" style="display: none;">
            <h3>C√¢u 12: BFGS method c·∫≠p nh·∫≠t approximation c·ªßa:</h3>
            <div class="options">
                <label><input type="radio" name="q12" value="a"> A) Hessian matrix</label>
                <label><input type="radio" name="q12" value="b"> B) Inverse Hessian matrix</label>
                <label><input type="radio" name="q12" value="c"> C) Gradient vector</label>
                <label><input type="radio" name="q12" value="d"> D) Objective function</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Inverse Hessian matrix</strong><br>
                BFGS directly updates approximation c·ªßa inverse Hessian, tr√°nh vi·ªác ph·∫£i solve linear system.
            </div>
        </div>

        <!-- C√¢u h·ªèi 13: Computational complexity -->
        <div class="question" id="q13" style="display: none;">
            <h3>C√¢u 13: Computational cost m·ªói iteration c·ªßa Newton method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q13" value="a"> A) \(O(n)\)</label>
                <label><input type="radio" name="q13" value="b"> B) \(O(n^2)\)</label>
                <label><input type="radio" name="q13" value="c"> C) \(O(n^3)\)</label>
                <label><input type="radio" name="q13" value="d"> D) \(O(n^4)\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) $$O(n^3)$$</strong><br>
                Newton method c·∫ßn $$O(n^2)$$ ƒë·ªÉ t√≠nh Hessian v√† $$O(n^3)$$ ƒë·ªÉ solve linear system $$H \Delta x = -g$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 14: Damped Newton method -->
        <div class="question" id="q14" style="display: none;">
            <h3>C√¢u 14: Damped Newton method s·ª≠ d·ª•ng:</h3>
            <div class="options">
                <label><input type="radio" name="q14" value="a"> A) Fixed step size nh·ªè</label>
                <label><input type="radio" name="q14" value="b"> B) Line search v·ªõi Newton direction</label>
                <label><input type="radio" name="q14" value="c"> C) Modified Hessian</label>
                <label><input type="radio" name="q14" value="d"> D) Gradient descent direction</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Line search v·ªõi Newton direction</strong><br>
                Damped Newton method k·∫øt h·ª£p Newton direction v·ªõi line search ƒë·ªÉ ƒë·∫£m b·∫£o global convergence.
            </div>
        </div>

        <!-- C√¢u h·ªèi 15: Trust region vs line search -->
        <div class="question" id="q15" style="display: none;">
            <h3>C√¢u 15: Trust region Newton method kh√°c v·ªõi line search Newton ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q15" value="a"> A) Gi·ªõi h·∫°n step size trong m·ªôt region</label>
                <label><input type="radio" name="q15" value="b"> B) Modify direction thay v√¨ step size</label>
                <label><input type="radio" name="q15" value="c"> C) S·ª≠ d·ª•ng quadratic model</label>
                <label><input type="radio" name="q15" value="d"> D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</strong><br>
                Trust region methods gi·ªõi h·∫°n step trong trusted region, c√≥ th·ªÉ modify direction, v√† s·ª≠ d·ª•ng quadratic model approximation.
            </div>
        </div>

        <!-- C√¢u h·ªèi 16: Newton method for root finding -->
        <div class="question" id="q16" style="display: none;">
            <h3>C√¢u 16: Newton method cho root finding \(g(x) = 0\) c√≥ c√¥ng th·ª©c:</h3>
            <div class="options">
                <label><input type="radio" name="q16" value="a"> A) \(x^{(k+1)} = x^{(k)} - \frac{g(x^{(k)})}{g'(x^{(k)})}\)</label>
                <label><input type="radio" name="q16" value="b"> B) \(x^{(k+1)} = x^{(k)} - g(x^{(k)})\)</label>
                <label><input type="radio" name="q16" value="c"> C) \(x^{(k+1)} = x^{(k)} - g'(x^{(k)})\)</label>
                <label><input type="radio" name="q16" value="d"> D) \(x^{(k+1)} = x^{(k)} + \frac{g(x^{(k)})}{g'(x^{(k)})}\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$x^{(k+1)} = x^{(k)} - \frac{g(x^{(k)})}{g'(x^{(k)})}$$</strong><br>
                Newton method cho root finding l√† tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát khi $$g(x) = \nabla f(x)$$ v√† t√¨m $$\nabla f(x) = 0$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 17: Hessian modification -->
        <div class="question" id="q17" style="display: none;">
            <h3>C√¢u 17: Khi Hessian kh√¥ng positive definite, c√≥ th·ªÉ:</h3>
            <div class="options">
                <label><input type="radio" name="q17" value="a"> A) Th√™m regularization: \(H + \lambda I\)</label>
                <label><input type="radio" name="q17" value="b"> B) S·ª≠ d·ª•ng absolute eigenvalues</label>
                <label><input type="radio" name="q17" value="c"> C) Modified Cholesky decomposition</label>
                <label><input type="radio" name="q17" value="d"> D) T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p tr√™n</strong><br>
                C√≥ nhi·ªÅu c√°ch modify Hessian ƒë·ªÉ ƒë·∫£m b·∫£o positive definiteness: regularization, eigenvalue modification, modified Cholesky.
            </div>
        </div>

        <!-- C√¢u h·ªèi 18: L-BFGS -->
        <div class="question" id="q18" style="display: none;">
            <h3>C√¢u 18: L-BFGS (Limited-memory BFGS) c√≥ ∆∞u ƒëi·ªÉm:</h3>
            <div class="options">
                <label><input type="radio" name="q18" value="a"> A) Memory requirement \(O(n)\) thay v√¨ \(O(n^2)\)</label>
                <label><input type="radio" name="q18" value="b"> B) Ph√π h·ª£p cho large-scale problems</label>
                <label><input type="radio" name="q18" value="c"> C) Kh√¥ng c·∫ßn store full matrix</label>
                <label><input type="radio" name="q18" value="d"> D) T·∫•t c·∫£ c√°c ∆∞u ƒëi·ªÉm tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ∆∞u ƒëi·ªÉm tr√™n</strong><br>
                L-BFGS ch·ªâ store limited history (th∆∞·ªùng 5-20 vectors) thay v√¨ full matrix, r·∫•t hi·ªáu qu·∫£ cho large-scale optimization.
            </div>
        </div>

        <!-- C√¢u h·ªèi 19: Newton method applications -->
        <div class="question" id="q19" style="display: none;">
            <h3>C√¢u 19: Newton method ƒë∆∞·ª£c ·ª©ng d·ª•ng trong:</h3>
            <div class="options">
                <label><input type="radio" name="q19" value="a"> A) Interior point methods</label>
                <label><input type="radio" name="q19" value="b"> B) Neural network training</label>
                <label><input type="radio" name="q19" value="c"> C) Maximum likelihood estimation</label>
                <label><input type="radio" name="q19" value="d"> D) T·∫•t c·∫£ c√°c ·ª©ng d·ª•ng tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ·ª©ng d·ª•ng tr√™n</strong><br>
                Newton method c√≥ ·ª©ng d·ª•ng r·ªông r√£i: interior point methods, second-order optimization trong ML, statistical estimation.
            </div>
        </div>

        <!-- C√¢u h·ªèi 20: Comparison with first-order methods -->
        <div class="question" id="q20" style="display: none;">
            <h3>C√¢u 20: So v·ªõi first-order methods, Newton method:</h3>
            <div class="options">
                <label><input type="radio" name="q20" value="a"> A) √çt iterations h∆°n nh∆∞ng m·ªói iteration expensive h∆°n</label>
                <label><input type="radio" name="q20" value="b"> B) Quadratic convergence vs linear convergence</label>
                <label><input type="radio" name="q20" value="c"> C) Affine invariant vs coordinate dependent</label>
                <label><input type="radio" name="q20" value="d"> D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</strong><br>
                Newton method trade-off: fewer iterations v·ªõi faster convergence, nh∆∞ng m·ªói iteration expensive h∆°n. Affine invariant l√† advantage l·ªõn.
            </div>
        </div>
        <!-- C√¢u h·ªèi 21: Th·ª±c h√†nh -->
        <div class="question" id="q21" style="display: none;">
            <h3>C√¢u 21: Newton method s·ª≠ d·ª•ng th√¥ng tin:</h3>
            <div class="options">
                <label><input type="radio" name="q21" value="a"> A) Gradient v√† Hessian</label>
                <label><input type="radio" name="q21" value="b"> B) Ch·ªâ gradient</label>
                <label><input type="radio" name="q21" value="c"> C) Ch·ªâ Hessian</label>
                <label><input type="radio" name="q21" value="d"> D) Ch·ªâ gi√° tr·ªã h√†m</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Gradient v√† Hessian</strong><br>
                Newton method: $$x^{(k+1)} = x^{(k)} - [\nabla^2 f(x^{(k)})]^{-1} \nabla f(x^{(k)})$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 22: Th·ª±c h√†nh -->
        <div class="question" id="q22" style="display: none;">
            <h3>C√¢u 22: T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa Newton method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q22" value="a"> A) Quadratic (b·∫≠c 2)</label>
                <label><input type="radio" name="q22" value="b"> B) Linear</label>
                <label><input type="radio" name="q22" value="c"> C) Sublinear</label>
                <label><input type="radio" name="q22" value="d"> D) Cubic</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) Quadratic (b·∫≠c 2)</strong><br>
                Newton method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª• quadratic khi g·∫ßn nghi·ªám v√† Hessian kh·∫£ ngh·ªãch.
            </div>
        </div>

        <!-- C√¢u h·ªèi 23: Th·ª±c h√†nh -->
        <div class="question" id="q23" style="display: none;">
            <h3>C√¢u 23: Quasi-Newton methods kh√°c Newton method ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q23" value="a"> A) X·∫•p x·ªâ Hessian thay v√¨ t√≠nh ch√≠nh x√°c</label>
                <label><input type="radio" name="q23" value="b"> B) Kh√¥ng d√πng gradient</label>
                <label><input type="radio" name="q23" value="c"> C) Ch·ªâ cho h√†m tuy·∫øn t√≠nh</label>
                <label><input type="radio" name="q23" value="d"> D) Lu√¥n ch·∫≠m h∆°n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) X·∫•p x·ªâ Hessian thay v√¨ t√≠nh ch√≠nh x√°c</strong><br>
                Quasi-Newton (BFGS, L-BFGS) x√¢y d·ª±ng x·∫•p x·ªâ Hessian t·ª´ th√¥ng tin gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 24: Th·ª±c h√†nh -->
        <div class="question" id="q24" style="display: none;">
            <h3>C√¢u 24: BFGS update formula c·∫≠p nh·∫≠t:</h3>
            <div class="options">
                <label><input type="radio" name="q24" value="a"> A) X·∫•p x·ªâ Hessian ngh·ªãch ƒë·∫£o</label>
                <label><input type="radio" name="q24" value="b"> B) Gradient</label>
                <label><input type="radio" name="q24" value="c"> C) ƒêi·ªÉm hi·ªán t·∫°i</label>
                <label><input type="radio" name="q24" value="d"> D) K√≠ch th∆∞·ªõc b∆∞·ªõc</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) X·∫•p x·ªâ Hessian ngh·ªãch ƒë·∫£o</strong><br>
                BFGS c·∫≠p nh·∫≠t ma tr·∫≠n x·∫•p x·ªâ $$B_k \approx [\nabla^2 f]^{-1}$$ s·ª≠ d·ª•ng th√¥ng tin gradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 25: Th·ª±c h√†nh -->
        <div class="question" id="q25" style="display: none;">
            <h3>C√¢u 25: V·ªõi h√†m $$f(x) = x^2$$, m·ªôt b∆∞·ªõc Newton t·ª´ $$x_0 = 2$$ cho:</h3>
            <div class="options">
                <label><input type="radio" name="q25" value="a"> A) $$x_1 = 0$$</label>
                <label><input type="radio" name="q25" value="b"> B) $$x_1 = 1$$</label>
                <label><input type="radio" name="q25" value="c"> C) $$x_1 = -2$$</label>
                <label><input type="radio" name="q25" value="d"> D) $$x_1 = 2$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$x_1 = 0$$</strong><br>
                $$\nabla f(2) = 4$$, $$\nabla^2 f(2) = 2$$. $$x_1 = 2 - 4/2 = 0$$.
            </div>
        </div>

    </div>

    <div class="quiz-controls">
        <button onclick="previousQuestion()" id="prev-btn" disabled>‚Üê C√¢u tr∆∞·ªõc</button>
        <button onclick="nextQuestion()" id="next-btn">C√¢u ti·∫øp ‚Üí</button>
        <button onclick="submitQuiz()" id="submit-btn" style="display: none;">N·ªôp b√†i</button>
        <button onclick="resetQuiz()" id="reset-btn" style="display: none;">L√†m l·∫°i</button>
    </div>

    <div id="final-result" style="display: none;" class="final-score">
        <h2>üéâ Ho√†n th√†nh Quiz!</h2>
        <p>ƒêi·ªÉm s·ªë c·ªßa b·∫°n: <span id="final-score-text"></span></p>
        <p id="performance-message"></p>
    </div>
</div>

<script>
let currentQuestion = 1;
const totalQuestions = 25;
let userAnswers = {};
let quizSubmitted = false;

// ƒê√°p √°n ƒë√∫ng
const correctAnswers = {
    q1: 'c', q2: 'b', q3: 'a', q4: 'c', q5: 'a',
    q6: 'a', q7: 'a', q8: 'd', q9: 'd', q10: 'a',
    q11: 'd', q12: 'b', q13: 'c', q14: 'b', q15: 'd',
    q16: 'a', q17: 'd', q18: 'd', q19: 'd', q20: 'd'
,
    q21: 'a', q22: 'a', q23: 'a', q24: 'a', q25: 'a'};

function showQuestion(n) {
    // ·∫®n t·∫•t c·∫£ c√¢u h·ªèi
    for (let i = 1; i <= totalQuestions; i++) {
        document.getElementById(`q${i}`).style.display = 'none';
    }
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi hi·ªán t·∫°i
    document.getElementById(`q${n}`).style.display = 'block';
    
    // C·∫≠p nh·∫≠t progress bar
    const progress = (n / totalQuestions) * 100;
    document.getElementById('progress-fill').style.width = progress + '%';
    
    // C·∫≠p nh·∫≠t s·ªë c√¢u h·ªèi
    document.getElementById('current-q').textContent = n;
    
    // C·∫≠p nh·∫≠t n√∫t ƒëi·ªÅu h∆∞·ªõng
    document.getElementById('prev-btn').disabled = (n === 1);
    document.getElementById('next-btn').style.display = (n === totalQuestions) ? 'none' : 'inline-block';
    document.getElementById('submit-btn').style.display = (n === totalQuestions) ? 'inline-block' : 'none';
    
    currentQuestion = n;
}

function nextQuestion() {
    if (currentQuestion < totalQuestions) {
        showQuestion(currentQuestion + 1);
    }
}

function previousQuestion() {
    if (currentQuestion > 1) {
        showQuestion(currentQuestion - 1);
    }
}

function saveAnswer(questionId, answer) {
    userAnswers[questionId] = answer;
    
    // Highlight selected option
    const questionDiv = document.getElementById(questionId);
    const labels = questionDiv.querySelectorAll('.options label');
    labels.forEach(label => {
        label.classList.remove('selected');
        const input = label.querySelector('input[type="radio"]');
        if (input.checked) {
            label.classList.add('selected');
        }
    });
}

function calculateScore() {
    let correct = 0;
    for (let q in correctAnswers) {
        if (userAnswers[q] === correctAnswers[q]) {
            correct++;
        }
    }
    return correct;
}

function submitQuiz() {
    quizSubmitted = true;
    const score = calculateScore();
    const percentage = Math.round((score / totalQuestions) * 100);
    
    // Hi·ªÉn th·ªã t·∫•t c·∫£ gi·∫£i th√≠ch
    for (let i = 1; i <= totalQuestions; i++) {
        const explanation = document.querySelector(`#q${i} .explanation`);
        explanation.style.display = 'block';
        
        // Highlight ƒë√°p √°n ƒë√∫ng v√† sai
        const questionDiv = document.getElementById(`q${i}`);
        const labels = questionDiv.querySelectorAll('.options label');
        labels.forEach(label => {
            const input = label.querySelector('input[type="radio"]');
            const value = input.value;
            
            if (value === correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#d4edda';
                label.style.border = '2px solid #28a745';
            } else if (value === userAnswers[`q${i}`] && value !== correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#f8d7da';
                label.style.border = '2px solid #dc3545';
            }
        });
    }
    
    // Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi
    document.getElementById('final-result').style.display = 'block';
    document.getElementById('final-score-text').textContent = `${score}/${totalQuestions} (${percentage}%)`;
    
    let message = '';
    if (percentage >= 90) {
        message = 'üåü Xu·∫•t s·∫Øc! B·∫°n ƒë√£ th√†nh th·∫°o Newton method v√† second-order optimization!';
    } else if (percentage >= 80) {
        message = 'üëç R·∫•t t·ªët! B·∫°n hi·ªÉu t·ªët v·ªÅ Newton method v√† convergence analysis.';
    } else if (percentage >= 70) {
        message = 'üìö Kh√° ·ªïn! H√£y xem l·∫°i self-concordance v√† quasi-Newton methods.';
    } else if (percentage >= 60) {
        message = 'üí™ C·∫ßn c·∫£i thi·ªán! √în l·∫°i Newton direction v√† backtracking line search.';
    } else {
        message = 'üìñ H√£y h·ªçc l·∫°i t·ª´ ƒë·∫ßu v·ªÅ Newton method nh√©!';
    }
    
    document.getElementById('performance-message').textContent = message;
    
    // ·∫®n n√∫t submit, hi·ªán n√∫t reset
    document.getElementById('submit-btn').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'inline-block';
    
    // Cu·ªôn ƒë·∫øn k·∫øt qu·∫£
    document.getElementById('final-result').scrollIntoView({ behavior: 'smooth' });
}

function resetQuiz() {
    // Reset tr·∫°ng th√°i
    currentQuestion = 1;
    userAnswers = {};
    quizSubmitted = false;
    
    // ·∫®n t·∫•t c·∫£ gi·∫£i th√≠ch
    document.querySelectorAll('.explanation').forEach(exp => {
        exp.style.display = 'none';
    });
    
    // Reset style c·ªßa labels
    document.querySelectorAll('.options label').forEach(label => {
        label.style.backgroundColor = '';
        label.style.border = '';
        label.classList.remove('selected');
    });
    
    // Uncheck t·∫•t c·∫£ radio buttons
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.checked = false;
    });
    
    document.querySelectorAll('.options label').forEach(label => {
        label.classList.remove('selected');
    });
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi ƒë·∫ßu ti√™n
    showQuestion(1);
    
    // Cu·ªôn l√™n ƒë·∫ßu
    document.getElementById('quiz-header').scrollIntoView({ behavior: 'smooth' });
    
    // ·∫®n k·∫øt qu·∫£ cu·ªëi v√† reset button
    document.getElementById('final-result').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'none';
}

// Kh·ªüi t·∫°o b√†i t·∫≠p khi trang ƒë∆∞·ª£c t·∫£i
document.addEventListener('DOMContentLoaded', function() {
    showQuestion(1);
    
    // Th√™m event listener cho t·∫•t c·∫£ radio button
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.addEventListener('change', function() {
            const questionId = this.name;
            const answer = this.value;
            saveAnswer(questionId, answer);
        });
    });
    
    // Render MathJax sau khi DOM ƒë∆∞·ª£c t·∫£i
    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});

// X·ª≠ l√Ω ph√≠m t·∫Øt
document.addEventListener('keydown', function(event) {
    if (quizSubmitted) return;
    
    switch(event.key) {
        case 'ArrowLeft':
            if (currentQuestion > 1) previousQuestion();
            break;
        case 'ArrowRight':
            if (currentQuestion < totalQuestions) nextQuestion();
            break;
        case 'Enter':
            if (currentQuestion === totalQuestions) submitQuiz();
            break;
    }
});
</script>
