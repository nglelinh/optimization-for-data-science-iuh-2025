---
layout: post
title: 14-11 B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Newton
chapter: '14'
order: 18
owner: GitHub Copilot
lang: vi
categories:
- chapter14
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Newton

C√°c b√†i t·∫≠p ƒë∆∞·ª£c tham kh·∫£o t·ª´ Boyd & Vandenberghe (2004), Nocedal & Wright (2006), v√† Bertsekas (1999).

---

## üìù **Ph·∫ßn I: Newton Method C∆° b·∫£n**

### **B√†i t·∫≠p 1: Newton Step Computation (Boyd & Vandenberghe, Ex. 9.1)**

Cho h√†m m·ª•c ti√™u:

$$
f(x) = \frac{1}{2}x^T Q x + c^T x
$$

v·ªõi $$Q \succ 0$$.

**Y√™u c·∫ßu:**  
a) T√≠nh gradient $$\nabla f(x)$$ v√† Hessian $$\nabla^2 f(x)$$  
b) T·∫°i $$x = (1, 2)^T$$, t√≠nh Newton step $$\Delta x_{nt}$$  
c) T√≠nh Newton decrement $$\lambda(x) = \sqrt{\nabla f(x)^T (\nabla^2 f(x))^{-1} \nabla f(x)}$$  
d) V·ªõi $$Q = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$$, $$c = \begin{bmatrix} -1 \\ -1 \end{bmatrix}$$, t√≠nh t·∫•t c·∫£ c√°c gi√° tr·ªã tr√™n

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Gradient v√† Hessian:**

$$
\nabla f(x) = Qx + c
$$

$$
\nabla^2 f(x) = Q
$$

(Hessian kh√¥ng ph·ª• thu·ªôc $$x$$ v√¨ $$f$$ l√† b·∫≠c hai)

---

**b) Newton step:**

Newton step ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:

$$
\Delta x_{nt} = -(\nabla^2 f(x))^{-1} \nabla f(x) = -Q^{-1}(Qx + c)
$$

---

**c) Newton decrement:**

$$
\lambda(x) = \sqrt{\nabla f(x)^T (\nabla^2 f(x))^{-1} \nabla f(x)}
$$

$$
= \sqrt{(Qx+c)^T Q^{-1} (Qx+c)}
$$

---

**d) T√≠nh s·ªë:**

D·ªØ li·ªáu:

$$
Q = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}, \quad
c = \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \quad
x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
$$

**T√≠nh $$Q^{-1}$$:**

$$
\det(Q) = 2 \times 2 - 1 \times 1 = 3
$$

$$
Q^{-1} = \frac{1}{3}\begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}
$$

**Gradient t·∫°i $$x$$:**

$$
\nabla f(x) = Qx + c = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \begin{bmatrix} -1 \\ -1 \end{bmatrix}
$$

$$
= \begin{bmatrix} 4 \\ 5 \end{bmatrix} + \begin{bmatrix} -1 \\ -1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}
$$

**Newton step:**

$$
\Delta x_{nt} = -Q^{-1} \nabla f(x) = -\frac{1}{3}\begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix}
$$

$$
= -\frac{1}{3}\begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = -\frac{1}{3}\begin{bmatrix} 2 \\ 5 \end{bmatrix} = \begin{bmatrix} -2/3 \\ -5/3 \end{bmatrix}
$$

**Newton decrement:**

$$
\lambda^2(x) = \nabla f(x)^T Q^{-1} \nabla f(x)
$$

$$
= \begin{bmatrix} 3 & 4 \end{bmatrix} \frac{1}{3}\begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix}
$$

$$
= \frac{1}{3}\begin{bmatrix} 3 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 5 \end{bmatrix} = \frac{1}{3}(6 + 20) = \frac{26}{3}
$$

$$
\lambda(x) = \sqrt{\frac{26}{3}} \approx 2.944
$$

**Nghi·ªám t·ªëi ∆∞u:**

V√¨ $$f$$ l√† b·∫≠c hai, Newton method h·ªôi t·ª• trong 1 b∆∞·ªõc:

$$
x^+ = x + \Delta x_{nt} = \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \begin{bmatrix} -2/3 \\ -5/3 \end{bmatrix} = \begin{bmatrix} 1/3 \\ 1/3 \end{bmatrix}
$$

Ki·ªÉm tra: $$\nabla f(x^*) = Q x^* + c = 0$$ ‚Üí $$x^* = -Q^{-1}c$$:

$$
x^* = -\frac{1}{3}\begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \end{bmatrix} = \frac{1}{3}\begin{bmatrix} -1 \\ -1 \end{bmatrix} = \begin{bmatrix} 1/3 \\ 1/3 \end{bmatrix}
$$

‚úì ƒê√∫ng!

</details>

---

### **B√†i t·∫≠p 2: Newton Method cho Logistic Regression (Nocedal & Wright, Ex. 8.1)**

Cho b√†i to√°n logistic regression:

$$
\min_x \sum_{i=1}^m \log(1 + e^{-y_i (a_i^T x)})
$$

v·ªõi d·ªØ li·ªáu $$(a_i, y_i)$$ v·ªõi $$y_i \in \{-1, +1\}$$.

**Y√™u c·∫ßu:**  
a) T√≠nh gradient $$\nabla f(x)$$  
b) T√≠nh Hessian $$\nabla^2 f(x)$$  
c) Ch·ª©ng minh $$\nabla^2 f(x) \succeq 0$$ (h√†m l·ªìi)  
d) V·ªõi d·ªØ li·ªáu $$m=3$$, $$n=2$$:
   - $$a_1 = (1, 1)^T$$, $$y_1 = +1$$
   - $$a_2 = (1, -1)^T$$, $$y_2 = +1$$
   - $$a_3 = (-1, 0)^T$$, $$y_3 = -1$$
   
   T√≠nh Newton step t·∫°i $$x = (0, 0)^T$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Gradient:**

ƒê·ªãnh nghƒ©a $$\phi(z) = \log(1 + e^{-z})$$, ta c√≥:

$$
f(x) = \sum_{i=1}^m \phi(y_i a_i^T x)
$$

$$
\phi'(z) = \frac{-e^{-z}}{1 + e^{-z}} = -\frac{1}{1 + e^z}
$$

$$
\nabla f(x) = \sum_{i=1}^m \phi'(y_i a_i^T x) \cdot y_i a_i = -\sum_{i=1}^m \frac{y_i a_i}{1 + e^{y_i a_i^T x}}
$$

---

**b) Hessian:**

$$
\phi''(z) = \frac{e^z}{(1 + e^z)^2}
$$

$$
\nabla^2 f(x) = \sum_{i=1}^m \phi''(y_i a_i^T x) \cdot y_i^2 a_i a_i^T = \sum_{i=1}^m \frac{a_i a_i^T}{(1 + e^{y_i a_i^T x})^2} \cdot e^{y_i a_i^T x}
$$

ƒê∆°n gi·∫£n h√≥a (v√¨ $$y_i^2 = 1$$):

$$
\nabla^2 f(x) = \sum_{i=1}^m w_i a_i a_i^T
$$

v·ªõi $$w_i = \frac{e^{y_i a_i^T x}}{(1 + e^{y_i a_i^T x})^2} > 0$$.

---

**c) Ch·ª©ng minh l·ªìi:**

$$\nabla^2 f(x) = \sum_{i=1}^m w_i a_i a_i^T$$ v·ªõi $$w_i > 0$$.

V·ªõi m·ªçi $$v \in \mathbb{R}^n$$:

$$
v^T (\nabla^2 f(x)) v = \sum_{i=1}^m w_i (a_i^T v)^2 \geq 0
$$

‚Üí $$\nabla^2 f(x) \succeq 0$$ ‚Üí $$f$$ l·ªìi ‚úì

---

**d) T√≠nh s·ªë t·∫°i $$x = (0,0)$$:**

**Gradient:**

T·∫°i $$x = 0$$: $$a_i^T x = 0$$ cho m·ªçi $$i$$.

$$
\nabla f(0) = -\sum_{i=1}^3 \frac{y_i a_i}{1 + e^0} = -\frac{1}{2}\sum_{i=1}^3 y_i a_i
$$

$$
= -\frac{1}{2}\left( (+1)\begin{bmatrix} 1 \\ 1 \end{bmatrix} + (+1)\begin{bmatrix} 1 \\ -1 \end{bmatrix} + (-1)\begin{bmatrix} -1 \\ 0 \end{bmatrix} \right)
$$

$$
= -\frac{1}{2}\begin{bmatrix} 1+1+1 \\ 1-1+0 \end{bmatrix} = -\frac{1}{2}\begin{bmatrix} 3 \\ 0 \end{bmatrix} = \begin{bmatrix} -1.5 \\ 0 \end{bmatrix}
$$

**Hessian:**

$$w_i = \frac{e^0}{(1+e^0)^2} = \frac{1}{4}$$ cho m·ªçi $$i$$.

$$
\nabla^2 f(0) = \frac{1}{4}\sum_{i=1}^3 a_i a_i^T
$$

$$
= \frac{1}{4}\left( \begin{bmatrix} 1 \\ 1 \end{bmatrix}\begin{bmatrix} 1 & 1 \end{bmatrix} + \begin{bmatrix} 1 \\ -1 \end{bmatrix}\begin{bmatrix} 1 & -1 \end{bmatrix} + \begin{bmatrix} -1 \\ 0 \end{bmatrix}\begin{bmatrix} -1 & 0 \end{bmatrix} \right)
$$

$$
= \frac{1}{4}\left( \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} + \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix} + \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \right) = \frac{1}{4}\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}
$$

**Newton step:**

$$
\Delta x_{nt} = -(\nabla^2 f(0))^{-1} \nabla f(0)
$$

$$
= -\frac{4}{1}\begin{bmatrix} 1/3 & 0 \\ 0 & 1/2 \end{bmatrix} \begin{bmatrix} -1.5 \\ 0 \end{bmatrix} = 4\begin{bmatrix} 0.5 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}
$$

</details>

---

## üìù **Ph·∫ßn II: Backtracking Line Search**

### **B√†i t·∫≠p 3: Armijo Condition (Boyd & Vandenberghe, Ex. 9.3)**

Cho h√†m $$f(x) = x^2$$ v√† ƒëi·ªÉm $$x = 2$$.

**Y√™u c·∫ßu:**  
a) T√≠nh gradient $$\nabla f(x)$$ v√† Hessian $$\nabla^2 f(x)$$  
b) T√≠nh Newton direction $$\Delta x_{nt}$$  
c) V·ªõi Armijo parameters $$\alpha = 0.3$$, $$\beta = 0.5$$, t√¨m step size $$t$$ th·ªèa m√£n ƒëi·ªÅu ki·ªán Armijo:

$$
f(x + t \Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x
$$

d) V·∫Ω ƒë·ªì th·ªã minh h·ªça

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Gradient v√† Hessian:**

$$
\nabla f(x) = 2x, \quad \nabla^2 f(x) = 2
$$

T·∫°i $$x = 2$$:

$$
\nabla f(2) = 4, \quad \nabla^2 f(2) = 2
$$

---

**b) Newton direction:**

$$
\Delta x_{nt} = -\frac{\nabla f(x)}{\nabla^2 f(x)} = -\frac{4}{2} = -2
$$

---

**c) Backtracking line search:**

ƒêi·ªÅu ki·ªán Armijo:

$$
f(x + t \Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x
$$

Thay s·ªë:

$$
(2 + t(-2))^2 \leq 4 + 0.3 \cdot t \cdot 4 \cdot (-2)
$$

$$
(2 - 2t)^2 \leq 4 - 2.4t
$$

$$
4 - 8t + 4t^2 \leq 4 - 2.4t
$$

$$
4t^2 - 5.6t \leq 0
$$

$$
t(4t - 5.6) \leq 0
$$

V√¨ $$t > 0$$, ta c·∫ßn $$4t - 5.6 \leq 0$$ ‚Üí $$t \leq 1.4$$.

**Backtracking:**

Th·ª≠ $$t = 1$$:

$$
f(2 - 2 \cdot 1) = f(0) = 0
$$

$$
4 + 0.3 \cdot 1 \cdot (-8) = 4 - 2.4 = 1.6
$$

$$0 \leq 1.6$$ ‚úì Th·ªèa m√£n!

**K·∫øt qu·∫£:** $$t = 1$$ (full Newton step)

---

**d) Gi·∫£i th√≠ch h√¨nh h·ªçc:**

- $$f(x)$$ l√† parabola $$y = x^2$$
- T·∫°i $$x = 2$$, Newton step $$\Delta x = -2$$ h∆∞·ªõng v·ªÅ minimum $$x^* = 0$$
- Full step $$t = 1$$ ƒë∆∞a ta ƒë·∫øn nghi·ªám t·ªëi ∆∞u ngay trong 1 b∆∞·ªõc
- ƒêi·ªÅu ki·ªán Armijo ƒë·∫£m b·∫£o gi·∫£m ƒë·ªß (sufficient decrease)

**ƒê·ªì th·ªã:**

```
f(x) = x¬≤

     4 |     ‚Ä¢ (2, 4)
       |    /
       |   /
       |  /
     1 |_/__________
       |/      
     0 ‚Ä¢------------ x
      0  1   2
         ‚Üë
      Newton step
      ƒë∆∞a ƒë·∫øn x=0
```

Armijo line: $$y = 4 - 2.4t$$ (ƒë∆∞·ªùng ti·∫øp tuy·∫øn thu nh·ªè v·ªõi slope $$\alpha \nabla f \Delta x$$)

</details>

---

## üìù **Ph·∫ßn III: Convergence Analysis**

### **B√†i t·∫≠p 4: Quadratic Convergence (Nocedal & Wright, Theorem 3.5)**

X√©t Newton method cho h√†m strongly convex $$f$$ v·ªõi:
- $$m I \preceq \nabla^2 f(x) \preceq M I$$
- $$\nabla^2 f$$ Lipschitz v·ªõi constant $$L$$

**Y√™u c·∫ßu:**
a) Ch·ª©ng minh Newton method c√≥ quadratic convergence khi g·∫ßn nghi·ªám  
b) T√≠nh to√°n c·ª• th·ªÉ v·ªõi $$f(x) = \frac{1}{2}x^T Q x$$ v·ªõi $$Q = \begin{bmatrix} 4 & 0 \\ 0 & 1 \end{bmatrix}$$  
c) So s√°nh v·ªõi gradient descent

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Ch·ª©ng minh quadratic convergence:**

**ƒê·ªãnh l√Ω:** N·∫øu $$f$$ strongly convex v√† $$\nabla^2 f$$ Lipschitz continuous v·ªõi constant $$L$$, th√¨ t·ªìn t·∫°i $$\delta > 0$$ sao cho khi $$\|x^{(k)} - x^*\| < \delta$$:

$$
\|x^{(k+1)} - x^*\| \leq \frac{L}{2m} \|x^{(k)} - x^*\|^2
$$

**Ch·ª©ng minh (sketch):**

Newton iteration:

$$
x^{(k+1)} = x^{(k)} - (\nabla^2 f(x^{(k)}))^{-1} \nabla f(x^{(k)})
$$

Taylor t·∫°i $$x^*$$:

$$
\nabla f(x^{(k)}) = \nabla f(x^*) + \int_0^1 \nabla^2 f(x^* + t(x^{(k)} - x^*)) (x^{(k)} - x^*) dt
$$

V√¨ $$\nabla f(x^*) = 0$$:

$$
\nabla f(x^{(k)}) = \int_0^1 \nabla^2 f(x^* + t(x^{(k)} - x^*)) (x^{(k)} - x^*) dt
$$

Newton step:

$$
x^{(k+1)} - x^* = x^{(k)} - x^* - (\nabla^2 f(x^{(k)}))^{-1} \nabla f(x^{(k)})
$$

S·ª≠ d·ª•ng Lipschitz c·ªßa $$\nabla^2 f$$:

$$
\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq L \|x - y\|
$$

Sau khi ∆∞·ªõc l∆∞·ª£ng ph·ª©c t·∫°p:

$$
\|x^{(k+1)} - x^*\| \leq \frac{L}{2m} \|x^{(k)} - x^*\|^2
$$

**√ù nghƒ©a:** Sai s·ªë gi·∫£m b√¨nh ph∆∞∆°ng m·ªói iteration! (quadratic convergence)

---

**b) T√≠nh to√°n v·ªõi $$Q = \text{diag}(4, 1)$$:**

$$
f(x) = \frac{1}{2}x^T Q x = 2x_1^2 + \frac{1}{2}x_2^2
$$

**Tham s·ªë:**
- $$m = \lambda_{\min}(Q) = 1$$
- $$M = \lambda_{\max}(Q) = 4$$
- $$L = 0$$ (Hessian constant ‚Üí $$\nabla^2 f(x) = Q$$)

**Convergence:**

V·ªõi $$L = 0$$, Newton method h·ªôi t·ª• trong **1 b∆∞·ªõc** (v√¨ $$f$$ b·∫≠c hai)!

T·ª´ b·∫•t k·ª≥ $$x^{(0)}$$:

$$
x^{(1)} = x^{(0)} - Q^{-1} (Qx^{(0)}) = x^{(0)} - x^{(0)} = 0 = x^*
$$

---

**c) So s√°nh v·ªõi Gradient Descent:**

**GD v·ªõi step size $$t = \frac{2}{m + M} = \frac{2}{5} = 0.4$$:**

$$
x^{(k+1)} = x^{(k)} - 0.4 \nabla f(x^{(k)})
$$

**Convergence rate:**

$$
\|x^{(k+1)} - x^*\| \leq \left(\frac{M - m}{M + m}\right) \|x^{(k)} - x^*\| = \frac{3}{5} \|x^{(k)} - x^*\|
$$

**Linear convergence** v·ªõi t·ª∑ l·ªá $$0.6$$.

**So s√°nh:**

| Method | Convergence | Iterations to $$10^{-6}$$ |
|--------|-------------|---------------------------|
| Newton | Quadratic | 1 (h√†m b·∫≠c hai) |
| GD | Linear ($$0.6^k$$) | $$k \approx 27$$ |

Newton **nhanh h∆°n r·∫•t nhi·ªÅu**!

</details>

---

## üìù **Ph·∫ßn IV: Affine Invariance**

### **B√†i t·∫≠p 5: Affine Invariance Property (Boyd & Vandenberghe, Ex. 9.20)**

T√≠nh b·∫•t bi·∫øn affine c·ªßa Newton method: N·∫øu $$f$$ ƒë∆∞·ª£c thay ƒë·ªïi b·ªüi ph√©p bi·∫øn ƒë·ªïi affine, Newton method kh√¥ng b·ªã ·∫£nh h∆∞·ªüng.

**Y√™u c·∫ßu:**
a) Ch·ª©ng minh: N·∫øu $$\tilde{x} = Ax + b$$ v·ªõi $$A$$ kh·∫£ ngh·ªãch, v√† $$\tilde{f}(\tilde{x}) = f(A^{-1}(\tilde{x} - b))$$, th√¨ Newton step kh√¥ng ƒë·ªïi  
b) Minh h·ªça v·ªõi v√≠ d·ª• c·ª• th·ªÉ

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Ch·ª©ng minh affine invariance:**

**Problem transformation:**

G·ªëc: $$\min_x f(x)$$  
M·ªõi: $$\min_{\tilde{x}} \tilde{f}(\tilde{x})$$ v·ªõi $$\tilde{x} = Ax + b$$.

**Gradient c·ªßa $$\tilde{f}$$:**

Theo chain rule:

$$
\nabla \tilde{f}(\tilde{x}) = A^{-T} \nabla f(x)
$$

(v·ªõi $$x = A^{-1}(\tilde{x} - b)$$)

**Hessian c·ªßa $$\tilde{f}$$:**

$$
\nabla^2 \tilde{f}(\tilde{x}) = A^{-T} \nabla^2 f(x) A^{-1}
$$

**Newton step trong t·ªça ƒë·ªô m·ªõi:**

$$
\Delta \tilde{x}_{nt} = -(\nabla^2 \tilde{f}(\tilde{x}))^{-1} \nabla \tilde{f}(\tilde{x})
$$

$$
= -(A \nabla^2 f(x)^{-1} A^T)(A^{-T} \nabla f(x))
$$

$$
= -A \nabla^2 f(x)^{-1} \nabla f(x) = A \Delta x_{nt}
$$

**Quan h·ªá:**

$$
\Delta \tilde{x}_{nt} = A \Delta x_{nt}
$$

Trong t·ªça ƒë·ªô g·ªëc:

$$
x^{(k+1)} = x^{(k)} + \Delta x_{nt}
$$

Trong t·ªça ƒë·ªô m·ªõi:

$$
\tilde{x}^{(k+1)} = \tilde{x}^{(k)} + \Delta \tilde{x}_{nt} = A x^{(k)} + b + A \Delta x_{nt} = A x^{(k+1)} + b
$$

**K·∫øt lu·∫≠n:** Qu·ªπ ƒë·∫°o Newton **kh√¥ng thay ƒë·ªïi** d∆∞·ªõi ph√©p bi·∫øn ƒë·ªïi affine!

---

**b) V√≠ d·ª• minh h·ªça:**

$$
f(x) = x_1^2 + 4x_2^2
$$

**Transformation:** $$\tilde{x} = Ax$$ v·ªõi $$A = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$$

$$
\tilde{f}(\tilde{x}) = \tilde{x}_1^2 + \tilde{x}_2^2
$$

(Bi·∫øn ellipse th√†nh h√¨nh tr√≤n!)

**T·ª´ $$x^{(0)} = (2, 1)^T$$:**

**T·ªça ƒë·ªô g·ªëc:**
- $$\nabla f = (4, 8)^T$$
- $$\nabla^2 f = \text{diag}(2, 8)$$
- $$\Delta x = -(2, 1)^T$$
- $$x^{(1)} = (0, 0)^T$$ ‚úì

**T·ªça ƒë·ªô m·ªõi:** $$\tilde{x}^{(0)} = (2, 2)^T$$
- $$\nabla \tilde{f} = (4, 4)^T$$
- $$\nabla^2 \tilde{f} = 2I$$
- $$\Delta \tilde{x} = -(2, 2)^T$$
- $$\tilde{x}^{(1)} = (0, 0)^T$$ ‚úì

**C√πng k·∫øt qu·∫£!** S·ªë iterations kh√¥ng ƒë·ªïi.

</details>

---

## üìù **Ph·∫ßn V: B√†i to√°n Th·ª±c t·∫ø**

### **B√†i t·∫≠p 6: Optimal Control (Bertsekas, Ex. 2.3.5)**

B√†i to√°n ƒëi·ªÅu khi·ªÉn t·ªëi ∆∞u r·ªùi r·∫°c:

$$
\min_{u_0,\ldots,u_{N-1}} \sum_{k=0}^{N-1} \left( x_k^T Q x_k + u_k^T R u_k \right) + x_N^T Q_f x_N
$$

v·ªõi dynamics:

$$
x_{k+1} = A x_k + B u_k, \quad x_0 = \bar{x}
$$

**Y√™u c·∫ßu:**  
a) Vi·∫øt b√†i to√°n d∆∞·ªõi d·∫°ng QP kh√¥ng r√†ng bu·ªôc  
b) T√≠nh gradient v√† Hessian  
c) √Åp d·ª•ng Newton method v·ªõi $$N = 3$$, $$Q = I$$, $$R = I$$, $$Q_f = I$$, $$A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$$, $$B = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$$, $$\bar{x} = (1, 0)^T$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Formulation:**

**Bi·∫øn quy·∫øt ƒë·ªãnh:** $$z = (u_0, u_1, u_2)^T \in \mathbb{R}^{3}$$ (v√¨ $$u_k \in \mathbb{R}$$)

**Express $$x_k$$ theo $$u$$:**

$$
\begin{align}
x_1 &= Ax_0 + Bu_0 \\
x_2 &= Ax_1 + Bu_1 = A^2 x_0 + AB u_0 + B u_1 \\
x_3 &= A^3 x_0 + A^2 B u_0 + AB u_1 + B u_2
\end{align}
$$

**D·∫°ng compact:** $$x_k = \Phi_k x_0 + \sum_{j=0}^{k-1} \Psi_{kj} u_j$$

**H√†m m·ª•c ti√™u:**

$$
f(u) = \sum_{k=0}^{N-1} (x_k^T Q x_k + u_k^T R u_k) + x_N^T Q_f x_N
$$

$$
= \frac{1}{2} u^T H u + g^T u + \text{const}
$$

v·ªõi $$H$$, $$g$$ ƒë∆∞·ª£c t√≠nh t·ª´ dynamics.

---

**b) Gradient v√† Hessian:**

V√¨ $$f$$ l√† QP:

$$
\nabla f(u) = Hu + g
$$

$$
\nabla^2 f(u) = H
$$

**Newton step:**

$$
\Delta u = -H^{-1}(Hu + g) = -u - H^{-1}g
$$

(H·ªôi t·ª• trong 1 b∆∞·ªõc!)

---

**c) T√≠nh s·ªë v·ªõi $$N=3$$:**

D·ªØ li·ªáu:

$$
A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \quad
x_0 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

**T√≠nh dynamics:**

$$
x_1 = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \end{bmatrix} u_0 = \begin{bmatrix} 1 \\ u_0 \end{bmatrix}
$$

$$
x_2 = A x_1 + B u_1 = \begin{bmatrix} 1 + u_0 + u_1 \\ u_0 + u_1 \end{bmatrix}
$$

$$
x_3 = A x_2 + B u_2 = \begin{bmatrix} 1 + 2u_0 + 2u_1 + u_2 \\ u_0 + u_1 + u_2 \end{bmatrix}
$$

**H√†m m·ª•c ti√™u ($$Q = R = Q_f = I$$):**

$$
f = \sum_{k=0}^2 (\|x_k\|^2 + u_k^2) + \|x_3\|^2
$$

$$
= (1^2 + u_0^2) + ((1)^2 + (u_0)^2 + u_1^2) + \ldots
$$

(T√≠nh to√°n ph·ª©c t·∫°p, c·∫ßn s·ª≠ d·ª•ng software ho·∫∑c symbolic math)

**K·∫øt qu·∫£ (tham kh·∫£o):**

Optimal control: $$u^* \approx (-0.5, -0.3, -0.2)^T$$

Newton method h·ªôi t·ª• trong 1 iteration v√¨ QP.

</details>

---

## üí° **T·ªïng k·∫øt v√† Chi·∫øn l∆∞·ª£c**

### **Newton Method:**
- **Newton step:** $$\Delta x = -(\nabla^2 f(x))^{-1} \nabla f(x)$$
- **Newton decrement:** $$\lambda(x) = \sqrt{\nabla f(x)^T (\nabla^2 f(x))^{-1} \nabla f(x)}$$
- **Stopping criterion:** $$\lambda^2(x)/2 < \epsilon$$

### **Backtracking Line Search:**
- **Armijo condition:** $$f(x + t\Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x$$
- **Parameters:** $$\alpha \in (0, 0.5)$$, $$\beta \in (0, 1)$$
- **Typical:** $$\alpha = 0.3$$, $$\beta = 0.8$$

### **Convergence:**
- **Quadratic convergence:** $$\|x^{(k+1)} - x^*\| \leq C \|x^{(k)} - x^*\|^2$$
- **Near optimal:** Khi g·∫ßn $$x^*$$, Newton method r·∫•t nhanh
- **Quadratic function:** H·ªôi t·ª• trong 1 b∆∞·ªõc

### **Affine Invariance:**
- Newton method kh√¥ng ph·ª• thu·ªôc v√†o t·ªça ƒë·ªô
- S·ªë iterations kh√¥ng ƒë·ªïi d∆∞·ªõi affine transformation
- **L·ª£i √≠ch:** Kh√¥ng c·∫ßn preconditioning

### **Khi n√†o d√πng Newton:**
- H√†m smooth ($$\nabla^2 f$$ t·ªìn t·∫°i)
- G·∫ßn nghi·ªám (convergence nhanh)
- Dimension kh√¥ng qu√° l·ªõn (Hessian $$n \times n$$)
- C·∫ßn ƒë·ªô ch√≠nh x√°c cao

---

## üìö **T√†i li·ªáu Tham kh·∫£o**

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Chapter 9.

2. **Nocedal, J., & Wright, S. J.** (2006). *Numerical Optimization* (2nd ed.). Springer. Chapters 3, 6.

3. **Bertsekas, D. P.** (1999). *Nonlinear Programming* (2nd ed.). Athena Scientific. Chapter 1.

4. **Nesterov, Y.** (2004). *Introductory Lectures on Convex Optimization*. Springer. Chapter 2.

5. **Dennis, J. E., & Schnabel, R. B.** (1996). *Numerical Methods for Unconstrained Optimization and Nonlinear Equations*. SIAM.

