---
layout: post
title: 14-01 Phương pháp Newton
chapter: '14'
order: 2
owner: Minjoo Lee
categories:
- chapter14
lang: vi
lesson_type: required
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

Các phương pháp gradient descent mà chúng ta đã thảo luận được gọi là **phương pháp bậc nhất** vì các nghiệm dựa trên đạo hàm bậc nhất của hàm số. Phương pháp Newton là một **phương pháp bậc hai**, có nghĩa là nghiệm yêu cầu tính toán đạo hàm bậc hai.


Hãy xem xét một bài toán tối ưu hóa cho hàm số $$f$$ không có ràng buộc, khả vi hai lần, lồi, và có dom($$f$$) = $$\mathbb{R}^{n}$$.

>$$
>\begin{align}
>\min_{x} f(x)
>\end{align}
>$$

Trong [Gradient descent]({% multilang_post_url contents/chapter06/21-03-20-06_00_gradient_descent %}), chúng ta đã thực hiện quy trình sau đây cho hàm số $$f$$:
 
1. Thực hiện xấp xỉ Taylor bậc hai
2. Giả sử ma trận Hessian tương ứng với số hạng đạo hàm bậc hai là $$I/t$$, tức là ma trận đơn vị chia cho t (kích thước bước)
3. Thực hiện xấp xỉ bậc hai để tiến hành bước cập nhật

Quy trình chi tiết được giải thích trong bước cập nhật gradient descent ở trang tiếp theo. Công thức bước cập nhật tại mỗi lần lặp như sau:

>$$
>\begin{align}
>&\text{chọn giá trị khởi tạo } x^{(0)} \in \mathbb{R}^{n},\\\\
>&x^{(k)} = x^{(k-1)} - t_{k} \cdot \nabla f(x^{(k-1)}), \qquad k = 1,2,3,...
>\end{align}
>$$

Phương pháp Newton (phương pháp Newton thuần túy) thực sự tính toán số hạng đạo hàm bậc hai mà được giả sử là $$\frac{1}{t}I$$ trong gradient descent, thực hiện xấp xỉ bậc hai, và tiến hành bước cập nhật. Quy trình này cũng được giải thích trong bước cập nhật phương pháp Newton ở trang tiếp theo. Công thức bước cập nhật tại mỗi lần lặp như sau:

>$$
>\begin{align}
>&\text{chọn giá trị khởi tạo } x^{(0)} \in \mathbb{R}^{n},\\\\
>&x^{(k)} = x^{(k-1)} - \Big(\nabla^{2}f(x^{(k-1)})\Big)^{-1} \nabla f(x^{(k-1)}), \qquad k = 1,2,3,...
>\end{align}
>$$
