---
layout: post
title: 14-01 Newton's method
chapter: '14'
order: 2
owner: Minjoo Lee
categories:
- chapter14
lang: en
lesson_type: required
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

The gradient descent methods we have discussed are called **first-order methods** because the solutions are based on the first derivative of the function. Newton's method is a **second-order method**, meaning the solution requires computing the second derivative.


Let us consider an optimization problem for a function $$f$$ that is unconstrained, twice differentiable, convex, and has dom($$f$$) = $$\mathbb{R}^{n}$$.

>$$
>\begin{align}
>\min_{x} f(x)
>\end{align}
>$$

In [Gradient descent]({% multilang_post_url contents/chapter06/21-03-20-06_00_gradient_descent %}), we performed the following process for this function $$f$$:
 
1. Perform second-order Taylor approximation
2. Assume the Hessian matrix corresponding to the second derivative term as $$I/t$$, i.e., the identity matrix divided by t (step size)
3. Perform quadratic approximation to proceed with the update step

The detailed process is explained in the gradient descent update step on the next page. The update step formula at each iteration is as follows:

>$$
>\begin{align}
>&\text{choose initial } x^{(0)} \in \mathbb{R}^{n},\\\\
>&x^{(k)} = x^{(k-1)} - t_{k} \cdot \nabla f(x^{(k-1)}), \qquad k = 1,2,3,...
>\end{align}
>$$

Newton's method (pure Newton's method) actually computes the second derivative term that was assumed as $$\frac{1}{t}I$$ in gradient descent, performs quadratic approximation, and proceeds with the update step. This process is also explained in the Newton's method update step on the next page. The update step formula at each iteration is as follows:

>$$
>\begin{align}
>&\text{choose initial } x^{(0)} \in \mathbb{R}^{n},\\\\
>&x^{(k)} = x^{(k-1)} - \Big(\nabla^{2}f(x^{(k-1)})\Big)^{-1} \nabla f(x^{(k-1)}), \qquad k = 1,2,3,...
>\end{align}
>$$
