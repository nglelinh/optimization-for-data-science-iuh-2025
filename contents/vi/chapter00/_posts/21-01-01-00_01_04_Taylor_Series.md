---
layout: post
title: 00-01-04 Chuỗi Taylor
chapter: '00'
order: 6
owner: GitHub Copilot
lang: vi
categories:
- chapter00
lesson_type: required
---

Bài học này bao gồm việc khai triển chuỗi Taylor, điều này rất cơ bản để xấp xỉ các hàm và hiểu hành vi cục bộ của các hàm trong các thuật toán tối ưu hóa.

---

## Định nghĩa Chuỗi Taylor

Chuỗi Taylor là một biểu diễn của một hàm dưới dạng tổng vô hạn các số hạng được tính từ giá trị các đạo hàm của hàm tại một điểm duy nhất. Nó cung cấp một cách để xấp xỉ các hàm phức tạp bằng cách sử dụng đa thức.

### Chuỗi Taylor Một Biến

Chuỗi Taylor là một khai triển chuỗi của hàm $$f(x)$$ tại điểm $$a$$:

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n$$

Dưới dạng khai triển:

$$f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \dots$$

### Chuỗi Maclaurin

Khi khai triển tại $$a = 0$$, chuỗi Taylor được gọi là **chuỗi Maclaurin**:

$$f(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \dots$$

### Các Chuỗi Maclaurin Thông Dụng

**Hàm Mũ:**
$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$

**Hàm Sin:**
$$\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \dots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}$$

**Hàm Cosin:**
$$\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}$$

**Logarit Tự Nhiên (với $$|x| < 1$$):**
$$\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \dots = \sum_{n=1}^{\infty} \frac{(-1)^{n+1} x^n}{n}$$

---

## Multivariable Taylor Series

For functions of multiple variables, Taylor series become more complex but follow similar principles. The expansion around a point $$\mathbf{x}_0$$ involves partial derivatives.

### First-Order Taylor Expansion

The **linear approximation** of $$f(\mathbf{x})$$ around $$\mathbf{x}_0$$:

$$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)$$

This is the equation of the tangent plane to the function at $$\mathbf{x}_0$$.

### Second-Order Taylor Expansion

The **quadratic approximation** includes curvature information:

$$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \nabla^2 f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)$$

where $$\nabla^2 f(\mathbf{x}_0)$$ is the Hessian matrix at $$\mathbf{x}_0$$.

### General Form

The complete multivariable Taylor series involves higher-order tensors:

$$f(\mathbf{x}) = \sum_{|\alpha|=0}^{\infty} \frac{D^{\alpha} f(\mathbf{x}_0)}{\alpha!} (\mathbf{x} - \mathbf{x}_0)^{\alpha}$$

where $$\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)$$ is a multi-index.

---

## Applications in Optimization

Taylor series are fundamental to optimization theory and algorithms for several reasons:

### 1. Local Function Approximation

**Linear Approximation (First-Order Methods):**
- Used in gradient descent algorithms
- Assumes the function is approximately linear in a small neighborhood
- Step size must be small enough for the approximation to be valid

**Quadratic Approximation (Second-Order Methods):**
- Used in Newton's method and quasi-Newton methods
- Captures curvature information through the Hessian
- Often provides faster convergence than first-order methods

### 2. Optimality Conditions

**First-Order Necessary Condition:**
At a local minimum $$\mathbf{x}^*$$, we must have $$\nabla f(\mathbf{x}^*) = \mathbf{0}$$.

This comes from the first-order Taylor expansion: if $$\nabla f(\mathbf{x}^*) \neq \mathbf{0}$$, we could move in the direction $$-\nabla f(\mathbf{x}^*)$$ to decrease the function value.

**Second-Order Sufficient Condition:**
If $$\nabla f(\mathbf{x}^*) = \mathbf{0}$$ and $$\nabla^2 f(\mathbf{x}^*)$$ is positive definite, then $$\mathbf{x}^*$$ is a local minimum.

This follows from the second-order Taylor expansion: the quadratic term dominates near $$\mathbf{x}^*$$.

### 3. Algorithm Design

**Newton's Method:**
Uses the second-order Taylor approximation to find the minimum of the quadratic model:

$$\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)$$

**Trust Region Methods:**
Use Taylor approximations within a trusted region where the approximation is believed to be accurate.

**Line Search Methods:**
Use Taylor expansions to determine appropriate step sizes along search directions.

---

## Example: Quadratic Function Analysis

Consider $$f(x, y) = x^2 + 2xy + 3y^2$$ around the point $$(0, 0)$$:

**Gradient:**
$$\nabla f = \begin{pmatrix} 2x + 2y \\ 2x + 6y \end{pmatrix}$$

At $$(0, 0)$$: $$\nabla f(0, 0) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$

**Hessian:**
$$\nabla^2 f = \begin{pmatrix} 2 & 2 \\ 2 & 6 \end{pmatrix}$$

**Second-Order Taylor Expansion around $$(0, 0)$$:**
$$f(x, y) \approx f(0, 0) + 0 + \frac{1}{2} \begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} 2 & 2 \\ 2 & 6 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}$$

$$= 0 + \frac{1}{2}(2x^2 + 4xy + 6y^2) = x^2 + 2xy + 3y^2$$

In this case, the function is exactly quadratic, so the second-order Taylor expansion is exact.

Since the Hessian has eigenvalues $$\lambda_1 = 2 + 2\sqrt{2} > 0$$ and $$\lambda_2 = 2 - 2\sqrt{2} < 0$$, the point $$(0, 0)$$ is a saddle point, not a minimum.

---

## Practical Considerations

### Convergence and Accuracy

1. **Radius of Convergence**: Taylor series only converge within a certain radius from the expansion point
2. **Truncation Error**: Using finite terms introduces approximation errors
3. **Computational Cost**: Higher-order terms require more derivative computations

### Optimization Algorithm Choice

- **First-order methods** (gradient descent): Use only gradient information, slower but cheaper per iteration
- **Second-order methods** (Newton): Use Hessian information, faster convergence but expensive per iteration
- **Quasi-Newton methods**: Approximate the Hessian, balancing speed and computational cost

The Taylor series expansion helps us approximate complex functions with simpler polynomial functions around a specific point, which is vital for optimization algorithms and understanding local behavior of functions.
