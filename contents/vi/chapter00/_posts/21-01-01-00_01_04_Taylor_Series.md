---
layout: post
title: 00-01-04 Taylor Series
chapter: '00'
order: 6
owner: GitHub Copilot
lang: en
categories:
- chapter00
lesson_type: required
---

This lesson covers Taylor series expansions, which are fundamental for approximating functions and understanding the local behavior of functions in optimization algorithms.

---

## Taylor Series Definition

The Taylor series is a representation of a function as an infinite sum of terms calculated from the values of the function's derivatives at a single point. It provides a way to approximate complex functions using polynomials.

### Single Variable Taylor Series

A Taylor series is a series expansion of a function $$f(x)$$ about a point $$a$$:

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n$$

In expanded form:

$$f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \dots$$

### Maclaurin Series

When the expansion is around $$a = 0$$, the Taylor series is called a **Maclaurin series**:

$$f(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \dots$$

### Common Maclaurin Series

**Exponential Function:**
$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$

**Sine Function:**
$$\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \dots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}$$

**Cosine Function:**
$$\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}$$

**Natural Logarithm (for $$|x| < 1$$):**
$$\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \dots = \sum_{n=1}^{\infty} \frac{(-1)^{n+1} x^n}{n}$$

---

## Multivariable Taylor Series

For functions of multiple variables, Taylor series become more complex but follow similar principles. The expansion around a point $$\mathbf{x}_0$$ involves partial derivatives.

### First-Order Taylor Expansion

The **linear approximation** of $$f(\mathbf{x})$$ around $$\mathbf{x}_0$$:

$$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)$$

This is the equation of the tangent plane to the function at $$\mathbf{x}_0$$.

### Second-Order Taylor Expansion

The **quadratic approximation** includes curvature information:

$$f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \nabla^2 f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)$$

where $$\nabla^2 f(\mathbf{x}_0)$$ is the Hessian matrix at $$\mathbf{x}_0$$.

### General Form

The complete multivariable Taylor series involves higher-order tensors:

$$f(\mathbf{x}) = \sum_{|\alpha|=0}^{\infty} \frac{D^{\alpha} f(\mathbf{x}_0)}{\alpha!} (\mathbf{x} - \mathbf{x}_0)^{\alpha}$$

where $$\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)$$ is a multi-index.

---

## Applications in Optimization

Taylor series are fundamental to optimization theory and algorithms for several reasons:

### 1. Local Function Approximation

**Linear Approximation (First-Order Methods):**
- Used in gradient descent algorithms
- Assumes the function is approximately linear in a small neighborhood
- Step size must be small enough for the approximation to be valid

**Quadratic Approximation (Second-Order Methods):**
- Used in Newton's method and quasi-Newton methods
- Captures curvature information through the Hessian
- Often provides faster convergence than first-order methods

### 2. Optimality Conditions

**First-Order Necessary Condition:**
At a local minimum $$\mathbf{x}^*$$, we must have $$\nabla f(\mathbf{x}^*) = \mathbf{0}$$.

This comes from the first-order Taylor expansion: if $$\nabla f(\mathbf{x}^*) \neq \mathbf{0}$$, we could move in the direction $$-\nabla f(\mathbf{x}^*)$$ to decrease the function value.

**Second-Order Sufficient Condition:**
If $$\nabla f(\mathbf{x}^*) = \mathbf{0}$$ and $$\nabla^2 f(\mathbf{x}^*)$$ is positive definite, then $$\mathbf{x}^*$$ is a local minimum.

This follows from the second-order Taylor expansion: the quadratic term dominates near $$\mathbf{x}^*$$.

### 3. Algorithm Design

**Newton's Method:**
Uses the second-order Taylor approximation to find the minimum of the quadratic model:

$$\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)$$

**Trust Region Methods:**
Use Taylor approximations within a trusted region where the approximation is believed to be accurate.

**Line Search Methods:**
Use Taylor expansions to determine appropriate step sizes along search directions.

---

## Example: Quadratic Function Analysis

Consider $$f(x, y) = x^2 + 2xy + 3y^2$$ around the point $$(0, 0)$$:

**Gradient:**
$$\nabla f = \begin{pmatrix} 2x + 2y \\ 2x + 6y \end{pmatrix}$$

At $$(0, 0)$$: $$\nabla f(0, 0) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$

**Hessian:**
$$\nabla^2 f = \begin{pmatrix} 2 & 2 \\ 2 & 6 \end{pmatrix}$$

**Second-Order Taylor Expansion around $$(0, 0)$$:**
$$f(x, y) \approx f(0, 0) + 0 + \frac{1}{2} \begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} 2 & 2 \\ 2 & 6 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}$$

$$= 0 + \frac{1}{2}(2x^2 + 4xy + 6y^2) = x^2 + 2xy + 3y^2$$

In this case, the function is exactly quadratic, so the second-order Taylor expansion is exact.

Since the Hessian has eigenvalues $$\lambda_1 = 2 + 2\sqrt{2} > 0$$ and $$\lambda_2 = 2 - 2\sqrt{2} < 0$$, the point $$(0, 0)$$ is a saddle point, not a minimum.

---

## Practical Considerations

### Convergence and Accuracy

1. **Radius of Convergence**: Taylor series only converge within a certain radius from the expansion point
2. **Truncation Error**: Using finite terms introduces approximation errors
3. **Computational Cost**: Higher-order terms require more derivative computations

### Optimization Algorithm Choice

- **First-order methods** (gradient descent): Use only gradient information, slower but cheaper per iteration
- **Second-order methods** (Newton): Use Hessian information, faster convergence but expensive per iteration
- **Quasi-Newton methods**: Approximate the Hessian, balancing speed and computational cost

The Taylor series expansion helps us approximate complex functions with simpler polynomial functions around a specific point, which is vital for optimization algorithms and understanding local behavior of functions.
