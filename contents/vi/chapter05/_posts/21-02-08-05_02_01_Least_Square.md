---
layout: post
title: 05-02-01 Bài Toán Bình Phương Tối Thiểu Tuyến Tính
chapter: '05'
order: 8
owner: Hooncheol Shin
categories:
- chapter05
lang: vi
lesson_type: required
---

## Bài Toán Bình Phương Tối Thiểu Tuyến Tính

Bài toán bình phương tối thiểu tuyến tính là một bài toán tối ưu không có ràng buộc, trong đó chúng ta tối thiểu hóa tổng các sai số bình phương:

$$\text{minimize}_{x} \quad f_0(x) = \|Ax - b\|_2^2 = \sum_{i=1}^{k} (a_i^T x - b_i)^2$$

**trong đó:**
- $$A \in \mathbb{R}^{k \times n}$$ là một ma trận với $$k \geq n$$
- $$a_i^T$$ là các hàng của $$A$$
- $$x \in \mathbb{R}^n$$ là biến chúng ta muốn tìm
- $$b \in \mathbb{R}^k$$ là vector mục tiêu

**Mục tiêu:** Tìm $$x$$ để tối thiểu hóa tổng các phần dư bình phương.

### Ví dụ: Hồi quy Tuyến tính với hàm một biến

Tìm đường thẳng khớp nhất $$y = mx + c$$ đi qua các điểm dữ liệu. Chúng ta tối thiểu hóa tổng các khoảng cách thẳng đứng bình phương từ các điểm đến đường thẳng.

**Mục tiêu:** Tìm $$m, c$$.

<div id="linear-regression-demo" style="border: 2px solid #ddd; padding: 20px; margin: 20px 0; border-radius: 10px; background-color: #f9f9f9;">
    <h4 style="text-align: center; color: #333;">Minh họa Hồi quy Tuyến tính Tương tác</h4>
    
    <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
        <!-- Canvas for visualization -->
        <div style="flex: 1; min-width: 400px;">
            <canvas id="regressionCanvas" width="400" height="300" style="border: 1px solid #ccc; background: white; cursor: crosshair;"></canvas>
            <p style="font-size: 12px; color: #666; margin-top: 5px;">
                <strong>Hướng dẫn:</strong> Nhấp vào canvas để thêm các điểm dữ liệu. Đường thẳng đỏ hiển thị đường khớp nhất.
            </p>
        </div>
        
        <!-- Controls and information -->
        <div style="flex: 1; min-width: 250px;">
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h5 style="margin-top: 0; color: #444;">Tham số Hồi quy</h5>
                <div id="regression-params" style="font-family: monospace; font-size: 14px; line-height: 1.6;">
                    <div><strong>Hệ số góc (m):</strong> <span id="slope-value">0.000</span></div>
                    <div><strong>Tung độ gốc (c):</strong> <span id="intercept-value">0.000</span></div>
                    <div><strong>Điểm R²:</strong> <span id="r2-value">N/A</span></div>
                    <div><strong>MSE:</strong> <span id="mse-value">N/A</span></div>
                </div>
                
                <div style="margin-top: 15px;">
                    <h5 style="color: #444;">Phương trình</h5>
                    <div id="equation" style="font-family: monospace; font-size: 16px; background: #f0f0f0; padding: 8px; border-radius: 4px;">
                        y = 0.000x + 0.000
                    </div>
                </div>
                
                <div style="margin-top: 15px;">
                    <button onclick="clearPoints()" style="background: #ff6b6b; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; margin-right: 10px;">Xóa Điểm</button>
                    <button onclick="addRandomPoints()" style="background: #4ecdc4; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Thêm Điểm Ngẫu nhiên</button>
                </div>
            </div>
            
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-top: 15px;">
                <h5 style="margin-top: 0; color: #444;">Công thức Toán học</h5>
                <div style="font-size: 13px; line-height: 1.5;">
                    <p><strong>Mục tiêu:</strong> Tối thiểu hóa tổng các phần dư bình phương</p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace;">
                        S(m,c) = Σ(yᵢ - mxᵢ - c)²
                    </div>
                    <p style="margin-top: 10px;"><strong>Nghiệm:</strong></p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace; font-size: 11px;">
                        m = Σ(xᵢ-x̄)(yᵢ-ȳ) / Σ(xᵢ-x̄)²<br>
                        c = ȳ - mx̄
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
// Linear Regression Interactive Demo
class LinearRegressionDemo {
    constructor() {
        this.canvas = document.getElementById('regressionCanvas');
        this.ctx = this.canvas.getContext('2d');
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        
        // Set up canvas
        this.canvas.addEventListener('click', (e) => this.addPoint(e));
        
        // Initialize with some sample points
        this.addRandomPoints();
        this.draw();
    }
    
    addPoint(event) {
        const rect = this.canvas.getBoundingClientRect();
        const x = event.clientX - rect.left;
        const y = event.clientY - rect.top;
        
        // Convert canvas coordinates to data coordinates
        const dataX = (x / this.canvas.width) * 10;
        const dataY = ((this.canvas.height - y) / this.canvas.height) * 10;
        
        this.points.push({x: dataX, y: dataY});
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    addRandomPoints() {
        // Add some sample points with a trend
        const baseSlope = 0.8;
        const baseIntercept = 2;
        
        for (let i = 0; i < 8; i++) {
            const x = Math.random() * 8 + 1;
            const y = baseSlope * x + baseIntercept + (Math.random() - 0.5) * 2;
            this.points.push({x: x, y: Math.max(0, Math.min(10, y))});
        }
        
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    clearPoints() {
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        this.draw();
        this.updateDisplay();
    }
    
    calculateRegression() {
        if (this.points.length < 2) {
            this.slope = 0;
            this.intercept = 0;
            return;
        }
        
        const n = this.points.length;
        const sumX = this.points.reduce((sum, p) => sum + p.x, 0);
        const sumY = this.points.reduce((sum, p) => sum + p.y, 0);
        const sumXY = this.points.reduce((sum, p) => sum + p.x * p.y, 0);
        const sumXX = this.points.reduce((sum, p) => sum + p.x * p.x, 0);
        
        const meanX = sumX / n;
        const meanY = sumY / n;
        
        const numerator = sumXY - n * meanX * meanY;
        const denominator = sumXX - n * meanX * meanX;
        
        if (Math.abs(denominator) < 1e-10) {
            this.slope = 0;
            this.intercept = meanY;
        } else {
            this.slope = numerator / denominator;
            this.intercept = meanY - this.slope * meanX;
        }
    }
    
    calculateR2() {
        if (this.points.length < 2) return 0;
        
        const meanY = this.points.reduce((sum, p) => sum + p.y, 0) / this.points.length;
        let ssRes = 0;
        let ssTot = 0;
        
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            ssRes += Math.pow(point.y - predicted, 2);
            ssTot += Math.pow(point.y - meanY, 2);
        }
        
        return ssTot === 0 ? 1 : 1 - (ssRes / ssTot);
    }
    
    calculateMSE() {
        if (this.points.length === 0) return 0;
        
        let mse = 0;
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            mse += Math.pow(point.y - predicted, 2);
        }
        
        return mse / this.points.length;
    }
    
    draw() {
        // Clear canvas
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw grid
        this.drawGrid();
        
        // Draw regression line
        if (this.points.length >= 2) {
            this.drawRegressionLine();
        }
        
        // Draw points and residuals
        this.drawPoints();
        
        // Draw axes labels
        this.drawLabels();
    }
    
    drawGrid() {
        this.ctx.strokeStyle = '#f0f0f0';
        this.ctx.lineWidth = 1;
        
        // Vertical lines
        for (let i = 0; i <= 10; i++) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.beginPath();
            this.ctx.moveTo(x, 0);
            this.ctx.lineTo(x, this.canvas.height);
            this.ctx.stroke();
        }
        
        // Horizontal lines
        for (let i = 0; i <= 10; i++) {
            const y = (i / 10) * this.canvas.height;
            this.ctx.beginPath();
            this.ctx.moveTo(0, y);
            this.ctx.lineTo(this.canvas.width, y);
            this.ctx.stroke();
        }
    }
    
    drawRegressionLine() {
        this.ctx.strokeStyle = '#ff4757';
        this.ctx.lineWidth = 2;
        
        const x1 = 0;
        const y1 = this.intercept;
        const x2 = 10;
        const y2 = this.slope * x2 + this.intercept;
        
        const canvasX1 = (x1 / 10) * this.canvas.width;
        const canvasY1 = this.canvas.height - (y1 / 10) * this.canvas.height;
        const canvasX2 = (x2 / 10) * this.canvas.width;
        const canvasY2 = this.canvas.height - (y2 / 10) * this.canvas.height;
        
        this.ctx.beginPath();
        this.ctx.moveTo(canvasX1, canvasY1);
        this.ctx.lineTo(canvasX2, canvasY2);
        this.ctx.stroke();
    }
    
    drawPoints() {
        for (const point of this.points) {
            const canvasX = (point.x / 10) * this.canvas.width;
            const canvasY = this.canvas.height - (point.y / 10) * this.canvas.height;
            
            // Draw residual line (vertical distance to regression line)
            if (this.points.length >= 2) {
                const predictedY = this.slope * point.x + this.intercept;
                const predictedCanvasY = this.canvas.height - (predictedY / 10) * this.canvas.height;
                
                this.ctx.strokeStyle = '#ff6b6b';
                this.ctx.lineWidth = 1;
                this.ctx.setLineDash([2, 2]);
                this.ctx.beginPath();
                this.ctx.moveTo(canvasX, canvasY);
                this.ctx.lineTo(canvasX, predictedCanvasY);
                this.ctx.stroke();
                this.ctx.setLineDash([]);
            }
            
            // Draw point
            this.ctx.fillStyle = '#2f3542';
            this.ctx.beginPath();
            this.ctx.arc(canvasX, canvasY, 4, 0, 2 * Math.PI);
            this.ctx.fill();
        }
    }
    
    drawLabels() {
        this.ctx.fillStyle = '#666';
        this.ctx.font = '12px Arial';
        
        // X-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.fillText(i.toString(), x - 5, this.canvas.height - 5);
        }
        
        // Y-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const y = this.canvas.height - (i / 10) * this.canvas.height;
            this.ctx.fillText(i.toString(), 5, y + 3);
        }
    }
    
    updateDisplay() {
        document.getElementById('slope-value').textContent = this.slope.toFixed(3);
        document.getElementById('intercept-value').textContent = this.intercept.toFixed(3);
        document.getElementById('equation').textContent = `y = ${this.slope.toFixed(3)}x + ${this.intercept.toFixed(3)}`;
        
        if (this.points.length >= 2) {
            document.getElementById('r2-value').textContent = this.calculateR2().toFixed(3);
            document.getElementById('mse-value').textContent = this.calculateMSE().toFixed(3);
        } else {
            document.getElementById('r2-value').textContent = 'N/A';
            document.getElementById('mse-value').textContent = 'N/A';
        }
    }
}

// Global functions for buttons
function clearPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
    }
}

function addRandomPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
        window.regressionDemo.addRandomPoints();
    }
}

// Initialize when DOM is loaded
document.addEventListener('DOMContentLoaded', function() {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
});

// Initialize immediately if DOM is already loaded
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
        if (document.getElementById('regressionCanvas')) {
            window.regressionDemo = new LinearRegressionDemo();
        }
    });
} else {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
}
</script>

**Bài toán:** Cho $$n$$ điểm dữ liệu $$(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$$, tìm đường thẳng $$y = mx + c$$ tối thiểu hóa tổng các khoảng cách thẳng đứng bình phương từ các điểm đến đường thẳng.

**Hàm Mục tiêu:** Chúng ta muốn tối thiểu hóa
$$S(m,c) = \sum_{i=1}^{n} (y_i - mx_i - c)^2$$

**Nghiệm:** Để tìm cực tiểu, chúng ta lấy đạo hàm riêng và cho bằng không.

Lấy đạo hàm riêng theo $$c$$:
> $$\frac{\partial S}{\partial c} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-1) = -2\sum_{i=1}^{n} (y_i - mx_i - c) = 0$$

Điều này cho ta:
$$\sum_{i=1}^{n} y_i = m\sum_{i=1}^{n} x_i + nc$$

Do đó:

> $$c = \frac{1}{n}\sum_{i=1}^{n} y_i - m\frac{1}{n}\sum_{i=1}^{n} x_i = \bar{y} - m\bar{x}$$

trong đó $$\bar{x}$$ và $$\bar{y}$$ là giá trị trung bình của các giá trị $$x$$ và $$y$$.

Lấy đạo hàm riêng theo $$m$$:

> $$\frac{\partial S}{\partial m} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-x_i) = -2\sum_{i=1}^{n} x_i(y_i - mx_i - c) = 0$$

Thay thế $$c = \bar{y} - m\bar{x}$$:
$$\sum_{i=1}^{n} x_i(y_i - mx_i - \bar{y} + m\bar{x}) = 0$$

Sắp xếp lại:
$$\sum_{i=1}^{n} x_iy_i - m\sum_{i=1}^{n} x_i^2 - \bar{y}\sum_{i=1}^{n} x_i + m\bar{x}\sum_{i=1}^{n} x_i = 0$$

Vì $$\sum_{i=1}^{n} x_i = n\bar{x}$$ và $$\sum_{i=1}^{n} x_iy_i - \bar{y}\sum_{i=1}^{n} x_i = \sum_{i=1}^{n} x_i(y_i - \bar{y})$$:

> $$\sum_{i=1}^{n} x_i(y_i - \bar{y}) = m\left(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2\right)$$

Lưu ý rằng $$\sum_{i=1}^{n} x_i^2 - n\bar{x}^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2$$

Do đó:

> $$m = \frac{\sum_{i=1}^{n} x_i(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

**Kết quả Cuối cùng:** Đường thẳng khớp nhất có các tham số:

> $$\boxed{m = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \quad \text{và} \quad c = \bar{y} - m\bar{x}}$$

Đây là nghiệm bình phương tối thiểu kinh điển cho hồi quy tuyến tính, cũng được biết đến với tên gọi **Phương trình Chuẩn**.

---

## Nghiệm Tối ưu của Hồi quy Tuyến tính với hàm nhiều biến

**Phát biểu Bài toán:**

Trong Hồi quy Tuyến tính, chúng ta muốn tìm một vector các hệ số $$\mathbf{w}$$ khớp nhất với một mô hình tuyến tính cho một tập dữ liệu cho trước. Chúng ta có $$n$$ điểm dữ liệu, mỗi điểm có $$p$$ đặc trưng.

Gọi $$X$$ là ma trận thiết kế kích thước $$n \times p$$, trong đó mỗi hàng đại diện cho một điểm dữ liệu và mỗi cột đại diện cho một đặc trưng.

Gọi $$\mathbf{y}$$ là vector các giá trị mục tiêu kích thước $$n \times 1$$.

Mô hình tuyến tính của chúng ta dự đoán các giá trị mục tiêu $$\hat{\mathbf{y}}$$ như:
$$\hat{\mathbf{y}} = X\mathbf{w}$$
trong đó $$\mathbf{w}$$ là vector các hệ số chưa biết kích thước $$p \times 1$$.

**Hàm Mục tiêu (Hàm Chi phí):**

Mục tiêu là tối thiểu hóa tổng các sai số bình phương (phần dư) giữa các giá trị mục tiêu thực tế $$\mathbf{y}$$ và các giá trị dự đoán $$\hat{\mathbf{y}}$$. Điều này được biết đến với tên gọi hàm mục tiêu Bình phương Tối thiểu Thông thường (OLS):
$$J(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \| \mathbf{y} - X\mathbf{w} \|^2$$

Chúng ta có thể biểu diễn điều này dưới dạng ma trận bằng cách khai triển chuẩn Euclidean bình phương:
$$J(\mathbf{w}) = (\mathbf{y} - X\mathbf{w})^{\text{T}}(\mathbf{y} - X\mathbf{w})$$

Khai triển biểu thức này:
$$J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - \mathbf{y}^{\text{T}}X\mathbf{w} - (X\mathbf{w})^{\text{T}}\mathbf{y} + (X\mathbf{w})^{\text{T}}X\mathbf{w}$$

Sử dụng tính chất $$(AB)^{\text{T}} = B^{\text{T}}A^{\text{T}}$$, chúng ta có $$(X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}$$.

Ngoài ra, vì $$\mathbf{y}^{\text{T}}X\mathbf{w}$$ là một số vô hướng, chuyển vị của nó chính là nó: $$(\mathbf{y}^{\text{T}}X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y}$$.

Do đó, hai số hạng ở giữa là giống nhau:
$$J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - 2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y} + \mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w}$$

**Tối thiểu hóa sử dụng Giải tích:**

Để tìm $$\mathbf{w}$$ tối ưu tối thiểu hóa $$J(\mathbf{w})$$, chúng ta lấy đạo hàm của $$J(\mathbf{w})$$ theo $$\mathbf{w}$$ và cho bằng không. Chúng ta sử dụng các quy tắc giải tích ma trận sau:

1. $$\frac{\partial (\mathbf{a}^{\text{T}}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{a}$$
2. $$\frac{\partial (\mathbf{x}^{\text{T}}A\mathbf{x})}{\partial \mathbf{x}} = (A + A^{\text{T}})\mathbf{x}$$ (Nếu $$A$$ đối xứng, điều này được rút gọn thành $$2A\mathbf{x}$$)

Áp dụng các quy tắc này cho $$J(\mathbf{w})$$:
$$\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = \frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} - \frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} + \frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}}$$

Hãy tính từng số hạng:
- $$\frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 0$$ (vì $$\mathbf{y}^{\text{T}}\mathbf{y}$$ là một hằng số vô hướng đối với $$\mathbf{w}$$)
- $$\frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 2X^{\text{T}}\mathbf{y}$$ (sử dụng quy tắc 1, với $$\mathbf{a} = X^{\text{T}}\mathbf{y}$$)
- Đối với số hạng thứ ba, đặt $$A = X^{\text{T}}X$$. Lưu ý rằng $$A$$ là ma trận đối xứng vì $$(X^{\text{T}}X)^{\text{T}} = X^{\text{T}}(X^{\text{T}})^{\text{T}} = X^{\text{T}}X$$.
  
  Vậy, $$\frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}} = 2X^{\text{T}}X\mathbf{w}$$ (sử dụng quy tắc 2 cho ma trận đối xứng $$A$$)

Kết hợp các thành phần này, đạo hàm là:

> $$\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = 0 - 2X^{\text{T}}\mathbf{y} + 2X^{\text{T}}X\mathbf{w} = 2X^{\text{T}}X\mathbf{w} - 2X^{\text{T}}\mathbf{y}$$

**Tìm Nghiệm Tối ưu:**

Để tìm cực tiểu của $$J(\mathbf{w})$$, chúng ta cho đạo hàm bằng không:

$$\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = 2X^{\text{T}}X\mathbf{w} - 2X^{\text{T}}\mathbf{y} = 0$$

Chia cho 2 và sắp xếp lại:
$$X^{\text{T}}X\mathbf{w} = X^{\text{T}}\mathbf{y}$$

Đây được gọi là **Phương trình Chuẩn**. Nếu $$X^{\text{T}}X$$ khả nghịch (điều này xảy ra khi $$X$$ có hạng cột đầy đủ), chúng ta có thể giải để tìm $$\mathbf{w}$$:

> $$\boxed{\mathbf{w}^* = (X^{\text{T}}X)^{-1}X^{\text{T}}\mathbf{y}}$$

Đây là **nghiệm dạng đóng** cho hồi quy bình phương tối thiểu tuyến tính, còn được gọi là **nghiệm Phương trình Chuẩn**.

**Tính chất Quan trọng:**

1. **Tính duy nhất:** Nếu $$X^{\text{T}}X$$ khả nghịch, nghiệm là duy nhất.
2. **Ý nghĩa Hình học:** Nghiệm $$\mathbf{w}^*$$ làm cho vector phần dư $$\mathbf{y} - X\mathbf{w}^*$$ vuông góc với không gian cột của $$X$$.
3. **Độ phức tạp Tính toán:** $$O(p^3 + np^2)$$ trong đó $$n$$ là số mẫu và $$p$$ là số đặc trưng.

**Khi $$X^{\text{T}}X$$ không khả nghịch:**
- Điều này xảy ra khi $$X$$ không có hạng cột đầy đủ (tức là một số đặc trưng phụ thuộc tuyến tính)
- Trong trường hợp này, chúng ta có thể sử dụng **nghịch đảo giả Moore-Penrose**: $$\mathbf{w}^* = X^{\dagger}\mathbf{y}$$
- Hoặc thêm điều chuẩn hóa (Ridge regression): $$\mathbf{w}^* = (X^{\text{T}}X + \lambda I)^{-1}X^{\text{T}}\mathbf{y}$$

**Hiệu suất:**
- Độ phức tạp thời gian: khoảng $$n^2k$$ phép toán
- Một máy tính tiêu chuẩn giải quyết các bài toán với hàng trăm biến và hàng nghìn số hạng trong vài giây
- Ma trận thưa (nhiều phần tử bằng không) có thể được giải nhanh hơn nhiều

**Ví dụ:** Một ma trận thưa để xử lý ảnh có thể chỉ có 5 phần tử khác không trên mỗi hàng trong ma trận 10,000 × 10,000.