---
layout: post
title: 05-02-04 H·ªìi quy Logistic
chapter: '05'
order: 8
owner: GitHub Copilot
categories:
- chapter05
lang: vi
lesson_type: required
---

# H·ªìi quy Logistic (Logistic Regression)

H·ªìi quy logistic l√† m·ªôt trong nh·ªØng ·ª©ng d·ª•ng quan tr·ªçng nh·∫•t c·ªßa t·ªëi ∆∞u h√≥a l·ªìi trong machine learning, ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i cho c√°c b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n.

## üéØ B√†i to√°n Ph√¢n lo·∫°i Nh·ªã ph√¢n

Kh√°c v·ªõi h·ªìi quy tuy·∫øn t√≠nh d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c, **ph√¢n lo·∫°i nh·ªã ph√¢n** d·ª± ƒëo√°n x√°c su·∫•t m·ªôt m·∫´u thu·ªôc v·ªÅ m·ªôt trong hai l·ªõp.

**V√≠ d·ª• th·ª±c t·∫ø:**
- **Y t·∫ø**: D·ª± ƒëo√°n b·ªánh nh√¢n c√≥ m·∫Øc b·ªánh hay kh√¥ng (0: kh·ªèe m·∫°nh, 1: b·ªánh)
- **Marketing**: Kh√°ch h√†ng c√≥ mua s·∫£n ph·∫©m hay kh√¥ng (0: kh√¥ng mua, 1: mua)
- **T√†i ch√≠nh**: Email c√≥ ph·∫£i spam hay kh√¥ng (0: kh√¥ng spam, 1: spam)
- **C√¥ng ngh·ªá**: Giao d·ªãch c√≥ gian l·∫≠n hay kh√¥ng (0: h·ª£p l·ªá, 1: gian l·∫≠n)

**D·ªØ li·ªáu hu·∫•n luy·ªán:**
- $$\{(\mathbf{x}_i, y_i)\}_{i=1}^n$$ v·ªõi $$\mathbf{x}_i \in \mathbb{R}^p$$, $$y_i \in \{0, 1\}$$
- $$\mathbf{x}_i$$: vector ƒë·∫∑c tr∆∞ng (features)
- $$y_i$$: nh√£n l·ªõp (class label)

## üìä T·ª´ Linear Regression ƒë·∫øn Logistic Regression

### V·∫•n ƒë·ªÅ v·ªõi Linear Regression cho Ph√¢n lo·∫°i

N·∫øu √°p d·ª•ng h·ªìi quy tuy·∫øn t√≠nh tr·ª±c ti·∫øp: $$\hat{y} = \mathbf{w}^T \mathbf{x} + b$$

**V·∫•n ƒë·ªÅ:**
- $$\hat{y}$$ c√≥ th·ªÉ nh·∫≠n gi√° tr·ªã b·∫•t k·ª≥: $$(-\infty, +\infty)$$
- Nh∆∞ng x√°c su·∫•t ph·∫£i trong kho·∫£ng $$[0, 1]$$
- Kh√¥ng ph√π h·ª£p v·ªõi b·∫£n ch·∫•t x√°c su·∫•t c·ªßa ph√¢n lo·∫°i

### Sigmoid Function - C·∫ßu n·ªëi Quan tr·ªçng

**H√†m Sigmoid:**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**T√≠nh ch·∫•t quan tr·ªçng:**
- **Mi·ªÅn gi√° tr·ªã**: $$\sigma(z) \in (0, 1)$$ $$\forall z \in \mathbb{R}$$
- **ƒê∆°n ƒëi·ªáu tƒÉng**: $$\sigma'(z) > 0$$ $$\forall z$$
- **ƒê·ªëi x·ª©ng**: $$\sigma(-z) = 1 - \sigma(z)$$
- **Gi·ªõi h·∫°n**: $$\lim_{z \to -\infty} \sigma(z) = 0$$, $$\lim_{z \to +\infty} \sigma(z) = 1$$

**ƒê·∫°o h√†m ƒë·∫∑c bi·ªát:**
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$


### M√¥ h√¨nh Logistic Regression

**X√°c su·∫•t d·ª± ƒëo√°n:**
$$P(y = 1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}$$

$$P(y = 0 | \mathbf{x}) = 1 - P(y = 1 | \mathbf{x}) = \frac{e^{-\mathbf{w}^T \mathbf{x}}}{1 + e^{-\mathbf{w}^T \mathbf{x}}}$$

**Quy t·∫Øc quy·∫øt ƒë·ªãnh:**
$$\hat{y} = \begin{cases}
1 & \text{n·∫øu } P(y = 1 | \mathbf{x}) \geq 0.5 \\
0 & \text{n·∫øu } P(y = 1 | \mathbf{x}) < 0.5
\end{cases}$$

## üßÆ H√†m M·∫•t m√°t Cross-Entropy

Trong logistic regression, ch√∫ng ta s·ª≠ d·ª•ng **Cross-Entropy Loss** ƒë·ªÉ ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa d·ª± ƒëo√°n v√† th·ª±c t·∫ø:

$$J(\mathbf{w}) = -\sum_{i=1}^n \left[ y_i \log p_i + (1-y_i) \log (1-p_i) \right]$$

v·ªõi $$p_i = \sigma(\mathbf{w}^T \mathbf{x}_i)$$ l√† x√°c su·∫•t d·ª± ƒëo√°n.

**ƒê·∫∑c ƒëi·ªÉm quan tr·ªçng:**
- **Ph·∫°t n·∫∑ng d·ª± ƒëo√°n sai:** Khi d·ª± ƒëo√°n c√†ng sai, loss tƒÉng exponentially
- **Smooth v√† convex:** ƒê·∫£m b·∫£o t·ªëi ∆∞u h√≥a hi·ªáu qu·∫£
- **Probabilistic:** Ph√π h·ª£p v·ªõi b·∫£n ch·∫•t x√°c su·∫•t c·ªßa b√†i to√°n

## üîç Ch·ª©ng minh T√≠nh L·ªìi

### ƒê·ªãnh l√Ω: H√†m m·∫•t m√°t Logistic l√† l·ªìi

**Ch·ª©ng minh:**

**B∆∞·ªõc 1:** Ch·ª©ng minh $$-\log \sigma(z)$$ l√† l·ªìi

$$-\log \sigma(z) = -\log \frac{1}{1 + e^{-z}} = \log(1 + e^{-z})$$

ƒê·∫°o h√†m b·∫≠c hai:
$$\frac{d^2}{dz^2} \log(1 + e^{-z}) = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z)(1 - \sigma(z)) > 0$$

‚üπ $$-\log \sigma(z)$$ l√† l·ªìi theo $$z$$

**B∆∞·ªõc 2:** Ch·ª©ng minh $$-\log(1 - \sigma(z))$$ l√† l·ªìi

$$-\log(1 - \sigma(z)) = -\log \frac{e^{-z}}{1 + e^{-z}} = z + \log(1 + e^{-z})$$

ƒê·∫°o h√†m b·∫≠c hai:
$$\frac{d^2}{dz^2} [z + \log(1 + e^{-z})] = \frac{e^{-z}}{(1 + e^{-z})^2} > 0$$

‚üπ $$-\log(1 - \sigma(z))$$ l√† l·ªìi theo $$z$$

**B∆∞·ªõc 3:** K·∫øt lu·∫≠n

V√¨ $$z = \mathbf{w}^T \mathbf{x}_i$$ l√† affine theo $$\mathbf{w}$$, v√† t·ªïng c·ªßa c√°c h√†m l·ªìi l√† l·ªìi:

$$J(\mathbf{w}) = \sum_{i=1}^n \left[ -y_i \log \sigma(\mathbf{w}^T \mathbf{x}_i) - (1-y_i) \log (1 - \sigma(\mathbf{w}^T \mathbf{x}_i)) \right]$$

l√† **h√†m l·ªìi** theo $$\mathbf{w}$$.

## üìà Gradient c·ªßa H√†m M·∫•t m√°t

**Gradient:**
$$\nabla_{\mathbf{w}} J(\mathbf{w}) = \sum_{i=1}^n (\sigma(\mathbf{w}^T \mathbf{x}_i) - y_i) \mathbf{x}_i = \mathbf{X}^T (\boldsymbol{\sigma} - \mathbf{y})$$

v·ªõi:
- $$\mathbf{X} \in \mathbb{R}^{n \times p}$$: ma tr·∫≠n ƒë·∫∑c tr∆∞ng
- $$\boldsymbol{\sigma} = [\sigma(\mathbf{w}^T \mathbf{x}_1), ..., \sigma(\mathbf{w}^T \mathbf{x}_n)]^T$$
- $$\mathbf{y} = [y_1, ..., y_n]^T$$

**Hessian:**
$$\nabla^2_{\mathbf{w}} J(\mathbf{w}) = \mathbf{X}^T \mathbf{S} \mathbf{X}$$

v·ªõi $$\mathbf{S} = \text{diag}(\sigma(\mathbf{w}^T \mathbf{x}_i)(1 - \sigma(\mathbf{w}^T \mathbf{x}_i)))$$

V√¨ $$\mathbf{S} \succ 0$$, ta c√≥ $$\nabla^2_{\mathbf{w}} J(\mathbf{w}) \succeq 0$$ ‚üπ **h√†m l·ªìi**.

### üéØ √ù nghƒ©a c·ªßa T√≠nh L·ªìi

**T·∫°i sao t√≠nh l·ªìi quan tr·ªçng?**

1. **ƒê·∫£m b·∫£o Global Optimum:**
   - H√†m l·ªìi c√≥ **duy nh·∫•t m·ªôt ƒëi·ªÉm t·ªëi ∆∞u to√†n c·ª•c**
   - Kh√¥ng c√≥ local minima "gi·∫£" nh∆∞ trong neural networks
   - M·ªçi thu·∫≠t to√°n t·ªëi ∆∞u ƒë·ªÅu h·ªôi t·ª• v·ªÅ c√πng m·ªôt nghi·ªám

2. **Gradient Descent ho·∫°t ƒë·ªông hi·ªáu qu·∫£:**
   - **Lu√¥n h·ªôi t·ª•** v·ªÅ nghi·ªám t·ªëi ∆∞u (v·ªõi learning rate ph√π h·ª£p)
   - **Kh√¥ng b·ªã k·∫πt** t·∫°i local minima
   - **ƒê∆°n gi·∫£n v√† ·ªïn ƒë·ªãnh** - kh√¥ng c·∫ßn k·ªπ thu·∫≠t ph·ª©c t·∫°p

3. **L√Ω thuy·∫øt t·ªëi ∆∞u m·∫°nh m·∫Ω:**
   - C√≥ th·ªÉ ch·ª©ng minh **t·ªëc ƒë·ªô h·ªôi t·ª•**
   - C√≥ th·ªÉ **∆∞·ªõc l∆∞·ª£ng sai s·ªë**
   - C√≥ **ƒëi·ªÅu ki·ªán d·ª´ng** r√µ r√†ng

### üöÄ T·ª´ T√≠nh L·ªìi ƒë·∫øn Gradient Descent

**V√¨ h√†m m·∫•t m√°t logistic l√† l·ªìi v√† kh·∫£ vi**, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng **Gradient Descent** m·ªôt c√°ch t·ª± tin:

**Thu·∫≠t to√°n c∆° b·∫£n:**
$$\mathbf{w}^{(k+1)} = \mathbf{w}^{(k)} - \alpha \nabla J(\mathbf{w}^{(k)})$$

v·ªõi gradient:
$$\nabla J(\mathbf{w}) = \mathbf{X}^T (\boldsymbol{\sigma} - \mathbf{y})$$

**ƒê·∫£m b·∫£o h·ªôi t·ª•:**
- Ch·ªçn learning rate $$\alpha$$ ph√π h·ª£p ‚Üí **lu√¥n h·ªôi t·ª•**
- Kh√¥ng c·∫ßn lo l·∫Øng v·ªÅ local minima
- C√≥ th·ªÉ d·ª± ƒëo√°n ƒë∆∞·ª£c h√†nh vi c·ªßa thu·∫≠t to√°n

**K·∫øt n·ªëi v·ªõi Ch∆∞∆°ng 6:**
Trong **Ch∆∞∆°ng 6 - Gradient Descent**, ch√∫ng ta s·∫Ω h·ªçc chi ti·∫øt:
- C√°ch ch·ªçn learning rate t·ªëi ∆∞u
- Ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª•
- C√°c bi·∫øn th·ªÉ nh∆∞ Stochastic Gradient Descent
- **Logistic Regression s·∫Ω l√† v√≠ d·ª• ch√≠nh** ƒë·ªÉ minh h·ªça c√°c kh√°i ni·ªám n√†y!

## üîÑ So s√°nh v·ªõi Linear Regression

| Kh√≠a c·∫°nh | Linear Regression | Logistic Regression |
|-----------|-------------------|---------------------|
| **B√†i to√°n** | H·ªìi quy (regression) | Ph√¢n lo·∫°i (classification) |
| **ƒê·∫ßu ra** | Gi√° tr·ªã li√™n t·ª•c | X√°c su·∫•t $$\in [0,1]$$ |
| **H√†m k√≠ch ho·∫°t** | Identity: $$f(z) = z$$ | Sigmoid: $$\sigma(z) = \frac{1}{1+e^{-z}}$$ |
| **H√†m m·∫•t m√°t** | MSE: $$\frac{1}{2n}\sum(y_i - \hat{y}_i)^2$$ | Cross-entropy: $$-\sum[y_i \log p_i + (1-y_i)\log(1-p_i)]$$ |
| **Nghi·ªám ƒë√≥ng** | C√≥: $$\mathbf{w}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$ | Kh√¥ng c√≥ |
| **Thu·∫≠t to√°n** | Normal equation ho·∫∑c GD | Gradient Descent |
| **T√≠nh l·ªìi** | L·ªìi (quadratic) | L·ªìi (log-convex) |

## üí° V√≠ d·ª• Minh h·ªça: D·ª± ƒëo√°n Spam Email

**B√†i to√°n:** Ph√¢n lo·∫°i email spam d·ª±a tr√™n s·ªë l∆∞·ª£ng t·ª´ kh√≥a ƒë√°ng ng·ªù.

**D·ªØ li·ªáu:**
- $$x$$: s·ªë t·ª´ kh√≥a spam trong email
- $$y$$: 1 n·∫øu spam, 0 n·∫øu kh√¥ng spam

**M√¥ h√¨nh:**
$$P(\text{spam} | x) = \sigma(w_0 + w_1 x) = \frac{1}{1 + e^{-(w_0 + w_1 x)}}$$

**Gi·∫£i th√≠ch:**
- N·∫øu $$w_1 > 0$$: c√†ng nhi·ªÅu t·ª´ kh√≥a spam, x√°c su·∫•t spam c√†ng cao
- ƒêi·ªÉm quy·∫øt ƒë·ªãnh: $$x^* = -\frac{w_0}{w_1}$$ (khi $$P = 0.5$$)

## üéõÔ∏è Interactive Visualization

<div id="logistic-regression-demo" style="margin: 20px 0;">
    <div style="display: flex; gap: 20px; margin-bottom: 20px;">
        <div style="flex: 1;">
            <h4>üéöÔ∏è ƒêi·ªÅu khi·ªÉn Tham s·ªë</h4>
            <div style="margin: 10px 0;">
                <label for="w0-slider">Bias (w‚ÇÄ): <span id="w0-value">0</span></label>
                <input type="range" id="w0-slider" min="-5" max="5" step="0.1" value="0" style="width: 100%;">
            </div>
            <div style="margin: 10px 0;">
                <label for="w1-slider">Weight (w‚ÇÅ): <span id="w1-value">1</span></label>
                <input type="range" id="w1-slider" min="-3" max="3" step="0.1" value="1" style="width: 100%;">
            </div>
            <div style="margin: 10px 0;">
                <button id="add-data-btn" style="padding: 8px 16px; margin: 5px;">Th√™m D·ªØ li·ªáu</button>
                <button id="clear-data-btn" style="padding: 8px 16px; margin: 5px;">X√≥a D·ªØ li·ªáu</button>
                <button id="fit-model-btn" style="padding: 8px 16px; margin: 5px;">Fit Model</button>
            </div>
        </div>
        <div style="flex: 1;">
            <h4>üìä Th√¥ng tin M√¥ h√¨nh</h4>
            <div id="model-info">
                <p><strong>Ph∆∞∆°ng tr√¨nh:</strong> P(y=1|x) = œÉ(<span id="equation">0 + 1¬∑x</span>)</p>
                <p><strong>ƒêi·ªÉm quy·∫øt ƒë·ªãnh:</strong> x = <span id="decision-boundary">0</span></p>
                <p><strong>Loss:</strong> <span id="current-loss">-</span></p>
                <p><strong>Accuracy:</strong> <span id="accuracy">-</span></p>
            </div>
        </div>
    </div>
    
    <div style="display: flex; gap: 20px;">
        <div style="flex: 1;">
            <h4>üìà Sigmoid Function</h4>
            <canvas id="sigmoid-canvas" width="400" height="300" style="border: 1px solid #ccc;"></canvas>
        </div>
        <div style="flex: 1;">
            <h4>üéØ Classification Results</h4>
            <canvas id="classification-canvas" width="400" height="300" style="border: 1px solid #ccc;"></canvas>
        </div>
    </div>
    
    <div style="margin-top: 20px;">
        <h4>üìâ Loss Function Visualization</h4>
        <canvas id="loss-canvas" width="800" height="300" style="border: 1px solid #ccc;"></canvas>
    </div>
</div>

<script>
class LogisticRegressionDemo {
    constructor() {
        this.w0 = 0;
        this.w1 = 1;
        this.dataPoints = [];
        this.isAddingData = false;
        
        this.initializeCanvases();
        this.setupEventListeners();
        this.generateSampleData();
        this.updateVisualization();
    }
    
    initializeCanvases() {
        this.sigmoidCanvas = document.getElementById('sigmoid-canvas');
        this.sigmoidCtx = this.sigmoidCanvas.getContext('2d');
        
        this.classificationCanvas = document.getElementById('classification-canvas');
        this.classificationCtx = this.classificationCanvas.getContext('2d');
        
        this.lossCanvas = document.getElementById('loss-canvas');
        this.lossCtx = this.lossCanvas.getContext('2d');
    }
    
    setupEventListeners() {
        const w0Slider = document.getElementById('w0-slider');
        const w1Slider = document.getElementById('w1-slider');
        const addDataBtn = document.getElementById('add-data-btn');
        const clearDataBtn = document.getElementById('clear-data-btn');
        const fitModelBtn = document.getElementById('fit-model-btn');
        
        w0Slider.addEventListener('input', (e) => {
            this.w0 = parseFloat(e.target.value);
            document.getElementById('w0-value').textContent = this.w0.toFixed(1);
            this.updateVisualization();
        });
        
        w1Slider.addEventListener('input', (e) => {
            this.w1 = parseFloat(e.target.value);
            document.getElementById('w1-value').textContent = this.w1.toFixed(1);
            this.updateVisualization();
        });
        
        addDataBtn.addEventListener('click', () => {
            this.isAddingData = !this.isAddingData;
            addDataBtn.textContent = this.isAddingData ? 'D·ª´ng Th√™m' : 'Th√™m D·ªØ li·ªáu';
            addDataBtn.style.backgroundColor = this.isAddingData ? '#ff6b6b' : '';
        });
        
        clearDataBtn.addEventListener('click', () => {
            this.dataPoints = [];
            this.updateVisualization();
        });
        
        fitModelBtn.addEventListener('click', () => {
            this.fitModel();
        });
        
        // Click to add data points
        this.classificationCanvas.addEventListener('click', (e) => {
            if (this.isAddingData) {
                const rect = this.classificationCanvas.getBoundingClientRect();
                const x = (e.clientX - rect.left - 50) / 300 * 10 - 5; // Scale to [-5, 5]
                const y = e.clientY - rect.top > 150 ? 0 : 1; // Top half = class 1, bottom = class 0
                this.dataPoints.push({x, y});
                this.updateVisualization();
            }
        });
    }
    
    generateSampleData() {
        // Generate some sample data
        const sampleData = [
            {x: -3, y: 0}, {x: -2.5, y: 0}, {x: -2, y: 0}, {x: -1.5, y: 0},
            {x: -1, y: 0}, {x: -0.5, y: 0}, {x: 0, y: 0}, {x: 0.5, y: 1},
            {x: 1, y: 1}, {x: 1.5, y: 1}, {x: 2, y: 1}, {x: 2.5, y: 1}, {x: 3, y: 1}
        ];
        
        // Add some noise
        this.dataPoints = sampleData.map(point => ({
            x: point.x + (Math.random() - 0.5) * 0.5,
            y: Math.random() < 0.1 ? 1 - point.y : point.y // 10% label noise
        }));
    }
    
    sigmoid(z) {
        return 1 / (1 + Math.exp(-z));
    }
    
    predict(x) {
        return this.sigmoid(this.w0 + this.w1 * x);
    }
    
    calculateLoss() {
        if (this.dataPoints.length === 0) return 0;
        
        let loss = 0;
        for (const point of this.dataPoints) {
            const p = this.predict(point.x);
            const epsilon = 1e-15; // Prevent log(0)
            const clampedP = Math.max(epsilon, Math.min(1 - epsilon, p));
            loss += -point.y * Math.log(clampedP) - (1 - point.y) * Math.log(1 - clampedP);
        }
        return loss / this.dataPoints.length;
    }
    
    calculateAccuracy() {
        if (this.dataPoints.length === 0) return 0;
        
        let correct = 0;
        for (const point of this.dataPoints) {
            const prediction = this.predict(point.x) >= 0.5 ? 1 : 0;
            if (prediction === point.y) correct++;
        }
        return correct / this.dataPoints.length;
    }
    
    fitModel() {
        if (this.dataPoints.length === 0) return;
        
        // Simple gradient descent
        const learningRate = 0.1;
        const iterations = 1000;
        
        for (let iter = 0; iter < iterations; iter++) {
            let gradW0 = 0, gradW1 = 0;
            
            for (const point of this.dataPoints) {
                const p = this.predict(point.x);
                const error = p - point.y;
                gradW0 += error;
                gradW1 += error * point.x;
            }
            
            gradW0 /= this.dataPoints.length;
            gradW1 /= this.dataPoints.length;
            
            this.w0 -= learningRate * gradW0;
            this.w1 -= learningRate * gradW1;
        }
        
        // Update sliders
        document.getElementById('w0-slider').value = this.w0;
        document.getElementById('w1-slider').value = this.w1;
        document.getElementById('w0-value').textContent = this.w0.toFixed(1);
        document.getElementById('w1-value').textContent = this.w1.toFixed(1);
        
        this.updateVisualization();
    }
    
    drawSigmoid() {
        const ctx = this.sigmoidCtx;
        const canvas = this.sigmoidCanvas;
        
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw axes
        ctx.strokeStyle = '#ccc';
        ctx.lineWidth = 1;
        
        // X-axis
        ctx.beginPath();
        ctx.moveTo(50, canvas.height - 50);
        ctx.lineTo(canvas.width - 20, canvas.height - 50);
        ctx.stroke();
        
        // Y-axis
        ctx.beginPath();
        ctx.moveTo(50, 20);
        ctx.lineTo(50, canvas.height - 50);
        ctx.stroke();
        
        // Draw sigmoid curve
        ctx.strokeStyle = '#2196F3';
        ctx.lineWidth = 2;
        ctx.beginPath();
        
        for (let i = 0; i <= 300; i++) {
            const x = (i / 300) * 10 - 5; // x from -5 to 5
            const z = this.w0 + this.w1 * x;
            const y = this.sigmoid(z);
            
            const canvasX = 50 + (x + 5) / 10 * 300;
            const canvasY = canvas.height - 50 - y * 200;
            
            if (i === 0) {
                ctx.moveTo(canvasX, canvasY);
            } else {
                ctx.lineTo(canvasX, canvasY);
            }
        }
        ctx.stroke();
        
        // Draw decision boundary
        if (this.w1 !== 0) {
            const decisionX = -this.w0 / this.w1;
            if (decisionX >= -5 && decisionX <= 5) {
                const canvasX = 50 + (decisionX + 5) / 10 * 300;
                ctx.strokeStyle = '#ff6b6b';
                ctx.lineWidth = 2;
                ctx.setLineDash([5, 5]);
                ctx.beginPath();
                ctx.moveTo(canvasX, 20);
                ctx.lineTo(canvasX, canvas.height - 50);
                ctx.stroke();
                ctx.setLineDash([]);
            }
        }
        
        // Labels
        ctx.fillStyle = '#333';
        ctx.font = '12px Arial';
        ctx.fillText('P(y=1|x)', 10, 30);
        ctx.fillText('x', canvas.width - 15, canvas.height - 30);
        ctx.fillText('0', 45, canvas.height - 30);
        ctx.fillText('1', 45, 25);
        ctx.fillText('0.5', 35, canvas.height - 150);
    }
    
    drawClassification() {
        const ctx = this.classificationCtx;
        const canvas = this.classificationCanvas;
        
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw background regions
        ctx.fillStyle = 'rgba(255, 107, 107, 0.1)'; // Red for class 0
        ctx.fillRect(50, canvas.height/2, 300, canvas.height/2 - 50);
        
        ctx.fillStyle = 'rgba(76, 175, 80, 0.1)'; // Green for class 1
        ctx.fillRect(50, 20, 300, canvas.height/2 - 20);
        
        // Draw axes
        ctx.strokeStyle = '#ccc';
        ctx.lineWidth = 1;
        
        // X-axis
        ctx.beginPath();
        ctx.moveTo(50, canvas.height - 50);
        ctx.lineTo(350, canvas.height - 50);
        ctx.stroke();
        
        // Y-axis (class separator)
        ctx.beginPath();
        ctx.moveTo(50, canvas.height/2);
        ctx.lineTo(350, canvas.height/2);
        ctx.stroke();
        
        // Draw data points
        for (const point of this.dataPoints) {
            const canvasX = 50 + (point.x + 5) / 10 * 300;
            const canvasY = point.y === 1 ? canvas.height/4 : 3*canvas.height/4;
            
            ctx.fillStyle = point.y === 1 ? '#4CAF50' : '#f44336';
            ctx.beginPath();
            ctx.arc(canvasX, canvasY, 5, 0, 2 * Math.PI);
            ctx.fill();
        }
        
        // Draw decision boundary
        if (this.w1 !== 0) {
            const decisionX = -this.w0 / this.w1;
            if (decisionX >= -5 && decisionX <= 5) {
                const canvasX = 50 + (decisionX + 5) / 10 * 300;
                ctx.strokeStyle = '#2196F3';
                ctx.lineWidth = 3;
                ctx.beginPath();
                ctx.moveTo(canvasX, 20);
                ctx.lineTo(canvasX, canvas.height - 50);
                ctx.stroke();
            }
        }
        
        // Labels
        ctx.fillStyle = '#333';
        ctx.font = '12px Arial';
        ctx.fillText('Class 1', 10, 40);
        ctx.fillText('Class 0', 10, canvas.height - 60);
        ctx.fillText('x', 360, canvas.height - 30);
    }
    
    drawLoss() {
        const ctx = this.lossCtx;
        const canvas = this.lossCanvas;
        
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        if (this.dataPoints.length === 0) return;
        
        // Draw axes
        ctx.strokeStyle = '#ccc';
        ctx.lineWidth = 1;
        
        // X-axis
        ctx.beginPath();
        ctx.moveTo(50, canvas.height - 50);
        ctx.lineTo(canvas.width - 20, canvas.height - 50);
        ctx.stroke();
        
        // Y-axis
        ctx.beginPath();
        ctx.moveTo(50, 20);
        ctx.lineTo(50, canvas.height - 50);
        ctx.stroke();
        
        // Calculate loss surface
        const originalW0 = this.w0;
        const originalW1 = this.w1;
        
        const w0Range = [-3, 3];
        const w1Range = [-2, 2];
        const resolution = 50;
        
        let minLoss = Infinity;
        let maxLoss = -Infinity;
        const lossData = [];
        
        for (let i = 0; i <= resolution; i++) {
            lossData[i] = [];
            for (let j = 0; j <= resolution; j++) {
                this.w0 = w0Range[0] + (w0Range[1] - w0Range[0]) * i / resolution;
                this.w1 = w1Range[0] + (w1Range[1] - w1Range[0]) * j / resolution;
                const loss = this.calculateLoss();
                lossData[i][j] = loss;
                minLoss = Math.min(minLoss, loss);
                maxLoss = Math.max(maxLoss, loss);
            }
        }
        
        // Draw contour lines
        const numContours = 10;
        for (let c = 0; c < numContours; c++) {
            const level = minLoss + (maxLoss - minLoss) * c / numContours;
            ctx.strokeStyle = `hsl(${240 - c * 20}, 70%, 60%)`;
            ctx.lineWidth = 1;
            
            // Simple contour drawing (could be improved)
            for (let i = 0; i < resolution; i++) {
                for (let j = 0; j < resolution; j++) {
                    if (Math.abs(lossData[i][j] - level) < (maxLoss - minLoss) / numContours / 2) {
                        const x = 50 + i / resolution * (canvas.width - 70);
                        const y = canvas.height - 50 - j / resolution * (canvas.height - 70);
                        ctx.fillRect(x, y, 2, 2);
                    }
                }
            }
        }
        
        // Mark current position
        this.w0 = originalW0;
        this.w1 = originalW1;
        
        const currentX = 50 + (this.w0 - w0Range[0]) / (w0Range[1] - w0Range[0]) * (canvas.width - 70);
        const currentY = canvas.height - 50 - (this.w1 - w1Range[0]) / (w1Range[1] - w1Range[0]) * (canvas.height - 70);
        
        ctx.fillStyle = '#ff6b6b';
        ctx.beginPath();
        ctx.arc(currentX, currentY, 6, 0, 2 * Math.PI);
        ctx.fill();
        
        // Labels
        ctx.fillStyle = '#333';
        ctx.font = '12px Arial';
        ctx.fillText('w‚ÇÅ', 10, canvas.height - 30);
        ctx.fillText('w‚ÇÄ', canvas.width - 20, canvas.height - 30);
    }
    
    updateVisualization() {
        this.drawSigmoid();
        this.drawClassification();
        this.drawLoss();
        
        // Update info panel
        const equation = `${this.w0.toFixed(1)} + ${this.w1.toFixed(1)}¬∑x`;
        document.getElementById('equation').textContent = equation;
        
        const decisionBoundary = this.w1 !== 0 ? (-this.w0 / this.w1).toFixed(2) : '‚àû';
        document.getElementById('decision-boundary').textContent = decisionBoundary;
        
        const loss = this.calculateLoss();
        document.getElementById('current-loss').textContent = loss.toFixed(3);
        
        const accuracy = this.calculateAccuracy();
        document.getElementById('accuracy').textContent = (accuracy * 100).toFixed(1) + '%';
    }
}

// Initialize demo when page loads
document.addEventListener('DOMContentLoaded', () => {
    new LogisticRegressionDemo();
});
</script>

<style>
#logistic-regression-demo {
    font-family: Arial, sans-serif;
    max-width: 1000px;
    margin: 0 auto;
}

#logistic-regression-demo canvas {
    border-radius: 4px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

#logistic-regression-demo input[type="range"] {
    width: 100%;
    margin: 5px 0;
}

#logistic-regression-demo button {
    background-color: #2196F3;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s;
}

#logistic-regression-demo button:hover {
    background-color: #1976D2;
}

#model-info {
    background-color: #f5f5f5;
    padding: 15px;
    border-radius: 4px;
    font-size: 14px;
}

#model-info p {
    margin: 5px 0;
}
</style>


---

üí° **B√†i t·∫≠p th·ª±c h√†nh:** H√£y th·ª≠ thay ƒë·ªïi c√°c tham s·ªë trong visualization tr√™n ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ c√°ch logistic regression ho·∫°t ƒë·ªông!

---

## üìñ ƒê·ªçc th√™m: C√°c Link Functions thay th·∫ø cho Sigmoid

*Ph·∫ßn n√†y d√†nh cho nh·ªØng b·∫°n mu·ªën t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ l√Ω do ch·ªçn sigmoid function, t·∫°i sao s·ª≠ d·ª•ng cross-entropy loss, v√† c√°c l·ª±a ch·ªçn thay th·∫ø trong generalized linear models.*

## üßÆ T·∫°i sao Cross-Entropy l√† H√†m M·∫•t m√°t?

### üéØ M·ª•c ti√™u: T√¨m tr·ªçng s·ªë t·ªët nh·∫•t

**C√¢u h·ªèi c·ªët l√µi:** L√†m th·∫ø n√†o ƒë·ªÉ t√¨m tr·ªçng s·ªë $$\mathbf{w}$$ sao cho m√¥ h√¨nh d·ª± ƒëo√°n ch√≠nh x√°c nh·∫•t?

**√ù t∆∞·ªüng:** Ch·ªçn $$\mathbf{w}$$ sao cho **x√°c su·∫•t quan s√°t ƒë∆∞·ª£c d·ªØ li·ªáu th·ª±c t·∫ø l√† cao nh·∫•t**.

### üîç T·ª´ X√°c su·∫•t ƒë·∫øn Cross-Entropy

#### **B∆∞·ªõc 1: M√¥ h√¨nh h√≥a x√°c su·∫•t**

V·ªõi m·ªói m·∫´u d·ªØ li·ªáu $$(\mathbf{x}_i, y_i)$$, m√¥ h√¨nh d·ª± ƒëo√°n:
- N·∫øu $$y_i = 1$$: x√°c su·∫•t l√† $$p_i = \sigma(\mathbf{w}^T \mathbf{x}_i)$$
- N·∫øu $$y_i = 0$$: x√°c su·∫•t l√† $$1 - p_i = 1 - \sigma(\mathbf{w}^T \mathbf{x}_i)$$

#### **B∆∞·ªõc 2: Likelihood - "Kh·∫£ nƒÉng x·∫£y ra"**

**C√¢u h·ªèi:** V·ªõi tr·ªçng s·ªë $$\mathbf{w}$$ hi·ªán t·∫°i, kh·∫£ nƒÉng quan s√°t ƒë∆∞·ª£c to√†n b·ªô d·ªØ li·ªáu l√† bao nhi·ªÅu?

**Tr·∫£ l·ªùi:** Nh√¢n x√°c su·∫•t c·ªßa t·∫•t c·∫£ c√°c m·∫´u:
$$\text{Likelihood} = p_1^{y_1} \cdot (1-p_1)^{1-y_1} \times p_2^{y_2} \cdot (1-p_2)^{1-y_2} \times \ldots$$

#### **B∆∞·ªõc 3: Maximum Likelihood Principle**

**Nguy√™n l√Ω:** Ch·ªçn $$\mathbf{w}$$ ƒë·ªÉ **t·ªëi ƒëa h√≥a likelihood** = ch·ªçn $$\mathbf{w}$$ l√†m cho d·ªØ li·ªáu quan s√°t ƒë∆∞·ª£c c√≥ kh·∫£ nƒÉng x·∫£y ra cao nh·∫•t.

**V·∫•n ƒë·ªÅ th·ª±c t·∫ø:** Likelihood l√† t√≠ch c·ªßa nhi·ªÅu s·ªë nh·ªè ‚Üí kh√≥ t√≠nh to√°n v√† kh√¥ng ·ªïn ƒë·ªãnh.

#### **B∆∞·ªõc 4: Log-Likelihood**

**Gi·∫£i ph√°p:** L·∫•y logarithm ƒë·ªÉ chuy·ªÉn t√≠ch th√†nh t·ªïng:
$$\log(\text{Likelihood}) = \sum_{i=1}^n \left[ y_i \log p_i + (1-y_i) \log (1-p_i) \right]$$

**L·ª£i √≠ch:**
- T√≠nh to√°n ·ªïn ƒë·ªãnh h∆°n
- T·ªëi ƒëa h√≥a log-likelihood = t·ªëi ƒëa h√≥a likelihood

#### **B∆∞·ªõc 5: Cross-Entropy Loss**

**Chuy·ªÉn ƒë·ªïi cu·ªëi c√πng:** T·ªëi ƒëa h√≥a = T·ªëi thi·ªÉu h√≥a √¢m c·ªßa n√≥

$$J(\mathbf{w}) = -\log(\text{Likelihood}) = -\sum_{i=1}^n \left[ y_i \log p_i + (1-y_i) \log (1-p_i) \right]$$

**ƒê√¢y ch√≠nh l√† Cross-Entropy Loss!**

### üí° T·∫°i sao Cross-Entropy "H·ª£p l√Ω"?

#### **1. Ph·∫°t n·∫∑ng d·ª± ƒëo√°n sai**

**V√≠ d·ª• minh h·ªça:**
- **Tr∆∞·ªùng h·ª£p 1:** $$y = 1$$ (th·ª±c t·∫ø), $$p = 0.9$$ (d·ª± ƒëo√°n)
  - Loss = $$-1 \times \log(0.9) = 0.105$$ (th·∫•p - t·ªët!)

- **Tr∆∞·ªùng h·ª£p 2:** $$y = 1$$ (th·ª±c t·∫ø), $$p = 0.1$$ (d·ª± ƒëo√°n)  
  - Loss = $$-1 \times \log(0.1) = 2.303$$ (cao - t·ªá!)

**K·∫øt lu·∫≠n:** D·ª± ƒëo√°n c√†ng sai, loss c√†ng l·ªõn m·ªôt c√°ch **exponential**.

#### **2. Khuy·∫øn kh√≠ch d·ª± ƒëo√°n t·ª± tin**

Cross-entropy "th∆∞·ªüng" cho nh·ªØng d·ª± ƒëo√°n t·ª± tin v√† ƒë√∫ng:
- $$p = 0.99$$ khi $$y = 1$$ ‚Üí loss r·∫•t th·∫•p
- $$p = 0.51$$ khi $$y = 1$$ ‚Üí loss v·∫´n cao (m·∫∑c d√π ƒë√∫ng)

#### **3. Smooth v√† kh·∫£ vi**

Kh√°c v·ªõi accuracy (0-1 loss), cross-entropy:
- **Smooth**: C√≥ ƒë·∫°o h√†m m·ªçi n∆°i
- **Convex**: ƒê·∫£m b·∫£o t·ªëi ∆∞u to√†n c·ª•c
- **Informative gradient**: Gradient l·ªõn khi d·ª± ƒëo√°n sai

### üîÑ So s√°nh v·ªõi c√°c Loss kh√°c

| Loss Function | C√¥ng th·ª©c | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|---------------|-----------|---------|------------|
| **Cross-Entropy** | $$-[y\log p + (1-y)\log(1-p)]$$ | Smooth, convex, principled | Sensitive to outliers |
| **Mean Squared Error** | $$(y - p)^2$$ | Simple, familiar | Not probabilistic |
| **0-1 Loss** | $$\mathbf{1}[y \neq \text{round}(p)]$$ | Direct accuracy | Non-differentiable |

### üéØ K·∫øt lu·∫≠n v·ªÅ Cross-Entropy

**Cross-Entropy kh√¥ng ph·∫£i l√† l·ª±a ch·ªçn t√πy √Ω** - n√≥ xu·∫•t ph√°t t·ª± nhi√™n t·ª´:

1. **Nguy√™n l√Ω Maximum Likelihood** - c√°ch t·ªët nh·∫•t ƒë·ªÉ fit m√¥ h√¨nh x√°c su·∫•t
2. **T√≠nh ch·∫•t to√°n h·ªçc t·ªët** - convex, smooth, differentiable  
3. **√ù nghƒ©a th·ª±c t·∫ø** - ph·∫°t n·∫∑ng d·ª± ƒëo√°n sai, khuy·∫øn kh√≠ch t·ª± tin

**ƒê√¢y l√† l√Ω do t·∫°i sao cross-entropy l√† "gold standard" cho classification problems!**

## ü§î T·∫°i sao ch·ªçn Sigmoid Function?

### 1. **Y√™u c·∫ßu To√°n h·ªçc cho Activation Function**

ƒê·ªÉ chuy·ªÉn ƒë·ªïi t·ª´ $$\mathbb{R}$$ sang x√°c su·∫•t $$[0,1]$$, ta c·∫ßn m·ªôt h√†m $$g: \mathbb{R} \to [0,1]$$ th·ªèa m√£n:

**Y√™u c·∫ßu b·∫Øt bu·ªôc:**
- ‚úÖ **Monotonic**: $$g'(z) > 0$$ (ƒë·∫£m b·∫£o th·ª© t·ª±)
- ‚úÖ **Bounded**: $$g(z) \in [0,1]$$ (x√°c su·∫•t h·ª£p l·ªá)
- ‚úÖ **Smooth**: Kh·∫£ vi ƒë·ªÉ t√≠nh gradient
- ‚úÖ **Asymptotic**: $$\lim_{z \to -\infty} g(z) = 0$$, $$\lim_{z \to +\infty} g(z) = 1$$

**Y√™u c·∫ßu mong mu·ªën:**
- üéØ **Simple derivative**: ƒê·∫°o h√†m d·ªÖ t√≠nh
- üéØ **Probabilistic interpretation**: C√≥ √Ω nghƒ©a x√°c su·∫•t
- üéØ **Symmetric**: $$g(-z) = 1 - g(z)$$

### 2. **Ngu·ªìn g·ªëc t·ª´ Odds v√† Logit**

**Odds (T·ª∑ s·ªë c∆∞·ª£c):**
$$\text{Odds} = \frac{P(y=1|x)}{P(y=0|x)} = \frac{P(y=1|x)}{1-P(y=1|x)}$$

**Log-odds (Logit):**
$$\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$$

**M√¥ h√¨nh tuy·∫øn t√≠nh cho log-odds:**
$$\log\left(\frac{P(y=1|x)}{1-P(y=1|x)}\right) = \mathbf{w}^T \mathbf{x}$$

**Gi·∫£i ng∆∞·ª£c ƒë·ªÉ t√¨m P:**
$$\frac{P(y=1|x)}{1-P(y=1|x)} = e^{\mathbf{w}^T \mathbf{x}}$$

$$P(y=1|x) = (1-P(y=1|x)) \cdot e^{\mathbf{w}^T \mathbf{x}}$$

$$P(y=1|x) = e^{\mathbf{w}^T \mathbf{x}} - P(y=1|x) \cdot e^{\mathbf{w}^T \mathbf{x}}$$

$$P(y=1|x)(1 + e^{\mathbf{w}^T \mathbf{x}}) = e^{\mathbf{w}^T \mathbf{x}}$$

$$P(y=1|x) = \frac{e^{\mathbf{w}^T \mathbf{x}}}{1 + e^{\mathbf{w}^T \mathbf{x}}} = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}} = \sigma(\mathbf{w}^T \mathbf{x})$$

‚üπ **Sigmoid xu·∫•t hi·ªán t·ª± nhi√™n t·ª´ m√¥ h√¨nh log-odds tuy·∫øn t√≠nh!**

### 3. **∆Øu ƒëi·ªÉm ƒê·∫∑c bi·ªát c·ªßa Sigmoid**

#### **a) ƒê·∫°o h√†m Elegant**
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

**L·ª£i √≠ch:**
- Kh√¥ng c·∫ßn t√≠nh l·∫°i $$e^{-z}$$
- Gradient descent hi·ªáu qu·∫£
- Backpropagation ƒë∆°n gi·∫£n

#### **b) T√≠nh ƒê·ªëi x·ª©ng**
$$\sigma(-z) = 1 - \sigma(z)$$

**√ù nghƒ©a:** N·∫øu $$P(y=1|x) = 0.7$$ th√¨ $$P(y=1|-x) = 0.3$$

#### **c) Gi·∫£i th√≠ch X√°c su·∫•t T·ª± nhi√™n**
- $$z = 0 \Rightarrow P = 0.5$$ (kh√¥ng thi√™n v·ªã)
- $$z > 0 \Rightarrow P > 0.5$$ (thi√™n v·ªÅ class 1)
- $$z < 0 \Rightarrow P < 0.5$$ (thi√™n v·ªÅ class 0)

### üîÑ T·∫°i sao c·∫ßn c√°c h√†m thay th·∫ø?

M·∫∑c d√π sigmoid l√† l·ª±a ch·ªçn ph·ªï bi·∫øn nh·∫•t, trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát, c√°c link functions kh√°c c√≥ th·ªÉ ph√π h·ª£p h∆°n:

- **D·ªØ li·ªáu c√≥ tail behavior kh√°c bi·ªát**
- **Rare events ho·∫∑c imbalanced data**
- **Y√™u c·∫ßu m√¥ h√¨nh h√≥a ƒë·∫∑c bi·ªát**
- **Theoretical assumptions kh√°c nhau**

### 1. **Tanh Function**

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = \frac{e^{2z} - 1}{e^{2z} + 1}$$

**M·ªëi quan h·ªá v·ªõi Sigmoid:**
$$\tanh(z) = 2\sigma(2z) - 1$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ **Mi·ªÅn gi√° tr·ªã**: $$(-1, 1)$$
- ‚úÖ **ƒê·ªëi x·ª©ng qua g·ªëc**: $$\tanh(-z) = -\tanh(z)$$
- ‚úÖ **ƒê·∫°o h√†m**: $$\tanh'(z) = 1 - \tanh^2(z)$$

**Chuy·ªÉn ƒë·ªïi v·ªÅ x√°c su·∫•t:** $$p = \frac{\tanh(z) + 1}{2}$$

**·ª®ng d·ª•ng:** Neural networks (hidden layers), khi c·∫ßn output centered around 0

### 2. **Probit Function (Inverse Normal CDF)**

$$\Phi^{-1}(p) = z \Leftrightarrow p = \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ **Mi·ªÅn gi√° tr·ªã**: $$(0, 1)$$
- ‚úÖ **ƒê·ªëi x·ª©ng**: $$\Phi(-z) = 1 - \Phi(z)$$
- ‚ùå **Kh√¥ng c√≥ d·∫°ng ƒë√≥ng**: C·∫ßn t√≠nh t√≠ch ph√¢n s·ªë

**So s√°nh v·ªõi Sigmoid:**
- **Sigmoid**: ƒëu√¥i exponential decay $$\sim e^{-|z|}$$
- **Probit**: ƒëu√¥i Gaussian decay $$\sim e^{-z^2/2}$$ (nhanh h∆°n)

**·ª®ng d·ª•ng:** 
- Econometrics (probit regression)
- Khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi g·∫ßn Gaussian
- Threshold models

### 3. **Gumbel (Log-log) Function**

$$F(z) = e^{-e^{-z}}$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ **Mi·ªÅn gi√° tr·ªã**: $$(0, 1)$$
- ‚ùå **Kh√¥ng ƒë·ªëi x·ª©ng**: Left-skewed
- **CDF c·ªßa Gumbel distribution**

**·ª®ng d·ª•ng:**
- Extreme value theory
- Survival analysis
- Modeling maximum values

### 4. **Complementary Log-log**

$$F(z) = 1 - e^{-e^z}$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ **Mi·ªÅn gi√° tr·ªã**: $$(0, 1)$$
- ‚ùå **Kh√¥ng ƒë·ªëi x·ª©ng**: Right-skewed
- **Complement c·ªßa Gumbel**

**·ª®ng d·ª•ng:**
- **Rare events modeling** (y=1 hi·∫øm)
- Survival analysis (hazard modeling)
- Time-to-event data
- Poisson regression v·ªõi binary outcomes

### 5. **Cauchit Function**

$$F(z) = \frac{1}{2} + \frac{1}{\pi} \arctan(z)$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ **Mi·ªÅn gi√° tr·ªã**: $$(0, 1)$$
- ‚úÖ **ƒê·ªëi x·ª©ng**: $$F(-z) = 1 - F(z)$$
- **Heavy tails**: Slower decay than sigmoid/probit

**·ª®ng d·ª•ng:**
- Robust regression (outlier-resistant)
- Heavy-tailed error distributions
- Financial modeling

## üìä Interactive Comparison Tool

<div id="link-functions-comparison" style="margin: 20px 0;">
    <div style="margin-bottom: 20px;">
        <h4>üéõÔ∏è ƒêi·ªÅu khi·ªÉn Hi·ªÉn th·ªã</h4>
        <div style="display: flex; gap: 15px; flex-wrap: wrap;">
            <label><input type="checkbox" id="show-sigmoid" checked> Sigmoid</label>
            <label><input type="checkbox" id="show-tanh" checked> Tanh (scaled)</label>
            <label><input type="checkbox" id="show-probit" checked> Probit</label>
            <label><input type="checkbox" id="show-gumbel"> Gumbel</label>
            <label><input type="checkbox" id="show-cauchit"> Cauchit</label>
        </div>
    </div>
    
    <div style="display: flex; gap: 20px;">
        <div style="flex: 1;">
            <h4>üìà Link Functions Comparison</h4>
            <canvas id="link-functions-canvas" width="500" height="400" style="border: 1px solid #ccc;"></canvas>
        </div>
        <div style="flex: 1;">
            <h4>üìä Properties Comparison</h4>
            <div id="properties-table" style="font-size: 14px;">
                <table style="width: 100%; border-collapse: collapse;">
                    <thead>
                        <tr style="background-color: #f0f0f0;">
                            <th style="border: 1px solid #ccc; padding: 8px;">Function</th>
                            <th style="border: 1px solid #ccc; padding: 8px;">Symmetric</th>
                            <th style="border: 1px solid #ccc; padding: 8px;">Tail Behavior</th>
                            <th style="border: 1px solid #ccc; padding: 8px;">Computation</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="border: 1px solid #ccc; padding: 8px;"><strong>Sigmoid</strong></td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚úÖ Yes</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">Exponential</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚ö° Fast</td>
                        </tr>
                        <tr>
                            <td style="border: 1px solid #ccc; padding: 8px;"><strong>Tanh</strong></td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚úÖ Yes</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">Exponential</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚ö° Fast</td>
                        </tr>
                        <tr>
                            <td style="border: 1px solid #ccc; padding: 8px;"><strong>Probit</strong></td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚úÖ Yes</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">Gaussian</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">üêå Slow</td>
                        </tr>
                        <tr>
                            <td style="border: 1px solid #ccc; padding: 8px;"><strong>Gumbel</strong></td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚ùå No</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">Left-skewed</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚ö° Fast</td>
                        </tr>
                        <tr>
                            <td style="border: 1px solid #ccc; padding: 8px;"><strong>Cauchit</strong></td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚úÖ Yes</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">Heavy tails</td>
                            <td style="border: 1px solid #ccc; padding: 8px;">‚ö° Fast</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>
</div>

<script>
class LinkFunctionsComparison {
    constructor() {
        this.canvas = document.getElementById('link-functions-canvas');
        if (!this.canvas) return;
        this.ctx = this.canvas.getContext('2d');
        this.setupEventListeners();
        this.draw();
    }
    
    setupEventListeners() {
        const checkboxes = ['sigmoid', 'tanh', 'probit', 'gumbel', 'cauchit'];
        checkboxes.forEach(name => {
            const checkbox = document.getElementById(`show-${name}`);
            if (checkbox) {
                checkbox.addEventListener('change', () => {
                    this.draw();
                });
            }
        });
    }
    
    sigmoid(z) {
        return 1 / (1 + Math.exp(-z));
    }
    
    tanh_scaled(z) {
        return (Math.tanh(z) + 1) / 2; // Scale tanh from [-1,1] to [0,1]
    }
    
    // Approximation of probit function (inverse normal CDF)
    probit(z) {
        // Using rational approximation for normal CDF
        const a1 = 0.254829592;
        const a2 = -0.284496736;
        const a3 = 1.421413741;
        const a4 = -1.453152027;
        const a5 = 1.061405429;
        const p = 0.3275911;
        
        const sign = z < 0 ? -1 : 1;
        z = Math.abs(z) / Math.sqrt(2);
        
        const t = 1.0 / (1.0 + p * z);
        const y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-z * z);
        
        return 0.5 * (1.0 + sign * y);
    }
    
    gumbel(z) {
        return Math.exp(-Math.exp(-z));
    }
    
    cauchit(z) {
        return 0.5 + Math.atan(z) / Math.PI;
    }
    
    draw() {
        const ctx = this.ctx;
        const canvas = this.canvas;
        
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw axes
        ctx.strokeStyle = '#ccc';
        ctx.lineWidth = 1;
        
        // X-axis
        ctx.beginPath();
        ctx.moveTo(50, canvas.height - 50);
        ctx.lineTo(canvas.width - 20, canvas.height - 50);
        ctx.stroke();
        
        // Y-axis
        ctx.beginPath();
        ctx.moveTo(50, 20);
        ctx.lineTo(50, canvas.height - 50);
        ctx.stroke();
        
        // Grid lines
        ctx.strokeStyle = '#f0f0f0';
        ctx.lineWidth = 0.5;
        
        // Horizontal grid lines
        for (let i = 1; i <= 4; i++) {
            const y = 20 + (canvas.height - 70) * i / 5;
            ctx.beginPath();
            ctx.moveTo(50, y);
            ctx.lineTo(canvas.width - 20, y);
            ctx.stroke();
        }
        
        // Vertical grid lines
        for (let i = 1; i <= 9; i++) {
            const x = 50 + (canvas.width - 70) * i / 10;
            ctx.beginPath();
            ctx.moveTo(x, 20);
            ctx.lineTo(x, canvas.height - 50);
            ctx.stroke();
        }
        
        // Draw functions
        const functions = [
            { name: 'sigmoid', func: this.sigmoid, color: '#2196F3', label: 'Sigmoid' },
            { name: 'tanh', func: this.tanh_scaled, color: '#4CAF50', label: 'Tanh (scaled)' },
            { name: 'probit', func: this.probit, color: '#FF9800', label: 'Probit' },
            { name: 'gumbel', func: this.gumbel, color: '#9C27B0', label: 'Gumbel' },
            { name: 'cauchit', func: this.cauchit, color: '#F44336', label: 'Cauchit' }
        ];
        
        functions.forEach(({name, func, color, label}) => {
            const checkbox = document.getElementById(`show-${name}`);
            if (checkbox && checkbox.checked) {
                ctx.strokeStyle = color;
                ctx.lineWidth = 2;
                ctx.beginPath();
                
                for (let i = 0; i <= 400; i++) {
                    const z = (i / 400) * 10 - 5; // z from -5 to 5
                    let y;
                    
                    try {
                        y = func.call(this, z);
                        if (isNaN(y) || !isFinite(y)) continue;
                    } catch (e) {
                        continue;
                    }
                    
                    const canvasX = 50 + (z + 5) / 10 * (canvas.width - 70);
                    const canvasY = canvas.height - 50 - y * (canvas.height - 70);
                    
                    if (i === 0) {
                        ctx.moveTo(canvasX, canvasY);
                    } else {
                        ctx.lineTo(canvasX, canvasY);
                    }
                }
                ctx.stroke();
            }
        });
        
        // Draw legend
        let legendY = 30;
        functions.forEach(({name, color, label}) => {
            const checkbox = document.getElementById(`show-${name}`);
            if (checkbox && checkbox.checked) {
                ctx.strokeStyle = color;
                ctx.lineWidth = 3;
                ctx.beginPath();
                ctx.moveTo(canvas.width - 150, legendY);
                ctx.lineTo(canvas.width - 120, legendY);
                ctx.stroke();
                
                ctx.fillStyle = '#333';
                ctx.font = '12px Arial';
                ctx.fillText(label, canvas.width - 115, legendY + 4);
                
                legendY += 20;
            }
        });
        
        // Labels
        ctx.fillStyle = '#333';
        ctx.font = '12px Arial';
        ctx.fillText('z', canvas.width - 15, canvas.height - 30);
        ctx.fillText('P(y=1|z)', 10, 15);
        
        // Axis labels
        ctx.fillText('-5', 45, canvas.height - 30);
        ctx.fillText('0', 245, canvas.height - 30);
        ctx.fillText('5', 445, canvas.height - 30);
        ctx.fillText('0', 35, canvas.height - 45);
        ctx.fillText('0.5', 30, canvas.height/2 + 5);
        ctx.fillText('1', 35, 25);
    }
}

// Initialize comparison when page loads
document.addEventListener('DOMContentLoaded', () => {
    new LinkFunctionsComparison();
});
</script>

## üéØ H∆∞·ªõng d·∫´n L·ª±a ch·ªçn Link Function

### **Sigmoid (Logistic)** - L·ª±a ch·ªçn M·∫∑c ƒë·ªãnh ‚≠ê
**S·ª≠ d·ª•ng khi:**
- B√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n chu·∫©n
- C·∫ßn t·ªëc ƒë·ªô t√≠nh to√°n cao
- Mu·ªën gi·∫£i th√≠ch odds ratio
- D·ªØ li·ªáu c√¢n b·∫±ng, kh√¥ng c√≥ outliers c·ª±c ƒëoan

### **Probit** - Cho D·ªØ li·ªáu Gaussian
**S·ª≠ d·ª•ng khi:**
- D·ªØ li·ªáu c√≥ ph√¢n ph·ªëi g·∫ßn Gaussian
- C·∫ßn m√¥ h√¨nh h√≥a threshold effects
- Quan t√¢m ƒë·∫øn tail behavior ch√≠nh x√°c
- C√≥ th·ªùi gian t√≠nh to√°n (ch·∫≠m h∆°n sigmoid)

### **Complementary Log-log** - Cho Rare Events
**S·ª≠ d·ª•ng khi:**
- Modeling rare events (y=1 hi·∫øm)
- Survival analysis
- Time-to-event data
- Asymmetric data distribution

### **Cauchit** - Cho Heavy-tailed Data
**S·ª≠ d·ª•ng khi:**
- D·ªØ li·ªáu c√≥ nhi·ªÅu outliers
- Heavy-tailed distributions
- Robust modeling c·∫ßn thi·∫øt

## üí° K·∫øt lu·∫≠n v·ªÅ Link Functions

### **T·∫°i sao Sigmoid v·∫´n l√† l·ª±a ch·ªçn t·ªët nh·∫•t?**

1. **üßÆ To√°n h·ªçc Elegant:**
   - ƒê·∫°o h√†m ƒë∆°n gi·∫£n: $$\sigma'(z) = \sigma(z)(1-\sigma(z))$$
   - T√≠nh to√°n nhanh, ·ªïn ƒë·ªãnh s·ªë h·ªçc
   - Kh√¥ng c·∫ßn tra b·∫£ng hay t√≠nh t√≠ch ph√¢n

2. **üìä √ù nghƒ©a X√°c su·∫•t T·ª± nhi√™n:**
   - Xu·∫•t ph√°t t·ª´ log-odds model
   - Gi·∫£i th√≠ch tr·ª±c quan (odds ratio)
   - Symmetric around 0.5

3. **‚ö° Hi·ªáu su·∫•t T√≠nh to√°n:**
   - Ch·ªâ c·∫ßn m·ªôt ph√©p chia v√† exponential
   - Gradient descent hi·ªáu qu·∫£
   - Ph√π h·ª£p cho big data

4. **üîÑ T√≠nh L·ªìi ƒê·∫£m b·∫£o:**
   - Cross-entropy v·ªõi sigmoid lu√¥n l·ªìi
   - Global optimum guaranteed
   - Thu·∫≠t to√°n h·ªôi t·ª• ·ªïn ƒë·ªãnh

**Khuy·∫øn ngh·ªã:** B·∫Øt ƒë·∫ßu v·ªõi sigmoid, ch·ªâ chuy·ªÉn sang link function kh√°c khi c√≥ l√Ω do c·ª• th·ªÉ v√† ƒë√£ th·ª≠ nghi·ªám so s√°nh hi·ªáu su·∫•t.

## üî¢ M·ªü r·ªông: Softmax Regression (Multi-class Classification)

*Logistic Regression ch·ªâ x·ª≠ l√Ω ƒë∆∞·ª£c 2 l·ªõp. V·∫≠y l√†m th·∫ø n√†o ƒë·ªÉ ph√¢n lo·∫°i nhi·ªÅu l·ªõp?*

### üéØ T·ª´ Binary ƒë·∫øn Multi-class

**B√†i to√°n Multi-class Classification:**
- **Input**: $$\mathbf{x} \in \mathbb{R}^p$$
- **Output**: $$y \in \{1, 2, 3, ..., K\}$$ (K l·ªõp)
- **M·ª•c ti√™u**: D·ª± ƒëo√°n x√°c su·∫•t cho m·ªói l·ªõp

**V√≠ d·ª• th·ª±c t·∫ø:**
- **Nh·∫≠n d·∫°ng ch·ªØ s·ªë**: 0, 1, 2, ..., 9 (10 l·ªõp)
- **Ph√¢n lo·∫°i email**: spam, promotion, social, primary (4 l·ªõp)
- **Ch·∫©n ƒëo√°n b·ªánh**: kh·ªèe m·∫°nh, c·∫£m l·∫°nh, vi√™m h·ªçng, COVID (4 l·ªõp)

### üßÆ Softmax Function

**M·ªü r·ªông t·ª´ Sigmoid:**
- **Sigmoid**: $$\sigma(z) = \frac{e^z}{1 + e^z}$$ (2 l·ªõp)
- **Softmax**: $$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$$ (K l·ªõp)

**T√≠nh ch·∫•t Softmax:**
- ‚úÖ **T·ªïng b·∫±ng 1**: $$\sum_{i=1}^K \text{softmax}(\mathbf{z})_i = 1$$
- ‚úÖ **Lu√¥n d∆∞∆°ng**: $$\text{softmax}(\mathbf{z})_i > 0$$ $$\forall i$$
- ‚úÖ **Monotonic**: L·ªõp c√≥ $$z_i$$ l·ªõn h∆°n ‚Üí x√°c su·∫•t cao h∆°n
- ‚úÖ **Smooth**: Kh·∫£ vi m·ªçi n∆°i

### üèóÔ∏è M√¥ h√¨nh Softmax Regression

**Tham s·ªë m√¥ h√¨nh:**
- $$\mathbf{W} \in \mathbb{R}^{p \times K}$$: ma tr·∫≠n tr·ªçng s·ªë
- $$\mathbf{b} \in \mathbb{R}^K$$: bias vector

**Forward pass:**
$$\mathbf{z} = \mathbf{W}^T \mathbf{x} + \mathbf{b}$$

$$P(y = k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}$$

**D·ª± ƒëo√°n:**
$$\hat{y} = \arg\max_k P(y = k | \mathbf{x})$$

### üìä Cross-Entropy Loss cho Multi-class

**One-hot encoding:**
- $$y = 2$$ (l·ªõp 2) ‚Üí $$\mathbf{y} = [0, 1, 0, 0]$$ (n·∫øu K=4)

**Multi-class Cross-Entropy:**
$$J(\mathbf{W}) = -\sum_{i=1}^n \sum_{k=1}^K y_{i,k} \log p_{i,k}$$

v·ªõi:
- $$y_{i,k}$$: one-hot encoding c·ªßa sample $$i$$
- $$p_{i,k} = P(y = k | \mathbf{x}_i)$$: x√°c su·∫•t d·ª± ƒëo√°n

**ƒê·∫∑c ƒëi·ªÉm:**
- **M·ªü r·ªông t·ª± nhi√™n** t·ª´ binary cross-entropy
- **V·∫´n convex** ‚Üí gradient descent ho·∫°t ƒë·ªông t·ªët
- **Ph·∫°t n·∫∑ng d·ª± ƒëo√°n sai** nh∆∞ binary case

### üîÑ So s√°nh Binary vs Multi-class

| Kh√≠a c·∫°nh | Binary Logistic | Softmax Regression |
|-----------|-----------------|-------------------|
| **S·ªë l·ªõp** | 2 | K ‚â• 2 |
| **Activation** | Sigmoid | Softmax |
| **Tham s·ªë** | $$\mathbf{w} \in \mathbb{R}^p$$ | $$\mathbf{W} \in \mathbb{R}^{p \times K}$$ |
| **Output** | 1 x√°c su·∫•t | K x√°c su·∫•t |
| **Loss** | Binary cross-entropy | Multi-class cross-entropy |
| **Gradient** | $$\mathbf{X}^T(\boldsymbol{\sigma} - \mathbf{y})$$ | $$\mathbf{X}^T(\mathbf{P} - \mathbf{Y})$$ |

### üéõÔ∏è Interactive Softmax Demo

<div id="softmax-demo" style="margin: 20px 0;">
    <div style="display: flex; gap: 20px; margin-bottom: 20px;">
        <div style="flex: 1;">
            <h4>üéöÔ∏è ƒêi·ªÅu khi·ªÉn Logits</h4>
            <div style="margin: 10px 0;">
                <label for="z1-slider">z‚ÇÅ (Class 1): <span id="z1-value">1.0</span></label>
                <input type="range" id="z1-slider" min="-3" max="3" step="0.1" value="1.0" style="width: 100%;">
            </div>
            <div style="margin: 10px 0;">
                <label for="z2-slider">z‚ÇÇ (Class 2): <span id="z2-value">0.5</span></label>
                <input type="range" id="z2-slider" min="-3" max="3" step="0.1" value="0.5" style="width: 100%;">
            </div>
            <div style="margin: 10px 0;">
                <label for="z3-slider">z‚ÇÉ (Class 3): <span id="z3-value">-0.5</span></label>
                <input type="range" id="z3-slider" min="-3" max="3" step="0.1" value="-0.5" style="width: 100%;">
            </div>
            <div style="margin: 10px 0;">
                <label for="z4-slider">z‚ÇÑ (Class 4): <span id="z4-value">-1.0</span></label>
                <input type="range" id="z4-slider" min="-3" max="3" step="0.1" value="-1.0" style="width: 100%;">
            </div>
        </div>
        <div style="flex: 1;">
            <h4>üìä Softmax Probabilities</h4>
            <div id="probabilities-display" style="font-family: monospace; font-size: 14px;">
                <div>Class 1: <span id="p1">0.000</span></div>
                <div>Class 2: <span id="p2">0.000</span></div>
                <div>Class 3: <span id="p3">0.000</span></div>
                <div>Class 4: <span id="p4">0.000</span></div>
                <hr>
                <div><strong>Sum: <span id="sum">1.000</span></strong></div>
                <div><strong>Predicted: Class <span id="predicted">1</span></strong></div>
            </div>
        </div>
    </div>
    
    <div style="display: flex; gap: 20px;">
        <div style="flex: 1;">
            <h4>üìà Probability Bars</h4>
            <canvas id="prob-bars-canvas" width="400" height="300" style="border: 1px solid #ccc;"></canvas>
        </div>
        <div style="flex: 1;">
            <h4>ü•ß Probability Pie Chart</h4>
            <canvas id="prob-pie-canvas" width="400" height="300" style="border: 1px solid #ccc;"></canvas>
        </div>
    </div>
</div>

<script>
class SoftmaxDemo {
    constructor() {
        this.z = [1.0, 0.5, -0.5, -1.0];
        this.colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'];
        this.setupEventListeners();
        this.updateDisplay();
    }
    
    setupEventListeners() {
        for (let i = 1; i <= 4; i++) {
            const slider = document.getElementById(`z${i}-slider`);
            if (slider) {
                slider.addEventListener('input', (e) => {
                    this.z[i-1] = parseFloat(e.target.value);
                    document.getElementById(`z${i}-value`).textContent = this.z[i-1].toFixed(1);
                    this.updateDisplay();
                });
            }
        }
    }
    
    softmax(z) {
        const maxZ = Math.max(...z);
        const expZ = z.map(zi => Math.exp(zi - maxZ)); // Numerical stability
        const sumExpZ = expZ.reduce((sum, exp) => sum + exp, 0);
        return expZ.map(exp => exp / sumExpZ);
    }
    
    updateDisplay() {
        const probs = this.softmax(this.z);
        const sum = probs.reduce((s, p) => s + p, 0);
        const predicted = probs.indexOf(Math.max(...probs)) + 1;
        
        // Update probability display
        for (let i = 1; i <= 4; i++) {
            const elem = document.getElementById(`p${i}`);
            if (elem) {
                elem.textContent = probs[i-1].toFixed(3);
                elem.style.color = this.colors[i-1];
                elem.style.fontWeight = predicted === i ? 'bold' : 'normal';
            }
        }
        
        const sumElem = document.getElementById('sum');
        if (sumElem) sumElem.textContent = sum.toFixed(3);
        
        const predElem = document.getElementById('predicted');
        if (predElem) {
            predElem.textContent = predicted;
            predElem.style.color = this.colors[predicted-1];
        }
        
        this.drawBars(probs);
        this.drawPie(probs);
    }
    
    drawBars(probs) {
        const canvas = document.getElementById('prob-bars-canvas');
        if (!canvas) return;
        
        const ctx = canvas.getContext('2d');
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = 60;
        const maxHeight = 200;
        const startX = 50;
        const startY = canvas.height - 50;
        
        // Draw axes
        ctx.strokeStyle = '#ccc';
        ctx.lineWidth = 1;
        ctx.beginPath();
        ctx.moveTo(30, 30);
        ctx.lineTo(30, startY);
        ctx.lineTo(canvas.width - 20, startY);
        ctx.stroke();
        
        // Draw bars
        for (let i = 0; i < 4; i++) {
            const x = startX + i * (barWidth + 20);
            const height = probs[i] * maxHeight;
            
            ctx.fillStyle = this.colors[i];
            ctx.fillRect(x, startY - height, barWidth, height);
            
            // Labels
            ctx.fillStyle = '#333';
            ctx.font = '12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText(`Class ${i+1}`, x + barWidth/2, startY + 20);
            ctx.fillText(probs[i].toFixed(3), x + barWidth/2, startY - height - 5);
        }
        
        // Y-axis labels
        ctx.textAlign = 'right';
        ctx.fillText('0.0', 25, startY + 5);
        ctx.fillText('0.5', 25, startY - maxHeight/2 + 5);
        ctx.fillText('1.0', 25, startY - maxHeight + 5);
    }
    
    drawPie(probs) {
        const canvas = document.getElementById('prob-pie-canvas');
        if (!canvas) return;
        
        const ctx = canvas.getContext('2d');
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        const centerX = canvas.width / 2;
        const centerY = canvas.height / 2;
        const radius = 100;
        
        let currentAngle = -Math.PI / 2; // Start from top
        
        for (let i = 0; i < 4; i++) {
            const sliceAngle = probs[i] * 2 * Math.PI;
            
            // Draw slice
            ctx.beginPath();
            ctx.moveTo(centerX, centerY);
            ctx.arc(centerX, centerY, radius, currentAngle, currentAngle + sliceAngle);
            ctx.closePath();
            ctx.fillStyle = this.colors[i];
            ctx.fill();
            ctx.strokeStyle = '#fff';
            ctx.lineWidth = 2;
            ctx.stroke();
            
            // Draw label
            if (probs[i] > 0.05) { // Only show label if slice is big enough
                const labelAngle = currentAngle + sliceAngle / 2;
                const labelX = centerX + Math.cos(labelAngle) * radius * 0.7;
                const labelY = centerY + Math.sin(labelAngle) * radius * 0.7;
                
                ctx.fillStyle = '#fff';
                ctx.font = 'bold 12px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(`${i+1}`, labelX, labelY + 4);
            }
            
            currentAngle += sliceAngle;
        }
        
        // Legend
        for (let i = 0; i < 4; i++) {
            const legendY = 20 + i * 20;
            ctx.fillStyle = this.colors[i];
            ctx.fillRect(20, legendY, 15, 15);
            ctx.fillStyle = '#333';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText(`Class ${i+1}: ${(probs[i] * 100).toFixed(1)}%`, 40, legendY + 12);
        }
    }
}

// Initialize demo when page loads
document.addEventListener('DOMContentLoaded', () => {
    if (document.getElementById('softmax-demo')) {
        new SoftmaxDemo();
    }
});
</script>

<style>
#softmax-demo {
    font-family: Arial, sans-serif;
    max-width: 1000px;
    margin: 0 auto;
}

#softmax-demo canvas {
    border-radius: 4px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

#softmax-demo input[type="range"] {
    width: 100%;
    margin: 5px 0;
}

#probabilities-display {
    background-color: #f5f5f5;
    padding: 15px;
    border-radius: 4px;
    line-height: 1.6;
}

#probabilities-display div {
    margin: 5px 0;
}
</style>

### üéØ ·ª®ng d·ª•ng Th·ª±c t·∫ø c·ªßa Softmax

#### **1. Computer Vision**
- **Image Classification**: ImageNet (1000 l·ªõp)
- **Object Detection**: COCO dataset (80 l·ªõp)
- **Medical Imaging**: Ph√¢n lo·∫°i X-ray, MRI

#### **2. Natural Language Processing**
- **Sentiment Analysis**: positive, negative, neutral
- **Language Detection**: English, Vietnamese, Chinese, ...
- **Text Classification**: news categories, spam detection

#### **3. Recommendation Systems**
- **Content Categories**: movies, music, books
- **User Preferences**: age groups, interests
- **Product Classification**: electronics, fashion, food

### üîÑ K·∫øt n·ªëi v·ªõi Deep Learning

**Softmax l√† building block quan tr·ªçng:**
- **Output layer** c·ªßa neural networks
- **Attention mechanisms** trong Transformers
- **Policy networks** trong Reinforcement Learning

**T·ª´ Logistic ‚Üí Softmax ‚Üí Neural Networks:**
1. **Logistic Regression**: Linear + Sigmoid
2. **Softmax Regression**: Linear + Softmax  
3. **Neural Networks**: Multiple layers + Softmax

### üí° T√≥m t·∫Øt Softmax Regression

**∆Øu ƒëi·ªÉm:**
- ‚úÖ **M·ªü r·ªông t·ª± nhi√™n** t·ª´ binary classification
- ‚úÖ **Probabilistic output** c√≥ √Ω nghƒ©a
- ‚úÖ **Convex optimization** ‚Üí ƒë√°ng tin c·∫≠y
- ‚úÖ **Efficient training** v·ªõi gradient descent

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå **Linear decision boundaries** (nh∆∞ logistic)
- ‚ùå **Assumes linear separability**
- ‚ùå **May struggle** v·ªõi complex patterns

**Khi n√†o s·ª≠ d·ª•ng:**
- Multi-class classification problems
- C·∫ßn probabilistic outputs
- Baseline model tr∆∞·ªõc khi th·ª≠ complex methods
- Interpretability quan tr·ªçng

**Softmax Regression l√† b∆∞·ªõc ƒë·ªám ho√†n h·∫£o** gi·ªØa simple logistic regression v√† complex neural networks!
