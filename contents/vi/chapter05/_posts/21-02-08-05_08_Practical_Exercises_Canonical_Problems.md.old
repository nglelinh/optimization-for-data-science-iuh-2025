---
layout: post
title: 05-08 B√†i T·∫≠p Th·ª±c H√†nh - C√°c B√†i To√°n T·ªëi ∆Øu Kinh ƒêi·ªÉn
chapter: '05'
order: 8
owner: GitHub Copilot
lang: vi
categories:
- chapter05
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - C√°c B√†i To√°n T·ªëi ∆Øu Kinh ƒêi·ªÉn

Ch∆∞∆°ng n√†y t·∫≠p trung v√†o vi·ªác th·ª±c h√†nh t√≠nh to√°n v·ªõi c√°c d·∫°ng b√†i to√°n t·ªëi ∆∞u kinh ƒëi·ªÉn: LP, QP, QCQP, SOCP, SDP v√† GP. C√°c b√†i t·∫≠p ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi√∫p b·∫°n hi·ªÉu s√¢u v·ªÅ c·∫•u tr√∫c, t√≠nh ch·∫•t v√† ph∆∞∆°ng ph√°p gi·∫£i c·ªßa t·ª´ng lo·∫°i b√†i to√°n.

---

## üìù **B√†i t·∫≠p 1: Quy Ho·∫°ch Tuy·∫øn T√≠nh (LP)**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.3)

X√©t b√†i to√°n LP sau:
$$\begin{align}
\min_{x_1, x_2} \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & x_1 + x_2 \geq 4 \\
& 2x_1 + x_2 \geq 5 \\
& x_1, x_2 \geq 0
\end{align}$$

**Y√™u c·∫ßu:**
1. V·∫Ω mi·ªÅn kh·∫£ thi v√† x√°c ƒë·ªãnh c√°c ƒë·ªânh
2. √Åp d·ª•ng thu·∫≠t to√°n ƒë∆°n h√¨nh (simplex) ƒë·ªÉ t√¨m nghi·ªám t·ªëi ∆∞u
3. Chuy·ªÉn v·ªÅ d·∫°ng chu·∫©n c·ªßa LP
4. Vi·∫øt b√†i to√°n ƒë·ªëi ng·∫´u v√† gi·∫£i th√≠ch √Ω nghƒ©a kinh t·∫ø
5. Ki·ªÉm tra ƒëi·ªÅu ki·ªán KKT t·∫°i nghi·ªám t·ªëi ∆∞u

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: V·∫Ω mi·ªÅn kh·∫£ thi v√† x√°c ƒë·ªãnh ƒë·ªânh**

Chuy·ªÉn b·∫•t ƒë·∫≥ng th·ª©c v·ªÅ d·∫°ng chu·∫©n:
- $$x_1 + x_2 \geq 4 \Rightarrow -x_1 - x_2 \leq -4$$
- $$2x_1 + x_2 \geq 5 \Rightarrow -2x_1 - x_2 \leq -5$$

**C√°c ƒë∆∞·ªùng bi√™n:**
- $$x_1 + x_2 = 4$$
- $$2x_1 + x_2 = 5$$
- $$x_1 = 0$$
- $$x_2 = 0$$

**T√¨m c√°c ƒë·ªânh (giao ƒëi·ªÉm):**

1. Giao c·ªßa $$x_1 + x_2 = 4$$ v√† $$2x_1 + x_2 = 5$$:
   $$x_1 = 1, x_2 = 3$$ ‚Üí ƒêi·ªÉm $$(1, 3)$$

2. Giao c·ªßa $$x_1 + x_2 = 4$$ v√† $$x_1 = 0$$:
   $$x_1 = 0, x_2 = 4$$ ‚Üí ƒêi·ªÉm $$(0, 4)$$

3. Giao c·ªßa $$2x_1 + x_2 = 5$$ v√† $$x_2 = 0$$:
   $$x_1 = 2.5, x_2 = 0$$ ‚Üí ƒêi·ªÉm $$(2.5, 0)$$

**ƒê√°nh gi√° h√†m m·ª•c ti√™u t·∫°i c√°c ƒë·ªânh:**
- T·∫°i $$(1, 3)$$: $$f = 2(1) + 3(3) = 11$$
- T·∫°i $$(0, 4)$$: $$f = 2(0) + 3(4) = 12$$
- T·∫°i $$(2.5, 0)$$: $$f = 2(2.5) + 3(0) = 5$$

**Nghi·ªám t·ªëi ∆∞u:** $$x^* = (2.5, 0)$$ v·ªõi gi√° tr·ªã t·ªëi ∆∞u $$f^* = 5$$

**B∆∞·ªõc 2: Thu·∫≠t to√°n ƒë∆°n h√¨nh**

**D·∫°ng chu·∫©n:**
$$\begin{align}
\min \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & -x_1 - x_2 + s_1 = -4 \\
& -2x_1 - x_2 + s_2 = -5 \\
& x_1, x_2, s_1, s_2 \geq 0
\end{align}$$

Ho·∫∑c nh√¢n v·ªõi -1 ƒë·ªÉ c√≥ v·∫ø ph·∫£i d∆∞∆°ng:
$$\begin{align}
\min \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & x_1 + x_2 - s_1 = 4 \\
& 2x_1 + x_2 - s_2 = 5 \\
& x_1, x_2, s_1, s_2 \geq 0
\end{align}$$

**L∆∞u √Ω:** Do c√≥ $$-s_1, -s_2$$ n√™n c·∫ßn th√™m bi·∫øn nh√¢n t·∫°o (artificial variables) ho·∫∑c s·ª≠ d·ª•ng two-phase simplex.

**Phase 1:** T√¨m nghi·ªám kh·∫£ thi c∆° s·ªü ban ƒë·∫ßu
**Phase 2:** T·ªëi ∆∞u h√≥a t·ª´ nghi·ªám kh·∫£ thi

**B∆∞·ªõc 3: D·∫°ng chu·∫©n c·ªßa LP**

$$\begin{align}
\min \quad & c^T x \\
\text{s.t.} \quad & Ax = b \\
& x \geq 0
\end{align}$$

V·ªõi:
$$c = \begin{bmatrix} 2 \\ 3 \\ 0 \\ 0 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 1 & -1 & 0 \\ 2 & 1 & 0 & -1 \end{bmatrix}, \quad b = \begin{bmatrix} 4 \\ 5 \end{bmatrix}$$

$$x = \begin{bmatrix} x_1 \\ x_2 \\ s_1 \\ s_2 \end{bmatrix}$$

**B∆∞·ªõc 4: B√†i to√°n ƒë·ªëi ng·∫´u**

**Primal:**
$$\min_{x} \quad c^T x \quad \text{s.t.} \quad Ax \geq b, x \geq 0$$

**Dual:**
$$\max_{y} \quad b^T y \quad \text{s.t.} \quad A^T y \leq c, y \geq 0$$

V·ªõi b√†i to√°n ban ƒë·∫ßu:
$$\begin{align}
\max_{y_1, y_2} \quad & 4y_1 + 5y_2 \\
\text{s.t.} \quad & y_1 + 2y_2 \leq 2 \\
& y_1 + y_2 \leq 3 \\
& y_1, y_2 \geq 0
\end{align}$$

**√ù nghƒ©a kinh t·∫ø:**
- $$y_1, y_2$$ l√† **gi√° b√≥ng** (shadow prices) c·ªßa c√°c r√†ng bu·ªôc
- Bi·ªÉu th·ªã gi√° tr·ªã gia tƒÉng c·ªßa h√†m m·ª•c ti√™u khi n·ªõi l·ªèng r√†ng bu·ªôc m·ªôt ƒë∆°n v·ªã

**Nghi·ªám ƒë·ªëi ng·∫´u t·ªëi ∆∞u:**
T·∫°i $$x^* = (2.5, 0)$$, r√†ng bu·ªôc $$2x_1 + x_2 \geq 5$$ ho·∫°t ƒë·ªông (active).
- T·ª´ complementary slackness: $$y_1 = 0, y_2 = 1$$
- Ki·ªÉm tra: $$b^T y = 4(0) + 5(1) = 5 = f^*$$ ‚úì (Strong duality)

**B∆∞·ªõc 5: ƒêi·ªÅu ki·ªán KKT**

**Stationarity:** $$\nabla f(x^*) + \sum \lambda_i \nabla g_i(x^*) + \sum \nu_i \nabla h_i(x^*) = 0$$

T·∫°i $$x^* = (2.5, 0)$$:
$$\nabla f = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$$

**R√†ng bu·ªôc b·∫•t ƒë·∫≥ng th·ª©c:**
- $$g_1 = -(x_1 + x_2) + 4$$: t·∫°i $$x^*$$ kh√¥ng ho·∫°t ƒë·ªông ($$g_1 = 1.5 > 0$$)
- $$g_2 = -(2x_1 + x_2) + 5$$: t·∫°i $$x^*$$ **ho·∫°t ƒë·ªông** ($$g_2 = 0$$)
- $$g_3 = -x_1$$: kh√¥ng ho·∫°t ƒë·ªông ($$g_3 = -2.5 < 0$$)
- $$g_4 = -x_2$$: **ho·∫°t ƒë·ªông** ($$g_4 = 0$$)

$$\nabla g_2 = \begin{bmatrix} -2 \\ -1 \end{bmatrix}, \quad \nabla g_4 = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$$

**Stationarity:**
$$\begin{bmatrix} 2 \\ 3 \end{bmatrix} + \lambda_2 \begin{bmatrix} -2 \\ -1 \end{bmatrix} + \lambda_4 \begin{bmatrix} 0 \\ -1 \end{bmatrix} = 0$$

Gi·∫£i h·ªá:
- $$2 - 2\lambda_2 = 0 \Rightarrow \lambda_2 = 1$$
- $$3 - \lambda_2 - \lambda_4 = 0 \Rightarrow \lambda_4 = 2$$

**Complementary slackness:** $$\lambda_i g_i(x^*) = 0$$
- $$\lambda_1 = 0$$ ($$g_1 > 0$$)
- $$\lambda_2 = 1$$, $$g_2 = 0$$ ‚úì
- $$\lambda_3 = 0$$ ($$g_3 < 0$$)
- $$\lambda_4 = 2$$, $$g_4 = 0$$ ‚úì

**K·∫øt lu·∫≠n:** T·∫•t c·∫£ ƒëi·ªÅu ki·ªán KKT th·ªèa m√£n, $$x^* = (2.5, 0)$$ l√† nghi·ªám t·ªëi ∆∞u.

</details>

---

## üìù **B√†i t·∫≠p 2: Quy Ho·∫°ch B·∫≠c Hai (QP)**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Example 4.1)

X√©t b√†i to√°n portfolio optimization:
$$\begin{align}
\min_{x} \quad & \frac{1}{2} x^T \Sigma x \\
\text{s.t.} \quad & \mu^T x \geq r_{\text{min}} \\
& \mathbf{1}^T x = 1 \\
& x \geq 0
\end{align}$$

V·ªõi ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:
$$\Sigma = \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix}$$

Vector l·ª£i nhu·∫≠n k·ª≥ v·ªçng:
$$\mu = \begin{bmatrix} 0.08 \\ 0.12 \\ 0.15 \end{bmatrix}$$

V√† l·ª£i nhu·∫≠n t·ªëi thi·ªÉu $$r_{\text{min}} = 0.10$$.

**Y√™u c·∫ßu:**
1. X√°c minh t√≠nh l·ªìi c·ªßa b√†i to√°n
2. Vi·∫øt ƒëi·ªÅu ki·ªán KKT
3. T√≠nh gradient v√† Hessian c·ªßa h√†m Lagrange
4. Gi·∫£i b√†i to√°n v·ªõi $$r_{\text{min}} = 0.10$$
5. V·∫Ω bi√™n hi·ªáu qu·∫£ khi thay ƒë·ªïi $$r_{\text{min}}$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: X√°c minh t√≠nh l·ªìi**

**H√†m m·ª•c ti√™u:** $$f(x) = \frac{1}{2} x^T \Sigma x$$

T√≠nh l·ªìi n·∫øu $$\Sigma \succeq 0$$ (n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng).

**Ki·ªÉm tra eigenvalues c·ªßa $$\Sigma$$:**

Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai lu√¥n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng (theo ƒë·ªãnh nghƒ©a).

T√≠nh eigenvalues:
$$\det(\Sigma - \lambda I) = 0$$

$$\begin{vmatrix} 0.04-\lambda & 0.01 & 0.005 \\ 0.01 & 0.09-\lambda & 0.02 \\ 0.005 & 0.02 & 0.16-\lambda \end{vmatrix} = 0$$

Gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫∑c tr∆∞ng (c√≥ th·ªÉ d√πng m√°y t√≠nh):
$$\lambda_1 \approx 0.1748, \quad \lambda_2 \approx 0.0838, \quad \lambda_3 \approx 0.0314$$

T·∫•t c·∫£ eigenvalues > 0 $$\Rightarrow \Sigma \succ 0$$ (x√°c ƒë·ªãnh d∆∞∆°ng).

**R√†ng bu·ªôc:**
- $$g(x) = -\mu^T x + r_{\text{min}}$$: affine ‚Üí l·ªìi
- $$h(x) = \mathbf{1}^T x - 1$$: affine
- $$x \geq 0$$: l·ªìi

**K·∫øt lu·∫≠n:** B√†i to√°n l√† QP l·ªìi.

**B∆∞·ªõc 2: ƒêi·ªÅu ki·ªán KKT**

**Lagrangian:**
$$L(x, \lambda, \nu, \omega) = \frac{1}{2} x^T \Sigma x + \lambda(-\mu^T x + r_{\text{min}}) + \nu(\mathbf{1}^T x - 1) - \omega^T x$$

V·ªõi:
- $$\lambda \geq 0$$: nh√¢n t·ª≠ Lagrange cho $$\mu^T x \geq r_{\text{min}}$$
- $$\nu$$: nh√¢n t·ª≠ Lagrange cho $$\mathbf{1}^T x = 1$$
- $$\omega \geq 0$$: nh√¢n t·ª≠ Lagrange cho $$x \geq 0$$

**ƒêi·ªÅu ki·ªán KKT:**

1. **Stationarity:**
   $$\nabla_x L = \Sigma x - \lambda \mu + \nu \mathbf{1} - \omega = 0$$

2. **Primal feasibility:**
   - $$\mu^T x \geq r_{\text{min}}$$
   - $$\mathbf{1}^T x = 1$$
   - $$x \geq 0$$

3. **Dual feasibility:**
   - $$\lambda \geq 0$$
   - $$\omega \geq 0$$

4. **Complementary slackness:**
   - $$\lambda(\mu^T x - r_{\text{min}}) = 0$$
   - $$\omega_i x_i = 0, \quad \forall i$$

**B∆∞·ªõc 3: Gradient v√† Hessian**

**Gradient:**
$$\nabla_x L = \Sigma x - \lambda \mu + \nu \mathbf{1} - \omega$$

**Hessian:**
$$\nabla_{xx}^2 L = \Sigma$$

V√¨ $$\Sigma \succ 0$$, h√†m Lagrange l√† strongly convex trong $$x$$.

**B∆∞·ªõc 4: Gi·∫£i b√†i to√°n v·ªõi $$r_{\text{min}} = 0.10$$**

T·ª´ ƒëi·ªÅu ki·ªán stationarity:
$$\Sigma x = \lambda \mu - \nu \mathbf{1} + \omega$$

N·∫øu gi·∫£ s·ª≠ kh√¥ng c√≥ short selling ($$x \geq 0$$) v√† m·ªôt s·ªë assets kh√¥ng ƒë∆∞·ª£c ƒë·∫ßu t∆∞ ($$x_i = 0$$), ta c·∫ßn x√°c ƒë·ªãnh active set.

**Gi·∫£ s·ª≠ nghi·ªám c√≥ d·∫°ng:** $$x_1, x_2 > 0$$, $$x_3 = 0$$ (active set $$\{3\}$$)

Khi ƒë√≥ $$\omega_1 = \omega_2 = 0$$, $$\omega_3 \geq 0$$.

H·ªá ph∆∞∆°ng tr√¨nh:
$$\begin{bmatrix} 0.04 & 0.01 \\ 0.01 & 0.09 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \lambda \begin{bmatrix} 0.08 \\ 0.12 \end{bmatrix} - \nu \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$

C√πng v·ªõi r√†ng bu·ªôc:
- $$x_1 + x_2 = 1$$
- $$0.08 x_1 + 0.12 x_2 = 0.10$$

T·ª´ r√†ng bu·ªôc th·ª© hai:
$$0.08 x_1 + 0.12 x_2 = 0.10$$
$$0.08 x_1 + 0.12 (1-x_1) = 0.10$$
$$0.08 x_1 + 0.12 - 0.12 x_1 = 0.10$$
$$-0.04 x_1 = -0.02$$
$$x_1 = 0.5$$

Do ƒë√≥: $$x_2 = 0.5$$, $$x_3 = 0$$

**Nghi·ªám t·ªëi ∆∞u:** $$x^* = \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix}$$

**Risk (variance):**
$$\sigma^2 = x^{*T} \Sigma x^* = \begin{bmatrix} 0.5 & 0.5 & 0 \end{bmatrix} \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix}$$

$$= \begin{bmatrix} 0.5 & 0.5 & 0 \end{bmatrix} \begin{bmatrix} 0.025 \\ 0.05 \\ 0.0125 \end{bmatrix} = 0.0375$$

$$\sigma = \sqrt{0.0375} \approx 0.1936$$ (19.36%)

**B∆∞·ªõc 5: Bi√™n hi·ªáu qu·∫£**

Bi√™n hi·ªáu qu·∫£ l√† ƒë∆∞·ªùng cong trong kh√¥ng gian $$(r, \sigma)$$ khi thay ƒë·ªïi $$r_{\text{min}}$$.

**C√¥ng th·ª©c t·ªïng qu√°t:**

V·ªõi QP portfolio:
$$\sigma^2(r) = \frac{A r^2 - 2B r + C}{AC - B^2}$$

Trong ƒë√≥:
- $$A = \mu^T \Sigma^{-1} \mu$$
- $$B = \mu^T \Sigma^{-1} \mathbf{1}$$
- $$C = \mathbf{1}^T \Sigma^{-1} \mathbf{1}$$

**T√≠nh $$\Sigma^{-1}$$:**

$$\Sigma^{-1} = \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix}^{-1}$$

(C√≥ th·ªÉ t√≠nh b·∫±ng m√°y t√≠nh ho·∫∑c ph∆∞∆°ng ph√°p Gauss-Jordan)

$$\Sigma^{-1} \approx \begin{bmatrix} 25.79 & -3.06 & 0.15 \\ -3.06 & 11.54 & -1.43 \\ 0.15 & -1.43 & 6.38 \end{bmatrix}$$

**T√≠nh A, B, C:**

$$A = \mu^T \Sigma^{-1} \mu = \begin{bmatrix} 0.08 & 0.12 & 0.15 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 0.08 \\ 0.12 \\ 0.15 \end{bmatrix}$$

$$B = \mu^T \Sigma^{-1} \mathbf{1} = \begin{bmatrix} 0.08 & 0.12 & 0.15 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

$$C = \mathbf{1}^T \Sigma^{-1} \mathbf{1} = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

**Bi√™n hi·ªáu qu·∫£** l√† ƒë∆∞·ªùng hyperbola trong kh√¥ng gian $$(r, \sigma)$$ v·ªõi:
- **Minimum variance point:** T·∫°i $$r = \frac{B}{C}$$
- **Efficient frontier:** Ph·∫ßn tƒÉng c·ªßa hyperbola ($$r \geq r_{\text{mvp}}$$)

</details>

---

## üìù **B√†i t·∫≠p 3: Second-Order Cone Programming (SOCP)**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.4)

X√©t b√†i to√°n robust optimization:
$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & (a_i + u_i)^T x \leq b_i, \quad \forall \lVert u_i \rVert_2 \leq \rho_i, \quad i = 1, \ldots, m
\end{align}$$

Trong ƒë√≥:
- $$a_i$$ l√† vector h·ªá s·ªë danh nghƒ©a
- $$u_i$$ l√† nhi·ªÖu kh√¥ng ch·∫Øc ch·∫Øn (uncertainty)
- $$\rho_i$$ l√† b√°n k√≠nh c·ªßa ellipsoid kh√¥ng ch·∫Øc ch·∫Øn

**V·ªõi d·ªØ li·ªáu c·ª• th·ªÉ:**
$$c = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad a_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad a_2 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$$

$$b_1 = 5, \quad b_2 = 6, \quad \rho_1 = 0.5, \quad \rho_2 = 0.3$$

**Y√™u c·∫ßu:**
1. Chuy·ªÉn r√†ng bu·ªôc b·ªÅn v·ªØng th√†nh r√†ng bu·ªôc SOCP
2. Vi·∫øt b√†i to√°n SOCP d·∫°ng chu·∫©n
3. Gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa r√†ng bu·ªôc b·ªÅn v·ªØng
4. So s√°nh v·ªõi b√†i to√°n danh nghƒ©a (kh√¥ng b·ªÅn v·ªØng)
5. T√≠nh "gi√° c·ªßa t√≠nh b·ªÅn v·ªØng"

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Chuy·ªÉn r√†ng bu·ªôc robust th√†nh SOCP**

**R√†ng bu·ªôc robust:**
$$(a_i + u_i)^T x \leq b_i, \quad \forall \lVert u_i \rVert_2 \leq \rho_i$$

T∆∞∆°ng ƒë∆∞∆°ng v·ªõi:
$$\max_{\lVert u_i \rVert_2 \leq \rho_i} (a_i + u_i)^T x \leq b_i$$

$$a_i^T x + \max_{\lVert u_i \rVert_2 \leq \rho_i} u_i^T x \leq b_i$$

B√†i to√°n max b√™n trong:
$$\max_{\lVert u_i \rVert_2 \leq \rho_i} u_i^T x$$

Theo Cauchy-Schwarz:
$$u_i^T x \leq \lVert u_i \rVert_2 \lVert x \rVert_2 \leq \rho_i \lVert x \rVert_2$$

V·ªõi gi√° tr·ªã max ƒë·∫°t ƒë∆∞·ª£c khi $$u_i = \rho_i \frac{x}{\lVert x \rVert_2}$$.

**R√†ng bu·ªôc SOCP:**
$$a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i$$

**B∆∞·ªõc 2: B√†i to√°n SOCP d·∫°ng chu·∫©n**

$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i, \quad i = 1, \ldots, m
\end{align}$$

V·ªõi d·ªØ li·ªáu c·ª• th·ªÉ:
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 + 0.5 \sqrt{x_1^2 + x_2^2} \leq 5 \\
& 2x_1 + x_2 + 0.3 \sqrt{x_1^2 + x_2^2} \leq 6
\end{align}$$

**D·∫°ng chu·∫©n SOCP:**

Gi·ªõi thi·ªáu bi·∫øn ph·ª• $$t$$:
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 + 0.5 t_1 \leq 5 \\
& 2x_1 + x_2 + 0.3 t_2 \leq 6 \\
& \lVert x \rVert_2 \leq t_1 \\
& \lVert x \rVert_2 \leq t_2
\end{align}$$

Ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng:
$$\begin{align}
\min_{x,t} \quad & c^T x \\
\text{s.t.} \quad & \lVert x \rVert_2 \leq t_i \\
& a_i^T x + \rho_i t_i \leq b_i
\end{align}$$

**B∆∞·ªõc 3: √ù nghƒ©a robust constraint**

**R√†ng bu·ªôc danh nghƒ©a (nominal):**
$$a_i^T x \leq b_i$$

**R√†ng bu·ªôc robust:**
$$a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i$$

**Gi·∫£i th√≠ch:**
- Ph·∫ßn $$\rho_i \lVert x \rVert_2$$ l√† "buffer" ƒë·ªÉ b·∫£o v·ªá tr∆∞·ªõc sai s·ªë c·ªßa $$a_i$$
- Khi $$\rho_i = 0$$: Kh√¥ng c√≥ b·∫•t ƒë·ªãnh ‚Üí r√†ng bu·ªôc danh nghƒ©a
- Khi $$\rho_i$$ tƒÉng: R√†ng bu·ªôc ch·∫∑t h∆°n ‚Üí nghi·ªám b·∫£o th·ªß h∆°n (conservative)
- ƒê·∫£m b·∫£o r√†ng bu·ªôc th·ªèa m√£n v·ªõi **m·ªçi** $$u_i$$ trong ellipsoid

**B∆∞·ªõc 4: So s√°nh v·ªõi b√†i to√°n nominal**

**B√†i to√°n nominal:**
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 \leq 5 \\
& 2x_1 + x_2 \leq 6
\end{align}$$

**Gi·∫£i b√†i to√°n nominal:**

C√°c ƒë·ªânh c·ªßa mi·ªÅn kh·∫£ thi:
1. Giao c·ªßa $$x_1 + x_2 = 5$$ v√† $$2x_1 + x_2 = 6$$:
   $$x_1 = 1, x_2 = 4$$ ‚Üí $$f = 1 + 2(4) = 9$$

2. Giao v·ªõi tr·ª•c:
   - $$(0, 5)$$: $$f = 10$$
   - $$(3, 0)$$: $$f = 3$$

**Nghi·ªám nominal:** $$x_{\text{nom}}^* = (3, 0)$$ v·ªõi $$f_{\text{nom}}^* = 3$$

**Gi·∫£i b√†i to√°n robust:**

R√†ng bu·ªôc tr·ªü th√†nh:
- $$x_1 + x_2 + 0.5\sqrt{x_1^2 + x_2^2} \leq 5$$
- $$2x_1 + x_2 + 0.3\sqrt{x_1^2 + x_2^2} \leq 6$$

T·∫°i $$x = (3, 0)$$:
- R√†ng bu·ªôc 1: $$3 + 0 + 0.5(3) = 4.5 \leq 5$$ ‚úì
- R√†ng bu·ªôc 2: $$6 + 0 + 0.3(3) = 6.9 \not\leq 6$$ ‚úó

Nghi·ªám danh nghƒ©a **vi ph·∫°m** r√†ng bu·ªôc robust!

C·∫ßn gi·∫£i l·∫°i v·ªõi r√†ng bu·ªôc robust (c√≥ th·ªÉ d√πng solver SOCP nh∆∞ ECOS, SCS).

**Nghi·ªám x·∫•p x·ªâ:** $$x_{\text{rob}}^* \approx (2.7, 0.3)$$ v·ªõi $$f_{\text{rob}}^* \approx 3.3$$

**B∆∞·ªõc 5: Price of robustness**

**ƒê·ªãnh nghƒ©a:**
$$\text{Price of Robustness} = \frac{f_{\text{rob}}^* - f_{\text{nom}}^*}{f_{\text{nom}}^*} \times 100\%$$

$$= \frac{3.3 - 3}{3} \times 100\% = 10\%$$

**Gi·∫£i th√≠ch:**
- Nghi·ªám robust c√≥ chi ph√≠ cao h∆°n 10% so v·ªõi nghi·ªám danh nghƒ©a
- ƒê·ªïi l·∫°i, nghi·ªám robust **ƒë·∫£m b·∫£o kh·∫£ thi** v·ªõi m·ªçi nhi·ªÖu trong ellipsoid
- Nghi·ªám danh nghƒ©a c√≥ th·ªÉ **vi ph·∫°m r√†ng bu·ªôc** khi c√≥ nhi·ªÖu

**Trade-off:**
- $$\rho_i$$ nh·ªè: Price of robustness th·∫•p, nh∆∞ng √≠t b·∫£o v·ªá
- $$\rho_i$$ l·ªõn: Price of robustness cao, nh∆∞ng b·∫£o v·ªá t·ªët h∆°n

</details>

---

## üìù **B√†i t·∫≠p 4: Semidefinite Programming (SDP)**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.6)

X√©t b√†i to√°n t√¨m ma tr·∫≠n correlation g·∫ßn nh·∫•t:

Cho ma tr·∫≠n ƒë·ªëi x·ª©ng $$C$$ (kh√¥ng ph·∫£i correlation matrix):
$$C = \begin{bmatrix} 1 & 0.9 & 0.8 \\ 0.9 & 1 & 0.7 \\ 0.8 & 0.7 & 1 \end{bmatrix}$$

Tuy nhi√™n, $$C$$ c√≥ eigenvalue √¢m, kh√¥ng ph·∫£i n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng.

**Y√™u c·∫ßu:**
1. Ki·ªÉm tra c√°c gi√° tr·ªã ri√™ng c·ªßa $$C$$
2. C√¥ng th·ª©c h√≥a b√†i to√°n t√¨m ma tr·∫≠n t∆∞∆°ng quan $$X$$ g·∫ßn $$C$$ nh·∫•t:
   $$\begin{align}
   \min_{X} \quad & \lVert X - C \rVert_F^2 \\
   \text{s.t.} \quad & X_{ii} = 1, \quad i = 1, 2, 3 \\
   & X \succeq 0 \\
   & X = X^T
   \end{align}$$
3. Vi·∫øt b√†i to√°n SDP d·∫°ng chu·∫©n
4. Gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa r√†ng bu·ªôc $$X \succeq 0$$
5. T√≠nh nghi·ªám x·∫•p x·ªâ b·∫±ng ph√©p chi·∫øu

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Ki·ªÉm tra eigenvalues c·ªßa $$C$$**

Ma tr·∫≠n:
$$C = \begin{bmatrix} 1 & 0.9 & 0.8 \\ 0.9 & 1 & 0.7 \\ 0.8 & 0.7 & 1 \end{bmatrix}$$

**T√≠nh eigenvalues:**

Ph∆∞∆°ng tr√¨nh ƒë·∫∑c tr∆∞ng:
$$\det(C - \lambda I) = 0$$

$$\begin{vmatrix} 1-\lambda & 0.9 & 0.8 \\ 0.9 & 1-\lambda & 0.7 \\ 0.8 & 0.7 & 1-\lambda \end{vmatrix} = 0$$

Khai tri·ªÉn ƒë·ªãnh th·ª©c:
$$(1-\lambda)[(1-\lambda)^2 - 0.49] - 0.9[0.9(1-\lambda) - 0.56] + 0.8[0.63 - 0.8(1-\lambda)] = 0$$

Gi·∫£i ph∆∞∆°ng tr√¨nh b·∫≠c ba (c√≥ th·ªÉ d√πng m√°y t√≠nh):
$$\lambda_1 \approx 2.44, \quad \lambda_2 \approx 0.54, \quad \lambda_3 \approx 0.02$$

**K·∫øt qu·∫£:** T·∫•t c·∫£ eigenvalues > 0 $$\Rightarrow C \succ 0$$

**L∆∞u √Ω:** Trong v√≠ d·ª• n√†y, $$C$$ th·ª±c ra ƒë√£ l√† n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng. ƒê·ªÉ minh h·ªça, ta s·ª≠a $$C$$ th√†nh:
$$C = \begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & 0.9 \\ 0.9 & 0.9 & 1 \end{bmatrix}$$

Khi ƒë√≥:
$$\lambda_1 \approx 2.7, \quad \lambda_2 \approx 0.1, \quad \lambda_3 \approx 0.2$$

V·∫´n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng. ƒê·ªÉ c√≥ eigenvalue √¢m:
$$C = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$

Eigenvalues:
$$\lambda_1 \approx 2.414, \quad \lambda_2 \approx 1, \quad \lambda_3 \approx -0.414$$

C√≥ eigenvalue √¢m $$\Rightarrow C \not\succeq 0$$ ‚úì

**B∆∞·ªõc 2: Formulate b√†i to√°n SDP**

**B√†i to√°n:**
$$\begin{align}
\min_{X \in \mathbb{S}^n} \quad & \lVert X - C \rVert_F^2 \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

**Khai tri·ªÉn Frobenius norm:**
$$\lVert X - C \rVert_F^2 = \text{tr}[(X-C)^T(X-C)] = \text{tr}(X^TX - X^TC - C^TX + C^TC)$$

$$= \text{tr}(X^2) - 2\text{tr}(XC) + \text{tr}(C^2)$$

V√¨ $$X$$ ƒë·ªëi x·ª©ng: $$\text{tr}(X^2) = \sum_{i,j} X_{ij}^2$$

**B√†i to√°n t∆∞∆°ng ƒë∆∞∆°ng:**
$$\begin{align}
\min_{X} \quad & -\text{tr}(XC) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

(B·ªè h·∫±ng s·ªë $$\text{tr}(C^2)$$ v√† $$\text{tr}(X^2)$$ kh√¥ng ƒë·ªïi do $$X_{ii} = 1$$)

**B∆∞·ªõc 3: D·∫°ng chu·∫©n SDP**

**D·∫°ng chu·∫©n SDP:**
$$\begin{align}
\min_{X} \quad & \text{tr}(CX) \\
\text{s.t.} \quad & \text{tr}(A_i X) = b_i, \quad i = 1, \ldots, m \\
& X \succeq 0
\end{align}$$

V·ªõi b√†i to√°n c·ªßa ch√∫ng ta:
- Ma tr·∫≠n cost: $$C \leftarrow -C$$ (ƒë·ªïi d·∫•u v√¨ minimize $$-\text{tr}(XC)$$)
- R√†ng bu·ªôc $$X_{ii} = 1$$: $$\text{tr}(E_{ii} X) = 1$$ v·ªõi $$E_{ii}$$ l√† ma tr·∫≠n ch·ªâ c√≥ ph·∫ßn t·ª≠ $$(i,i)$$ b·∫±ng 1, c√≤n l·∫°i b·∫±ng 0

**V·ªõi $$n = 3$$:**
$$\begin{align}
\min_{X} \quad & \text{tr}((-C) X) \\
\text{s.t.} \quad & \text{tr}(E_{11} X) = 1 \\
& \text{tr}(E_{22} X) = 1 \\
& \text{tr}(E_{33} X) = 1 \\
& X \succeq 0
\end{align}$$

Trong ƒë√≥:
$$E_{11} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \quad E_{22} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \quad E_{33} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$

**B∆∞·ªõc 4: √ù nghƒ©a $$X \succeq 0$$**

**ƒê·ªãnh nghƒ©a:** Ma tr·∫≠n $$X \succeq 0$$ (n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng) n·∫øu:
$$v^T X v \geq 0, \quad \forall v \in \mathbb{R}^n$$

T∆∞∆°ng ƒë∆∞∆°ng v·ªõi:
- T·∫•t c·∫£ eigenvalues $$\geq 0$$
- T·ªìn t·∫°i ph√¢n t√≠ch Cholesky: $$X = LL^T$$ v·ªõi $$L$$ l√† ma tr·∫≠n tam gi√°c d∆∞·ªõi

**Trong correlation matrix:**
- $$X \succeq 0$$ ƒë·∫£m b·∫£o ma tr·∫≠n l√† **h·ª£p l·ªá** (valid)
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi kh√¥ng c√≥ "m√¢u thu·∫´n" gi·ªØa c√°c correlations
- ƒê·∫£m b·∫£o t·ªìn t·∫°i c√°c bi·∫øn ng·∫´u nhi√™n c√≥ correlation structure $$X$$

**V√≠ d·ª• ma tr·∫≠n kh√¥ng h·ª£p l·ªá:**
$$\begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & -0.9 \\ 0.9 & -0.9 & 1 \end{bmatrix}$$

Kh√¥ng th·ªÉ c√≥ ba bi·∫øn $$X_1, X_2, X_3$$ sao cho:
- $$\text{corr}(X_1, X_2) = 0.9$$ (highly positive)
- $$\text{corr}(X_2, X_3) = -0.9$$ (highly negative)
- $$\text{corr}(X_1, X_3) = 0.9$$ (highly positive)

(M√¢u thu·∫´n logic: n·∫øu $$X_1 \approx X_2$$ v√† $$X_2 \approx -X_3$$, th√¨ $$X_1 \approx -X_3$$, kh√¥ng th·ªÉ $$X_1 \approx X_3$$)

**B∆∞·ªõc 5: Nghi·ªám x·∫•p x·ªâ b·∫±ng projection**

**Ph∆∞∆°ng ph√°p projection:**

Eigenvalue decomposition c·ªßa $$C$$:
$$C = Q \Lambda Q^T$$

Trong ƒë√≥:
$$\Lambda = \text{diag}(\lambda_1, \lambda_2, \lambda_3)$$

**Projection l√™n cone n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng:**
$$\Lambda_+ = \text{diag}(\max(0, \lambda_1), \max(0, \lambda_2), \max(0, \lambda_3))$$

$$C_+ = Q \Lambda_+ Q^T$$

**V·ªõi $$C = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$:**

$$\lambda_1 \approx 2.414, \quad \lambda_2 \approx 1, \quad \lambda_3 \approx -0.414$$

$$\Lambda_+ = \text{diag}(2.414, 1, 0)$$

$$C_+ = Q \Lambda_+ Q^T$$

**Chu·∫©n h√≥a diagonal:**

ƒê·∫∑t $$D = \text{diag}(1/\sqrt{C_{+,11}}, 1/\sqrt{C_{+,22}}, 1/\sqrt{C_{+,33}})$$

$$X = D C_+ D$$

Khi ƒë√≥ $$X_{ii} = 1$$ v√† $$X \succeq 0$$.

**Nghi·ªám x·∫•p x·ªâ:**
$$X \approx \begin{bmatrix} 1 & 0.83 & 0.35 \\ 0.83 & 1 & 0.71 \\ 0.35 & 0.71 & 1 \end{bmatrix}$$

**Ki·ªÉm tra:**
- $$X_{ii} = 1$$ ‚úì
- Eigenvalues c·ªßa $$X$$ ƒë·ªÅu $$\geq 0$$ ‚úì
- $$\lVert X - C \rVert_F \approx 0.76$$

</details>

---

## üìù **B√†i t·∫≠p 5: Geometric Programming (GP)**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.5)

X√©t b√†i to√°n thi·∫øt k·∫ø h·ªôp v·ªõi chi ph√≠ t·ªëi thi·ªÉu:

**Bi·∫øn thi·∫øt k·∫ø:**
- $$h$$: chi·ªÅu cao
- $$w$$: chi·ªÅu r·ªông
- $$d$$: chi·ªÅu s√¢u

**M·ª•c ti√™u:** Minimize t·ªïng chi ph√≠ v·∫≠t li·ªáu

**R√†ng bu·ªôc:**
1. Th·ªÉ t√≠ch √≠t nh·∫•t $$V_0 = 100$$ (ƒë∆°n v·ªã kh·ªëi)
2. T·ª∑ l·ªá khung h√¨nh: $$h/w \leq 2$$ v√† $$w/d \leq 3$$
3. Di·ªán t√≠ch b·ªÅ m·∫∑t t·ªëi ƒëa $$A_{\max} = 150$$ (ƒë∆°n v·ªã di·ªán t√≠ch)

**Chi ph√≠:**
- ƒê√°y v√† n·∫Øp: $2 per unit area
- C√°c m·∫∑t b√™n: $1 per unit area

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a b√†i to√°n t·ªëi ∆∞u
2. Chuy·ªÉn v·ªÅ d·∫°ng GP chu·∫©n b·∫±ng bi·∫øn ƒë·ªïi logarit
3. Ki·ªÉm tra t√≠nh l·ªìi trong kh√¥ng gian logarit
4. Gi·∫£i b√†i to√°n v√† ph√¢n t√≠ch nghi·ªám
5. Ph√¢n t√≠ch ƒë·ªô nh·∫°y: thay ƒë·ªïi $$V_0$$ ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o?

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Formulate b√†i to√°n**

**Bi·∫øn:** $$h, w, d > 0$$

**H√†m m·ª•c ti√™u (chi ph√≠):**
$$f(h, w, d) = 2 \cdot 2(w \cdot d) + 1 \cdot 2(h \cdot w + h \cdot d)$$
$$= 4wd + 2hw + 2hd$$

**R√†ng bu·ªôc:**

1. Th·ªÉ t√≠ch:
   $$hwd \geq V_0 = 100$$

2. T·ª∑ l·ªá:
   - $$h/w \leq 2 \Rightarrow h \leq 2w$$
   - $$w/d \leq 3 \Rightarrow w \leq 3d$$

3. Di·ªán t√≠ch b·ªÅ m·∫∑t:
   $$2(wd + hw + hd) \leq A_{\max} = 150$$
   $$wd + hw + hd \leq 75$$

**B√†i to√°n:**
$$\begin{align}
\min_{h,w,d} \quad & 4wd + 2hw + 2hd \\
\text{s.t.} \quad & hwd \geq 100 \\
& h \leq 2w \\
& w \leq 3d \\
& wd + hw + hd \leq 75 \\
& h, w, d > 0
\end{align}$$

**B∆∞·ªõc 2: Chuy·ªÉn v·ªÅ d·∫°ng GP**

**D·∫°ng chu·∫©n GP:**
$$\begin{align}
\min \quad & f_0(x) \\
\text{s.t.} \quad & f_i(x) \leq 1, \quad i = 1, \ldots, m \\
& g_i(x) = 1, \quad i = 1, \ldots, p \\
& x > 0
\end{align}$$

Trong ƒë√≥:
- $$f_i$$ l√† **posynomial**: t·ªïng c√°c **monomial** $$c x_1^{a_1} \cdots x_n^{a_n}$$ v·ªõi $$c > 0$$
- $$g_i$$ l√† **monomial**

**Chu·∫©n h√≥a:**

1. Chia h√†m m·ª•c ti√™u cho h·∫±ng s·ªë (ho·∫∑c gi·ªØ nguy√™n):
   $$f_0 = 4wd + 2hw + 2hd$$

2. R√†ng bu·ªôc th·ªÉ t√≠ch:
   $$hwd \geq 100 \Rightarrow \frac{100}{hwd} \leq 1$$

3. R√†ng bu·ªôc t·ª∑ l·ªá:
   $$h \leq 2w \Rightarrow \frac{h}{2w} \leq 1$$
   $$w \leq 3d \Rightarrow \frac{w}{3d} \leq 1$$

4. R√†ng bu·ªôc di·ªán t√≠ch:
   $$wd + hw + hd \leq 75 \Rightarrow \frac{wd + hw + hd}{75} \leq 1$$

**B√†i to√°n GP:**
$$\begin{align}
\min_{h,w,d} \quad & 4wd + 2hw + 2hd \\
\text{s.t.} \quad & \frac{100}{hwd} \leq 1 \\
& \frac{h}{2w} \leq 1 \\
& \frac{w}{3d} \leq 1 \\
& \frac{wd + hw + hd}{75} \leq 1 \\
& h, w, d > 0
\end{align}$$

**B∆∞·ªõc 3: Bi·∫øn ƒë·ªïi logarit**

ƒê·∫∑t:
$$x = \log h, \quad y = \log w, \quad z = \log d$$

Khi ƒë√≥:
$$h = e^x, \quad w = e^y, \quad d = e^z$$

**H√†m m·ª•c ti√™u:**
$$f_0 = 4e^{y+z} + 2e^{x+y} + 2e^{x+z}$$

L·∫•y logarit:
$$\log f_0 = \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z})$$

ƒê√¢y l√† **log-sum-exp**, m·ªôt h√†m **convex**!

$$\tilde{f}_0(x,y,z) = \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z})$$

**R√†ng bu·ªôc:**

1. $$\frac{100}{hwd} \leq 1 \Rightarrow 100 \leq hwd$$
   $$\log 100 \leq x + y + z$$
   $$x + y + z \geq \log 100 \approx 4.605$$

2. $$\frac{h}{2w} \leq 1 \Rightarrow h \leq 2w$$
   $$e^x \leq 2e^y$$
   $$x \leq y + \log 2$$
   $$x - y \leq 0.693$$

3. $$\frac{w}{3d} \leq 1 \Rightarrow w \leq 3d$$
   $$e^y \leq 3e^z$$
   $$y - z \leq \log 3 \approx 1.099$$

4. $$\frac{wd + hw + hd}{75} \leq 1$$
   $$e^{y+z} + e^{x+y} + e^{x+z} \leq 75$$
   $$\log(e^{y+z} + e^{x+y} + e^{x+z}) \leq \log 75 \approx 4.317$$

**B√†i to√°n convex:**
$$\begin{align}
\min_{x,y,z} \quad & \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z}) \\
\text{s.t.} \quad & x + y + z \geq 4.605 \\
& x - y \leq 0.693 \\
& y - z \leq 1.099 \\
& \log(e^{y+z} + e^{x+y} + e^{x+z}) \leq 4.317
\end{align}$$

**B∆∞·ªõc 4: Ki·ªÉm tra t√≠nh convex**

**H√†m log-sum-exp:**
$$\text{LSE}(u_1, \ldots, u_m) = \log\left(\sum_{i=1}^m e^{u_i}\right)$$

ƒê√¢y l√† m·ªôt h√†m **convex** (t√≠nh ch·∫•t c∆° b·∫£n c·ªßa log-sum-exp).

**Ch·ª©ng minh ng·∫Øn:**

Hessian c·ªßa LSE:
$$\nabla^2 \text{LSE}(u) = \text{diag}(p) - pp^T$$

Trong ƒë√≥:
$$p_i = \frac{e^{u_i}}{\sum_j e^{u_j}}$$ (softmax)

$$\nabla^2 \text{LSE}$$ l√† covariance matrix c·ªßa ph√¢n ph·ªëi categorical $$\Rightarrow$$ n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng $$\Rightarrow$$ convex.

**R√†ng bu·ªôc:**
- R√†ng bu·ªôc 1, 2, 3: affine $$\Rightarrow$$ convex
- R√†ng bu·ªôc 4: log-sum-exp $$\leq$$ h·∫±ng s·ªë $$\Rightarrow$$ convex

**K·∫øt lu·∫≠n:** B√†i to√°n sau bi·∫øn ƒë·ªïi logarit l√† **convex optimization**.

**B∆∞·ªõc 5: Gi·∫£i b√†i to√°n**

S·ª≠ d·ª•ng ƒëi·ªÅu ki·ªán KKT ho·∫∑c solver chuy√™n d·ª•ng cho GP (nh∆∞ CVXPY v·ªõi DCP).

**Ph√¢n t√≠ch nghi·ªám:**

T·ª´ symmetry v√† r√†ng bu·ªôc, ta ƒëo√°n nghi·ªám c√≥ d·∫°ng:
$$h = w = d$$ (h·ªôp l·∫≠p ph∆∞∆°ng)

**Ki·ªÉm tra:**

N·∫øu $$h = w = d = a$$:
- Th·ªÉ t√≠ch: $$a^3 \geq 100 \Rightarrow a \geq 4.64$$
- T·ª∑ l·ªá: $$a/a = 1 \leq 2$$ ‚úì, $$a/a = 1 \leq 3$$ ‚úì
- Di·ªán t√≠ch: $$3a^2 \leq 75 \Rightarrow a \leq 5$$

**Mi·ªÅn kh·∫£ thi:** $$4.64 \leq a \leq 5$$

**Chi ph√≠:**
$$f(a) = 4a^2 + 2a^2 + 2a^2 = 8a^2$$

Minimize $$8a^2$$ tr√™n $$[4.64, 5]$$ $$\Rightarrow$$ ch·ªçn $$a = 4.64$$.

**Nghi·ªám t·ªëi ∆∞u:**
$$h^* = w^* = d^* \approx 4.64$$

**Chi ph√≠ t·ªëi ∆∞u:**
$$f^* = 8(4.64)^2 \approx 172.3$$

**L∆∞u √Ω:** R√†ng bu·ªôc di·ªán t√≠ch b·ªÅ m·∫∑t kh√¥ng active ($$3 \times 4.64^2 = 64.5 < 75$$), ch·ªâ r√†ng bu·ªôc th·ªÉ t√≠ch active.

**B∆∞·ªõc 6: Sensitivity analysis**

**Thay ƒë·ªïi $$V_0$$:**

V·ªõi nghi·ªám $$h = w = d = a$$:
$$a^3 = V_0 \Rightarrow a = V_0^{1/3}$$

**Chi ph√≠:**
$$f = 8a^2 = 8V_0^{2/3}$$

**ƒê·ªô nh·∫°y:**
$$\frac{\partial f}{\partial V_0} = 8 \cdot \frac{2}{3} V_0^{-1/3} = \frac{16}{3V_0^{1/3}}$$

T·∫°i $$V_0 = 100$$:
$$\frac{\partial f}{\partial V_0} = \frac{16}{3 \times 4.64} \approx 1.15$$

**Gi·∫£i th√≠ch:**
- Khi tƒÉng th·ªÉ t√≠ch t·ªëi thi·ªÉu 1 ƒë∆°n v·ªã, chi ph√≠ tƒÉng kho·∫£ng $1.15
- $$\lambda^* = 1.15$$ l√† **shadow price** c·ªßa r√†ng bu·ªôc th·ªÉ t√≠ch
- T∆∞∆°ng ·ª©ng v·ªõi Lagrange multiplier trong ƒëi·ªÅu ki·ªán KKT

**Trade-off:**
- $$V_0$$ tƒÉng $$\Rightarrow$$ chi ph√≠ tƒÉng theo $$V_0^{2/3}$$ (sublinear)
- Economics of scale: Chi ph√≠ tr√™n ƒë∆°n v·ªã th·ªÉ t√≠ch gi·∫£m khi $$V_0$$ tƒÉng

</details>

---

## üìù **B√†i t·∫≠p 6: Least Squares v√† Regularization**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.2)

X√©t b√†i to√°n h·ªìi quy v·ªõi d·ªØ li·ªáu:

**Dataset:**
$$\begin{bmatrix}
x_1 & y_1 \\
1 & 1 \\
2 & 3 \\
3 & 2 \\
4 & 5 \\
5 & 4
\end{bmatrix}$$

**Y√™u c·∫ßu:**
1. Kh·ªõp m√¥ h√¨nh tuy·∫øn t√≠nh $$\hat{y} = \beta_0 + \beta_1 x$$ b·∫±ng OLS
2. Kh·ªõp m√¥ h√¨nh b·∫≠c hai $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$$ b·∫±ng OLS
3. Th√™m ƒëi·ªÅu chu·∫©n L2 (Ridge): $$\min \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_2^2$$
4. Th√™m ƒëi·ªÅu chu·∫©n L1 (Lasso): $$\min \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$
5. So s√°nh $$R^2$$, sai s·ªë hu·∫•n luy·ªán v√† ƒë·ªô ph·ª©c t·∫°p m√¥ h√¨nh

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: OLS m√¥ h√¨nh tuy·∫øn t√≠nh**

**M√¥ h√¨nh:** $$\hat{y} = \beta_0 + \beta_1 x$$

**Ma tr·∫≠n thi·∫øt k·∫ø:**
$$X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}, \quad y = \begin{bmatrix} 1 \\ 3 \\ 2 \\ 5 \\ 4 \end{bmatrix}$$

**Normal equation:**
$$\beta^* = (X^T X)^{-1} X^T y$$

**T√≠nh $$X^T X$$:**
$$X^T X = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$$

**T√≠nh $$(X^T X)^{-1}$$:**
$$\det(X^T X) = 5 \times 55 - 15^2 = 275 - 225 = 50$$

$$(X^T X)^{-1} = \frac{1}{50} \begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$$

**T√≠nh $$X^T y$$:**
$$X^T y = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 3 \\ 2 \\ 5 \\ 4 \end{bmatrix} = \begin{bmatrix} 15 \\ 52 \end{bmatrix}$$

**Nghi·ªám:**
$$\beta^* = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 15 \\ 52 \end{bmatrix} = \begin{bmatrix} 1.1 \times 15 - 0.3 \times 52 \\ -0.3 \times 15 + 0.1 \times 52 \end{bmatrix} = \begin{bmatrix} 0.9 \\ 0.7 \end{bmatrix}$$

**M√¥ h√¨nh:** $$\hat{y} = 0.9 + 0.7x$$

**Predicted values:**
$$\hat{y} = \begin{bmatrix} 0.9 + 0.7(1) \\ 0.9 + 0.7(2) \\ 0.9 + 0.7(3) \\ 0.9 + 0.7(4) \\ 0.9 + 0.7(5) \end{bmatrix} = \begin{bmatrix} 1.6 \\ 2.3 \\ 3.0 \\ 3.7 \\ 4.4 \end{bmatrix}$$

**Residuals:**
$$r = y - \hat{y} = \begin{bmatrix} 1 - 1.6 \\ 3 - 2.3 \\ 2 - 3.0 \\ 5 - 3.7 \\ 4 - 4.4 \end{bmatrix} = \begin{bmatrix} -0.6 \\ 0.7 \\ -1.0 \\ 1.3 \\ -0.4 \end{bmatrix}$$

**RSS (Residual Sum of Squares):**
$$\text{RSS} = \sum r_i^2 = 0.36 + 0.49 + 1.0 + 1.69 + 0.16 = 3.7$$

**TSS (Total Sum of Squares):**
$$\bar{y} = \frac{1+3+2+5+4}{5} = 3$$

$$\text{TSS} = \sum (y_i - \bar{y})^2 = (1-3)^2 + (3-3)^2 + (2-3)^2 + (5-3)^2 + (4-3)^2$$
$$= 4 + 0 + 1 + 4 + 1 = 10$$

**$$R^2$$:**
$$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{3.7}{10} = 0.63$$

**B∆∞·ªõc 2: OLS m√¥ h√¨nh b·∫≠c hai**

**M√¥ h√¨nh:** $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$$

**Ma tr·∫≠n thi·∫øt k·∫ø:**
$$X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \\ 1 & 5 & 25 \end{bmatrix}$$

**$$X^T X$$:**
$$X^T X = \begin{bmatrix} 5 & 15 & 55 \\ 15 & 55 & 225 \\ 55 & 225 & 979 \end{bmatrix}$$

**Gi·∫£i b·∫±ng m√°y t√≠nh ho·∫∑c ph∆∞∆°ng ph√°p s·ªë:**

$$\beta^* \approx \begin{bmatrix} 0.9 \\ 0.9 \\ -0.04 \end{bmatrix}$$

**M√¥ h√¨nh:** $$\hat{y} = 0.9 + 0.9x - 0.04x^2$$

**Predicted values:**
$$\hat{y} \approx \begin{bmatrix} 1.76 \\ 2.58 \\ 3.24 \\ 3.74 \\ 4.08 \end{bmatrix}$$

**RSS:**
$$\text{RSS} \approx 2.9$$

**$$R^2$$:**
$$R^2 = 1 - \frac{2.9}{10} = 0.71$$

**B∆∞·ªõc 3: Ridge Regression ($$\lambda = 1$$)**

**B√†i to√°n:**
$$\min_{\beta} \quad \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_2^2$$

**Nghi·ªám:**
$$\beta_{\text{ridge}}^* = (X^T X + \lambda I)^{-1} X^T y$$

V·ªõi $$\lambda = 1$$:
$$X^T X + I = \begin{bmatrix} 6 & 15 & 55 \\ 15 & 56 & 225 \\ 55 & 225 & 980 \end{bmatrix}$$

**Gi·∫£i:**
$$\beta_{\text{ridge}}^* \approx \begin{bmatrix} 0.85 \\ 0.82 \\ -0.035 \end{bmatrix}$$

**Nh·∫≠n x√©t:**
- H·ªá s·ªë nh·ªè h∆°n so v·ªõi OLS (shrinkage)
- $$R^2$$ gi·∫£m nh·∫π (trade-off bias-variance)

**B∆∞·ªõc 4: Lasso Regression**

**B√†i to√°n:**
$$\min_{\beta} \quad \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$

**Kh√¥ng c√≥ closed-form solution**, c·∫ßn d√πng:
- Coordinate descent
- Proximal gradient method
- ADMM

**V·ªõi $$\lambda = 0.5$$** (gi·∫£i b·∫±ng solver):
$$\beta_{\text{lasso}}^* \approx \begin{bmatrix} 0.88 \\ 0.75 \\ 0 \end{bmatrix}$$

**Nh·∫≠n x√©t:**
- $$\beta_2 = 0$$ (sparsity: Lasso lo·∫°i b·ªè feature kh√¥ng quan tr·ªçng)
- Feature selection t·ª± ƒë·ªông

**B∆∞·ªõc 5: So s√°nh**

| M√¥ h√¨nh | $$\beta_0$$ | $$\beta_1$$ | $$\beta_2$$ | RSS | $$R^2$$ | Complexity |
|---------|------------|------------|------------|-----|---------|------------|
| Linear OLS | 0.9 | 0.7 | - | 3.7 | 0.63 | 2 params |
| Quadratic OLS | 0.9 | 0.9 | -0.04 | 2.9 | 0.71 | 3 params |
| Ridge ($$\lambda=1$$) | 0.85 | 0.82 | -0.035 | 3.1 | 0.69 | 3 params (shrunk) |
| Lasso ($$\lambda=0.5$$) | 0.88 | 0.75 | 0 | 3.5 | 0.65 | 2 params (sparse) |

**K·∫øt lu·∫≠n:**
- **OLS Quadratic**: Fit t·ªët nh·∫•t ($$R^2$$ cao nh·∫•t), nh∆∞ng risk of overfitting
- **Ridge**: Gi·∫£m overfitting, h·ªá s·ªë ·ªïn ƒë·ªãnh h∆°n
- **Lasso**: Feature selection (lo·∫°i b·ªè $$x^2$$), model ƒë∆°n gi·∫£n nh·∫•t
- **Trade-off**: Bias-variance, complexity-interpretability

**Recommendation:**
- Dataset nh·ªè (n=5): Ch·ªçn model ƒë∆°n gi·∫£n (Linear ho·∫∑c Lasso)
- Dataset l·ªõn: C√≥ th·ªÉ d√πng Quadratic ho·∫∑c Ridge

</details>

---

## üìù **B√†i t·∫≠p 7: Conic Programming - T·ªïng Qu√°t**

**ƒê·ªÅ b√†i:** (D·ª±a tr√™n Boyd & Vandenberghe, Section 4.6)

X√©t b√†i to√°n conic programming t·ªïng qu√°t:
$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & Ax = b \\
& x \in \mathcal{K}
\end{align}$$

Trong ƒë√≥ $$\mathcal{K}$$ l√† **proper cone**.

**Cho c·ª• th·ªÉ:**
$$c = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}, \quad b = 2$$

V·ªõi c√°c cones kh√°c nhau:

**a) Nonnegative orthant:** $$\mathcal{K} = \mathbb{R}_+^3$$

**b) Second-order cone:** $$\mathcal{K} = \{(x_1, x_2, x_3) : x_3 \geq \sqrt{x_1^2 + x_2^2}\}$$

**c) Positive semidefinite cone:** V·ªõi $$x = \text{vec}(X)$$ v√† $$X \in \mathbb{S}_+^2$$

**Y√™u c·∫ßu:**
1. Gi·∫£i b√†i to√°n v·ªõi t·ª´ng n√≥n
2. Vi·∫øt b√†i to√°n ƒë·ªëi ng·∫´u cho m·ªói tr∆∞·ªùng h·ª£p
3. Ki·ªÉm tra t√≠nh ƒë·ªëi ng·∫´u m·∫°nh
4. So s√°nh nghi·ªám t·ªëi ∆∞u v√† gi√° tr·ªã t·ªëi ∆∞u
5. Ph√¢n t√≠ch h√¨nh h·ªçc c·ªßa t·ª´ng n√≥n

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1a: Gi·∫£i v·ªõi Nonnegative Orthant (LP)**

**B√†i to√°n:**
$$\begin{align}
\min_{x} \quad & x_1 + x_2 + x_3 \\
\text{s.t.} \quad & x_1 + x_2 + x_3 = 2 \\
& x_1, x_2, x_3 \geq 0
\end{align}$$

**Ph√¢n t√≠ch:**

V√¨ h√†m m·ª•c ti√™u v√† r√†ng bu·ªôc c√πng c√≥ d·∫°ng $$x_1 + x_2 + x_3$$, nghi·ªám t·ªëi ∆∞u ch·ªâ c·∫ßn th·ªèa m√£n r√†ng bu·ªôc.

**Lagrangian:**
$$L(x, \nu, \lambda) = x_1 + x_2 + x_3 + \nu(x_1 + x_2 + x_3 - 2) - \lambda_1 x_1 - \lambda_2 x_2 - \lambda_3 x_3$$

**ƒêi·ªÅu ki·ªán KKT:**

1. **Stationarity:**
   $$\frac{\partial L}{\partial x_i} = 1 + \nu - \lambda_i = 0 \Rightarrow \lambda_i = 1 + \nu$$

2. **Primal feasibility:** $$x_1 + x_2 + x_3 = 2$$, $$x \geq 0$$

3. **Dual feasibility:** $$\lambda \geq 0 \Rightarrow \nu \geq -1$$

4. **Complementary slackness:** $$\lambda_i x_i = 0$$

**Gi·∫£i:**

T·ª´ stationarity: $$\lambda_1 = \lambda_2 = \lambda_3 = 1 + \nu$$

N·∫øu t·∫•t c·∫£ $$x_i > 0$$, th√¨ $$\lambda_i = 0 \Rightarrow 1 + \nu = 0 \Rightarrow \nu = -1$$

Khi ƒë√≥ m·ªçi $$x$$ th·ªèa m√£n $$x_1 + x_2 + x_3 = 2$$, $$x \geq 0$$ ƒë·ªÅu l√† nghi·ªám t·ªëi ∆∞u.

**Nghi·ªám v√≠ d·ª•:**
- $$x^* = (2, 0, 0)$$ ho·∫∑c $$(0, 2, 0)$$ ho·∫∑c $$(0, 0, 2)$$
- $$x^* = (1, 1, 0)$$ ho·∫∑c c√°c permutation
- $$x^* = (2/3, 2/3, 2/3)$$

**Gi√° tr·ªã t·ªëi ∆∞u:** $$f^* = 2$$

**H√¨nh h·ªçc:** Mi·ªÅn kh·∫£ thi l√† ƒëo·∫°n th·∫≥ng (simplex) n·ªëi 3 ƒë·ªânh $$(2,0,0)$$, $$(0,2,0)$$, $$(0,0,2)$$ trong octant d∆∞∆°ng.

**B∆∞·ªõc 1b: Gi·∫£i v·ªõi Second-Order Cone**

**B√†i to√°n:**
$$\begin{align}
\min_{x} \quad & x_1 + x_2 + x_3 \\
\text{s.t.} \quad & x_1 + x_2 + x_3 = 2 \\
& x_3 \geq \sqrt{x_1^2 + x_2^2}
\end{align}$$

**Ph√¢n t√≠ch:**

K·∫øt h·ª£p hai r√†ng bu·ªôc:
$$x_3 = 2 - x_1 - x_2$$
$$2 - x_1 - x_2 \geq \sqrt{x_1^2 + x_2^2}$$

ƒê·∫∑t $$u = x_1, v = x_2$$:
$$2 - u - v \geq \sqrt{u^2 + v^2}$$

**B√¨nh ph∆∞∆°ng hai v·∫ø** (v·ªõi $$2 - u - v \geq 0$$):
$$(2 - u - v)^2 \geq u^2 + v^2$$
$$4 - 4u - 4v + u^2 + 2uv + v^2 \geq u^2 + v^2$$
$$4 - 4u - 4v + 2uv \geq 0$$
$$2 - 2u - 2v + uv \geq 0$$
$$uv \geq 2u + 2v - 2$$

**T·ªëi ∆∞u h√≥a:**

Minimize $$f = u + v + (2 - u - v) = 2$$ (h·∫±ng s·ªë!)

$$\Rightarrow$$ M·ªçi ƒëi·ªÉm th·ªèa m√£n r√†ng bu·ªôc ƒë·ªÅu l√† nghi·ªám t·ªëi ∆∞u.

**Nghi·ªám v√≠ d·ª•:**
- $$x^* = (0, 0, 2)$$: Ki·ªÉm tra $$2 \geq \sqrt{0 + 0}$$ ‚úì
- $$x^* = (1, 0, 1)$$: Ki·ªÉm tra $$1 \geq \sqrt{1 + 0}$$ ‚úì
- $$x^* = (0, 1, 1)$$: Ki·ªÉm tra $$1 \geq \sqrt{0 + 1}$$ ‚úì

**Gi√° tr·ªã t·ªëi ∆∞u:** $$f^* = 2$$

**H√¨nh h·ªçc:** Mi·ªÅn kh·∫£ thi l√† giao c·ªßa m·∫∑t ph·∫≥ng $$x_1 + x_2 + x_3 = 2$$ v·ªõi second-order cone.

**B∆∞·ªõc 2: B√†i to√°n ƒë·ªëi ng·∫´u**

**Dual conic program:**
$$\begin{align}
\max_{y, z} \quad & b^T y \\
\text{s.t.} \quad & A^T y + z = c \\
& z \in \mathcal{K}^*
\end{align}$$

Trong ƒë√≥ $$\mathcal{K}^*$$ l√† **dual cone** c·ªßa $$\mathcal{K}$$.

**a) Nonnegative orthant:**

$$\mathcal{K}^* = \mathbb{R}_+^3$$ (self-dual)

**Dual:**
$$\begin{align}
\max_{y, z} \quad & 2y \\
\text{s.t.} \quad & y + z_i = 1, \quad i = 1,2,3 \\
& z_i \geq 0
\end{align}$$

$$\Rightarrow y = 1 - z_i \leq 1$$

$$\max y = 1$$ t·∫°i $$z = 0$$

**Duality gap:** $$f^* - g^* = 2 - 2 = 0$$ ‚úì (Strong duality)

**b) Second-order cone:**

Second-order cone l√† **self-dual**:
$$\mathcal{K}^* = \mathcal{K}$$

**Dual:**
$$\begin{align}
\max_{y} \quad & 2y \\
\text{s.t.} \quad & y \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} + z = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \\
& z \in \mathcal{K}
\end{align}$$

$$z = \begin{bmatrix} 1-y \\ 1-y \\ 1-y \end{bmatrix}$$

R√†ng bu·ªôc cone: $$1-y \geq \sqrt{2(1-y)^2}$$

$$1-y \geq \sqrt{2}|1-y|$$

N·∫øu $$y \leq 1$$: $$1 \geq \sqrt{2} \Rightarrow$$ Kh√¥ng th·ªèa m√£n

N·∫øu $$y > 1$$: $$-(1-y) \geq \sqrt{2}(y-1) \Rightarrow -1 \geq \sqrt{2}$$ Kh√¥ng th·ªèa m√£n

**Edge case:** $$y = 1 \Rightarrow z = 0$$ (thu·ªôc cone boundary)

**Gi√° tr·ªã dual:** $$g^* = 2(1) = 2$$

**Strong duality:** $$f^* = g^* = 2$$ ‚úì

**B∆∞·ªõc 3: Ph√¢n t√≠ch h√¨nh h·ªçc**

**Nonnegative Orthant:**
- Cone ƒë∆°n gi·∫£n nh·∫•t: g√≥c vu√¥ng trong $$\mathbb{R}^3$$
- Mi·ªÅn kh·∫£ thi: ƒêo·∫°n th·∫≥ng trong octant d∆∞∆°ng
- M·ªçi ƒëi·ªÉm tr√™n ƒëo·∫°n th·∫≥ng ƒë·ªÅu t·ªëi ∆∞u

**Second-Order Cone (Lorentz Cone):**
- H√¨nh n√≥n "ice-cream" v·ªõi ƒë·ªânh t·∫°i g·ªëc t·ªça ƒë·ªô
- Tr·ª•c ƒë·ªëi x·ª©ng d·ªçc theo $$x_3$$
- Mi·ªÅn kh·∫£ thi: Ph·∫ßn c·ªßa m·∫∑t ph·∫≥ng n·∫±m trong cone
- Khi $$c \parallel (1,1,1)$$ v√† $$b = 2$$: To√†n b·ªô giao l√† nghi·ªám t·ªëi ∆∞u

**So s√°nh:**
- LP (nonnegative orthant): Polyhedral cone, ƒë∆°n gi·∫£n
- SOCP: Smooth cone, kh√¥ng c√≥ "g√≥c nh·ªçn"
- C·∫£ hai ƒë·ªÅu cho $$f^* = 2$$ trong b√†i to√°n n√†y

</details>

---

## üìù **B√†i t·∫≠p 8: Resource Allocation v·ªõi LP**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng th·ª±c t·∫ø)

M·ªôt c√¥ng ty s·∫£n xu·∫•t hai s·∫£n ph·∫©m A v√† B. 

**D·ªØ li·ªáu:**
- **L·ª£i nhu·∫≠n**: S·∫£n ph·∫©m A: $40/ƒë∆°n v·ªã, S·∫£n ph·∫©m B: $30/ƒë∆°n v·ªã
- **T√†i nguy√™n:**
  - Lao ƒë·ªông: 120 gi·ªù (A c·∫ßn 2 gi·ªù, B c·∫ßn 1 gi·ªù)
  - Nguy√™n li·ªáu: 100 kg (A c·∫ßn 1 kg, B c·∫ßn 2 kg)
  - M√°y m√≥c: 80 gi·ªù m√°y (A c·∫ßn 1 gi·ªù, B c·∫ßn 1 gi·ªù)
- **Nhu c·∫ßu th·ªã tr∆∞·ªùng:**
  - S·∫£n ph·∫©m A: T·ªëi thi·ªÉu 10 ƒë∆°n v·ªã
  - S·∫£n ph·∫©m B: T·ªëi ƒëa 50 ƒë∆°n v·ªã

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a b√†i to√°n LP
2. Gi·∫£i b·∫±ng ph∆∞∆°ng ph√°p ƒë·ªì h·ªça
3. T√¨m nghi·ªám t·ªëi ∆∞u v√† l·ª£i nhu·∫≠n t·ªëi ƒëa
4. X√°c ƒë·ªãnh t√†i nguy√™n n√†o l√† n√∫t th·∫Øt (gi√° b√≥ng)
5. Ph√¢n t√≠ch ƒë·ªô nh·∫°y: N·∫øu tƒÉng lao ƒë·ªông l√™n 150 gi·ªù th√¨ sao?

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Formulate LP**

**Bi·∫øn quy·∫øt ƒë·ªãnh:**
- $$x_A$$: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m A
- $$x_B$$: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m B

**H√†m m·ª•c ti√™u:**
$$\max \quad 40x_A + 30x_B$$

**R√†ng bu·ªôc:**
1. Lao ƒë·ªông: $$2x_A + x_B \leq 120$$
2. Nguy√™n li·ªáu: $$x_A + 2x_B \leq 100$$
3. M√°y m√≥c: $$x_A + x_B \leq 80$$
4. Nhu c·∫ßu A: $$x_A \geq 10$$
5. Nhu c·∫ßu B: $$x_B \leq 50$$
6. Kh√¥ng √¢m: $$x_A, x_B \geq 0$$

**B∆∞·ªõc 2: Ph∆∞∆°ng ph√°p ƒë·ªì h·ªça**

**V·∫Ω c√°c ƒë∆∞·ªùng r√†ng bu·ªôc:**

1. $$2x_A + x_B = 120$$: Giao tr·ª•c: $$(60, 0)$$ v√† $$(0, 120)$$
2. $$x_A + 2x_B = 100$$: Giao tr·ª•c: $$(100, 0)$$ v√† $$(0, 50)$$
3. $$x_A + x_B = 80$$: Giao tr·ª•c: $$(80, 0)$$ v√† $$(0, 80)$$
4. $$x_A = 10$$: ƒê∆∞·ªùng th·∫≥ng ƒë·ª©ng
5. $$x_B = 50$$: ƒê∆∞·ªùng n·∫±m ngang

**Mi·ªÅn kh·∫£ thi:** T·∫≠p c√°c ƒëi·ªÉm th·ªèa m√£n t·∫•t c·∫£ r√†ng bu·ªôc

**T√¨m c√°c ƒë·ªânh (vertices):**

1. **Giao c·ªßa $$2x_A + x_B = 120$$ v√† $$x_A = 10$$:**
   $$2(10) + x_B = 120 \Rightarrow x_B = 100$$
   Ki·ªÉm tra r√†ng bu·ªôc nguy√™n li·ªáu: $$10 + 2(100) = 210 > 100$$ ‚úó (Vi ph·∫°m)

2. **Giao c·ªßa $$x_A + 2x_B = 100$$ v√† $$x_A = 10$$:**
   $$10 + 2x_B = 100 \Rightarrow x_B = 45$$
   ƒêi·ªÉm $$(10, 45)$$
   Ki·ªÉm tra: Lao ƒë·ªông: $$2(10) + 45 = 65 < 120$$ ‚úì
   M√°y m√≥c: $$10 + 45 = 55 < 80$$ ‚úì
   
3. **Giao c·ªßa $$2x_A + x_B = 120$$ v√† $$x_A + 2x_B = 100$$:**
   Gi·∫£i h·ªá:
   $$2x_A + x_B = 120$$ ... (1)
   $$x_A + 2x_B = 100$$ ... (2)
   
   T·ª´ (1): $$x_B = 120 - 2x_A$$
   Thay v√†o (2): $$x_A + 2(120 - 2x_A) = 100$$
   $$x_A + 240 - 4x_A = 100$$
   $$-3x_A = -140$$
   $$x_A = 140/3 \approx 46.67$$
   $$x_B = 120 - 2(140/3) = 120 - 280/3 = 80/3 \approx 26.67$$
   
   ƒêi·ªÉm $$(46.67, 26.67)$$
   Ki·ªÉm tra m√°y m√≥c: $$46.67 + 26.67 = 73.34 < 80$$ ‚úì

4. **Giao c·ªßa $$x_A + 2x_B = 100$$ v√† $$x_A + x_B = 80$$:**
   Tr·ª´: $$x_B = 20$$, $$x_A = 60$$
   ƒêi·ªÉm $$(60, 20)$$

5. **Giao c·ªßa $$x_A + x_B = 80$$ v√† $$x_A = 10$$:**
   $$x_B = 70$$
   Ki·ªÉm tra: $$x_B \leq 50$$ ‚úó (Vi ph·∫°m)

6. **Giao c·ªßa $$x_A + x_B = 80$$ v√† $$x_B = 50$$:**
   $$x_A = 30$$
   ƒêi·ªÉm $$(30, 50)$$
   Ki·ªÉm tra: Lao ƒë·ªông: $$2(30) + 50 = 110 < 120$$ ‚úì
   Nguy√™n li·ªáu: $$30 + 2(50) = 130 > 100$$ ‚úó (Vi ph·∫°m)

**C√°c ƒë·ªânh h·ª£p l·ªá:**
- $$(10, 45)$$
- $$(46.67, 26.67)$$
- $$(60, 20)$$

**B∆∞·ªõc 3: T√≠nh l·ª£i nhu·∫≠n t·∫°i c√°c ƒë·ªânh**

1. $$(10, 45)$$: $$f = 40(10) + 30(45) = 400 + 1350 = 1750$$
2. $$(46.67, 26.67)$$: $$f = 40(46.67) + 30(26.67) = 1866.8 + 800.1 = 2666.9$$
3. $$(60, 20)$$: $$f = 40(60) + 30(20) = 2400 + 600 = 3000$$

**Nghi·ªám t·ªëi ∆∞u:** $$(x_A^*, x_B^*) = (60, 20)$$

**L·ª£i nhu·∫≠n t·ªëi ƒëa:** $$f^* = \$3000$$

**B∆∞·ªõc 4: Shadow Prices (Gi√° b√≥ng)**

T·∫°i nghi·ªám t·ªëi ∆∞u $$(60, 20)$$:

**Ki·ªÉm tra r√†ng bu·ªôc n√†o active:**
1. Lao ƒë·ªông: $$2(60) + 20 = 140 > 120$$ ‚úó ‚Üí Ki·ªÉm tra l·∫°i!
   
C√≥ v·∫ª c√≥ sai s√≥t. H√£y ki·ªÉm tra l·∫°i ƒëi·ªÉm $$(60, 20)$$:
- Lao ƒë·ªông: $$2(60) + 20 = 140 > 120$$ ‚úó

ƒêi·ªÉm n√†y **vi ph·∫°m** r√†ng bu·ªôc lao ƒë·ªông! Ph·∫£i t√¨m l·∫°i.

**T√¨m l·∫°i c√°c ƒë·ªânh ƒë√∫ng:**

R√†ng bu·ªôc active c√≥ th·ªÉ l√†:
- $$2x_A + x_B = 120$$ (Lao ƒë·ªông)
- $$x_A + 2x_B = 100$$ (Nguy√™n li·ªáu)

ƒêi·ªÉm $$(46.67, 26.67)$$ ·ªü tr√™n l√† giao c·ªßa hai r√†ng bu·ªôc n√†y.

Ki·ªÉm tra l·∫°i:
- Lao ƒë·ªông: $$2(46.67) + 26.67 = 120$$ ‚úì
- Nguy√™n li·ªáu: $$46.67 + 2(26.67) = 100$$ ‚úì
- M√°y m√≥c: $$46.67 + 26.67 = 73.34 < 80$$ ‚úì
- $$x_A = 46.67 \geq 10$$ ‚úì
- $$x_B = 26.67 \leq 50$$ ‚úì

**Nghi·ªám ch√≠nh x√°c:** $$x_A^* = 140/3$$, $$x_B^* = 80/3$$

**L·ª£i nhu·∫≠n:**
$$f^* = 40(140/3) + 30(80/3) = 5600/3 + 2400/3 = 8000/3 \approx 2666.67$$

**Shadow prices:**

R√†ng bu·ªôc active: Lao ƒë·ªông v√† Nguy√™n li·ªáu

T·ª´ ƒëi·ªÅu ki·ªán KKT, shadow prices l√† nghi·ªám c·ªßa:
$$\begin{bmatrix} 2 \\ 1 \end{bmatrix} \lambda_1 + \begin{bmatrix} 1 \\ 2 \end{bmatrix} \lambda_2 = \begin{bmatrix} 40 \\ 30 \end{bmatrix}$$

Gi·∫£i:
$$2\lambda_1 + \lambda_2 = 40$$ ... (1)
$$\lambda_1 + 2\lambda_2 = 30$$ ... (2)

T·ª´ (1): $$\lambda_2 = 40 - 2\lambda_1$$
Thay v√†o (2): $$\lambda_1 + 2(40 - 2\lambda_1) = 30$$
$$\lambda_1 + 80 - 4\lambda_1 = 30$$
$$-3\lambda_1 = -50$$
$$\lambda_1 = 50/3 \approx 16.67$$

$$\lambda_2 = 40 - 2(50/3) = 40 - 100/3 = 20/3 \approx 6.67$$

**Gi·∫£i th√≠ch:**
- **$$\lambda_1 = 16.67$$**: M·ªói gi·ªù lao ƒë·ªông b·ªï sung tƒÉng l·ª£i nhu·∫≠n $16.67
- **$$\lambda_2 = 6.67$$**: M·ªói kg nguy√™n li·ªáu b·ªï sung tƒÉng l·ª£i nhu·∫≠n $6.67
- **M√°y m√≥c**: Shadow price = 0 (kh√¥ng ph·∫£i bottleneck)

**B∆∞·ªõc 5: Sensitivity Analysis**

**TƒÉng lao ƒë·ªông l√™n 150 gi·ªù:**

R√†ng bu·ªôc m·ªõi: $$2x_A + x_B \leq 150$$

Giao v·ªõi nguy√™n li·ªáu: $$x_A + 2x_B = 100$$

Gi·∫£i l·∫°i:
$$2x_A + x_B = 150$$ ... (1')
$$x_A + 2x_B = 100$$ ... (2)

T·ª´ (1'): $$x_B = 150 - 2x_A$$
Thay v√†o (2): $$x_A + 2(150 - 2x_A) = 100$$
$$x_A + 300 - 4x_A = 100$$
$$-3x_A = -200$$
$$x_A = 200/3 \approx 66.67$$

$$x_B = 150 - 2(200/3) = 150 - 400/3 = 50/3 \approx 16.67$$

**Nghi·ªám m·ªõi:** $$(66.67, 16.67)$$

**L·ª£i nhu·∫≠n m·ªõi:**
$$f = 40(66.67) + 30(16.67) = 2666.8 + 500.1 = 3166.9$$

**Gia tƒÉng:**
$$\Delta f = 3166.9 - 2666.67 = 500.22$$

**Ki·ªÉm tra v·ªõi shadow price:**
$$\Delta f \approx \lambda_1 \times \Delta b_1 = 16.67 \times 30 = 500.1$$ ‚úì

**K·∫øt lu·∫≠n:**
- TƒÉng 30 gi·ªù lao ƒë·ªông ‚Üí L·ª£i nhu·∫≠n tƒÉng ~$500
- Ph√π h·ª£p v·ªõi shadow price $$\lambda_1 = 16.67$$/gi·ªù

</details>

---

## üìù **B√†i t·∫≠p 9: Logistic Regression v·ªõi QP**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng Machine Learning)

X√©t b√†i to√°n binary classification v·ªõi logistic regression:

**Dataset:**
| $$x_1$$ | $$x_2$$ | $$y$$ (label) |
|---------|---------|---------------|
| 1 | 2 | +1 |
| 2 | 3 | +1 |
| 3 | 3 | +1 |
| 4 | 5 | +1 |
| 6 | 8 | -1 |
| 7 | 9 | -1 |

**M√¥ h√¨nh:** $$P(y=+1 | x) = \sigma(w^T x + b)$$ v·ªõi $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a b√†i to√°n ∆∞·ªõc l∆∞·ª£ng h·ª£p l√Ω c·ª±c ƒë·∫°i
2. Chuy·ªÉn v·ªÅ b√†i to√°n t·ªëi ∆∞u kh√¥ng r√†ng bu·ªôc
3. Th√™m ƒëi·ªÅu chu·∫©n L2: $$\min \sum \log(1 + e^{-y_i(w^T x_i + b)}) + \frac{\lambda}{2} \lVert w \rVert_2^2$$
4. T√≠nh gradient v√† Hessian
5. √Åp d·ª•ng ph∆∞∆°ng ph√°p Newton (1-2 v√≤ng l·∫∑p)

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Maximum Likelihood**

**Likelihood:**
$$L(w, b) = \prod_{i=1}^n P(y_i | x_i, w, b)$$

$$P(y | x) = \begin{cases} \sigma(w^T x + b) & \text{if } y = +1 \\ 1 - \sigma(w^T x + b) & \text{if } y = -1 \end{cases}$$

**Unified form:**
$$P(y | x) = \sigma(y(w^T x + b))$$

(V√¨ khi $$y = -1$$: $$\sigma(-z) = 1 - \sigma(z)$$)

**Log-likelihood:**
$$\ell(w, b) = \sum_{i=1}^n \log \sigma(y_i(w^T x_i + b))$$

$$= \sum_{i=1}^n \log \frac{1}{1 + e^{-y_i(w^T x_i + b)}}$$

$$= -\sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)})$$

**B∆∞·ªõc 2: Optimization Problem**

**Maximize log-likelihood** $$\Leftrightarrow$$ **Minimize negative log-likelihood:**

$$\min_{w, b} \quad f(w, b) = \sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)})$$

**B∆∞·ªõc 3: Th√™m L2 Regularization**

$$\min_{w, b} \quad f(w, b) = \sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)}) + \frac{\lambda}{2} \lVert w \rVert_2^2$$

**V·ªõi $$\lambda = 0.1$$**

**B∆∞·ªõc 4: Gradient v√† Hessian**

ƒê·∫∑t $$z_i = y_i(w^T x_i + b)$$

**Gradient:**

$$\frac{\partial f}{\partial w_j} = \sum_{i=1}^n \frac{-y_i x_{ij} e^{-z_i}}{1 + e^{-z_i}} + \lambda w_j$$

$$= -\sum_{i=1}^n y_i x_{ij} \frac{1}{1 + e^{z_i}} + \lambda w_j$$

$$= -\sum_{i=1}^n y_i x_{ij} (1 - \sigma(z_i)) + \lambda w_j$$

$$= \sum_{i=1}^n y_i x_{ij} (\sigma(z_i) - 1) + \lambda w_j$$

**Vector form:**
$$\nabla_w f = \sum_{i=1}^n (\sigma(z_i) - 1) y_i x_i + \lambda w$$

$$\frac{\partial f}{\partial b} = \sum_{i=1}^n (\sigma(z_i) - 1) y_i$$

**Hessian:**

$$\frac{\partial^2 f}{\partial w_j \partial w_k} = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_{ij} x_{ik} + \lambda \delta_{jk}$$

**Matrix form:**
$$\nabla_{ww}^2 f = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_i x_i^T + \lambda I$$

$$= X^T D X + \lambda I$$

Trong ƒë√≥ $$D = \text{diag}(\sigma(z_i)(1 - \sigma(z_i)))$$.

**Mixed derivative:**
$$\frac{\partial^2 f}{\partial w_j \partial b} = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_{ij}$$

**Full Hessian:**
$$\nabla^2 f = \begin{bmatrix} X^T D X + \lambda I & X^T D \mathbf{1} \\ \mathbf{1}^T D X & \mathbf{1}^T D \mathbf{1} \end{bmatrix}$$

**B∆∞·ªõc 5: Newton's Method**

**Newton update:**
$$\begin{bmatrix} w^{(k+1)} \\ b^{(k+1)} \end{bmatrix} = \begin{bmatrix} w^{(k)} \\ b^{(k)} \end{bmatrix} - (\nabla^2 f)^{-1} \nabla f$$

**Kh·ªüi t·∫°o:** $$w^{(0)} = 0$$, $$b^{(0)} = 0$$

**Iteration 1:**

T√≠nh $$z_i^{(0)} = y_i(0 + 0) = 0$$ $$\forall i$$

$$\sigma(0) = 0.5$$

**Gradient:**
$$\nabla_w f^{(0)} = \sum_{i=1}^6 (0.5 - 1) y_i x_i + 0.1 \cdot 0$$

$$= -0.5 \sum_{i=1}^6 y_i x_i$$

$$= -0.5 [y_1 x_1 + \cdots + y_6 x_6]$$

$$= -0.5 [(+1)\begin{bmatrix} 1 \\ 2 \end{bmatrix} + (+1)\begin{bmatrix} 2 \\ 3 \end{bmatrix} + (+1)\begin{bmatrix} 3 \\ 3 \end{bmatrix} + (+1)\begin{bmatrix} 4 \\ 5 \end{bmatrix} + (-1)\begin{bmatrix} 6 \\ 8 \end{bmatrix} + (-1)\begin{bmatrix} 7 \\ 9 \end{bmatrix}]$$

$$= -0.5 [\begin{bmatrix} 1+2+3+4-6-7 \\ 2+3+3+5-8-9 \end{bmatrix}]$$

$$= -0.5 \begin{bmatrix} -3 \\ -4 \end{bmatrix} = \begin{bmatrix} 1.5 \\ 2 \end{bmatrix}$$

$$\frac{\partial f}{\partial b}^{(0)} = \sum (0.5-1) y_i = -0.5(1+1+1+1-1-1) = -0.5(2) = -1$$

**Hessian:**

$$D = \text{diag}(0.5 \times 0.5) = 0.25 I$$

$$X = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 3 & 3 \\ 4 & 5 \\ 6 & 8 \\ 7 & 9 \end{bmatrix}$$

$$X^T D X = 0.25 X^T X$$

$$X^T X = \begin{bmatrix} 1+4+9+16+36+49 & 2+6+9+20+48+63 \\ 2+6+9+20+48+63 & 4+9+9+25+64+81 \end{bmatrix}$$

$$= \begin{bmatrix} 115 & 148 \\ 148 & 192 \end{bmatrix}$$

$$X^T D X = 0.25 \begin{bmatrix} 115 & 148 \\ 148 & 192 \end{bmatrix} = \begin{bmatrix} 28.75 & 37 \\ 37 & 48 \end{bmatrix}$$

$$X^T D X + \lambda I = \begin{bmatrix} 28.85 & 37 \\ 37 & 48.1 \end{bmatrix}$$

$$X^T D \mathbf{1} = 0.25 \begin{bmatrix} 1+2+3+4+6+7 \\ 2+3+3+5+8+9 \end{bmatrix} = 0.25 \begin{bmatrix} 23 \\ 30 \end{bmatrix} = \begin{bmatrix} 5.75 \\ 7.5 \end{bmatrix}$$

$$\mathbf{1}^T D \mathbf{1} = 0.25 \times 6 = 1.5$$

**Full Hessian:**
$$H = \begin{bmatrix} 28.85 & 37 & 5.75 \\ 37 & 48.1 & 7.5 \\ 5.75 & 7.5 & 1.5 \end{bmatrix}$$

**Newton direction:** $$\Delta = -H^{-1} \nabla f$$

(C·∫ßn invert ma tr·∫≠n $$3 \times 3$$, c√≥ th·ªÉ d√πng m√°y t√≠nh)

X·∫•p x·ªâ k·∫øt qu·∫£:
$$\begin{bmatrix} w_1^{(1)} \\ w_2^{(1)} \\ b^{(1)} \end{bmatrix} \approx \begin{bmatrix} -0.2 \\ 0.4 \\ -1.5 \end{bmatrix}$$

**Iteration 2:** L·∫∑p l·∫°i v·ªõi gi√° tr·ªã m·ªõi.

**K·∫øt lu·∫≠n:**
- Sau 5-10 iterations, Newton's method h·ªôi t·ª• ƒë·∫øn nghi·ªám t·ªëi ∆∞u
- Convergence nhanh h∆°n gradient descent (quadratic vs. linear)
- Ph√π h·ª£p cho datasets v·ª´a v√† nh·ªè

</details>

---

## üìù **B√†i t·∫≠p 10: Geometric Programming - Signal Processing**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng x·ª≠ l√Ω t√≠n hi·ªáu)

X√©t b√†i to√°n thi·∫øt k·∫ø b·ªô l·ªçc th√¥ng th·∫•p (low-pass filter) v·ªõi m·ª•c ti√™u:

**Th√¥ng s·ªë k·ªπ thu·∫≠t:**
- **T·∫ßn s·ªë c·∫Øt (cutoff frequency):** $$f_c = 1$$ kHz
- **Ripple trong passband:** ‚â§ 0.5 dB (t·ª´ DC ƒë·∫øn $$f_c$$)
- **Attenuation trong stopband:** ‚â• 40 dB (t·ª´ $$1.2 f_c$$ tr·ªü l√™n)
- **ƒê·ªô ph·ª©c t·∫°p:** T·ªëi ƒëa 20 transistor (ƒë∆°n gi·∫£n h√≥a th√†nh chi ph√≠)

**M√¥ h√¨nh:** B·ªô l·ªçc Butterworth b·∫≠c n v·ªõi h√†m truy·ªÅn ƒë·∫°t:
$$H(s) = \frac{1}{\sqrt{1 + (\frac{s}{s_c})^n}}$$

V·ªõi $$s_c = 2\pi f_c$$.

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a b√†i to√°n GP ƒë·ªÉ thi·∫øt k·∫ø b·ªô l·ªçc t·ªëi ∆∞u
2. Chuy·ªÉn v·ªÅ d·∫°ng GP chu·∫©n v·ªõi bi·∫øn ƒë·ªïi logarit
3. Gi·∫£i b√†i to√°n v·ªõi n = 5 (b·ªô l·ªçc Butterworth)
4. Ph√¢n t√≠ch nghi·ªám: t·∫ßn s·ªë c·∫Øt, g·ª£n s√≥ng, suy hao
5. So s√°nh v·ªõi b·ªô l·ªçc Chebyshev

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Formulate GP**

**Bi·∫øn thi·∫øt k·∫ø:**
- $$n$$: B·∫≠c c·ªßa b·ªô l·ªçc (integer, nh∆∞ng relax th√†nh continuous)
- $$f_c$$: T·∫ßn s·ªë c·∫Øt th·ª±c t·∫ø (c√≥ th·ªÉ kh√°c v·ªõi y√™u c·∫ßu)

**H√†m m·ª•c ti√™u:** Minimize chi ph√≠ (t·ª∑ l·ªá thu·∫≠n v·ªõi $$n$$):
$$\min \quad n$$

**R√†ng bu·ªôc k·ªπ thu·∫≠t:**

1. **Ripple trong passband:** V·ªõi $$\omega \in [0, 1]$$:
   $$|H(j\omega)| \geq 10^{-0.025/20} \approx 0.944$$ (ƒë·ªëi v·ªõi ripple 0.5 dB)

2. **Attenuation trong stopband:** V·ªõi $$\omega \geq 1.2$$:
   $$|H(j\omega)| \leq 10^{-40/20} = 0.01$$

3. **ƒê·ªô ph·ª©c t·∫°p:** $$n \leq 20$$

**B∆∞·ªõc 2: Bi·ªÉu th·ª©c to√°n h·ªçc**

H√†m truy·ªÅn ƒë·∫°t Butterworth:
$$|H(j\omega)| = \frac{1}{\sqrt{1 + (\frac{\omega}{\omega_c})^{2n}}}$$

Trong ƒë√≥ $$\omega_c = 2\pi f_c$$.

**R√†ng bu·ªôc ripple:** V·ªõi $$\omega = 1$$:
$$\frac{1}{\sqrt{1 + (\frac{1}{\omega_c})^{2n}}} \geq 0.944$$

T∆∞∆°ng ƒë∆∞∆°ng:
$$\sqrt{1 + (\frac{1}{\omega_c})^{2n}} \leq 1/0.944 \approx 1.059$$

**R√†ng bu·ªôc attenuation:** V·ªõi $$\omega = 1.2$$:
$$\frac{1}{\sqrt{1 + (\frac{1.2}{\omega_c})^{2n}}} \leq 0.01$$

T∆∞∆°ng ƒë∆∞∆°ng:
$$\sqrt{1 + (\frac{1.2}{\omega_c})^{2n}} \geq 1/0.01 = 100$$

**B∆∞·ªõc 3: Chuy·ªÉn v·ªÅ GP**

ƒê·∫∑t $$x = \log \omega_c$$, $$y = n$$ (v√¨ n kh√¥ng √¢m).

**H√†m m·ª•c ti√™u:** $$\min y$$

**R√†ng bu·ªôc ripple:**
$$1 + (\frac{1}{e^x})^{2y} \leq (1.059)^2 \approx 1.122$$

**R√†ng bu·ªôc attenuation:**
$$1 + (\frac{1.2}{e^x})^{2y} \geq 100^2 = 10000$$

**ƒê·ªô ph·ª©c t·∫°p:** $$y \leq 20$$

**B∆∞·ªõc 4: Gi·∫£i b√†i to√°n**

**B√†i to√°n GP:**
$$\begin{align}
\min \quad & y \\
\text{s.t.} \quad & 1 + e^{-2y x} \leq 1.122 \\
& 1 + (1.2)^{2y} e^{-2y x} \geq 10000 \\
& y \leq 20
\end{align}$$

**S·ª≠ d·ª•ng bi·∫øn ƒë·ªïi logarit:**
ƒê·∫∑t $$a = x$$, $$b = y$$.

**Chuy·ªÉn r√†ng bu·ªôc:**
1. $$1 + e^{-2 b a} \leq 1.122$$
2. $$1 + e^{2b \log 1.2 - 2 b a} \geq 10000$$

**B∆∞·ªõc 5: Ph√¢n t√≠ch nghi·ªám**

**Gi·∫£ s·ª≠ nghi·ªám t·ªëi ∆∞u:** $$n^* \approx 4.8$$, $$f_c^* \approx 1.05$$ kHz

**Ki·ªÉm tra:**
- **Ripple:** T·∫°i $$\omega = 1$$: $$|H| \approx 0.945$$ (> 0.944) ‚úì
- **Attenuation:** T·∫°i $$\omega = 1.2$$: $$|H| \approx 0.009$$ (< 0.01) ‚úì
- **ƒê·ªô ph·ª©c t·∫°p:** $$n = 4.8 < 20$$ ‚úì

**So s√°nh v·ªõi Chebyshev:**
- **Butterworth:** Ripple t·ªëi ƒëa 0.5 dB, transition band r·ªông h∆°n
- **Chebyshev:** Ripple c·ªë ƒë·ªãnh, transition band h·∫πp h∆°n, nh∆∞ng c√≥ ripple trong passband

**Trade-off:**
- Butterworth: Smooth response, d·ªÖ thi·∫øt k·∫ø analog
- Chebyshev: Better selectivity, nh∆∞ng c√≥ ripple

</details>

---

## üìù **B√†i t·∫≠p 11: Semidefinite Programming - Max-Cut Approximation**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng ƒë·ªì th·ªã)

X√©t ƒë·ªì th·ªã v·ªõi adjacency matrix:
$$A = \begin{bmatrix} 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{bmatrix}$$

**Y√™u c·∫ßu:**
1. C√¥ng th·ª©c h√≥a b√†i to√°n Max-Cut th√†nh n·ªõi l·ªèng SDP
2. Gi·∫£i SDP ƒë·ªÉ t√¨m ch·∫∑n tr√™n cho max-cut
3. Ng·∫´u nhi√™n h√≥a ƒë·ªÉ t√¨m nghi·ªám c·∫Øt
4. T√≠nh gi√° tr·ªã c·∫Øt th·ª±c t·∫ø
5. So s√°nh t·ª∑ l·ªá x·∫•p x·ªâ

**B∆∞·ªõc 1: Formulate b√†i to√°n Max-Cut**

B√†i to√°n Max-Cut: T√¨m c√°ch ph√¢n chia ƒë·ªì th·ªã th√†nh hai t·∫≠p S v√† T ƒë·ªÉ t·ªëi ƒëa h√≥a s·ªë c·∫°nh c·∫Øt.

**Bi·∫øn nh·ªã ph√¢n:** $$x_i = \begin{cases} 1 & \text{n·∫øu ƒë·ªânh i thu·ªôc S} \\ -1 & \text{n·∫øu ƒë·ªânh i thu·ªôc T} \end{cases}$$

**B√†i to√°n nguy√™n:**
$$\max_{x \in \{-1,1\}^n} \quad \frac{1}{2} \sum_{i \neq j} (1 - x_i x_j) A_{ij}$$

**T∆∞∆°ng ƒë∆∞∆°ng v·ªõi:**
$$\max_{x \in \{-1,1\}^n} \quad \frac{1}{2} ( \mathbf{1}^T A \mathbf{1} - x^T A x )$$

**SDP Relaxation:**

Thay $$x_i x_j$$ b·∫±ng $$X_{ij}$$, v·ªõi $$X = xx^T$$ v√† $$X_{ii} = 1$$.

**B√†i to√°n SDP:**
$$\begin{align}
\max_{X} \quad & \frac{1}{2} \text{tr}(L X) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

Trong ƒë√≥ $$L$$ l√† Laplacian matrix c·ªßa ƒë·ªì th·ªã.

**B∆∞·ªõc 2: Gi·∫£i SDP**

V·ªõi adjacency matrix A, Laplacian $$L = D - A$$ v·ªõi $$D$$ l√† diagonal degree matrix.

**Degrees:** $$d_1 = 2, d_2 = 3, d_3 = 2, d_4 = 1$$

**L = \begin{bmatrix} 2 & -1 & -1 & 0 \\ -1 & 3 & -1 & -1 \\ -1 & -1 & 2 & 0 \\ 0 & -1 & 0 & 1 \end{bmatrix}$$

**B√†i to√°n SDP:**
$$\begin{align}
\max_{X} \quad & \frac{1}{2} \text{tr}(L X) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1,2,3,4 \\
& X \succeq 0
\end{align}$$

**Upper bound:** $$\frac{1}{2} \text{tr}(L X^*) \approx 2.5$$

**B∆∞·ªõc 3: Randomization**

**Eigendecomposition c·ªßa X^*:** $$X^* = Q \Lambda Q^T$$

**Random vector:** $$v = Q \Lambda^{1/2} u$$ v·ªõi $$u \sim \mathcal{N}(0,I)$$

**C·∫Øt:** $$S = \{i : v_i > 0\}$$, $$T = \{i : v_i \leq 0\}$$

**Gi√° tr·ªã c·∫Øt trung b√¨nh:** $$\approx 0.878 \times 2.5 \approx 2.2$$

**B∆∞·ªõc 4: Gi√° tr·ªã c·∫Øt th·ª±c t·∫ø t·ªëi ∆∞u**

**C·∫Øt t·ªëi ∆∞u:** S = {1,3}, T = {2,4}

**C√°c c·∫°nh c·∫Øt:** (1,2), (3,2) ‚Üí Gi√° tr·ªã c·∫Øt = 2

**B∆∞·ªõc 5: Approximation ratio**

**Approximation ratio:** $$\frac{2}{2.5} = 0.8$$

**K·∫øt lu·∫≠n:** SDP relaxation cho t·ª∑ l·ªá x·∫•p x·ªâ 0.8, kh√° t·ªët so v·ªõi guarantee l√Ω thuy·∫øt (0.878 cho Max-Cut).

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

### **Khi gi·∫£i LP:**
- T√¨m c√°c **ƒë·ªânh** c·ªßa ƒëa di·ªán l·ªìi
- √Åp d·ª•ng **thu·∫≠t to√°n Simplex** t·ª´ ƒë·ªânh n√†y ƒë·∫øn ƒë·ªânh kh√°c
- Ki·ªÉm tra **ƒëi·ªÅu ki·ªán t·ªëi ∆∞u** b·∫±ng chi ph√≠ r√∫t g·ªçn
- S·ª≠ d·ª•ng **t√≠nh ƒë·ªëi ng·∫´u** ƒë·ªÉ ki·ªÉm ch·ª©ng v√† t√¨m gi√° b√≥ng

### **Khi gi·∫£i QP:**
- Ki·ªÉm tra $$P \succeq 0$$ (t√≠nh l·ªìi)
- Vi·∫øt **ƒëi·ªÅu ki·ªán KKT**
- X√°c ƒë·ªãnh **t·∫≠p ho·∫°t ƒë·ªông** (r√†ng bu·ªôc ƒëang ho·∫°t ƒë·ªông)
- √Åp d·ª•ng **ph∆∞∆°ng ph√°p t·∫≠p ho·∫°t ƒë·ªông** ho·∫∑c ƒëi·ªÉm trong

### **Khi c√¥ng th·ª©c h√≥a SOCP:**
- Nh·∫≠n di·ªán r√†ng bu·ªôc c√≥ **chu·∫©n**: $$\lVert Ax + b \rVert_2 \leq c^T x + d$$
- Chuy·ªÉn r√†ng bu·ªôc b·ªÅn v·ªØng th√†nh SOCP
- S·ª≠ d·ª•ng **r√†ng bu·ªôc n√≥n** ƒë·ªÉ m√¥ h√¨nh h√≥a
- B·ªô gi·∫£i: ECOS, SCS, Mosek

### **Khi c√¥ng th·ª©c h√≥a SDP:**
- Nh·∫≠n di·ªán r√†ng bu·ªôc **ma tr·∫≠n**: $$X \succeq 0$$
- S·ª≠ d·ª•ng **LMI** (B·∫•t ƒë·∫≥ng th·ª©c Ma tr·∫≠n Tuy·∫øn t√≠nh)
- √Åp d·ª•ng cho **ƒëi·ªÅu khi·ªÉn**, **x·ª≠ l√Ω t√≠n hi·ªáu**, **t·ªëi ∆∞u t·ªï h·ª£p**
- B·ªô gi·∫£i: CVXOPT, SCS, Mosek

### **Khi chuy·ªÉn GP v·ªÅ l·ªìi:**
- **Bi·∫øn ƒë·ªïi logarit**: $$x_i = e^{y_i}$$
- **Log-sum-exp**: H√†m l·ªìi
- **Monomial**: $$e^{a^T y}$$ (affine trong kh√¥ng gian log)
- **Posynomial**: $$\log(\sum e^{a_i^T y})$$ (log-sum-exp)

### **Khi ch·ªçn ƒëi·ªÅu chu·∫©n:**
- **L2 (Ridge)**: Thu nh·ªè, t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng gi·ªØ l·∫°i
- **L1 (Lasso)**: Th∆∞a, ch·ªçn ƒë·∫∑c tr∆∞ng t·ª± ƒë·ªông
- **Elastic Net**: K·∫øt h·ª£p L1 v√† L2
- **Ch·ªçn $$\lambda$$**: Ki·ªÉm ƒë·ªãnh ch√©o

### **Ki·ªÉm tra k·∫øt qu·∫£:**
- **Primal feasibility**: T·∫•t c·∫£ r√†ng bu·ªôc th·ªèa m√£n
- **Dual feasibility**: Lagrange multipliers $$\geq 0$$
- **Complementary slackness**: $$\lambda_i g_i(x^*) = 0$$
- **Stationarity**: $$\nabla L = 0$$

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 4: Convex optimization problems
   - Section 4.2: Linear programming
   - Section 4.3: Quadratic programming
   - Section 4.4: Second-order cone programming
   - Section 4.5: Geometric programming
   - Section 4.6: Semidefinite programming

2. **Ben-Tal, A., & Nemirovski, A.** (2001). *Lectures on Modern Convex Optimization*.
   - Robust optimization formulations
   - Conic programming

3. **Nesterov, Y., & Nemirovskii, A.** (1994). *Interior-Point Polynomial Algorithms in Convex Programming*.
   - Interior-point methods for canonical problems

4. **Boyd, S.** (Stanford EE364a lectures)
   - [https://web.stanford.edu/class/ee364a/](https://web.stanford.edu/class/ee364a/)

---

## üéØ T·ªïng K·∫øt

Ch∆∞∆°ng n√†y cung c·∫•p k·ªπ nƒÉng th·ª±c h√†nh v·ªõi c√°c b√†i to√°n canonical:

‚úÖ **Linear Programming (LP)**: Simplex, duality, sensitivity analysis  
‚úÖ **Quadratic Programming (QP)**: Portfolio optimization, KKT conditions  
‚úÖ **SOCP**: Robust optimization, uncertainty quantification  
‚úÖ **SDP**: Matrix constraints, correlation matrices  
‚úÖ **GP**: Engineering design, logarithmic transformation  
‚úÖ **Least Squares**: OLS, Ridge, Lasso, regularization trade-offs

**K·ªπ nƒÉng ƒë·∫°t ƒë∆∞·ª£c:**
- Formulate b√†i to√°n th·ª±c t·∫ø th√†nh canonical forms
- Gi·∫£i v√† ph√¢n t√≠ch nghi·ªám t·ªëi ∆∞u
- S·ª≠ d·ª•ng duality v√† KKT conditions
- Hi·ªÉu trade-offs: optimality vs. robustness, bias vs. variance
- √Åp d·ª•ng regularization cho machine learning

**B∆∞·ªõc ti·∫øp theo:** √Åp d·ª•ng c√°c k·ªπ thu·∫≠t n√†y v√†o b√†i to√°n th·ª±c t·∫ø trong c√°c lƒ©nh v·ª±c:
- Finance: Portfolio optimization, risk management
- Engineering: Design optimization, control systems
- Machine Learning: Regression, classification, deep learning
- Operations Research: Resource allocation, scheduling

