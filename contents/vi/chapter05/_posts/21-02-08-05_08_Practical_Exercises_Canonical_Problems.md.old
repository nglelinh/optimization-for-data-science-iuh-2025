---
layout: post
title: 05-08 Bài Tập Thực Hành - Các Bài Toán Tối Ưu Kinh Điển
chapter: '05'
order: 8
owner: GitHub Copilot
lang: vi
categories:
- chapter05
lesson_type: required
---

# Bài Tập Thực Hành - Các Bài Toán Tối Ưu Kinh Điển

Chương này tập trung vào việc thực hành tính toán với các dạng bài toán tối ưu kinh điển: LP, QP, QCQP, SOCP, SDP và GP. Các bài tập được thiết kế để giúp bạn hiểu sâu về cấu trúc, tính chất và phương pháp giải của từng loại bài toán.

---

## 📝 **Bài tập 1: Quy Hoạch Tuyến Tính (LP)**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.3)

Xét bài toán LP sau:
$$\begin{align}
\min_{x_1, x_2} \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & x_1 + x_2 \geq 4 \\
& 2x_1 + x_2 \geq 5 \\
& x_1, x_2 \geq 0
\end{align}$$

**Yêu cầu:**
1. Vẽ miền khả thi và xác định các đỉnh
2. Áp dụng thuật toán đơn hình (simplex) để tìm nghiệm tối ưu
3. Chuyển về dạng chuẩn của LP
4. Viết bài toán đối ngẫu và giải thích ý nghĩa kinh tế
5. Kiểm tra điều kiện KKT tại nghiệm tối ưu

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Vẽ miền khả thi và xác định đỉnh**

Chuyển bất đẳng thức về dạng chuẩn:
- $$x_1 + x_2 \geq 4 \Rightarrow -x_1 - x_2 \leq -4$$
- $$2x_1 + x_2 \geq 5 \Rightarrow -2x_1 - x_2 \leq -5$$

**Các đường biên:**
- $$x_1 + x_2 = 4$$
- $$2x_1 + x_2 = 5$$
- $$x_1 = 0$$
- $$x_2 = 0$$

**Tìm các đỉnh (giao điểm):**

1. Giao của $$x_1 + x_2 = 4$$ và $$2x_1 + x_2 = 5$$:
   $$x_1 = 1, x_2 = 3$$ → Điểm $$(1, 3)$$

2. Giao của $$x_1 + x_2 = 4$$ và $$x_1 = 0$$:
   $$x_1 = 0, x_2 = 4$$ → Điểm $$(0, 4)$$

3. Giao của $$2x_1 + x_2 = 5$$ và $$x_2 = 0$$:
   $$x_1 = 2.5, x_2 = 0$$ → Điểm $$(2.5, 0)$$

**Đánh giá hàm mục tiêu tại các đỉnh:**
- Tại $$(1, 3)$$: $$f = 2(1) + 3(3) = 11$$
- Tại $$(0, 4)$$: $$f = 2(0) + 3(4) = 12$$
- Tại $$(2.5, 0)$$: $$f = 2(2.5) + 3(0) = 5$$

**Nghiệm tối ưu:** $$x^* = (2.5, 0)$$ với giá trị tối ưu $$f^* = 5$$

**Bước 2: Thuật toán đơn hình**

**Dạng chuẩn:**
$$\begin{align}
\min \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & -x_1 - x_2 + s_1 = -4 \\
& -2x_1 - x_2 + s_2 = -5 \\
& x_1, x_2, s_1, s_2 \geq 0
\end{align}$$

Hoặc nhân với -1 để có vế phải dương:
$$\begin{align}
\min \quad & 2x_1 + 3x_2 \\
\text{s.t.} \quad & x_1 + x_2 - s_1 = 4 \\
& 2x_1 + x_2 - s_2 = 5 \\
& x_1, x_2, s_1, s_2 \geq 0
\end{align}$$

**Lưu ý:** Do có $$-s_1, -s_2$$ nên cần thêm biến nhân tạo (artificial variables) hoặc sử dụng two-phase simplex.

**Phase 1:** Tìm nghiệm khả thi cơ sở ban đầu
**Phase 2:** Tối ưu hóa từ nghiệm khả thi

**Bước 3: Dạng chuẩn của LP**

$$\begin{align}
\min \quad & c^T x \\
\text{s.t.} \quad & Ax = b \\
& x \geq 0
\end{align}$$

Với:
$$c = \begin{bmatrix} 2 \\ 3 \\ 0 \\ 0 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 1 & -1 & 0 \\ 2 & 1 & 0 & -1 \end{bmatrix}, \quad b = \begin{bmatrix} 4 \\ 5 \end{bmatrix}$$

$$x = \begin{bmatrix} x_1 \\ x_2 \\ s_1 \\ s_2 \end{bmatrix}$$

**Bước 4: Bài toán đối ngẫu**

**Primal:**
$$\min_{x} \quad c^T x \quad \text{s.t.} \quad Ax \geq b, x \geq 0$$

**Dual:**
$$\max_{y} \quad b^T y \quad \text{s.t.} \quad A^T y \leq c, y \geq 0$$

Với bài toán ban đầu:
$$\begin{align}
\max_{y_1, y_2} \quad & 4y_1 + 5y_2 \\
\text{s.t.} \quad & y_1 + 2y_2 \leq 2 \\
& y_1 + y_2 \leq 3 \\
& y_1, y_2 \geq 0
\end{align}$$

**Ý nghĩa kinh tế:**
- $$y_1, y_2$$ là **giá bóng** (shadow prices) của các ràng buộc
- Biểu thị giá trị gia tăng của hàm mục tiêu khi nới lỏng ràng buộc một đơn vị

**Nghiệm đối ngẫu tối ưu:**
Tại $$x^* = (2.5, 0)$$, ràng buộc $$2x_1 + x_2 \geq 5$$ hoạt động (active).
- Từ complementary slackness: $$y_1 = 0, y_2 = 1$$
- Kiểm tra: $$b^T y = 4(0) + 5(1) = 5 = f^*$$ ✓ (Strong duality)

**Bước 5: Điều kiện KKT**

**Stationarity:** $$\nabla f(x^*) + \sum \lambda_i \nabla g_i(x^*) + \sum \nu_i \nabla h_i(x^*) = 0$$

Tại $$x^* = (2.5, 0)$$:
$$\nabla f = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$$

**Ràng buộc bất đẳng thức:**
- $$g_1 = -(x_1 + x_2) + 4$$: tại $$x^*$$ không hoạt động ($$g_1 = 1.5 > 0$$)
- $$g_2 = -(2x_1 + x_2) + 5$$: tại $$x^*$$ **hoạt động** ($$g_2 = 0$$)
- $$g_3 = -x_1$$: không hoạt động ($$g_3 = -2.5 < 0$$)
- $$g_4 = -x_2$$: **hoạt động** ($$g_4 = 0$$)

$$\nabla g_2 = \begin{bmatrix} -2 \\ -1 \end{bmatrix}, \quad \nabla g_4 = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$$

**Stationarity:**
$$\begin{bmatrix} 2 \\ 3 \end{bmatrix} + \lambda_2 \begin{bmatrix} -2 \\ -1 \end{bmatrix} + \lambda_4 \begin{bmatrix} 0 \\ -1 \end{bmatrix} = 0$$

Giải hệ:
- $$2 - 2\lambda_2 = 0 \Rightarrow \lambda_2 = 1$$
- $$3 - \lambda_2 - \lambda_4 = 0 \Rightarrow \lambda_4 = 2$$

**Complementary slackness:** $$\lambda_i g_i(x^*) = 0$$
- $$\lambda_1 = 0$$ ($$g_1 > 0$$)
- $$\lambda_2 = 1$$, $$g_2 = 0$$ ✓
- $$\lambda_3 = 0$$ ($$g_3 < 0$$)
- $$\lambda_4 = 2$$, $$g_4 = 0$$ ✓

**Kết luận:** Tất cả điều kiện KKT thỏa mãn, $$x^* = (2.5, 0)$$ là nghiệm tối ưu.

</details>

---

## 📝 **Bài tập 2: Quy Hoạch Bậc Hai (QP)**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Example 4.1)

Xét bài toán portfolio optimization:
$$\begin{align}
\min_{x} \quad & \frac{1}{2} x^T \Sigma x \\
\text{s.t.} \quad & \mu^T x \geq r_{\text{min}} \\
& \mathbf{1}^T x = 1 \\
& x \geq 0
\end{align}$$

Với ma trận hiệp phương sai:
$$\Sigma = \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix}$$

Vector lợi nhuận kỳ vọng:
$$\mu = \begin{bmatrix} 0.08 \\ 0.12 \\ 0.15 \end{bmatrix}$$

Và lợi nhuận tối thiểu $$r_{\text{min}} = 0.10$$.

**Yêu cầu:**
1. Xác minh tính lồi của bài toán
2. Viết điều kiện KKT
3. Tính gradient và Hessian của hàm Lagrange
4. Giải bài toán với $$r_{\text{min}} = 0.10$$
5. Vẽ biên hiệu quả khi thay đổi $$r_{\text{min}}$$

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Xác minh tính lồi**

**Hàm mục tiêu:** $$f(x) = \frac{1}{2} x^T \Sigma x$$

Tính lồi nếu $$\Sigma \succeq 0$$ (nửa xác định dương).

**Kiểm tra eigenvalues của $$\Sigma$$:**

Ma trận hiệp phương sai luôn nửa xác định dương (theo định nghĩa).

Tính eigenvalues:
$$\det(\Sigma - \lambda I) = 0$$

$$\begin{vmatrix} 0.04-\lambda & 0.01 & 0.005 \\ 0.01 & 0.09-\lambda & 0.02 \\ 0.005 & 0.02 & 0.16-\lambda \end{vmatrix} = 0$$

Giải phương trình đặc trưng (có thể dùng máy tính):
$$\lambda_1 \approx 0.1748, \quad \lambda_2 \approx 0.0838, \quad \lambda_3 \approx 0.0314$$

Tất cả eigenvalues > 0 $$\Rightarrow \Sigma \succ 0$$ (xác định dương).

**Ràng buộc:**
- $$g(x) = -\mu^T x + r_{\text{min}}$$: affine → lồi
- $$h(x) = \mathbf{1}^T x - 1$$: affine
- $$x \geq 0$$: lồi

**Kết luận:** Bài toán là QP lồi.

**Bước 2: Điều kiện KKT**

**Lagrangian:**
$$L(x, \lambda, \nu, \omega) = \frac{1}{2} x^T \Sigma x + \lambda(-\mu^T x + r_{\text{min}}) + \nu(\mathbf{1}^T x - 1) - \omega^T x$$

Với:
- $$\lambda \geq 0$$: nhân tử Lagrange cho $$\mu^T x \geq r_{\text{min}}$$
- $$\nu$$: nhân tử Lagrange cho $$\mathbf{1}^T x = 1$$
- $$\omega \geq 0$$: nhân tử Lagrange cho $$x \geq 0$$

**Điều kiện KKT:**

1. **Stationarity:**
   $$\nabla_x L = \Sigma x - \lambda \mu + \nu \mathbf{1} - \omega = 0$$

2. **Primal feasibility:**
   - $$\mu^T x \geq r_{\text{min}}$$
   - $$\mathbf{1}^T x = 1$$
   - $$x \geq 0$$

3. **Dual feasibility:**
   - $$\lambda \geq 0$$
   - $$\omega \geq 0$$

4. **Complementary slackness:**
   - $$\lambda(\mu^T x - r_{\text{min}}) = 0$$
   - $$\omega_i x_i = 0, \quad \forall i$$

**Bước 3: Gradient và Hessian**

**Gradient:**
$$\nabla_x L = \Sigma x - \lambda \mu + \nu \mathbf{1} - \omega$$

**Hessian:**
$$\nabla_{xx}^2 L = \Sigma$$

Vì $$\Sigma \succ 0$$, hàm Lagrange là strongly convex trong $$x$$.

**Bước 4: Giải bài toán với $$r_{\text{min}} = 0.10$$**

Từ điều kiện stationarity:
$$\Sigma x = \lambda \mu - \nu \mathbf{1} + \omega$$

Nếu giả sử không có short selling ($$x \geq 0$$) và một số assets không được đầu tư ($$x_i = 0$$), ta cần xác định active set.

**Giả sử nghiệm có dạng:** $$x_1, x_2 > 0$$, $$x_3 = 0$$ (active set $$\{3\}$$)

Khi đó $$\omega_1 = \omega_2 = 0$$, $$\omega_3 \geq 0$$.

Hệ phương trình:
$$\begin{bmatrix} 0.04 & 0.01 \\ 0.01 & 0.09 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \lambda \begin{bmatrix} 0.08 \\ 0.12 \end{bmatrix} - \nu \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$

Cùng với ràng buộc:
- $$x_1 + x_2 = 1$$
- $$0.08 x_1 + 0.12 x_2 = 0.10$$

Từ ràng buộc thứ hai:
$$0.08 x_1 + 0.12 x_2 = 0.10$$
$$0.08 x_1 + 0.12 (1-x_1) = 0.10$$
$$0.08 x_1 + 0.12 - 0.12 x_1 = 0.10$$
$$-0.04 x_1 = -0.02$$
$$x_1 = 0.5$$

Do đó: $$x_2 = 0.5$$, $$x_3 = 0$$

**Nghiệm tối ưu:** $$x^* = \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix}$$

**Risk (variance):**
$$\sigma^2 = x^{*T} \Sigma x^* = \begin{bmatrix} 0.5 & 0.5 & 0 \end{bmatrix} \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix}$$

$$= \begin{bmatrix} 0.5 & 0.5 & 0 \end{bmatrix} \begin{bmatrix} 0.025 \\ 0.05 \\ 0.0125 \end{bmatrix} = 0.0375$$

$$\sigma = \sqrt{0.0375} \approx 0.1936$$ (19.36%)

**Bước 5: Biên hiệu quả**

Biên hiệu quả là đường cong trong không gian $$(r, \sigma)$$ khi thay đổi $$r_{\text{min}}$$.

**Công thức tổng quát:**

Với QP portfolio:
$$\sigma^2(r) = \frac{A r^2 - 2B r + C}{AC - B^2}$$

Trong đó:
- $$A = \mu^T \Sigma^{-1} \mu$$
- $$B = \mu^T \Sigma^{-1} \mathbf{1}$$
- $$C = \mathbf{1}^T \Sigma^{-1} \mathbf{1}$$

**Tính $$\Sigma^{-1}$$:**

$$\Sigma^{-1} = \begin{bmatrix} 0.04 & 0.01 & 0.005 \\ 0.01 & 0.09 & 0.02 \\ 0.005 & 0.02 & 0.16 \end{bmatrix}^{-1}$$

(Có thể tính bằng máy tính hoặc phương pháp Gauss-Jordan)

$$\Sigma^{-1} \approx \begin{bmatrix} 25.79 & -3.06 & 0.15 \\ -3.06 & 11.54 & -1.43 \\ 0.15 & -1.43 & 6.38 \end{bmatrix}$$

**Tính A, B, C:**

$$A = \mu^T \Sigma^{-1} \mu = \begin{bmatrix} 0.08 & 0.12 & 0.15 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 0.08 \\ 0.12 \\ 0.15 \end{bmatrix}$$

$$B = \mu^T \Sigma^{-1} \mathbf{1} = \begin{bmatrix} 0.08 & 0.12 & 0.15 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

$$C = \mathbf{1}^T \Sigma^{-1} \mathbf{1} = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \Sigma^{-1} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

**Biên hiệu quả** là đường hyperbola trong không gian $$(r, \sigma)$$ với:
- **Minimum variance point:** Tại $$r = \frac{B}{C}$$
- **Efficient frontier:** Phần tăng của hyperbola ($$r \geq r_{\text{mvp}}$$)

</details>

---

## 📝 **Bài tập 3: Second-Order Cone Programming (SOCP)**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.4)

Xét bài toán robust optimization:
$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & (a_i + u_i)^T x \leq b_i, \quad \forall \lVert u_i \rVert_2 \leq \rho_i, \quad i = 1, \ldots, m
\end{align}$$

Trong đó:
- $$a_i$$ là vector hệ số danh nghĩa
- $$u_i$$ là nhiễu không chắc chắn (uncertainty)
- $$\rho_i$$ là bán kính của ellipsoid không chắc chắn

**Với dữ liệu cụ thể:**
$$c = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad a_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad a_2 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$$

$$b_1 = 5, \quad b_2 = 6, \quad \rho_1 = 0.5, \quad \rho_2 = 0.3$$

**Yêu cầu:**
1. Chuyển ràng buộc bền vững thành ràng buộc SOCP
2. Viết bài toán SOCP dạng chuẩn
3. Giải thích ý nghĩa của ràng buộc bền vững
4. So sánh với bài toán danh nghĩa (không bền vững)
5. Tính "giá của tính bền vững"

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Chuyển ràng buộc robust thành SOCP**

**Ràng buộc robust:**
$$(a_i + u_i)^T x \leq b_i, \quad \forall \lVert u_i \rVert_2 \leq \rho_i$$

Tương đương với:
$$\max_{\lVert u_i \rVert_2 \leq \rho_i} (a_i + u_i)^T x \leq b_i$$

$$a_i^T x + \max_{\lVert u_i \rVert_2 \leq \rho_i} u_i^T x \leq b_i$$

Bài toán max bên trong:
$$\max_{\lVert u_i \rVert_2 \leq \rho_i} u_i^T x$$

Theo Cauchy-Schwarz:
$$u_i^T x \leq \lVert u_i \rVert_2 \lVert x \rVert_2 \leq \rho_i \lVert x \rVert_2$$

Với giá trị max đạt được khi $$u_i = \rho_i \frac{x}{\lVert x \rVert_2}$$.

**Ràng buộc SOCP:**
$$a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i$$

**Bước 2: Bài toán SOCP dạng chuẩn**

$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i, \quad i = 1, \ldots, m
\end{align}$$

Với dữ liệu cụ thể:
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 + 0.5 \sqrt{x_1^2 + x_2^2} \leq 5 \\
& 2x_1 + x_2 + 0.3 \sqrt{x_1^2 + x_2^2} \leq 6
\end{align}$$

**Dạng chuẩn SOCP:**

Giới thiệu biến phụ $$t$$:
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 + 0.5 t_1 \leq 5 \\
& 2x_1 + x_2 + 0.3 t_2 \leq 6 \\
& \lVert x \rVert_2 \leq t_1 \\
& \lVert x \rVert_2 \leq t_2
\end{align}$$

Hoặc tương đương:
$$\begin{align}
\min_{x,t} \quad & c^T x \\
\text{s.t.} \quad & \lVert x \rVert_2 \leq t_i \\
& a_i^T x + \rho_i t_i \leq b_i
\end{align}$$

**Bước 3: Ý nghĩa robust constraint**

**Ràng buộc danh nghĩa (nominal):**
$$a_i^T x \leq b_i$$

**Ràng buộc robust:**
$$a_i^T x + \rho_i \lVert x \rVert_2 \leq b_i$$

**Giải thích:**
- Phần $$\rho_i \lVert x \rVert_2$$ là "buffer" để bảo vệ trước sai số của $$a_i$$
- Khi $$\rho_i = 0$$: Không có bất định → ràng buộc danh nghĩa
- Khi $$\rho_i$$ tăng: Ràng buộc chặt hơn → nghiệm bảo thủ hơn (conservative)
- Đảm bảo ràng buộc thỏa mãn với **mọi** $$u_i$$ trong ellipsoid

**Bước 4: So sánh với bài toán nominal**

**Bài toán nominal:**
$$\begin{align}
\min_{x_1, x_2} \quad & x_1 + 2x_2 \\
\text{s.t.} \quad & x_1 + x_2 \leq 5 \\
& 2x_1 + x_2 \leq 6
\end{align}$$

**Giải bài toán nominal:**

Các đỉnh của miền khả thi:
1. Giao của $$x_1 + x_2 = 5$$ và $$2x_1 + x_2 = 6$$:
   $$x_1 = 1, x_2 = 4$$ → $$f = 1 + 2(4) = 9$$

2. Giao với trục:
   - $$(0, 5)$$: $$f = 10$$
   - $$(3, 0)$$: $$f = 3$$

**Nghiệm nominal:** $$x_{\text{nom}}^* = (3, 0)$$ với $$f_{\text{nom}}^* = 3$$

**Giải bài toán robust:**

Ràng buộc trở thành:
- $$x_1 + x_2 + 0.5\sqrt{x_1^2 + x_2^2} \leq 5$$
- $$2x_1 + x_2 + 0.3\sqrt{x_1^2 + x_2^2} \leq 6$$

Tại $$x = (3, 0)$$:
- Ràng buộc 1: $$3 + 0 + 0.5(3) = 4.5 \leq 5$$ ✓
- Ràng buộc 2: $$6 + 0 + 0.3(3) = 6.9 \not\leq 6$$ ✗

Nghiệm danh nghĩa **vi phạm** ràng buộc robust!

Cần giải lại với ràng buộc robust (có thể dùng solver SOCP như ECOS, SCS).

**Nghiệm xấp xỉ:** $$x_{\text{rob}}^* \approx (2.7, 0.3)$$ với $$f_{\text{rob}}^* \approx 3.3$$

**Bước 5: Price of robustness**

**Định nghĩa:**
$$\text{Price of Robustness} = \frac{f_{\text{rob}}^* - f_{\text{nom}}^*}{f_{\text{nom}}^*} \times 100\%$$

$$= \frac{3.3 - 3}{3} \times 100\% = 10\%$$

**Giải thích:**
- Nghiệm robust có chi phí cao hơn 10% so với nghiệm danh nghĩa
- Đổi lại, nghiệm robust **đảm bảo khả thi** với mọi nhiễu trong ellipsoid
- Nghiệm danh nghĩa có thể **vi phạm ràng buộc** khi có nhiễu

**Trade-off:**
- $$\rho_i$$ nhỏ: Price of robustness thấp, nhưng ít bảo vệ
- $$\rho_i$$ lớn: Price of robustness cao, nhưng bảo vệ tốt hơn

</details>

---

## 📝 **Bài tập 4: Semidefinite Programming (SDP)**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.6)

Xét bài toán tìm ma trận correlation gần nhất:

Cho ma trận đối xứng $$C$$ (không phải correlation matrix):
$$C = \begin{bmatrix} 1 & 0.9 & 0.8 \\ 0.9 & 1 & 0.7 \\ 0.8 & 0.7 & 1 \end{bmatrix}$$

Tuy nhiên, $$C$$ có eigenvalue âm, không phải nửa xác định dương.

**Yêu cầu:**
1. Kiểm tra các giá trị riêng của $$C$$
2. Công thức hóa bài toán tìm ma trận tương quan $$X$$ gần $$C$$ nhất:
   $$\begin{align}
   \min_{X} \quad & \lVert X - C \rVert_F^2 \\
   \text{s.t.} \quad & X_{ii} = 1, \quad i = 1, 2, 3 \\
   & X \succeq 0 \\
   & X = X^T
   \end{align}$$
3. Viết bài toán SDP dạng chuẩn
4. Giải thích ý nghĩa của ràng buộc $$X \succeq 0$$
5. Tính nghiệm xấp xỉ bằng phép chiếu

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Kiểm tra eigenvalues của $$C$$**

Ma trận:
$$C = \begin{bmatrix} 1 & 0.9 & 0.8 \\ 0.9 & 1 & 0.7 \\ 0.8 & 0.7 & 1 \end{bmatrix}$$

**Tính eigenvalues:**

Phương trình đặc trưng:
$$\det(C - \lambda I) = 0$$

$$\begin{vmatrix} 1-\lambda & 0.9 & 0.8 \\ 0.9 & 1-\lambda & 0.7 \\ 0.8 & 0.7 & 1-\lambda \end{vmatrix} = 0$$

Khai triển định thức:
$$(1-\lambda)[(1-\lambda)^2 - 0.49] - 0.9[0.9(1-\lambda) - 0.56] + 0.8[0.63 - 0.8(1-\lambda)] = 0$$

Giải phương trình bậc ba (có thể dùng máy tính):
$$\lambda_1 \approx 2.44, \quad \lambda_2 \approx 0.54, \quad \lambda_3 \approx 0.02$$

**Kết quả:** Tất cả eigenvalues > 0 $$\Rightarrow C \succ 0$$

**Lưu ý:** Trong ví dụ này, $$C$$ thực ra đã là nửa xác định dương. Để minh họa, ta sửa $$C$$ thành:
$$C = \begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & 0.9 \\ 0.9 & 0.9 & 1 \end{bmatrix}$$

Khi đó:
$$\lambda_1 \approx 2.7, \quad \lambda_2 \approx 0.1, \quad \lambda_3 \approx 0.2$$

Vẫn nửa xác định dương. Để có eigenvalue âm:
$$C = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$

Eigenvalues:
$$\lambda_1 \approx 2.414, \quad \lambda_2 \approx 1, \quad \lambda_3 \approx -0.414$$

Có eigenvalue âm $$\Rightarrow C \not\succeq 0$$ ✓

**Bước 2: Formulate bài toán SDP**

**Bài toán:**
$$\begin{align}
\min_{X \in \mathbb{S}^n} \quad & \lVert X - C \rVert_F^2 \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

**Khai triển Frobenius norm:**
$$\lVert X - C \rVert_F^2 = \text{tr}[(X-C)^T(X-C)] = \text{tr}(X^TX - X^TC - C^TX + C^TC)$$

$$= \text{tr}(X^2) - 2\text{tr}(XC) + \text{tr}(C^2)$$

Vì $$X$$ đối xứng: $$\text{tr}(X^2) = \sum_{i,j} X_{ij}^2$$

**Bài toán tương đương:**
$$\begin{align}
\min_{X} \quad & -\text{tr}(XC) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

(Bỏ hằng số $$\text{tr}(C^2)$$ và $$\text{tr}(X^2)$$ không đổi do $$X_{ii} = 1$$)

**Bước 3: Dạng chuẩn SDP**

**Dạng chuẩn SDP:**
$$\begin{align}
\min_{X} \quad & \text{tr}(CX) \\
\text{s.t.} \quad & \text{tr}(A_i X) = b_i, \quad i = 1, \ldots, m \\
& X \succeq 0
\end{align}$$

Với bài toán của chúng ta:
- Ma trận cost: $$C \leftarrow -C$$ (đổi dấu vì minimize $$-\text{tr}(XC)$$)
- Ràng buộc $$X_{ii} = 1$$: $$\text{tr}(E_{ii} X) = 1$$ với $$E_{ii}$$ là ma trận chỉ có phần tử $$(i,i)$$ bằng 1, còn lại bằng 0

**Với $$n = 3$$:**
$$\begin{align}
\min_{X} \quad & \text{tr}((-C) X) \\
\text{s.t.} \quad & \text{tr}(E_{11} X) = 1 \\
& \text{tr}(E_{22} X) = 1 \\
& \text{tr}(E_{33} X) = 1 \\
& X \succeq 0
\end{align}$$

Trong đó:
$$E_{11} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \quad E_{22} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \quad E_{33} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$

**Bước 4: Ý nghĩa $$X \succeq 0$$**

**Định nghĩa:** Ma trận $$X \succeq 0$$ (nửa xác định dương) nếu:
$$v^T X v \geq 0, \quad \forall v \in \mathbb{R}^n$$

Tương đương với:
- Tất cả eigenvalues $$\geq 0$$
- Tồn tại phân tích Cholesky: $$X = LL^T$$ với $$L$$ là ma trận tam giác dưới

**Trong correlation matrix:**
- $$X \succeq 0$$ đảm bảo ma trận là **hợp lệ** (valid)
- Tương đương với không có "mâu thuẫn" giữa các correlations
- Đảm bảo tồn tại các biến ngẫu nhiên có correlation structure $$X$$

**Ví dụ ma trận không hợp lệ:**
$$\begin{bmatrix} 1 & 0.9 & 0.9 \\ 0.9 & 1 & -0.9 \\ 0.9 & -0.9 & 1 \end{bmatrix}$$

Không thể có ba biến $$X_1, X_2, X_3$$ sao cho:
- $$\text{corr}(X_1, X_2) = 0.9$$ (highly positive)
- $$\text{corr}(X_2, X_3) = -0.9$$ (highly negative)
- $$\text{corr}(X_1, X_3) = 0.9$$ (highly positive)

(Mâu thuẫn logic: nếu $$X_1 \approx X_2$$ và $$X_2 \approx -X_3$$, thì $$X_1 \approx -X_3$$, không thể $$X_1 \approx X_3$$)

**Bước 5: Nghiệm xấp xỉ bằng projection**

**Phương pháp projection:**

Eigenvalue decomposition của $$C$$:
$$C = Q \Lambda Q^T$$

Trong đó:
$$\Lambda = \text{diag}(\lambda_1, \lambda_2, \lambda_3)$$

**Projection lên cone nửa xác định dương:**
$$\Lambda_+ = \text{diag}(\max(0, \lambda_1), \max(0, \lambda_2), \max(0, \lambda_3))$$

$$C_+ = Q \Lambda_+ Q^T$$

**Với $$C = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$:**

$$\lambda_1 \approx 2.414, \quad \lambda_2 \approx 1, \quad \lambda_3 \approx -0.414$$

$$\Lambda_+ = \text{diag}(2.414, 1, 0)$$

$$C_+ = Q \Lambda_+ Q^T$$

**Chuẩn hóa diagonal:**

Đặt $$D = \text{diag}(1/\sqrt{C_{+,11}}, 1/\sqrt{C_{+,22}}, 1/\sqrt{C_{+,33}})$$

$$X = D C_+ D$$

Khi đó $$X_{ii} = 1$$ và $$X \succeq 0$$.

**Nghiệm xấp xỉ:**
$$X \approx \begin{bmatrix} 1 & 0.83 & 0.35 \\ 0.83 & 1 & 0.71 \\ 0.35 & 0.71 & 1 \end{bmatrix}$$

**Kiểm tra:**
- $$X_{ii} = 1$$ ✓
- Eigenvalues của $$X$$ đều $$\geq 0$$ ✓
- $$\lVert X - C \rVert_F \approx 0.76$$

</details>

---

## 📝 **Bài tập 5: Geometric Programming (GP)**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.5)

Xét bài toán thiết kế hộp với chi phí tối thiểu:

**Biến thiết kế:**
- $$h$$: chiều cao
- $$w$$: chiều rộng
- $$d$$: chiều sâu

**Mục tiêu:** Minimize tổng chi phí vật liệu

**Ràng buộc:**
1. Thể tích ít nhất $$V_0 = 100$$ (đơn vị khối)
2. Tỷ lệ khung hình: $$h/w \leq 2$$ và $$w/d \leq 3$$
3. Diện tích bề mặt tối đa $$A_{\max} = 150$$ (đơn vị diện tích)

**Chi phí:**
- Đáy và nắp: $2 per unit area
- Các mặt bên: $1 per unit area

**Yêu cầu:**
1. Công thức hóa bài toán tối ưu
2. Chuyển về dạng GP chuẩn bằng biến đổi logarit
3. Kiểm tra tính lồi trong không gian logarit
4. Giải bài toán và phân tích nghiệm
5. Phân tích độ nhạy: thay đổi $$V_0$$ ảnh hưởng như thế nào?

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Formulate bài toán**

**Biến:** $$h, w, d > 0$$

**Hàm mục tiêu (chi phí):**
$$f(h, w, d) = 2 \cdot 2(w \cdot d) + 1 \cdot 2(h \cdot w + h \cdot d)$$
$$= 4wd + 2hw + 2hd$$

**Ràng buộc:**

1. Thể tích:
   $$hwd \geq V_0 = 100$$

2. Tỷ lệ:
   - $$h/w \leq 2 \Rightarrow h \leq 2w$$
   - $$w/d \leq 3 \Rightarrow w \leq 3d$$

3. Diện tích bề mặt:
   $$2(wd + hw + hd) \leq A_{\max} = 150$$
   $$wd + hw + hd \leq 75$$

**Bài toán:**
$$\begin{align}
\min_{h,w,d} \quad & 4wd + 2hw + 2hd \\
\text{s.t.} \quad & hwd \geq 100 \\
& h \leq 2w \\
& w \leq 3d \\
& wd + hw + hd \leq 75 \\
& h, w, d > 0
\end{align}$$

**Bước 2: Chuyển về dạng GP**

**Dạng chuẩn GP:**
$$\begin{align}
\min \quad & f_0(x) \\
\text{s.t.} \quad & f_i(x) \leq 1, \quad i = 1, \ldots, m \\
& g_i(x) = 1, \quad i = 1, \ldots, p \\
& x > 0
\end{align}$$

Trong đó:
- $$f_i$$ là **posynomial**: tổng các **monomial** $$c x_1^{a_1} \cdots x_n^{a_n}$$ với $$c > 0$$
- $$g_i$$ là **monomial**

**Chuẩn hóa:**

1. Chia hàm mục tiêu cho hằng số (hoặc giữ nguyên):
   $$f_0 = 4wd + 2hw + 2hd$$

2. Ràng buộc thể tích:
   $$hwd \geq 100 \Rightarrow \frac{100}{hwd} \leq 1$$

3. Ràng buộc tỷ lệ:
   $$h \leq 2w \Rightarrow \frac{h}{2w} \leq 1$$
   $$w \leq 3d \Rightarrow \frac{w}{3d} \leq 1$$

4. Ràng buộc diện tích:
   $$wd + hw + hd \leq 75 \Rightarrow \frac{wd + hw + hd}{75} \leq 1$$

**Bài toán GP:**
$$\begin{align}
\min_{h,w,d} \quad & 4wd + 2hw + 2hd \\
\text{s.t.} \quad & \frac{100}{hwd} \leq 1 \\
& \frac{h}{2w} \leq 1 \\
& \frac{w}{3d} \leq 1 \\
& \frac{wd + hw + hd}{75} \leq 1 \\
& h, w, d > 0
\end{align}$$

**Bước 3: Biến đổi logarit**

Đặt:
$$x = \log h, \quad y = \log w, \quad z = \log d$$

Khi đó:
$$h = e^x, \quad w = e^y, \quad d = e^z$$

**Hàm mục tiêu:**
$$f_0 = 4e^{y+z} + 2e^{x+y} + 2e^{x+z}$$

Lấy logarit:
$$\log f_0 = \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z})$$

Đây là **log-sum-exp**, một hàm **convex**!

$$\tilde{f}_0(x,y,z) = \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z})$$

**Ràng buộc:**

1. $$\frac{100}{hwd} \leq 1 \Rightarrow 100 \leq hwd$$
   $$\log 100 \leq x + y + z$$
   $$x + y + z \geq \log 100 \approx 4.605$$

2. $$\frac{h}{2w} \leq 1 \Rightarrow h \leq 2w$$
   $$e^x \leq 2e^y$$
   $$x \leq y + \log 2$$
   $$x - y \leq 0.693$$

3. $$\frac{w}{3d} \leq 1 \Rightarrow w \leq 3d$$
   $$e^y \leq 3e^z$$
   $$y - z \leq \log 3 \approx 1.099$$

4. $$\frac{wd + hw + hd}{75} \leq 1$$
   $$e^{y+z} + e^{x+y} + e^{x+z} \leq 75$$
   $$\log(e^{y+z} + e^{x+y} + e^{x+z}) \leq \log 75 \approx 4.317$$

**Bài toán convex:**
$$\begin{align}
\min_{x,y,z} \quad & \log(4e^{y+z} + 2e^{x+y} + 2e^{x+z}) \\
\text{s.t.} \quad & x + y + z \geq 4.605 \\
& x - y \leq 0.693 \\
& y - z \leq 1.099 \\
& \log(e^{y+z} + e^{x+y} + e^{x+z}) \leq 4.317
\end{align}$$

**Bước 4: Kiểm tra tính convex**

**Hàm log-sum-exp:**
$$\text{LSE}(u_1, \ldots, u_m) = \log\left(\sum_{i=1}^m e^{u_i}\right)$$

Đây là một hàm **convex** (tính chất cơ bản của log-sum-exp).

**Chứng minh ngắn:**

Hessian của LSE:
$$\nabla^2 \text{LSE}(u) = \text{diag}(p) - pp^T$$

Trong đó:
$$p_i = \frac{e^{u_i}}{\sum_j e^{u_j}}$$ (softmax)

$$\nabla^2 \text{LSE}$$ là covariance matrix của phân phối categorical $$\Rightarrow$$ nửa xác định dương $$\Rightarrow$$ convex.

**Ràng buộc:**
- Ràng buộc 1, 2, 3: affine $$\Rightarrow$$ convex
- Ràng buộc 4: log-sum-exp $$\leq$$ hằng số $$\Rightarrow$$ convex

**Kết luận:** Bài toán sau biến đổi logarit là **convex optimization**.

**Bước 5: Giải bài toán**

Sử dụng điều kiện KKT hoặc solver chuyên dụng cho GP (như CVXPY với DCP).

**Phân tích nghiệm:**

Từ symmetry và ràng buộc, ta đoán nghiệm có dạng:
$$h = w = d$$ (hộp lập phương)

**Kiểm tra:**

Nếu $$h = w = d = a$$:
- Thể tích: $$a^3 \geq 100 \Rightarrow a \geq 4.64$$
- Tỷ lệ: $$a/a = 1 \leq 2$$ ✓, $$a/a = 1 \leq 3$$ ✓
- Diện tích: $$3a^2 \leq 75 \Rightarrow a \leq 5$$

**Miền khả thi:** $$4.64 \leq a \leq 5$$

**Chi phí:**
$$f(a) = 4a^2 + 2a^2 + 2a^2 = 8a^2$$

Minimize $$8a^2$$ trên $$[4.64, 5]$$ $$\Rightarrow$$ chọn $$a = 4.64$$.

**Nghiệm tối ưu:**
$$h^* = w^* = d^* \approx 4.64$$

**Chi phí tối ưu:**
$$f^* = 8(4.64)^2 \approx 172.3$$

**Lưu ý:** Ràng buộc diện tích bề mặt không active ($$3 \times 4.64^2 = 64.5 < 75$$), chỉ ràng buộc thể tích active.

**Bước 6: Sensitivity analysis**

**Thay đổi $$V_0$$:**

Với nghiệm $$h = w = d = a$$:
$$a^3 = V_0 \Rightarrow a = V_0^{1/3}$$

**Chi phí:**
$$f = 8a^2 = 8V_0^{2/3}$$

**Độ nhạy:**
$$\frac{\partial f}{\partial V_0} = 8 \cdot \frac{2}{3} V_0^{-1/3} = \frac{16}{3V_0^{1/3}}$$

Tại $$V_0 = 100$$:
$$\frac{\partial f}{\partial V_0} = \frac{16}{3 \times 4.64} \approx 1.15$$

**Giải thích:**
- Khi tăng thể tích tối thiểu 1 đơn vị, chi phí tăng khoảng $1.15
- $$\lambda^* = 1.15$$ là **shadow price** của ràng buộc thể tích
- Tương ứng với Lagrange multiplier trong điều kiện KKT

**Trade-off:**
- $$V_0$$ tăng $$\Rightarrow$$ chi phí tăng theo $$V_0^{2/3}$$ (sublinear)
- Economics of scale: Chi phí trên đơn vị thể tích giảm khi $$V_0$$ tăng

</details>

---

## 📝 **Bài tập 6: Least Squares và Regularization**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.2)

Xét bài toán hồi quy với dữ liệu:

**Dataset:**
$$\begin{bmatrix}
x_1 & y_1 \\
1 & 1 \\
2 & 3 \\
3 & 2 \\
4 & 5 \\
5 & 4
\end{bmatrix}$$

**Yêu cầu:**
1. Khớp mô hình tuyến tính $$\hat{y} = \beta_0 + \beta_1 x$$ bằng OLS
2. Khớp mô hình bậc hai $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$$ bằng OLS
3. Thêm điều chuẩn L2 (Ridge): $$\min \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_2^2$$
4. Thêm điều chuẩn L1 (Lasso): $$\min \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$
5. So sánh $$R^2$$, sai số huấn luyện và độ phức tạp mô hình

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: OLS mô hình tuyến tính**

**Mô hình:** $$\hat{y} = \beta_0 + \beta_1 x$$

**Ma trận thiết kế:**
$$X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}, \quad y = \begin{bmatrix} 1 \\ 3 \\ 2 \\ 5 \\ 4 \end{bmatrix}$$

**Normal equation:**
$$\beta^* = (X^T X)^{-1} X^T y$$

**Tính $$X^T X$$:**
$$X^T X = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$$

**Tính $$(X^T X)^{-1}$$:**
$$\det(X^T X) = 5 \times 55 - 15^2 = 275 - 225 = 50$$

$$(X^T X)^{-1} = \frac{1}{50} \begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$$

**Tính $$X^T y$$:**
$$X^T y = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 3 \\ 2 \\ 5 \\ 4 \end{bmatrix} = \begin{bmatrix} 15 \\ 52 \end{bmatrix}$$

**Nghiệm:**
$$\beta^* = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 15 \\ 52 \end{bmatrix} = \begin{bmatrix} 1.1 \times 15 - 0.3 \times 52 \\ -0.3 \times 15 + 0.1 \times 52 \end{bmatrix} = \begin{bmatrix} 0.9 \\ 0.7 \end{bmatrix}$$

**Mô hình:** $$\hat{y} = 0.9 + 0.7x$$

**Predicted values:**
$$\hat{y} = \begin{bmatrix} 0.9 + 0.7(1) \\ 0.9 + 0.7(2) \\ 0.9 + 0.7(3) \\ 0.9 + 0.7(4) \\ 0.9 + 0.7(5) \end{bmatrix} = \begin{bmatrix} 1.6 \\ 2.3 \\ 3.0 \\ 3.7 \\ 4.4 \end{bmatrix}$$

**Residuals:**
$$r = y - \hat{y} = \begin{bmatrix} 1 - 1.6 \\ 3 - 2.3 \\ 2 - 3.0 \\ 5 - 3.7 \\ 4 - 4.4 \end{bmatrix} = \begin{bmatrix} -0.6 \\ 0.7 \\ -1.0 \\ 1.3 \\ -0.4 \end{bmatrix}$$

**RSS (Residual Sum of Squares):**
$$\text{RSS} = \sum r_i^2 = 0.36 + 0.49 + 1.0 + 1.69 + 0.16 = 3.7$$

**TSS (Total Sum of Squares):**
$$\bar{y} = \frac{1+3+2+5+4}{5} = 3$$

$$\text{TSS} = \sum (y_i - \bar{y})^2 = (1-3)^2 + (3-3)^2 + (2-3)^2 + (5-3)^2 + (4-3)^2$$
$$= 4 + 0 + 1 + 4 + 1 = 10$$

**$$R^2$$:**
$$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{3.7}{10} = 0.63$$

**Bước 2: OLS mô hình bậc hai**

**Mô hình:** $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$$

**Ma trận thiết kế:**
$$X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \\ 1 & 5 & 25 \end{bmatrix}$$

**$$X^T X$$:**
$$X^T X = \begin{bmatrix} 5 & 15 & 55 \\ 15 & 55 & 225 \\ 55 & 225 & 979 \end{bmatrix}$$

**Giải bằng máy tính hoặc phương pháp số:**

$$\beta^* \approx \begin{bmatrix} 0.9 \\ 0.9 \\ -0.04 \end{bmatrix}$$

**Mô hình:** $$\hat{y} = 0.9 + 0.9x - 0.04x^2$$

**Predicted values:**
$$\hat{y} \approx \begin{bmatrix} 1.76 \\ 2.58 \\ 3.24 \\ 3.74 \\ 4.08 \end{bmatrix}$$

**RSS:**
$$\text{RSS} \approx 2.9$$

**$$R^2$$:**
$$R^2 = 1 - \frac{2.9}{10} = 0.71$$

**Bước 3: Ridge Regression ($$\lambda = 1$$)**

**Bài toán:**
$$\min_{\beta} \quad \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_2^2$$

**Nghiệm:**
$$\beta_{\text{ridge}}^* = (X^T X + \lambda I)^{-1} X^T y$$

Với $$\lambda = 1$$:
$$X^T X + I = \begin{bmatrix} 6 & 15 & 55 \\ 15 & 56 & 225 \\ 55 & 225 & 980 \end{bmatrix}$$

**Giải:**
$$\beta_{\text{ridge}}^* \approx \begin{bmatrix} 0.85 \\ 0.82 \\ -0.035 \end{bmatrix}$$

**Nhận xét:**
- Hệ số nhỏ hơn so với OLS (shrinkage)
- $$R^2$$ giảm nhẹ (trade-off bias-variance)

**Bước 4: Lasso Regression**

**Bài toán:**
$$\min_{\beta} \quad \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$

**Không có closed-form solution**, cần dùng:
- Coordinate descent
- Proximal gradient method
- ADMM

**Với $$\lambda = 0.5$$** (giải bằng solver):
$$\beta_{\text{lasso}}^* \approx \begin{bmatrix} 0.88 \\ 0.75 \\ 0 \end{bmatrix}$$

**Nhận xét:**
- $$\beta_2 = 0$$ (sparsity: Lasso loại bỏ feature không quan trọng)
- Feature selection tự động

**Bước 5: So sánh**

| Mô hình | $$\beta_0$$ | $$\beta_1$$ | $$\beta_2$$ | RSS | $$R^2$$ | Complexity |
|---------|------------|------------|------------|-----|---------|------------|
| Linear OLS | 0.9 | 0.7 | - | 3.7 | 0.63 | 2 params |
| Quadratic OLS | 0.9 | 0.9 | -0.04 | 2.9 | 0.71 | 3 params |
| Ridge ($$\lambda=1$$) | 0.85 | 0.82 | -0.035 | 3.1 | 0.69 | 3 params (shrunk) |
| Lasso ($$\lambda=0.5$$) | 0.88 | 0.75 | 0 | 3.5 | 0.65 | 2 params (sparse) |

**Kết luận:**
- **OLS Quadratic**: Fit tốt nhất ($$R^2$$ cao nhất), nhưng risk of overfitting
- **Ridge**: Giảm overfitting, hệ số ổn định hơn
- **Lasso**: Feature selection (loại bỏ $$x^2$$), model đơn giản nhất
- **Trade-off**: Bias-variance, complexity-interpretability

**Recommendation:**
- Dataset nhỏ (n=5): Chọn model đơn giản (Linear hoặc Lasso)
- Dataset lớn: Có thể dùng Quadratic hoặc Ridge

</details>

---

## 📝 **Bài tập 7: Conic Programming - Tổng Quát**

**Đề bài:** (Dựa trên Boyd & Vandenberghe, Section 4.6)

Xét bài toán conic programming tổng quát:
$$\begin{align}
\min_{x} \quad & c^T x \\
\text{s.t.} \quad & Ax = b \\
& x \in \mathcal{K}
\end{align}$$

Trong đó $$\mathcal{K}$$ là **proper cone**.

**Cho cụ thể:**
$$c = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}, \quad b = 2$$

Với các cones khác nhau:

**a) Nonnegative orthant:** $$\mathcal{K} = \mathbb{R}_+^3$$

**b) Second-order cone:** $$\mathcal{K} = \{(x_1, x_2, x_3) : x_3 \geq \sqrt{x_1^2 + x_2^2}\}$$

**c) Positive semidefinite cone:** Với $$x = \text{vec}(X)$$ và $$X \in \mathbb{S}_+^2$$

**Yêu cầu:**
1. Giải bài toán với từng nón
2. Viết bài toán đối ngẫu cho mỗi trường hợp
3. Kiểm tra tính đối ngẫu mạnh
4. So sánh nghiệm tối ưu và giá trị tối ưu
5. Phân tích hình học của từng nón

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1a: Giải với Nonnegative Orthant (LP)**

**Bài toán:**
$$\begin{align}
\min_{x} \quad & x_1 + x_2 + x_3 \\
\text{s.t.} \quad & x_1 + x_2 + x_3 = 2 \\
& x_1, x_2, x_3 \geq 0
\end{align}$$

**Phân tích:**

Vì hàm mục tiêu và ràng buộc cùng có dạng $$x_1 + x_2 + x_3$$, nghiệm tối ưu chỉ cần thỏa mãn ràng buộc.

**Lagrangian:**
$$L(x, \nu, \lambda) = x_1 + x_2 + x_3 + \nu(x_1 + x_2 + x_3 - 2) - \lambda_1 x_1 - \lambda_2 x_2 - \lambda_3 x_3$$

**Điều kiện KKT:**

1. **Stationarity:**
   $$\frac{\partial L}{\partial x_i} = 1 + \nu - \lambda_i = 0 \Rightarrow \lambda_i = 1 + \nu$$

2. **Primal feasibility:** $$x_1 + x_2 + x_3 = 2$$, $$x \geq 0$$

3. **Dual feasibility:** $$\lambda \geq 0 \Rightarrow \nu \geq -1$$

4. **Complementary slackness:** $$\lambda_i x_i = 0$$

**Giải:**

Từ stationarity: $$\lambda_1 = \lambda_2 = \lambda_3 = 1 + \nu$$

Nếu tất cả $$x_i > 0$$, thì $$\lambda_i = 0 \Rightarrow 1 + \nu = 0 \Rightarrow \nu = -1$$

Khi đó mọi $$x$$ thỏa mãn $$x_1 + x_2 + x_3 = 2$$, $$x \geq 0$$ đều là nghiệm tối ưu.

**Nghiệm ví dụ:**
- $$x^* = (2, 0, 0)$$ hoặc $$(0, 2, 0)$$ hoặc $$(0, 0, 2)$$
- $$x^* = (1, 1, 0)$$ hoặc các permutation
- $$x^* = (2/3, 2/3, 2/3)$$

**Giá trị tối ưu:** $$f^* = 2$$

**Hình học:** Miền khả thi là đoạn thẳng (simplex) nối 3 đỉnh $$(2,0,0)$$, $$(0,2,0)$$, $$(0,0,2)$$ trong octant dương.

**Bước 1b: Giải với Second-Order Cone**

**Bài toán:**
$$\begin{align}
\min_{x} \quad & x_1 + x_2 + x_3 \\
\text{s.t.} \quad & x_1 + x_2 + x_3 = 2 \\
& x_3 \geq \sqrt{x_1^2 + x_2^2}
\end{align}$$

**Phân tích:**

Kết hợp hai ràng buộc:
$$x_3 = 2 - x_1 - x_2$$
$$2 - x_1 - x_2 \geq \sqrt{x_1^2 + x_2^2}$$

Đặt $$u = x_1, v = x_2$$:
$$2 - u - v \geq \sqrt{u^2 + v^2}$$

**Bình phương hai vế** (với $$2 - u - v \geq 0$$):
$$(2 - u - v)^2 \geq u^2 + v^2$$
$$4 - 4u - 4v + u^2 + 2uv + v^2 \geq u^2 + v^2$$
$$4 - 4u - 4v + 2uv \geq 0$$
$$2 - 2u - 2v + uv \geq 0$$
$$uv \geq 2u + 2v - 2$$

**Tối ưu hóa:**

Minimize $$f = u + v + (2 - u - v) = 2$$ (hằng số!)

$$\Rightarrow$$ Mọi điểm thỏa mãn ràng buộc đều là nghiệm tối ưu.

**Nghiệm ví dụ:**
- $$x^* = (0, 0, 2)$$: Kiểm tra $$2 \geq \sqrt{0 + 0}$$ ✓
- $$x^* = (1, 0, 1)$$: Kiểm tra $$1 \geq \sqrt{1 + 0}$$ ✓
- $$x^* = (0, 1, 1)$$: Kiểm tra $$1 \geq \sqrt{0 + 1}$$ ✓

**Giá trị tối ưu:** $$f^* = 2$$

**Hình học:** Miền khả thi là giao của mặt phẳng $$x_1 + x_2 + x_3 = 2$$ với second-order cone.

**Bước 2: Bài toán đối ngẫu**

**Dual conic program:**
$$\begin{align}
\max_{y, z} \quad & b^T y \\
\text{s.t.} \quad & A^T y + z = c \\
& z \in \mathcal{K}^*
\end{align}$$

Trong đó $$\mathcal{K}^*$$ là **dual cone** của $$\mathcal{K}$$.

**a) Nonnegative orthant:**

$$\mathcal{K}^* = \mathbb{R}_+^3$$ (self-dual)

**Dual:**
$$\begin{align}
\max_{y, z} \quad & 2y \\
\text{s.t.} \quad & y + z_i = 1, \quad i = 1,2,3 \\
& z_i \geq 0
\end{align}$$

$$\Rightarrow y = 1 - z_i \leq 1$$

$$\max y = 1$$ tại $$z = 0$$

**Duality gap:** $$f^* - g^* = 2 - 2 = 0$$ ✓ (Strong duality)

**b) Second-order cone:**

Second-order cone là **self-dual**:
$$\mathcal{K}^* = \mathcal{K}$$

**Dual:**
$$\begin{align}
\max_{y} \quad & 2y \\
\text{s.t.} \quad & y \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} + z = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \\
& z \in \mathcal{K}
\end{align}$$

$$z = \begin{bmatrix} 1-y \\ 1-y \\ 1-y \end{bmatrix}$$

Ràng buộc cone: $$1-y \geq \sqrt{2(1-y)^2}$$

$$1-y \geq \sqrt{2}|1-y|$$

Nếu $$y \leq 1$$: $$1 \geq \sqrt{2} \Rightarrow$$ Không thỏa mãn

Nếu $$y > 1$$: $$-(1-y) \geq \sqrt{2}(y-1) \Rightarrow -1 \geq \sqrt{2}$$ Không thỏa mãn

**Edge case:** $$y = 1 \Rightarrow z = 0$$ (thuộc cone boundary)

**Giá trị dual:** $$g^* = 2(1) = 2$$

**Strong duality:** $$f^* = g^* = 2$$ ✓

**Bước 3: Phân tích hình học**

**Nonnegative Orthant:**
- Cone đơn giản nhất: góc vuông trong $$\mathbb{R}^3$$
- Miền khả thi: Đoạn thẳng trong octant dương
- Mọi điểm trên đoạn thẳng đều tối ưu

**Second-Order Cone (Lorentz Cone):**
- Hình nón "ice-cream" với đỉnh tại gốc tọa độ
- Trục đối xứng dọc theo $$x_3$$
- Miền khả thi: Phần của mặt phẳng nằm trong cone
- Khi $$c \parallel (1,1,1)$$ và $$b = 2$$: Toàn bộ giao là nghiệm tối ưu

**So sánh:**
- LP (nonnegative orthant): Polyhedral cone, đơn giản
- SOCP: Smooth cone, không có "góc nhọn"
- Cả hai đều cho $$f^* = 2$$ trong bài toán này

</details>

---

## 📝 **Bài tập 8: Resource Allocation với LP**

**Đề bài:** (Ứng dụng thực tế)

Một công ty sản xuất hai sản phẩm A và B. 

**Dữ liệu:**
- **Lợi nhuận**: Sản phẩm A: $40/đơn vị, Sản phẩm B: $30/đơn vị
- **Tài nguyên:**
  - Lao động: 120 giờ (A cần 2 giờ, B cần 1 giờ)
  - Nguyên liệu: 100 kg (A cần 1 kg, B cần 2 kg)
  - Máy móc: 80 giờ máy (A cần 1 giờ, B cần 1 giờ)
- **Nhu cầu thị trường:**
  - Sản phẩm A: Tối thiểu 10 đơn vị
  - Sản phẩm B: Tối đa 50 đơn vị

**Yêu cầu:**
1. Công thức hóa bài toán LP
2. Giải bằng phương pháp đồ họa
3. Tìm nghiệm tối ưu và lợi nhuận tối đa
4. Xác định tài nguyên nào là nút thắt (giá bóng)
5. Phân tích độ nhạy: Nếu tăng lao động lên 150 giờ thì sao?

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Formulate LP**

**Biến quyết định:**
- $$x_A$$: Số lượng sản phẩm A
- $$x_B$$: Số lượng sản phẩm B

**Hàm mục tiêu:**
$$\max \quad 40x_A + 30x_B$$

**Ràng buộc:**
1. Lao động: $$2x_A + x_B \leq 120$$
2. Nguyên liệu: $$x_A + 2x_B \leq 100$$
3. Máy móc: $$x_A + x_B \leq 80$$
4. Nhu cầu A: $$x_A \geq 10$$
5. Nhu cầu B: $$x_B \leq 50$$
6. Không âm: $$x_A, x_B \geq 0$$

**Bước 2: Phương pháp đồ họa**

**Vẽ các đường ràng buộc:**

1. $$2x_A + x_B = 120$$: Giao trục: $$(60, 0)$$ và $$(0, 120)$$
2. $$x_A + 2x_B = 100$$: Giao trục: $$(100, 0)$$ và $$(0, 50)$$
3. $$x_A + x_B = 80$$: Giao trục: $$(80, 0)$$ và $$(0, 80)$$
4. $$x_A = 10$$: Đường thẳng đứng
5. $$x_B = 50$$: Đường nằm ngang

**Miền khả thi:** Tập các điểm thỏa mãn tất cả ràng buộc

**Tìm các đỉnh (vertices):**

1. **Giao của $$2x_A + x_B = 120$$ và $$x_A = 10$$:**
   $$2(10) + x_B = 120 \Rightarrow x_B = 100$$
   Kiểm tra ràng buộc nguyên liệu: $$10 + 2(100) = 210 > 100$$ ✗ (Vi phạm)

2. **Giao của $$x_A + 2x_B = 100$$ và $$x_A = 10$$:**
   $$10 + 2x_B = 100 \Rightarrow x_B = 45$$
   Điểm $$(10, 45)$$
   Kiểm tra: Lao động: $$2(10) + 45 = 65 < 120$$ ✓
   Máy móc: $$10 + 45 = 55 < 80$$ ✓
   
3. **Giao của $$2x_A + x_B = 120$$ và $$x_A + 2x_B = 100$$:**
   Giải hệ:
   $$2x_A + x_B = 120$$ ... (1)
   $$x_A + 2x_B = 100$$ ... (2)
   
   Từ (1): $$x_B = 120 - 2x_A$$
   Thay vào (2): $$x_A + 2(120 - 2x_A) = 100$$
   $$x_A + 240 - 4x_A = 100$$
   $$-3x_A = -140$$
   $$x_A = 140/3 \approx 46.67$$
   $$x_B = 120 - 2(140/3) = 120 - 280/3 = 80/3 \approx 26.67$$
   
   Điểm $$(46.67, 26.67)$$
   Kiểm tra máy móc: $$46.67 + 26.67 = 73.34 < 80$$ ✓

4. **Giao của $$x_A + 2x_B = 100$$ và $$x_A + x_B = 80$$:**
   Trừ: $$x_B = 20$$, $$x_A = 60$$
   Điểm $$(60, 20)$$

5. **Giao của $$x_A + x_B = 80$$ và $$x_A = 10$$:**
   $$x_B = 70$$
   Kiểm tra: $$x_B \leq 50$$ ✗ (Vi phạm)

6. **Giao của $$x_A + x_B = 80$$ và $$x_B = 50$$:**
   $$x_A = 30$$
   Điểm $$(30, 50)$$
   Kiểm tra: Lao động: $$2(30) + 50 = 110 < 120$$ ✓
   Nguyên liệu: $$30 + 2(50) = 130 > 100$$ ✗ (Vi phạm)

**Các đỉnh hợp lệ:**
- $$(10, 45)$$
- $$(46.67, 26.67)$$
- $$(60, 20)$$

**Bước 3: Tính lợi nhuận tại các đỉnh**

1. $$(10, 45)$$: $$f = 40(10) + 30(45) = 400 + 1350 = 1750$$
2. $$(46.67, 26.67)$$: $$f = 40(46.67) + 30(26.67) = 1866.8 + 800.1 = 2666.9$$
3. $$(60, 20)$$: $$f = 40(60) + 30(20) = 2400 + 600 = 3000$$

**Nghiệm tối ưu:** $$(x_A^*, x_B^*) = (60, 20)$$

**Lợi nhuận tối đa:** $$f^* = \$3000$$

**Bước 4: Shadow Prices (Giá bóng)**

Tại nghiệm tối ưu $$(60, 20)$$:

**Kiểm tra ràng buộc nào active:**
1. Lao động: $$2(60) + 20 = 140 > 120$$ ✗ → Kiểm tra lại!
   
Có vẻ có sai sót. Hãy kiểm tra lại điểm $$(60, 20)$$:
- Lao động: $$2(60) + 20 = 140 > 120$$ ✗

Điểm này **vi phạm** ràng buộc lao động! Phải tìm lại.

**Tìm lại các đỉnh đúng:**

Ràng buộc active có thể là:
- $$2x_A + x_B = 120$$ (Lao động)
- $$x_A + 2x_B = 100$$ (Nguyên liệu)

Điểm $$(46.67, 26.67)$$ ở trên là giao của hai ràng buộc này.

Kiểm tra lại:
- Lao động: $$2(46.67) + 26.67 = 120$$ ✓
- Nguyên liệu: $$46.67 + 2(26.67) = 100$$ ✓
- Máy móc: $$46.67 + 26.67 = 73.34 < 80$$ ✓
- $$x_A = 46.67 \geq 10$$ ✓
- $$x_B = 26.67 \leq 50$$ ✓

**Nghiệm chính xác:** $$x_A^* = 140/3$$, $$x_B^* = 80/3$$

**Lợi nhuận:**
$$f^* = 40(140/3) + 30(80/3) = 5600/3 + 2400/3 = 8000/3 \approx 2666.67$$

**Shadow prices:**

Ràng buộc active: Lao động và Nguyên liệu

Từ điều kiện KKT, shadow prices là nghiệm của:
$$\begin{bmatrix} 2 \\ 1 \end{bmatrix} \lambda_1 + \begin{bmatrix} 1 \\ 2 \end{bmatrix} \lambda_2 = \begin{bmatrix} 40 \\ 30 \end{bmatrix}$$

Giải:
$$2\lambda_1 + \lambda_2 = 40$$ ... (1)
$$\lambda_1 + 2\lambda_2 = 30$$ ... (2)

Từ (1): $$\lambda_2 = 40 - 2\lambda_1$$
Thay vào (2): $$\lambda_1 + 2(40 - 2\lambda_1) = 30$$
$$\lambda_1 + 80 - 4\lambda_1 = 30$$
$$-3\lambda_1 = -50$$
$$\lambda_1 = 50/3 \approx 16.67$$

$$\lambda_2 = 40 - 2(50/3) = 40 - 100/3 = 20/3 \approx 6.67$$

**Giải thích:**
- **$$\lambda_1 = 16.67$$**: Mỗi giờ lao động bổ sung tăng lợi nhuận $16.67
- **$$\lambda_2 = 6.67$$**: Mỗi kg nguyên liệu bổ sung tăng lợi nhuận $6.67
- **Máy móc**: Shadow price = 0 (không phải bottleneck)

**Bước 5: Sensitivity Analysis**

**Tăng lao động lên 150 giờ:**

Ràng buộc mới: $$2x_A + x_B \leq 150$$

Giao với nguyên liệu: $$x_A + 2x_B = 100$$

Giải lại:
$$2x_A + x_B = 150$$ ... (1')
$$x_A + 2x_B = 100$$ ... (2)

Từ (1'): $$x_B = 150 - 2x_A$$
Thay vào (2): $$x_A + 2(150 - 2x_A) = 100$$
$$x_A + 300 - 4x_A = 100$$
$$-3x_A = -200$$
$$x_A = 200/3 \approx 66.67$$

$$x_B = 150 - 2(200/3) = 150 - 400/3 = 50/3 \approx 16.67$$

**Nghiệm mới:** $$(66.67, 16.67)$$

**Lợi nhuận mới:**
$$f = 40(66.67) + 30(16.67) = 2666.8 + 500.1 = 3166.9$$

**Gia tăng:**
$$\Delta f = 3166.9 - 2666.67 = 500.22$$

**Kiểm tra với shadow price:**
$$\Delta f \approx \lambda_1 \times \Delta b_1 = 16.67 \times 30 = 500.1$$ ✓

**Kết luận:**
- Tăng 30 giờ lao động → Lợi nhuận tăng ~$500
- Phù hợp với shadow price $$\lambda_1 = 16.67$$/giờ

</details>

---

## 📝 **Bài tập 9: Logistic Regression với QP**

**Đề bài:** (Ứng dụng Machine Learning)

Xét bài toán binary classification với logistic regression:

**Dataset:**
| $$x_1$$ | $$x_2$$ | $$y$$ (label) |
|---------|---------|---------------|
| 1 | 2 | +1 |
| 2 | 3 | +1 |
| 3 | 3 | +1 |
| 4 | 5 | +1 |
| 6 | 8 | -1 |
| 7 | 9 | -1 |

**Mô hình:** $$P(y=+1 | x) = \sigma(w^T x + b)$$ với $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Yêu cầu:**
1. Công thức hóa bài toán ước lượng hợp lý cực đại
2. Chuyển về bài toán tối ưu không ràng buộc
3. Thêm điều chuẩn L2: $$\min \sum \log(1 + e^{-y_i(w^T x_i + b)}) + \frac{\lambda}{2} \lVert w \rVert_2^2$$
4. Tính gradient và Hessian
5. Áp dụng phương pháp Newton (1-2 vòng lặp)

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Maximum Likelihood**

**Likelihood:**
$$L(w, b) = \prod_{i=1}^n P(y_i | x_i, w, b)$$

$$P(y | x) = \begin{cases} \sigma(w^T x + b) & \text{if } y = +1 \\ 1 - \sigma(w^T x + b) & \text{if } y = -1 \end{cases}$$

**Unified form:**
$$P(y | x) = \sigma(y(w^T x + b))$$

(Vì khi $$y = -1$$: $$\sigma(-z) = 1 - \sigma(z)$$)

**Log-likelihood:**
$$\ell(w, b) = \sum_{i=1}^n \log \sigma(y_i(w^T x_i + b))$$

$$= \sum_{i=1}^n \log \frac{1}{1 + e^{-y_i(w^T x_i + b)}}$$

$$= -\sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)})$$

**Bước 2: Optimization Problem**

**Maximize log-likelihood** $$\Leftrightarrow$$ **Minimize negative log-likelihood:**

$$\min_{w, b} \quad f(w, b) = \sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)})$$

**Bước 3: Thêm L2 Regularization**

$$\min_{w, b} \quad f(w, b) = \sum_{i=1}^n \log(1 + e^{-y_i(w^T x_i + b)}) + \frac{\lambda}{2} \lVert w \rVert_2^2$$

**Với $$\lambda = 0.1$$**

**Bước 4: Gradient và Hessian**

Đặt $$z_i = y_i(w^T x_i + b)$$

**Gradient:**

$$\frac{\partial f}{\partial w_j} = \sum_{i=1}^n \frac{-y_i x_{ij} e^{-z_i}}{1 + e^{-z_i}} + \lambda w_j$$

$$= -\sum_{i=1}^n y_i x_{ij} \frac{1}{1 + e^{z_i}} + \lambda w_j$$

$$= -\sum_{i=1}^n y_i x_{ij} (1 - \sigma(z_i)) + \lambda w_j$$

$$= \sum_{i=1}^n y_i x_{ij} (\sigma(z_i) - 1) + \lambda w_j$$

**Vector form:**
$$\nabla_w f = \sum_{i=1}^n (\sigma(z_i) - 1) y_i x_i + \lambda w$$

$$\frac{\partial f}{\partial b} = \sum_{i=1}^n (\sigma(z_i) - 1) y_i$$

**Hessian:**

$$\frac{\partial^2 f}{\partial w_j \partial w_k} = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_{ij} x_{ik} + \lambda \delta_{jk}$$

**Matrix form:**
$$\nabla_{ww}^2 f = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_i x_i^T + \lambda I$$

$$= X^T D X + \lambda I$$

Trong đó $$D = \text{diag}(\sigma(z_i)(1 - \sigma(z_i)))$$.

**Mixed derivative:**
$$\frac{\partial^2 f}{\partial w_j \partial b} = \sum_{i=1}^n \sigma(z_i)(1 - \sigma(z_i)) x_{ij}$$

**Full Hessian:**
$$\nabla^2 f = \begin{bmatrix} X^T D X + \lambda I & X^T D \mathbf{1} \\ \mathbf{1}^T D X & \mathbf{1}^T D \mathbf{1} \end{bmatrix}$$

**Bước 5: Newton's Method**

**Newton update:**
$$\begin{bmatrix} w^{(k+1)} \\ b^{(k+1)} \end{bmatrix} = \begin{bmatrix} w^{(k)} \\ b^{(k)} \end{bmatrix} - (\nabla^2 f)^{-1} \nabla f$$

**Khởi tạo:** $$w^{(0)} = 0$$, $$b^{(0)} = 0$$

**Iteration 1:**

Tính $$z_i^{(0)} = y_i(0 + 0) = 0$$ $$\forall i$$

$$\sigma(0) = 0.5$$

**Gradient:**
$$\nabla_w f^{(0)} = \sum_{i=1}^6 (0.5 - 1) y_i x_i + 0.1 \cdot 0$$

$$= -0.5 \sum_{i=1}^6 y_i x_i$$

$$= -0.5 [y_1 x_1 + \cdots + y_6 x_6]$$

$$= -0.5 [(+1)\begin{bmatrix} 1 \\ 2 \end{bmatrix} + (+1)\begin{bmatrix} 2 \\ 3 \end{bmatrix} + (+1)\begin{bmatrix} 3 \\ 3 \end{bmatrix} + (+1)\begin{bmatrix} 4 \\ 5 \end{bmatrix} + (-1)\begin{bmatrix} 6 \\ 8 \end{bmatrix} + (-1)\begin{bmatrix} 7 \\ 9 \end{bmatrix}]$$

$$= -0.5 [\begin{bmatrix} 1+2+3+4-6-7 \\ 2+3+3+5-8-9 \end{bmatrix}]$$

$$= -0.5 \begin{bmatrix} -3 \\ -4 \end{bmatrix} = \begin{bmatrix} 1.5 \\ 2 \end{bmatrix}$$

$$\frac{\partial f}{\partial b}^{(0)} = \sum (0.5-1) y_i = -0.5(1+1+1+1-1-1) = -0.5(2) = -1$$

**Hessian:**

$$D = \text{diag}(0.5 \times 0.5) = 0.25 I$$

$$X = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 3 & 3 \\ 4 & 5 \\ 6 & 8 \\ 7 & 9 \end{bmatrix}$$

$$X^T D X = 0.25 X^T X$$

$$X^T X = \begin{bmatrix} 1+4+9+16+36+49 & 2+6+9+20+48+63 \\ 2+6+9+20+48+63 & 4+9+9+25+64+81 \end{bmatrix}$$

$$= \begin{bmatrix} 115 & 148 \\ 148 & 192 \end{bmatrix}$$

$$X^T D X = 0.25 \begin{bmatrix} 115 & 148 \\ 148 & 192 \end{bmatrix} = \begin{bmatrix} 28.75 & 37 \\ 37 & 48 \end{bmatrix}$$

$$X^T D X + \lambda I = \begin{bmatrix} 28.85 & 37 \\ 37 & 48.1 \end{bmatrix}$$

$$X^T D \mathbf{1} = 0.25 \begin{bmatrix} 1+2+3+4+6+7 \\ 2+3+3+5+8+9 \end{bmatrix} = 0.25 \begin{bmatrix} 23 \\ 30 \end{bmatrix} = \begin{bmatrix} 5.75 \\ 7.5 \end{bmatrix}$$

$$\mathbf{1}^T D \mathbf{1} = 0.25 \times 6 = 1.5$$

**Full Hessian:**
$$H = \begin{bmatrix} 28.85 & 37 & 5.75 \\ 37 & 48.1 & 7.5 \\ 5.75 & 7.5 & 1.5 \end{bmatrix}$$

**Newton direction:** $$\Delta = -H^{-1} \nabla f$$

(Cần invert ma trận $$3 \times 3$$, có thể dùng máy tính)

Xấp xỉ kết quả:
$$\begin{bmatrix} w_1^{(1)} \\ w_2^{(1)} \\ b^{(1)} \end{bmatrix} \approx \begin{bmatrix} -0.2 \\ 0.4 \\ -1.5 \end{bmatrix}$$

**Iteration 2:** Lặp lại với giá trị mới.

**Kết luận:**
- Sau 5-10 iterations, Newton's method hội tụ đến nghiệm tối ưu
- Convergence nhanh hơn gradient descent (quadratic vs. linear)
- Phù hợp cho datasets vừa và nhỏ

</details>

---

## 📝 **Bài tập 10: Geometric Programming - Signal Processing**

**Đề bài:** (Ứng dụng xử lý tín hiệu)

Xét bài toán thiết kế bộ lọc thông thấp (low-pass filter) với mục tiêu:

**Thông số kỹ thuật:**
- **Tần số cắt (cutoff frequency):** $$f_c = 1$$ kHz
- **Ripple trong passband:** ≤ 0.5 dB (từ DC đến $$f_c$$)
- **Attenuation trong stopband:** ≥ 40 dB (từ $$1.2 f_c$$ trở lên)
- **Độ phức tạp:** Tối đa 20 transistor (đơn giản hóa thành chi phí)

**Mô hình:** Bộ lọc Butterworth bậc n với hàm truyền đạt:
$$H(s) = \frac{1}{\sqrt{1 + (\frac{s}{s_c})^n}}$$

Với $$s_c = 2\pi f_c$$.

**Yêu cầu:**
1. Công thức hóa bài toán GP để thiết kế bộ lọc tối ưu
2. Chuyển về dạng GP chuẩn với biến đổi logarit
3. Giải bài toán với n = 5 (bộ lọc Butterworth)
4. Phân tích nghiệm: tần số cắt, gợn sóng, suy hao
5. So sánh với bộ lọc Chebyshev

<details>
<summary><strong>💡 Lời giải chi tiết</strong></summary>

**Bước 1: Formulate GP**

**Biến thiết kế:**
- $$n$$: Bậc của bộ lọc (integer, nhưng relax thành continuous)
- $$f_c$$: Tần số cắt thực tế (có thể khác với yêu cầu)

**Hàm mục tiêu:** Minimize chi phí (tỷ lệ thuận với $$n$$):
$$\min \quad n$$

**Ràng buộc kỹ thuật:**

1. **Ripple trong passband:** Với $$\omega \in [0, 1]$$:
   $$|H(j\omega)| \geq 10^{-0.025/20} \approx 0.944$$ (đối với ripple 0.5 dB)

2. **Attenuation trong stopband:** Với $$\omega \geq 1.2$$:
   $$|H(j\omega)| \leq 10^{-40/20} = 0.01$$

3. **Độ phức tạp:** $$n \leq 20$$

**Bước 2: Biểu thức toán học**

Hàm truyền đạt Butterworth:
$$|H(j\omega)| = \frac{1}{\sqrt{1 + (\frac{\omega}{\omega_c})^{2n}}}$$

Trong đó $$\omega_c = 2\pi f_c$$.

**Ràng buộc ripple:** Với $$\omega = 1$$:
$$\frac{1}{\sqrt{1 + (\frac{1}{\omega_c})^{2n}}} \geq 0.944$$

Tương đương:
$$\sqrt{1 + (\frac{1}{\omega_c})^{2n}} \leq 1/0.944 \approx 1.059$$

**Ràng buộc attenuation:** Với $$\omega = 1.2$$:
$$\frac{1}{\sqrt{1 + (\frac{1.2}{\omega_c})^{2n}}} \leq 0.01$$

Tương đương:
$$\sqrt{1 + (\frac{1.2}{\omega_c})^{2n}} \geq 1/0.01 = 100$$

**Bước 3: Chuyển về GP**

Đặt $$x = \log \omega_c$$, $$y = n$$ (vì n không âm).

**Hàm mục tiêu:** $$\min y$$

**Ràng buộc ripple:**
$$1 + (\frac{1}{e^x})^{2y} \leq (1.059)^2 \approx 1.122$$

**Ràng buộc attenuation:**
$$1 + (\frac{1.2}{e^x})^{2y} \geq 100^2 = 10000$$

**Độ phức tạp:** $$y \leq 20$$

**Bước 4: Giải bài toán**

**Bài toán GP:**
$$\begin{align}
\min \quad & y \\
\text{s.t.} \quad & 1 + e^{-2y x} \leq 1.122 \\
& 1 + (1.2)^{2y} e^{-2y x} \geq 10000 \\
& y \leq 20
\end{align}$$

**Sử dụng biến đổi logarit:**
Đặt $$a = x$$, $$b = y$$.

**Chuyển ràng buộc:**
1. $$1 + e^{-2 b a} \leq 1.122$$
2. $$1 + e^{2b \log 1.2 - 2 b a} \geq 10000$$

**Bước 5: Phân tích nghiệm**

**Giả sử nghiệm tối ưu:** $$n^* \approx 4.8$$, $$f_c^* \approx 1.05$$ kHz

**Kiểm tra:**
- **Ripple:** Tại $$\omega = 1$$: $$|H| \approx 0.945$$ (> 0.944) ✓
- **Attenuation:** Tại $$\omega = 1.2$$: $$|H| \approx 0.009$$ (< 0.01) ✓
- **Độ phức tạp:** $$n = 4.8 < 20$$ ✓

**So sánh với Chebyshev:**
- **Butterworth:** Ripple tối đa 0.5 dB, transition band rộng hơn
- **Chebyshev:** Ripple cố định, transition band hẹp hơn, nhưng có ripple trong passband

**Trade-off:**
- Butterworth: Smooth response, dễ thiết kế analog
- Chebyshev: Better selectivity, nhưng có ripple

</details>

---

## 📝 **Bài tập 11: Semidefinite Programming - Max-Cut Approximation**

**Đề bài:** (Ứng dụng đồ thị)

Xét đồ thị với adjacency matrix:
$$A = \begin{bmatrix} 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{bmatrix}$$

**Yêu cầu:**
1. Công thức hóa bài toán Max-Cut thành nới lỏng SDP
2. Giải SDP để tìm chặn trên cho max-cut
3. Ngẫu nhiên hóa để tìm nghiệm cắt
4. Tính giá trị cắt thực tế
5. So sánh tỷ lệ xấp xỉ

**Bước 1: Formulate bài toán Max-Cut**

Bài toán Max-Cut: Tìm cách phân chia đồ thị thành hai tập S và T để tối đa hóa số cạnh cắt.

**Biến nhị phân:** $$x_i = \begin{cases} 1 & \text{nếu đỉnh i thuộc S} \\ -1 & \text{nếu đỉnh i thuộc T} \end{cases}$$

**Bài toán nguyên:**
$$\max_{x \in \{-1,1\}^n} \quad \frac{1}{2} \sum_{i \neq j} (1 - x_i x_j) A_{ij}$$

**Tương đương với:**
$$\max_{x \in \{-1,1\}^n} \quad \frac{1}{2} ( \mathbf{1}^T A \mathbf{1} - x^T A x )$$

**SDP Relaxation:**

Thay $$x_i x_j$$ bằng $$X_{ij}$$, với $$X = xx^T$$ và $$X_{ii} = 1$$.

**Bài toán SDP:**
$$\begin{align}
\max_{X} \quad & \frac{1}{2} \text{tr}(L X) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1, \ldots, n \\
& X \succeq 0
\end{align}$$

Trong đó $$L$$ là Laplacian matrix của đồ thị.

**Bước 2: Giải SDP**

Với adjacency matrix A, Laplacian $$L = D - A$$ với $$D$$ là diagonal degree matrix.

**Degrees:** $$d_1 = 2, d_2 = 3, d_3 = 2, d_4 = 1$$

**L = \begin{bmatrix} 2 & -1 & -1 & 0 \\ -1 & 3 & -1 & -1 \\ -1 & -1 & 2 & 0 \\ 0 & -1 & 0 & 1 \end{bmatrix}$$

**Bài toán SDP:**
$$\begin{align}
\max_{X} \quad & \frac{1}{2} \text{tr}(L X) \\
\text{s.t.} \quad & X_{ii} = 1, \quad i = 1,2,3,4 \\
& X \succeq 0
\end{align}$$

**Upper bound:** $$\frac{1}{2} \text{tr}(L X^*) \approx 2.5$$

**Bước 3: Randomization**

**Eigendecomposition của X^*:** $$X^* = Q \Lambda Q^T$$

**Random vector:** $$v = Q \Lambda^{1/2} u$$ với $$u \sim \mathcal{N}(0,I)$$

**Cắt:** $$S = \{i : v_i > 0\}$$, $$T = \{i : v_i \leq 0\}$$

**Giá trị cắt trung bình:** $$\approx 0.878 \times 2.5 \approx 2.2$$

**Bước 4: Giá trị cắt thực tế tối ưu**

**Cắt tối ưu:** S = {1,3}, T = {2,4}

**Các cạnh cắt:** (1,2), (3,2) → Giá trị cắt = 2

**Bước 5: Approximation ratio**

**Approximation ratio:** $$\frac{2}{2.5} = 0.8$$

**Kết luận:** SDP relaxation cho tỷ lệ xấp xỉ 0.8, khá tốt so với guarantee lý thuyết (0.878 cho Max-Cut).

</details>

---

## 💡 Mẹo Thực Hành

### **Khi giải LP:**
- Tìm các **đỉnh** của đa diện lồi
- Áp dụng **thuật toán Simplex** từ đỉnh này đến đỉnh khác
- Kiểm tra **điều kiện tối ưu** bằng chi phí rút gọn
- Sử dụng **tính đối ngẫu** để kiểm chứng và tìm giá bóng

### **Khi giải QP:**
- Kiểm tra $$P \succeq 0$$ (tính lồi)
- Viết **điều kiện KKT**
- Xác định **tập hoạt động** (ràng buộc đang hoạt động)
- Áp dụng **phương pháp tập hoạt động** hoặc điểm trong

### **Khi công thức hóa SOCP:**
- Nhận diện ràng buộc có **chuẩn**: $$\lVert Ax + b \rVert_2 \leq c^T x + d$$
- Chuyển ràng buộc bền vững thành SOCP
- Sử dụng **ràng buộc nón** để mô hình hóa
- Bộ giải: ECOS, SCS, Mosek

### **Khi công thức hóa SDP:**
- Nhận diện ràng buộc **ma trận**: $$X \succeq 0$$
- Sử dụng **LMI** (Bất đẳng thức Ma trận Tuyến tính)
- Áp dụng cho **điều khiển**, **xử lý tín hiệu**, **tối ưu tổ hợp**
- Bộ giải: CVXOPT, SCS, Mosek

### **Khi chuyển GP về lồi:**
- **Biến đổi logarit**: $$x_i = e^{y_i}$$
- **Log-sum-exp**: Hàm lồi
- **Monomial**: $$e^{a^T y}$$ (affine trong không gian log)
- **Posynomial**: $$\log(\sum e^{a_i^T y})$$ (log-sum-exp)

### **Khi chọn điều chuẩn:**
- **L2 (Ridge)**: Thu nhỏ, tất cả đặc trưng giữ lại
- **L1 (Lasso)**: Thưa, chọn đặc trưng tự động
- **Elastic Net**: Kết hợp L1 và L2
- **Chọn $$\lambda$$**: Kiểm định chéo

### **Kiểm tra kết quả:**
- **Primal feasibility**: Tất cả ràng buộc thỏa mãn
- **Dual feasibility**: Lagrange multipliers $$\geq 0$$
- **Complementary slackness**: $$\lambda_i g_i(x^*) = 0$$
- **Stationarity**: $$\nabla L = 0$$

---

## 📚 Tài Liệu Tham Khảo

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 4: Convex optimization problems
   - Section 4.2: Linear programming
   - Section 4.3: Quadratic programming
   - Section 4.4: Second-order cone programming
   - Section 4.5: Geometric programming
   - Section 4.6: Semidefinite programming

2. **Ben-Tal, A., & Nemirovski, A.** (2001). *Lectures on Modern Convex Optimization*.
   - Robust optimization formulations
   - Conic programming

3. **Nesterov, Y., & Nemirovskii, A.** (1994). *Interior-Point Polynomial Algorithms in Convex Programming*.
   - Interior-point methods for canonical problems

4. **Boyd, S.** (Stanford EE364a lectures)
   - [https://web.stanford.edu/class/ee364a/](https://web.stanford.edu/class/ee364a/)

---

## 🎯 Tổng Kết

Chương này cung cấp kỹ năng thực hành với các bài toán canonical:

✅ **Linear Programming (LP)**: Simplex, duality, sensitivity analysis  
✅ **Quadratic Programming (QP)**: Portfolio optimization, KKT conditions  
✅ **SOCP**: Robust optimization, uncertainty quantification  
✅ **SDP**: Matrix constraints, correlation matrices  
✅ **GP**: Engineering design, logarithmic transformation  
✅ **Least Squares**: OLS, Ridge, Lasso, regularization trade-offs

**Kỹ năng đạt được:**
- Formulate bài toán thực tế thành canonical forms
- Giải và phân tích nghiệm tối ưu
- Sử dụng duality và KKT conditions
- Hiểu trade-offs: optimality vs. robustness, bias vs. variance
- Áp dụng regularization cho machine learning

**Bước tiếp theo:** Áp dụng các kỹ thuật này vào bài toán thực tế trong các lĩnh vực:
- Finance: Portfolio optimization, risk management
- Engineering: Design optimization, control systems
- Machine Learning: Regression, classification, deep learning
- Operations Research: Resource allocation, scheduling

