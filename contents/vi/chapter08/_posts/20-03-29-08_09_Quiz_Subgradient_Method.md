---
layout: post
title: 08-09 B√†i t·∫≠p tr·∫Øc nghi·ªám - Ph∆∞∆°ng Ph√°p Subgradient
chapter: '08'
order: 17
owner: GitHub Copilot
lang: vi
categories:
- chapter08
lesson_type: quiz
---

## üìö √în t·∫≠p l√Ω thuy·∫øt

Tr∆∞·ªõc khi l√†m b√†i t·∫≠p, h√£y √¥n l·∫°i c√°c kh√°i ni·ªám ch√≠nh trong ch∆∞∆°ng n√†y:

### ‚ö° **Ph∆∞∆°ng Ph√°p Subgradient**

#### **ƒê·ªãnh nghƒ©a thu·∫≠t to√°n**
Ph∆∞∆°ng ph√°p subgradient l√† m·ªü r·ªông c·ªßa gradient descent cho h√†m l·ªìi kh√¥ng kh·∫£ vi:

$$x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}, \quad k = 1, 2, 3, \ldots$$

v·ªõi $$g^{(k-1)} \in \partial f(x^{(k-1)})$$ l√† m·ªôt subgradient c·ªßa $$f$$ t·∫°i $$x^{(k-1)}$$

#### **Kh√°c bi·ªát quan tr·ªçng v·ªõi Gradient Descent**
- **Kh√¥ng ƒë·∫£m b·∫£o descent:** Gi√° tr·ªã h√†m s·ªë c√≥ th·ªÉ tƒÉng ·ªü m·ªôt s·ªë b∆∞·ªõc
- **Theo d√µi best value:** $$f(x_{\text{best}}^{(k)}) = \min_{i=0,\ldots,k} f(x^{(i)})$$
- **Step size c·ªë ƒë·ªãnh:** Kh√¥ng th·ªÉ s·ª≠ d·ª•ng line search nh∆∞ gradient descent

### üìè **L·ª±a Ch·ªçn K√≠ch Th∆∞·ªõc B∆∞·ªõc**

#### **1. K√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh**
$$t_k = t > 0, \quad \forall k$$

**ƒê·∫∑c ƒëi·ªÉm:**
- ƒê∆°n gi·∫£n ƒë·ªÉ implement
- H·ªôi t·ª• ƒë·∫øn l√¢n c·∫≠n c·ªßa nghi·ªám t·ªëi ∆∞u
- Kh√¥ng ƒë·∫°t ƒë∆∞·ª£c nghi·ªám ch√≠nh x√°c

#### **2. K√≠ch th∆∞·ªõc b∆∞·ªõc gi·∫£m d·∫ßn**
Th·ªèa m√£n ƒëi·ªÅu ki·ªán:
$$\sum_{k=1}^{\infty} t_k = \infty, \quad \sum_{k=1}^{\infty} t_k^2 < \infty$$

**V√≠ d·ª• ph·ªï bi·∫øn:**
- $$t_k = \frac{1}{k}$$ (chu·ªói ƒëi·ªÅu h√≤a)
- $$t_k = \frac{1}{\sqrt{k}}$$
- $$t_k = \frac{a}{b + k}$$ v·ªõi $$a, b > 0$$

**ƒê·∫∑c ƒëi·ªÉm:**
- H·ªôi t·ª• ch√≠nh x√°c ƒë·∫øn nghi·ªám t·ªëi ∆∞u
- T·ªëc ƒë·ªô h·ªôi t·ª• ch·∫≠m h∆°n k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh ban ƒë·∫ßu

#### **3. Polyak step sizes**
$$t_k = \frac{f(x^{(k)}) - f^*}{\lVert g^{(k)} \rVert_2^2}$$

**Y√™u c·∫ßu:** Bi·∫øt gi√° tr·ªã t·ªëi ∆∞u $$f^*$$
**∆Øu ƒëi·ªÉm:** H·ªôi t·ª• nhanh h∆°n khi bi·∫øt $$f^*$$

### üìä **Ph√¢n T√≠ch H·ªôi T·ª•**

#### **Gi·∫£ ƒë·ªãnh c∆° b·∫£n**
- $$f$$ l√† h√†m l·ªìi v·ªõi $$\text{dom}(f) = \mathbb{R}^n$$
- $$f$$ th·ªèa m√£n ƒëi·ªÅu ki·ªán Lipschitz:
  $$|f(x) - f(y)| \le G \lVert x - y \rVert_2, \quad \forall x, y$$
- Subgradients b·ªã ch·∫∑n: $$\lVert g \rVert_2 \le G$$ v·ªõi $$g \in \partial f(x)$$

#### **ƒê·ªãnh l√Ω h·ªôi t·ª• cho k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh**
$$\lim_{k \to \infty} f(x_{\text{best}}^{(k)}) \le f^* + \frac{G^2 t}{2}$$

**√ù nghƒ©a:**
- H·ªôi t·ª• ƒë·∫øn l√¢n c·∫≠n c·ªßa nghi·ªám t·ªëi ∆∞u
- Sai s·ªë cu·ªëi c√πng t·ª∑ l·ªá v·ªõi k√≠ch th∆∞·ªõc b∆∞·ªõc $$t$$
- Trade-off: $$t$$ nh·ªè ‚Üí sai s·ªë nh·ªè nh∆∞ng h·ªôi t·ª• ch·∫≠m

#### **ƒê·ªãnh l√Ω h·ªôi t·ª• cho k√≠ch th∆∞·ªõc b∆∞·ªõc gi·∫£m d·∫ßn**
$$\lim_{k \to \infty} f(x_{\text{best}}^{(k)}) = f^*$$

**√ù nghƒ©a:**
- H·ªôi t·ª• ch√≠nh x√°c ƒë·∫øn nghi·ªám t·ªëi ∆∞u
- T·ªëc ƒë·ªô h·ªôi t·ª•: $$O(1/\sqrt{k})$$ (sublinear)

#### **Ch·ª©ng minh c∆° b·∫£n**
T·ª´ b·∫•t ƒë·∫≥ng th·ª©c c∆° b·∫£n:
$$f_{\text{best}}^{(k)} - f^* \le \frac{R^2 + G^2 \sum_{i=1}^k t_i^2}{2 \sum_{i=1}^k t_i}$$

v·ªõi $$R = \lVert x^{(0)} - x^* \rVert_2$$

### üéØ **·ª®ng D·ª•ng Th·ª±c T·∫ø**

#### **1. LASSO Regression**
$$\min_\beta \frac{1}{2} \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_1$$

**Subgradient:**
$$\partial f(\beta) = X^T(X\beta - y) + \lambda \partial \lVert \beta \rVert_1$$

#### **2. Regularized Logistic Regression**
$$\min_\beta \sum_{i=1}^n \left[-y_i x_i^T \beta + \log(1 + \exp(x_i^T \beta))\right] + \lambda \lVert \beta \rVert_1$$

**So s√°nh:**
- **Ridge (L2):** Kh·∫£ vi ‚Üí Gradient descent
- **LASSO (L1):** Kh√¥ng kh·∫£ vi ‚Üí Subgradient method

#### **3. Support Vector Machine**
$$\min_w \frac{1}{2} \lVert w \rVert_2^2 + C \sum_{i=1}^n \max(0, 1 - y_i w^T x_i)$$

**Hinge loss:** $$\max(0, 1 - z)$$ kh√¥ng kh·∫£ vi t·∫°i $$z = 1$$

### üîÑ **Stochastic Subgradient Method**

#### **B√†i to√°n t·ªïng**
$$\min_x f(x) = \min_x \sum_{i=1}^m f_i(x)$$

#### **Stochastic update**
$$x^{(k)} = x^{(k-1)} - t_k g_i^{(k-1)}$$

v·ªõi $$g_i^{(k-1)} \in \partial f_i(x^{(k-1)})$$ v√† $$i$$ ƒë∆∞·ª£c ch·ªçn ng·∫´u nhi√™n

#### **∆Øu ƒëi·ªÉm**
- Chi ph√≠ t√≠nh to√°n: $$O(1)$$ thay v√¨ $$O(m)$$
- Ph√π h·ª£p v·ªõi big data
- C√≥ th·ªÉ tho√°t kh·ªèi saddle points

#### **Nh∆∞·ª£c ƒëi·ªÉm**
- Nhi·ªÖu cao h∆°n batch method
- C·∫ßn tuning learning rate c·∫©n th·∫≠n
- H·ªôi t·ª• ch·∫≠m h∆°n g·∫ßn nghi·ªám t·ªëi ∆∞u

### üìà **Projected Subgradient Method**

#### **B√†i to√°n c√≥ r√†ng bu·ªôc**
$$\min_{x \in C} f(x)$$

v·ªõi $$C$$ l√† t·∫≠p l·ªìi ƒë√≥ng

#### **Thu·∫≠t to√°n**
$$x^{(k)} = P_C(x^{(k-1)} - t_k g^{(k-1)})$$

v·ªõi $$P_C(\cdot)$$ l√† ph√©p chi·∫øu l√™n t·∫≠p $$C$$

#### **V√≠ d·ª•: Intersection of Sets**
$$\min_x \frac{1}{2} \lVert x \rVert_2^2 \quad \text{s.t.} \quad x \in \bigcap_{i=1}^m C_i$$

**Thu·∫≠t to√°n:**
1. Ch·ªçn t·∫≠p $$C_i$$ vi ph·∫°m nh·∫•t
2. Chi·∫øu l√™n $$C_i$$: $$x^{(k)} = P_{C_i}(x^{(k-1)})$$

### üîÑ **So S√°nh C√°c Ph∆∞∆°ng Ph√°p**

| Ph∆∞∆°ng ph√°p | ƒêi·ªÅu ki·ªán | T·ªëc ƒë·ªô h·ªôi t·ª• | ·ª®ng d·ª•ng |
|-------------|-----------|---------------|----------|
| **Gradient Descent** | H√†m kh·∫£ vi | $$O(1/k)$$ ho·∫∑c linear | Smooth optimization |
| **Subgradient** | H√†m l·ªìi | $$O(1/\sqrt{k})$$ | Non-smooth optimization |
| **SGD** | H√†m kh·∫£ vi | $$O(1/\sqrt{k})$$ | Large-scale problems |
| **Stochastic Subgradient** | H√†m l·ªìi | $$O(1/\sqrt{k})$$ | Large-scale non-smooth |

### üõ†Ô∏è **K·ªπ Thu·∫≠t Th·ª±c H√†nh**

#### **Ch·ªçn step size**
1. **Fixed step size:** Th·ª≠ $$t = 0.01, 0.1, 1$$ v√† ch·ªçn t·ªët nh·∫•t
2. **Diminishing:** S·ª≠ d·ª•ng $$t_k = \frac{1}{\sqrt{k}}$$ ho·∫∑c $$t_k = \frac{1}{k}$$
3. **Polyak:** N·∫øu bi·∫øt $$f^*$$, s·ª≠ d·ª•ng c√¥ng th·ª©c Polyak

#### **Monitoring convergence**
- Theo d√µi $$f(x_{\text{best}}^{(k)})$$ thay v√¨ $$f(x^{(k)})$$
- V·∫Ω ƒë·ªì th·ªã convergence ƒë·ªÉ ki·ªÉm tra
- S·ª≠ d·ª•ng moving average ƒë·ªÉ l√†m m∆∞·ª£t

#### **Debugging**
- Ki·ªÉm tra t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa subgradient
- Verify Lipschitz constant $$G$$
- Th·ª≠ nhi·ªÅu step size kh√°c nhau
- So s√°nh v·ªõi gradient descent (n·∫øu h√†m kh·∫£ vi)

### ‚ö° **C·∫£i Ti·∫øn Subgradient Method**

#### **1. Averaging**
$$\bar{x}^{(k)} = \frac{1}{k} \sum_{i=1}^k x^{(i)}$$

**∆Øu ƒëi·ªÉm:** Gi·∫£m nhi·ªÖu, c·∫£i thi·ªán convergence rate

#### **2. Momentum**
$$x^{(k)} = x^{(k-1)} - t_k g^{(k-1)} + \beta(x^{(k-1)} - x^{(k-2)})$$

**∆Øu ƒëi·ªÉm:** TƒÉng t·ªëc h·ªôi t·ª•, gi·∫£m oscillation

#### **3. Adaptive step sizes**
- **AdaGrad for subgradients**
- **RMSprop variants**
- **Adam-like methods**

### üéØ **V√≠ D·ª• Minh H·ªça**

#### **B√†i to√°n:** $$\min_x |x - 2| + |x + 1|$$

**B∆∞·ªõc 1:** T√≠nh subgradient
$$\partial f(x) = \partial |x - 2| + \partial |x + 1|$$

**B∆∞·ªõc 2:** Subgradient method v·ªõi $$t_k = 0.1$$
- B·∫Øt ƒë·∫ßu t·ª´ $$x^{(0)} = 5$$
- $$g^{(0)} = 1 + 1 = 2$$
- $$x^{(1)} = 5 - 0.1 \times 2 = 4.8$$
- Ti·∫øp t·ª•c cho ƒë·∫øn h·ªôi t·ª•

**K·∫øt qu·∫£:** H·ªôi t·ª• ƒë·∫øn $$x^* \in [-1, 2]$$

### üí° **M·∫πo Th·ª±c H√†nh**

#### **Khi n√†o s·ª≠ d·ª•ng Subgradient Method:**
- H√†m m·ª•c ti√™u kh√¥ng kh·∫£ vi (ch·ª©a $$\lvert \cdot \rvert$$, $$\max\{\cdot\}$$)
- Regularization v·ªõi L1 norm
- Constraint optimization v·ªõi indicator functions
- Large-scale problems v·ªõi stochastic variants

#### **Khi n√†o KH√îNG s·ª≠ d·ª•ng:**
- H√†m kh·∫£ vi (d√πng gradient descent)
- C·∫ßn convergence nhanh (d√πng Newton methods)
- H√†m kh√¥ng l·ªìi (kh√¥ng ƒë·∫£m b·∫£o global optimum)

#### **Best practices:**
- Lu√¥n theo d√µi $$f_{\text{best}}^{(k)}$$
- Th·ª≠ nhi·ªÅu step size strategies
- S·ª≠ d·ª•ng averaging ƒë·ªÉ gi·∫£m nhi·ªÖu
- K·∫øt h·ª£p v·ªõi projection cho constrained problems

---

üí° **M·∫πo:** Subgradient method l√† c√¥ng c·ª• m·∫°nh m·∫Ω cho non-smooth optimization, ƒë·∫∑c bi·ªát quan tr·ªçng trong machine learning v·ªõi regularization v√† robust optimization!

---

<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    ignoreHtmlClass: ".*|",
    processHtmlClass: "arithmatex"
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
/* S·ª≠a l·ªói alignment cho MathJax inline math */
.MathJax {
    vertical-align: baseline !important;
}

mjx-container[jax="CHTML"][display="false"] {
    vertical-align: baseline !important;
    display: inline !important;
}

/* ƒê·∫£m b·∫£o c√°c label trong quiz c√≥ alignment t·ªët */
.options label {
    display: flex;
    align-items: center;
    margin: 8px 0;
    padding: 8px;
    border-radius: 4px;
    transition: background-color 0.2s;
}

.options label:hover {
    background-color: #f0f0f0;
}

.options input[type="radio"] {
    margin-right: 8px;
    flex-shrink: 0;
}

/* Styling cho quiz container */
#quiz-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.question {
    background: #f8f9fa;
    border-left: 4px solid #007bff;
    padding: 20px;
    margin: 20px 0;
    border-radius: 8px;
}

.question h3 {
    color: #2c3e50;
    margin-bottom: 15px;
}

.explanation {
    background: #e8f5e8;
    border: 1px solid #28a745;
    padding: 15px;
    margin-top: 15px;
    border-radius: 6px;
}

.quiz-controls {
    text-align: center;
    margin: 30px 0;
}

.quiz-controls button {
    background: #007bff;
    color: white;
    border: none;
    padding: 12px 24px;
    margin: 0 10px;
    border-radius: 6px;
    cursor: pointer;
    font-size: 16px;
    transition: background-color 0.3s;
}

.quiz-controls button:hover {
    background: #0056b3;
}

.quiz-controls button:disabled {
    background: #6c757d;
    cursor: not-allowed;
}

.progress-bar {
    width: 100%;
    height: 8px;
    background: #e9ecef;
    border-radius: 4px;
    margin: 20px 0;
    overflow: hidden;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #007bff, #28a745);
    transition: width 0.3s ease;
}

.score-display {
    text-align: center;
    font-size: 18px;
    font-weight: bold;
    color: #2c3e50;
    margin: 20px 0;
}

.options label.selected {
    background-color: #e3f2fd;
    border: 2px solid #2196f3;
}

.final-score {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 30px;
    border-radius: 15px;
    text-align: center;
    margin: 30px 0;
}
</style>

B√†i t·∫≠p tr·∫Øc nghi·ªám n√†y ki·ªÉm tra hi·ªÉu bi·∫øt c·ªßa b·∫°n v·ªÅ ph∆∞∆°ng ph√°p subgradient, l·ª±a ch·ªçn k√≠ch th∆∞·ªõc b∆∞·ªõc, ph√¢n t√≠ch h·ªôi t·ª•, projected subgradient method v√† stochastic subgradient method.

---

<div id="quiz-container">
    <div id="quiz-header">
        <h2>üîÑ Quiz: Ph∆∞∆°ng Ph√°p Subgradient</h2>
        <div class="progress-bar">
            <div class="progress-fill" id="progress-fill" style="width: 5%"></div>
        </div>
        <div class="score-display" id="score-display">C√¢u h·ªèi: <span id="current-q">1</span> / <span id="total-q">20</span></div>
    </div>

    <div id="quiz-content">
        <!-- C√¢u h·ªèi 1: ƒê·ªãnh nghƒ©a subgradient method -->
        <div class="question" id="q1" style="display: block;">
            <h3>C√¢u 1: C√¥ng th·ª©c c·∫≠p nh·∫≠t c·ªßa ph∆∞∆°ng ph√°p subgradient l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q1" value="a"> A) \(x^{(k)} = x^{(k-1)} + t_k \cdot g^{(k-1)}\)</label>
                <label><input type="radio" name="q1" value="b"> B) \(x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}\)</label>
                <label><input type="radio" name="q1" value="c"> C) \(x^{(k)} = x^{(k-1)} - t_k \cdot \nabla f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q1" value="d"> D) \(x^{(k)} = x^{(k-1)} - g^{(k-1)}\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}$$</strong><br>
                Ph∆∞∆°ng ph√°p subgradient thay th·∫ø gradient b·∫±ng subgradient $$g^{(k-1)} \in \partial f(x^{(k-1)})$$ v·ªõi step size $$t_k$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 2: T√≠nh ch·∫•t descent -->
        <div class="question" id="q2" style="display: none;">
            <h3>C√¢u 2: Kh√°c v·ªõi gradient descent, ph∆∞∆°ng ph√°p subgradient:</h3>
            <div class="options">
                <label><input type="radio" name="q2" value="a"> A) Lu√¥n gi·∫£m gi√° tr·ªã h√†m s·ªë ·ªü m·ªói b∆∞·ªõc</label>
                <label><input type="radio" name="q2" value="b"> B) Kh√¥ng lu√¥n ƒë·∫£m b·∫£o gi·∫£m gi√° tr·ªã h√†m s·ªë ·ªü m·ªói b∆∞·ªõc</label>
                <label><input type="radio" name="q2" value="c"> C) Ch·ªâ √°p d·ª•ng cho h√†m kh·∫£ vi</label>
                <label><input type="radio" name="q2" value="d"> D) H·ªôi t·ª• nhanh h∆°n gradient descent</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Kh√¥ng lu√¥n ƒë·∫£m b·∫£o gi·∫£m gi√° tr·ªã h√†m s·ªë ·ªü m·ªói b∆∞·ªõc</strong><br>
                V√¨ v·∫≠y ph∆∞∆°ng ph√°p n√†y kh√¥ng ƒë∆∞·ª£c g·ªçi l√† "descent" subgradient v√† c·∫ßn theo d√µi gi√° tr·ªã t·ªët nh·∫•t $$f(x_{best}^{(k)})$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 3: Best value tracking -->
        <div class="question" id="q3" style="display: none;">
            <h3>C√¢u 3: Trong subgradient method, \(f(x_{best}^{(k)})\) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q3" value="a"> A) \(f(x^{(k)})\)</label>
                <label><input type="radio" name="q3" value="b"> B) \(\min_{i=0,...,k} f(x^{(i)})\)</label>
                <label><input type="radio" name="q3" value="c"> C) \(\max_{i=0,...,k} f(x^{(i)})\)</label>
                <label><input type="radio" name="q3" value="d"> D) \(\frac{1}{k+1} \sum_{i=0}^k f(x^{(i)})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\min_{i=0,...,k} f(x^{(i)})$$</strong><br>
                ƒê√¢y l√† gi√° tr·ªã nh·ªè nh·∫•t c·ªßa h√†m $$f$$ thu ƒë∆∞·ª£c trong $$k$$ l·∫ßn l·∫∑p c·ªßa ph∆∞∆°ng ph√°p subgradient.
            </div>
        </div>

        <!-- C√¢u h·ªèi 4: Fixed step size -->
        <div class="question" id="q4" style="display: none;">
            <h3>C√¢u 4: V·ªõi k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh \(t_k = t\), ph∆∞∆°ng ph√°p subgradient:</h3>
            <div class="options">
                <label><input type="radio" name="q4" value="a"> A) H·ªôi t·ª• ƒë·∫øn nghi·ªám t·ªëi ∆∞u ch√≠nh x√°c</label>
                <label><input type="radio" name="q4" value="b"> B) H·ªôi t·ª• ƒë·∫øn m·ªôt l√¢n c·∫≠n c·ªßa nghi·ªám t·ªëi ∆∞u</label>
                <label><input type="radio" name="q4" value="c"> C) Kh√¥ng h·ªôi t·ª•</label>
                <label><input type="radio" name="q4" value="d"> D) Ch·ªâ h·ªôi t·ª• cho h√†m kh·∫£ vi</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) H·ªôi t·ª• ƒë·∫øn m·ªôt l√¢n c·∫≠n c·ªßa nghi·ªám t·ªëi ∆∞u</strong><br>
                V·ªõi step size c·ªë ƒë·ªãnh, thu·∫≠t to√°n kh√¥ng h·ªôi t·ª• ch√≠nh x√°c m√† ch·ªâ ƒë·∫øn g·∫ßn nghi·ªám t·ªëi ∆∞u trong m·ªôt l√¢n c·∫≠n.
            </div>
        </div>

        <!-- C√¢u h·ªèi 5: Diminishing step size conditions -->
        <div class="question" id="q5" style="display: none;">
            <h3>C√¢u 5: ƒêi·ªÅu ki·ªán cho k√≠ch th∆∞·ªõc b∆∞·ªõc gi·∫£m d·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª• l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q5" value="a"> A) \(\sum_{k=1}^{\infty} t_k < \infty\) v√† \(\sum_{k=1}^{\infty} t_k^2 < \infty\)</label>
                <label><input type="radio" name="q5" value="b"> B) \(\sum_{k=1}^{\infty} t_k = \infty\) v√† \(\sum_{k=1}^{\infty} t_k^2 < \infty\)</label>
                <label><input type="radio" name="q5" value="c"> C) \(\sum_{k=1}^{\infty} t_k = \infty\) v√† \(\sum_{k=1}^{\infty} t_k^2 = \infty\)</label>
                <label><input type="radio" name="q5" value="d"> D) \(t_k = \text{constant}\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\sum_{k=1}^{\infty} t_k = \infty$$ v√† $$\sum_{k=1}^{\infty} t_k^2 < \infty$$</strong><br>
                ƒêi·ªÅu ki·ªán Robbins-Monro: t·ªïng v√¥ h·∫°n ƒë·ªÉ ƒë·∫£m b·∫£o ti·∫øn b·ªô, t·ªïng b√¨nh ph∆∞∆°ng h·ªØu h·∫°n ƒë·ªÉ ki·ªÉm so√°t nhi·ªÖu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 6: V√≠ d·ª• step size -->
        <div class="question" id="q6" style="display: none;">
            <h3>C√¢u 6: K√≠ch th∆∞·ªõc b∆∞·ªõc \(t_k = \frac{1}{k}\) th·ªèa m√£n ƒëi·ªÅu ki·ªán h·ªôi t·ª• v√¨:</h3>
            <div class="options">
                <label><input type="radio" name="q6" value="a"> A) \(\sum_{k=1}^{\infty} \frac{1}{k} = \infty\) (chu·ªói ƒëi·ªÅu h√≤a) v√† \(\sum_{k=1}^{\infty} \frac{1}{k^2} < \infty\) (b√†i to√°n Basel)</label>
                <label><input type="radio" name="q6" value="b"> B) C·∫£ hai t·ªïng ƒë·ªÅu h·ªØu h·∫°n</label>
                <label><input type="radio" name="q6" value="c"> C) C·∫£ hai t·ªïng ƒë·ªÅu v√¥ h·∫°n</label>
                <label><input type="radio" name="q6" value="d"> D) Kh√¥ng th·ªèa m√£n ƒëi·ªÅu ki·ªán</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\sum_{k=1}^{\infty} \frac{1}{k} = \infty$$ (chu·ªói ƒëi·ªÅu h√≤a) v√† $$\sum_{k=1}^{\infty} \frac{1}{k^2} < \infty$$ (b√†i to√°n Basel)</strong><br>
                ƒê√¢y l√† v√≠ d·ª• c·ªï ƒëi·ªÉn c·ªßa step size th·ªèa m√£n ƒëi·ªÅu ki·ªán Robbins-Monro.
            </div>
        </div>

        <!-- C√¢u h·ªèi 7: Convergence rate -->
        <div class="question" id="q7" style="display: none;">
            <h3>C√¢u 7: T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa subgradient method v·ªõi diminishing step size l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q7" value="a"> A) \(O(1/k^2)\)</label>
                <label><input type="radio" name="q7" value="b"> B) \(O(1/k)\)</label>
                <label><input type="radio" name="q7" value="c"> C) \(O(1/\sqrt{k})\)</label>
                <label><input type="radio" name="q7" value="d"> D) \(O(e^{-k})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: C) $$O(1/\sqrt{k})$$</strong><br>
                Subgradient method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª• $$O(1/\sqrt{k})$$, ch·∫≠m h∆°n gradient descent $$O(1/k)$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 8: Basic inequality -->
        <div class="question" id="q8" style="display: none;">
            <h3>C√¢u 8: B·∫•t ƒë·∫≥ng th·ª©c c∆° b·∫£n trong ph√¢n t√≠ch subgradient method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q8" value="a"> A) \(\|x^{(k)} - x^*\|^2 \leq \|x^{(k-1)} - x^*\|^2 - 2t_k(f(x^{(k-1)}) - f^*) + t_k^2\|g^{(k-1)}\|^2\)</label>
                <label><input type="radio" name="q8" value="b"> B) \(\|x^{(k)} - x^*\|^2 \geq \|x^{(k-1)} - x^*\|^2\)</label>
                <label><input type="radio" name="q8" value="c"> C) \(f(x^{(k)}) \leq f(x^{(k-1)})\)</label>
                <label><input type="radio" name="q8" value="d"> D) \(\|g^{(k)}\| \leq \|g^{(k-1)}\|\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\|x^{(k)} - x^*\|^2 \leq \|x^{(k-1)} - x^*\|^2 - 2t_k(f(x^{(k-1)}) - f^*) + t_k^2\|g^{(k-1)}\|^2$$</strong><br>
                ƒê√¢y l√† b·∫•t ƒë·∫≥ng th·ª©c c∆° b·∫£n d√πng ƒë·ªÉ ph√¢n t√≠ch h·ªôi t·ª•, c√¢n b·∫±ng gi·ªØa ti·∫øn b·ªô v√† nhi·ªÖu.
            </div>
        </div>

        <!-- C√¢u h·ªèi 9: Polyak step size -->
        <div class="question" id="q9" style="display: none;">
            <h3>C√¢u 9: Polyak step size ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q9" value="a"> A) \(t_k = \frac{1}{k}\)</label>
                <label><input type="radio" name="q9" value="b"> B) \(t_k = \frac{f(x^{(k-1)}) - f^*}{\|g^{(k-1)}\|^2}\)</label>
                <label><input type="radio" name="q9" value="c"> C) \(t_k = \text{constant}\)</label>
                <label><input type="radio" name="q9" value="d"> D) \(t_k = \frac{1}{\|g^{(k-1)}\|}\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$t_k = \frac{f(x^{(k-1)}) - f^*}{\|g^{(k-1)}\|^2}$$</strong><br>
                Polyak step size y√™u c·∫ßu bi·∫øt gi√° tr·ªã t·ªëi ∆∞u $$f^*$$ v√† th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët trong th·ª±c t·∫ø.
            </div>
        </div>

        <!-- C√¢u h·ªèi 10: Projected subgradient method -->
        <div class="question" id="q10" style="display: none;">
            <h3>C√¢u 10: Projected subgradient method cho b√†i to√°n c√≥ r√†ng bu·ªôc \(\min_{x \in C} f(x)\) c√≥ c√¥ng th·ª©c:</h3>
            <div class="options">
                <label><input type="radio" name="q10" value="a"> A) \(x^{(k)} = x^{(k-1)} - t_k g^{(k-1)}\)</label>
                <label><input type="radio" name="q10" value="b"> B) \(x^{(k)} = P_C(x^{(k-1)} - t_k g^{(k-1)})\)</label>
                <label><input type="radio" name="q10" value="c"> C) \(x^{(k)} = x^{(k-1)} - t_k P_C(g^{(k-1)})\)</label>
                <label><input type="radio" name="q10" value="d"> D) \(x^{(k)} = P_C(x^{(k-1)})\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$x^{(k)} = P_C(x^{(k-1)} - t_k g^{(k-1)})$$</strong><br>
                Projected subgradient method th·ª±c hi·ªán b∆∞·ªõc subgradient r·ªìi chi·∫øu k·∫øt qu·∫£ v·ªÅ t·∫≠p r√†ng bu·ªôc $$C$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 11: Stochastic subgradient -->
        <div class="question" id="q11" style="display: none;">
            <h3>C√¢u 11: Stochastic subgradient method cho b√†i to√°n \(\min \sum_{i=1}^m f_i(x)\) c√≥ c√¥ng th·ª©c:</h3>
            <div class="options">
                <label><input type="radio" name="q11" value="a"> A) \(x^{(k)} = x^{(k-1)} - t_k \sum_{i=1}^m g_i^{(k-1)}\)</label>
                <label><input type="radio" name="q11" value="b"> B) \(x^{(k)} = x^{(k-1)} - t_k g_{i_k}^{(k-1)}\)</label>
                <label><input type="radio" name="q11" value="c"> C) \(x^{(k)} = x^{(k-1)} - t_k \frac{1}{m} \sum_{i=1}^m g_i^{(k-1)}\)</label>
                <label><input type="radio" name="q11" value="d"> D) \(x^{(k)} = x^{(k-1)} - g_{i_k}^{(k-1)}\)</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$x^{(k)} = x^{(k-1)} - t_k g_{i_k}^{(k-1)}$$</strong><br>
                Stochastic version ch·ªâ s·ª≠ d·ª•ng subgradient c·ªßa m·ªôt h√†m $$f_{i_k}$$ ƒë∆∞·ª£c ch·ªçn t·∫°i m·ªói iteration.
            </div>
        </div>

        <!-- C√¢u h·ªèi 12: Advantage of stochastic -->
        <div class="question" id="q12" style="display: none;">
            <h3>C√¢u 12: ∆Øu ƒëi·ªÉm ch√≠nh c·ªßa stochastic subgradient method l√†:</h3>
            <div class="options">
                <label><input type="radio" name="q12" value="a"> A) H·ªôi t·ª• nhanh h∆°n batch method</label>
                <label><input type="radio" name="q12" value="b"> B) Chi ph√≠ t√≠nh to√°n th·∫•p h∆°n m·ªói iteration</label>
                <label><input type="radio" name="q12" value="c"> C) Lu√¥n gi·∫£m gi√° tr·ªã h√†m s·ªë</label>
                <label><input type="radio" name="q12" value="d"> D) Kh√¥ng c·∫ßn step size</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Chi ph√≠ t√≠nh to√°n th·∫•p h∆°n m·ªói iteration</strong><br>
                Stochastic method ch·ªâ c·∫ßn t√≠nh subgradient c·ªßa m·ªôt h√†m thay v√¨ t·ªïng t·∫•t c·∫£, gi·∫£m ƒë√°ng k·ªÉ chi ph√≠ t√≠nh to√°n.
            </div>
        </div>

        <!-- C√¢u h·ªèi 13: Convergence comparison -->
        <div class="question" id="q13" style="display: none;">
            <h3>C√¢u 13: So v·ªõi gradient descent, subgradient method:</h3>
            <div class="options">
                <label><input type="radio" name="q13" value="a"> A) H·ªôi t·ª• nhanh h∆°n</label>
                <label><input type="radio" name="q13" value="b"> B) H·ªôi t·ª• ch·∫≠m h∆°n</label>
                <label><input type="radio" name="q13" value="c"> C) C√≥ c√πng t·ªëc ƒë·ªô h·ªôi t·ª•</label>
                <label><input type="radio" name="q13" value="d"> D) Kh√¥ng h·ªôi t·ª•</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) H·ªôi t·ª• ch·∫≠m h∆°n</strong><br>
                Subgradient method c√≥ t·ªëc ƒë·ªô $$O(1/\sqrt{k})$$ so v·ªõi $$O(1/k)$$ c·ªßa gradient descent, nh∆∞ng √°p d·ª•ng ƒë∆∞·ª£c cho h√†m kh√¥ng kh·∫£ vi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 14: When to use subgradient -->
        <div class="question" id="q14" style="display: none;">
            <h3>C√¢u 14: Subgradient method ƒë∆∞·ª£c s·ª≠ d·ª•ng khi:</h3>
            <div class="options">
                <label><input type="radio" name="q14" value="a"> A) H√†m m·ª•c ti√™u kh·∫£ vi m·ªçi n∆°i</label>
                <label><input type="radio" name="q14" value="b"> B) H√†m m·ª•c ti√™u l·ªìi nh∆∞ng c√≥ th·ªÉ kh√¥ng kh·∫£ vi</label>
                <label><input type="radio" name="q14" value="c"> C) H√†m m·ª•c ti√™u kh√¥ng l·ªìi</label>
                <label><input type="radio" name="q14" value="d"> D) Ch·ªâ cho b√†i to√°n kh√¥ng r√†ng bu·ªôc</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) H√†m m·ª•c ti√™u l·ªìi nh∆∞ng c√≥ th·ªÉ kh√¥ng kh·∫£ vi</strong><br>
                Subgradient method ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho c√°c h√†m l·ªìi kh√¥ng kh·∫£ vi nh∆∞ norm L1, max function.
            </div>
        </div>

        <!-- C√¢u h·ªèi 15: Step size adaptation -->
        <div class="question" id="q15" style="display: none;">
            <h3>C√¢u 15: Kh√°c v·ªõi gradient descent, step size trong subgradient method:</h3>
            <div class="options">
                <label><input type="radio" name="q15" value="a"> A) C√≥ th·ªÉ s·ª≠ d·ª•ng backtracking line search</label>
                <label><input type="radio" name="q15" value="b"> B) Ph·∫£i ƒë∆∞·ª£c thi·∫øt l·∫≠p tr∆∞·ªõc, kh√¥ng th√≠ch ·ª©ng</label>
                <label><input type="radio" name="q15" value="c"> C) Lu√¥n b·∫±ng 1</label>
                <label><input type="radio" name="q15" value="d"> D) T·ª± ƒë·ªông t·ªëi ∆∞u</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Ph·∫£i ƒë∆∞·ª£c thi·∫øt l·∫≠p tr∆∞·ªõc, kh√¥ng th√≠ch ·ª©ng</strong><br>
                Subgradient method kh√¥ng th·ªÉ s·ª≠ d·ª•ng line search v√¨ kh√¥ng ƒë·∫£m b·∫£o descent direction.
            </div>
        </div>

        <!-- C√¢u h·ªèi 16: Subgradient bound -->
        <div class="question" id="q16" style="display: none;">
            <h3>C√¢u 16: Trong ph√¢n t√≠ch h·ªôi t·ª•, th∆∞·ªùng gi·∫£ s·ª≠ subgradient b·ªã ch·∫∑n:</h3>
            <div class="options">
                <label><input type="radio" name="q16" value="a"> A) \(\|g^{(k)}\| \leq G\) v·ªõi \(G > 0\)</label>
                <label><input type="radio" name="q16" value="b"> B) \(\|g^{(k)}\| = 1\)</label>
                <label><input type="radio" name="q16" value="c"> C) \(\|g^{(k)}\| \to 0\)</label>
                <label><input type="radio" name="q16" value="d"> D) \(\|g^{(k)}\|\) kh√¥ng b·ªã ch·∫∑n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$\|g^{(k)}\| \leq G$$ v·ªõi $$G > 0$$</strong><br>
                Gi·∫£ s·ª≠ subgradient b·ªã ch·∫∑n l√† c·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª• v√† ∆∞·ªõc l∆∞·ª£ng t·ªëc ƒë·ªô h·ªôi t·ª•.
            </div>
        </div>

        <!-- C√¢u h·ªèi 17: Intersection of sets -->
        <div class="question" id="q17" style="display: none;">
            <h3>C√¢u 17: ƒê·ªÉ t√¨m giao c·ªßa c√°c t·∫≠p l·ªìi \(C_1 \cap C_2 \cap ... \cap C_m\), ta c√≥ th·ªÉ s·ª≠ d·ª•ng:</h3>
            <div class="options">
                <label><input type="radio" name="q17" value="a"> A) Gradient descent</label>
                <label><input type="radio" name="q17" value="b"> B) Projected subgradient method v·ªõi h√†m ch·ªâ th·ªã</label>
                <label><input type="radio" name="q17" value="c"> C) Newton method</label>
                <label><input type="radio" name="q17" value="d"> D) Kh√¥ng th·ªÉ gi·∫£i ƒë∆∞·ª£c</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Projected subgradient method v·ªõi h√†m ch·ªâ th·ªã</strong><br>
                C√≥ th·ªÉ t·ªëi ∆∞u h√≥a t·ªïng c√°c h√†m ch·ªâ th·ªã $$\sum I_{C_i}(x)$$ ƒë·ªÉ t√¨m giao c·ªßa c√°c t·∫≠p.
            </div>
        </div>

        <!-- C√¢u h·ªèi 18: Regularized problems -->
        <div class="question" id="q18" style="display: none;">
            <h3>C√¢u 18: Subgradient method th√≠ch h·ª£p cho b√†i to√°n regularized nh∆∞:</h3>
            <div class="options">
                <label><input type="radio" name="q18" value="a"> A) \(\min f(x) + \lambda \|x\|_2^2\)</label>
                <label><input type="radio" name="q18" value="b"> B) \(\min f(x) + \lambda \|x\|_1\)</label>
                <label><input type="radio" name="q18" value="c"> C) \(\min f(x) + \lambda x^T x\)</label>
                <label><input type="radio" name="q18" value="d"> D) T·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$\min f(x) + \lambda \|x\|_1$$</strong><br>
                L1 regularization t·∫°o ra h√†m kh√¥ng kh·∫£ vi, ph√π h·ª£p v·ªõi subgradient method. L2 regularization v·∫´n kh·∫£ vi.
            </div>
        </div>

        <!-- C√¢u h·ªèi 19: Batch vs stochastic -->
        <div class="question" id="q19" style="display: none;">
            <h3>C√¢u 19: Trong stochastic subgradient method, vi·ªác ch·ªçn index \(i_k\):</h3>
            <div class="options">
                <label><input type="radio" name="q19" value="a"> A) Ph·∫£i ho√†n to√†n ng·∫´u nhi√™n</label>
                <label><input type="radio" name="q19" value="b"> B) C√≥ th·ªÉ tu·∫ßn ho√†n ho·∫∑c ng·∫´u nhi√™n</label>
                <label><input type="radio" name="q19" value="c"> C) Ph·∫£i theo th·ª© t·ª± tƒÉng d·∫ßn</label>
                <label><input type="radio" name="q19" value="d"> D) Kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn h·ªôi t·ª•</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) C√≥ th·ªÉ tu·∫ßn ho√†n ho·∫∑c ng·∫´u nhi√™n</strong><br>
                C·∫£ hai c√°ch ch·ªçn tu·∫ßn ho√†n v√† ng·∫´u nhi√™n ƒë·ªÅu c√≥ th·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª• v·ªõi ƒëi·ªÅu ki·ªán th√≠ch h·ª£p.
            </div>
        </div>

        <!-- C√¢u h·ªèi 20: Practical considerations -->
        <div class="question" id="q20" style="display: none;">
            <h3>C√¢u 20: Trong th·ª±c t·∫ø, ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa subgradient method, ta c√≥ th·ªÉ:</h3>
            <div class="options">
                <label><input type="radio" name="q20" value="a"> A) S·ª≠ d·ª•ng averaging c·ªßa c√°c iterate</label>
                <label><input type="radio" name="q20" value="b"> B) K·∫øt h·ª£p v·ªõi momentum</label>
                <label><input type="radio" name="q20" value="c"> C) S·ª≠ d·ª•ng adaptive step size</label>
                <label><input type="radio" name="q20" value="d"> D) T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p tr√™n</strong><br>
                Averaging, momentum, v√† adaptive step size ƒë·ªÅu l√† c√°c k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán subgradient method.
            </div>
        </div>
        <!-- C√¢u h·ªèi 21: Th·ª±c h√†nh -->
        <div class="question" id="q21" style="display: none;">
            <h3>C√¢u 21: Subgradient method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª•:</h3>
            <div class="options">
                <label><input type="radio" name="q21" value="a"> A) $$O(1/k)$$</label>
                <label><input type="radio" name="q21" value="b"> B) $$O(1/\sqrt{k})$$</label>
                <label><input type="radio" name="q21" value="c"> C) $$O(1/k^2)$$</label>
                <label><input type="radio" name="q21" value="d"> D) Tuy·∫øn t√≠nh</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) $$O(1/\sqrt{k})$$</strong><br>
                Subgradient method c√≥ t·ªëc ƒë·ªô h·ªôi t·ª• $$O(1/\sqrt{k})$$ cho h√†m l·ªìi kh√¥ng tr∆°n.
            </div>
        </div>

        <!-- C√¢u h·ªèi 22: Th·ª±c h√†nh -->
        <div class="question" id="q22" style="display: none;">
            <h3>C√¢u 22: K√≠ch th∆∞·ªõc b∆∞·ªõc trong subgradient method th∆∞·ªùng ch·ªçn:</h3>
            <div class="options">
                <label><input type="radio" name="q22" value="a"> A) C·ªë ƒë·ªãnh nh·ªè</label>
                <label><input type="radio" name="q22" value="b"> B) Gi·∫£m d·∫ßn theo $$1/\sqrt{k}$$</label>
                <label><input type="radio" name="q22" value="c"> C) TƒÉng d·∫ßn</label>
                <label><input type="radio" name="q22" value="d"> D) Ng·∫´u nhi√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: B) Gi·∫£m d·∫ßn theo $$1/\sqrt{k}$$</strong><br>
                K√≠ch th∆∞·ªõc b∆∞·ªõc th∆∞·ªùng ch·ªçn $$t_k = \frac{c}{\sqrt{k}}$$ ƒë·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª•.
            </div>
        </div>

        <!-- C√¢u h·ªèi 23: Th·ª±c h√†nh -->
        <div class="question" id="q23" style="display: none;">
            <h3>C√¢u 23: V·ªõi h√†m $$f(x) = |x|$$, t·ª´ $$x_0 = 2$$, m·ªôt b∆∞·ªõc subgradient v·ªõi $$t = 0.5$$ cho:</h3>
            <div class="options">
                <label><input type="radio" name="q23" value="a"> A) $$x_1 = 1.5$$</label>
                <label><input type="radio" name="q23" value="b"> B) $$x_1 = 2.5$$</label>
                <label><input type="radio" name="q23" value="c"> C) $$x_1 = 1$$</label>
                <label><input type="radio" name="q23" value="d"> D) $$x_1 = 0$$</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: A) $$x_1 = 1.5$$</strong><br>
                T·∫°i $$x_0 = 2$$: subgradient l√† $$g = 1$$. $$x_1 = 2 - 0.5 \cdot 1 = 1.5$$.
            </div>
        </div>

        <!-- C√¢u h·ªèi 24: Th·ª±c h√†nh -->
        <div class="question" id="q24" style="display: none;">
            <h3>C√¢u 24: Subgradient method kh√°c gradient descent ·ªü ch·ªó:</h3>
            <div class="options">
                <label><input type="radio" name="q24" value="a"> A) Kh√¥ng c·∫ßn h√†m kh·∫£ vi</label>
                <label><input type="radio" name="q24" value="b"> B) H·ªôi t·ª• ch·∫≠m h∆°n</label>
                <label><input type="radio" name="q24" value="c"> C) C√≥ th·ªÉ dao ƒë·ªông</label>
                <label><input type="radio" name="q24" value="d"> D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) T·∫•t c·∫£ c√°c ƒëi·ªÅu tr√™n</strong><br>
                Subgradient method √°p d·ª•ng cho h√†m kh√¥ng tr∆°n, h·ªôi t·ª• ch·∫≠m h∆°n v√† c√≥ th·ªÉ dao ƒë·ªông quanh nghi·ªám.
            </div>
        </div>

        <!-- C√¢u h·ªèi 25: Th·ª±c h√†nh -->
        <div class="question" id="q25" style="display: none;">
            <h3>C√¢u 25: ƒêi·ªÅu ki·ªán d·ª´ng trong subgradient method th∆∞·ªùng d·ª±a v√†o:</h3>
            <div class="options">
                <label><input type="radio" name="q25" value="a"> A) $$\|g_k\| < \epsilon$$</label>
                <label><input type="radio" name="q25" value="b"> B) S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa</label>
                <label><input type="radio" name="q25" value="c"> C) Gi√° tr·ªã h√†m m·ª•c ti√™u</label>
                <label><input type="radio" name="q25" value="d"> D) B v√† C</label>
            </div>
            <div class="explanation" style="display: none;">
                <strong>ƒê√°p √°n ƒë√∫ng: D) B v√† C</strong><br>
                Do subgradient c√≥ th·ªÉ kh√¥ng b·∫±ng 0 t·∫°i nghi·ªám, th∆∞·ªùng d√πng s·ªë l·∫ßn l·∫∑p ho·∫∑c gi√° tr·ªã h√†m ƒë·ªÉ d·ª´ng.
            </div>
        </div>

    </div>

    <div class="quiz-controls">
        <button onclick="previousQuestion()" id="prev-btn" disabled>‚Üê C√¢u tr∆∞·ªõc</button>
        <button onclick="nextQuestion()" id="next-btn">C√¢u ti·∫øp ‚Üí</button>
        <button onclick="submitQuiz()" id="submit-btn" style="display: none;">N·ªôp b√†i</button>
        <button onclick="resetQuiz()" id="reset-btn" style="display: none;">L√†m l·∫°i</button>
    </div>

    <div id="final-result" style="display: none;" class="final-score">
        <h2>üéâ Ho√†n th√†nh Quiz!</h2>
        <p>ƒêi·ªÉm s·ªë c·ªßa b·∫°n: <span id="final-score-text"></span></p>
        <p id="performance-message"></p>
    </div>
</div>

<script>
let currentQuestion = 1;
const totalQuestions = 25;
let userAnswers = {};
let quizSubmitted = false;

// ƒê√°p √°n ƒë√∫ng
const correctAnswers = {
    q1: 'b', q2: 'b', q3: 'b', q4: 'b', q5: 'b',
    q6: 'a', q7: 'c', q8: 'a', q9: 'b', q10: 'b',
    q11: 'b', q12: 'b', q13: 'b', q14: 'b', q15: 'b',
    q16: 'a', q17: 'b', q18: 'b', q19: 'b', q20: 'd'
,
    q21: 'b', q22: 'b', q23: 'a', q24: 'd', q25: 'd'};

function showQuestion(n) {
    // ·∫®n t·∫•t c·∫£ c√¢u h·ªèi
    for (let i = 1; i <= totalQuestions; i++) {
        document.getElementById(`q${i}`).style.display = 'none';
    }
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi hi·ªán t·∫°i
    document.getElementById(`q${n}`).style.display = 'block';
    
    // C·∫≠p nh·∫≠t progress bar
    const progress = (n / totalQuestions) * 100;
    document.getElementById('progress-fill').style.width = progress + '%';
    
    // C·∫≠p nh·∫≠t s·ªë c√¢u h·ªèi
    document.getElementById('current-q').textContent = n;
    
    // C·∫≠p nh·∫≠t n√∫t ƒëi·ªÅu h∆∞·ªõng
    document.getElementById('prev-btn').disabled = (n === 1);
    document.getElementById('next-btn').style.display = (n === totalQuestions) ? 'none' : 'inline-block';
    document.getElementById('submit-btn').style.display = (n === totalQuestions) ? 'inline-block' : 'none';
    
    currentQuestion = n;
}

function nextQuestion() {
    if (currentQuestion < totalQuestions) {
        showQuestion(currentQuestion + 1);
    }
}

function previousQuestion() {
    if (currentQuestion > 1) {
        showQuestion(currentQuestion - 1);
    }
}

function saveAnswer(questionId, answer) {
    userAnswers[questionId] = answer;
    
    // Highlight selected option
    const questionDiv = document.getElementById(questionId);
    const labels = questionDiv.querySelectorAll('.options label');
    labels.forEach(label => {
        label.classList.remove('selected');
        const input = label.querySelector('input[type="radio"]');
        if (input.checked) {
            label.classList.add('selected');
        }
    });
}

function calculateScore() {
    let correct = 0;
    for (let q in correctAnswers) {
        if (userAnswers[q] === correctAnswers[q]) {
            correct++;
        }
    }
    return correct;
}

function submitQuiz() {
    quizSubmitted = true;
    const score = calculateScore();
    const percentage = Math.round((score / totalQuestions) * 100);
    
    // Hi·ªÉn th·ªã t·∫•t c·∫£ gi·∫£i th√≠ch
    for (let i = 1; i <= totalQuestions; i++) {
        const explanation = document.querySelector(`#q${i} .explanation`);
        explanation.style.display = 'block';
        
        // Highlight ƒë√°p √°n ƒë√∫ng v√† sai
        const questionDiv = document.getElementById(`q${i}`);
        const labels = questionDiv.querySelectorAll('.options label');
        labels.forEach(label => {
            const input = label.querySelector('input[type="radio"]');
            const value = input.value;
            
            if (value === correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#d4edda';
                label.style.border = '2px solid #28a745';
            } else if (value === userAnswers[`q${i}`] && value !== correctAnswers[`q${i}`]) {
                label.style.backgroundColor = '#f8d7da';
                label.style.border = '2px solid #dc3545';
            }
        });
    }
    
    // Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi
    document.getElementById('final-result').style.display = 'block';
    document.getElementById('final-score-text').textContent = `${score}/${totalQuestions} (${percentage}%)`;
    
    let message = '';
    if (percentage >= 90) {
        message = 'üåü Xu·∫•t s·∫Øc! B·∫°n ƒë√£ n·∫Øm v·ªØng subgradient method!';
    } else if (percentage >= 80) {
        message = 'üëç R·∫•t t·ªët! B·∫°n hi·ªÉu kh√° t·ªët v·ªÅ thu·∫≠t to√°n v√† ph√¢n t√≠ch h·ªôi t·ª•.';
    } else if (percentage >= 70) {
        message = 'üìö Kh√° ·ªïn! H√£y xem l·∫°i step size choices v√† stochastic methods.';
    } else if (percentage >= 60) {
        message = 'üí™ C·∫ßn c·∫£i thi·ªán! H√£y √¥n l·∫°i c√°c kh√°i ni·ªám c∆° b·∫£n v·ªÅ subgradient.';
    } else {
        message = 'üìñ H√£y h·ªçc l·∫°i t·ª´ ƒë·∫ßu v·ªÅ subgradient method nh√©!';
    }
    
    document.getElementById('performance-message').textContent = message;
    
    // ·∫®n n√∫t submit, hi·ªán n√∫t reset
    document.getElementById('submit-btn').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'inline-block';
    
    // Cu·ªôn ƒë·∫øn k·∫øt qu·∫£
    document.getElementById('final-result').scrollIntoView({ behavior: 'smooth' });
}

function resetQuiz() {
    // Reset tr·∫°ng th√°i
    currentQuestion = 1;
    userAnswers = {};
    quizSubmitted = false;
    
    // ·∫®n t·∫•t c·∫£ gi·∫£i th√≠ch
    document.querySelectorAll('.explanation').forEach(exp => {
        exp.style.display = 'none';
    });
    
    // Reset style c·ªßa labels
    document.querySelectorAll('.options label').forEach(label => {
        label.style.backgroundColor = '';
        label.style.border = '';
        label.classList.remove('selected');
    });
    
    // Uncheck t·∫•t c·∫£ radio buttons
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.checked = false;
    });
    
    document.querySelectorAll('.options label').forEach(label => {
        label.classList.remove('selected');
    });
    
    // Hi·ªÉn th·ªã c√¢u h·ªèi ƒë·∫ßu ti√™n
    showQuestion(1);
    
    // Cu·ªôn l√™n ƒë·∫ßu
    document.getElementById('quiz-header').scrollIntoView({ behavior: 'smooth' });
    
    // ·∫®n k·∫øt qu·∫£ cu·ªëi v√† reset button
    document.getElementById('final-result').style.display = 'none';
    document.getElementById('reset-btn').style.display = 'none';
}

// Kh·ªüi t·∫°o b√†i t·∫≠p khi trang ƒë∆∞·ª£c t·∫£i
document.addEventListener('DOMContentLoaded', function() {
    showQuestion(1);
    
    // Th√™m event listener cho t·∫•t c·∫£ radio button
    document.querySelectorAll('input[type="radio"]').forEach(input => {
        input.addEventListener('change', function() {
            const questionId = this.name;
            const answer = this.value;
            saveAnswer(questionId, answer);
        });
    });
    
    // Render MathJax sau khi DOM ƒë∆∞·ª£c t·∫£i
    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});

// X·ª≠ l√Ω ph√≠m t·∫Øt
document.addEventListener('keydown', function(event) {
    if (quizSubmitted) return;
    
    switch(event.key) {
        case 'ArrowLeft':
            if (currentQuestion > 1) previousQuestion();
            break;
        case 'ArrowRight':
            if (currentQuestion < totalQuestions) nextQuestion();
            break;
        case 'Enter':
            if (currentQuestion === totalQuestions) submitQuiz();
            break;
    }
});
</script>
