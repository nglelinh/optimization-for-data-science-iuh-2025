---
layout: post
title: 08-10 BÃ i Táº­p Thá»±c HÃ nh - PhÆ°Æ¡ng phÃ¡p Subgradient
chapter: '08'
order: 11
owner: GitHub Copilot
lang: vi
categories:
- chapter08
lesson_type: required
---

# BÃ i Táº­p Thá»±c HÃ nh - PhÆ°Æ¡ng phÃ¡p Subgradient

CÃ¡c bÃ i táº­p Ä‘Æ°á»£c tham kháº£o tá»« Boyd et al. (2003) vÃ  Nesterov (2009).

---

## ğŸ“ **BÃ i táº­p 1: Dual Subgradient cho LP**

Cho bÃ i toÃ¡n LP: $$\min c^Tx$$ s.t. $$Ax = b$$, $$x \geq 0$$ vá»›i $$c = (1,2)^T$$, $$A = [1,1]$$, $$b = 3$$.

**a)** Viáº¿t dual problem  
**b)** TÃ­nh subgradient  
**c)** Thá»±c hiá»‡n dual subgradient iterations

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**Dual:** $$\max -3\nu$$ s.t. $$\nu \leq 1$$

**Solution:** $$\nu^* = 1$$

Iterations vá»›i $$\alpha = 0.1$$ tá»« $$\nu^{(0)} = 0$$ há»™i tá»¥ Ä‘áº¿n $$\nu^* = 1$$ sau 4 bÆ°á»›c.

</details>

---

## ğŸ“ **BÃ i táº­p 2: Stochastic Subgradient**

Cho $$\min \frac{1}{n}\sum_i \max(0, 1 - y_i w^T x_i)$$ vá»›i 3 Ä‘iá»ƒm dá»¯ liá»‡u.

**a)** Viáº¿t stochastic update  
**b)** So sÃ¡nh vá»›i batch method

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

Stochastic chá»n ngáº«u nhiÃªn 1 sample má»—i iteration:

$$w^{(k+1)} = w^{(k)} - \alpha_k g_i$$

vá»›i $$g_i = -y_i x_i$$ náº¿u margin violated.

**Advantage:** Cheaper per-iteration, makes progress khi batch stalls.

</details>

---

## ğŸ“ **BÃ i táº­p 3: Polyak-Ruppert Averaging**

Cho $$f(x) = \max\{2x, -x+3\}$$.

**a)** TÃ¬m $$x^*$$  
**b)** So sÃ¡nh $$x^{(k)}$$ vs averaged $$\bar{x}^{(k)}$$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**$$x^* = 1$$** (intersection point)

**Averaging:** $$\bar{x}^{(k)} = \frac{1}{k}\sum_{i=1}^k x^{(i)}$$

Smooths oscillations vÃ  converges faster Ä‘áº¿n $$x^*$$.

</details>

---

## ğŸ“ **BÃ i táº­p 4: Mirror Descent trÃªn Simplex**

Cho $$f(x) = c^Tx$$ trÃªn simplex $$\Delta_3$$.

**a)** Sá»­ dá»¥ng entropic mirror map $$\psi(x) = \sum x_i \log x_i$$  
**b)** Derive update rule

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**Update:**

$$x_i^{(k+1)} \propto x_i^{(k)} e^{-\alpha c_i}$$

Normalize Ä‘á»ƒ $$\sum x_i = 1$$.

**Advantage:** Naturally handles simplex constraints, geometry-aware.

</details>

---

## ğŸ“ **BÃ i táº­p 5: Cutting Plane Method**

Cho $$f(x) = \max\{x^2, (x-2)^2\}$$.

**a)** Build cutting planes $$\ell_k(x) = f(x^{(k)}) + g^{(k)}(x - x^{(k)})$$  
**b)** Minimize lower bound $$\hat{f}(x) = \max_k \ell_k(x)$$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

Tá»« $$x^{(0)} = 0$$:

$$\ell_0(x) = 4 - 4x$$

Tá»« $$x^{(1)} = 1$$:

$$\ell_1(x) = 1 + 2(x-1) = 2x - 1$$

Minimize $$\max\{4-4x, 2x-1\}$$ â†’ $$x^{(2)} = \frac{5}{6}$$

Iterate until gap closes.

</details>

---

## ğŸ’¡ **Tá»•ng káº¿t**

| Method | Convergence | Best for |
|--------|-------------|----------|
| Dual Subgradient | $$O(1/\sqrt{k})$$ | LP, convex programs |
| Stochastic | $$O(1/\sqrt{k})$$ | Large-scale ML |
| Averaged | $$O(1/\sqrt{k})$$ | Reduce oscillations |
| Mirror Descent | $$O(1/\sqrt{k})$$ | Constrained domains |
| Bundle/Cutting | $$O(1/k)$$ | Non-smooth, few vars |

---

## ğŸ“š **TÃ i liá»‡u Tham kháº£o**

1. Boyd, S., et al. (2003). "Subgradient Methods". Stanford.
2. Nesterov, Y. (2009). "Primal-dual subgradient methods".
3. Beck, A., & Teboulle, M. (2003). "Mirror descent".
