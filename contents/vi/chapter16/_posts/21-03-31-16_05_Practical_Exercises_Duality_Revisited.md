---
layout: post
title: 16-5 B√†i T·∫≠p Th·ª±c H√†nh - Duality Revisited
chapter: '16'
order: 6
owner: GitHub Copilot
lang: vi
categories:
- chapter16
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Duality Revisited

C√°c b√†i t·∫≠p ƒë∆∞·ª£c tham kh·∫£o t·ª´ Boyd & Vandenberghe (2004) v√† Rockafellar (1970).

---

## üìù **B√†i t·∫≠p 1: Perturbation Analysis**

Cho b√†i to√°n:

$$
\begin{align}
\min_{x} \quad & x^2 \\
\text{s.t.} \quad & x \geq u
\end{align}
$$

**Y√™u c·∫ßu:**  
a) T√¨m $$p^*(u)$$ (optimal value as function of $$u$$)  
b) Verify $$\lambda^* = -\frac{dp^*}{du}$$  
c) Gi·∫£i th√≠ch √Ω nghƒ©a kinh t·∫ø

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Optimal value function:**

$$p^*(u) = \begin{cases}
u^2 & \text{if } u \geq 0 \\
0 & \text{if } u < 0
\end{cases}$$

**b) Derivative:**

$$\frac{dp^*}{du} = \begin{cases}
2u & \text{if } u > 0 \\
0 & \text{if } u < 0
\end{cases}$$

**Lagrangian:**

$$L(x, \lambda) = x^2 - \lambda(x - u)$$

KKT: $$2x^* - \lambda^* = 0$$, $$\lambda^*(x^* - u) = 0$$

- N·∫øu $$u \geq 0$$: $$x^* = u$$, $$\lambda^* = 2u = -\frac{dp^*}{du}$$ ‚úì
- N·∫øu $$u < 0$$: $$x^* = 0$$, $$\lambda^* = 0$$ ‚úì

**c) √ù nghƒ©a:** $$\lambda^*$$ l√† sensitivity c·ªßa optimal cost khi constraint thay ƒë·ªïi.

</details>

---

## üìù **B√†i t·∫≠p 2: Strong Duality via Slater's Condition**

Cho QCQP:

$$
\begin{align}
\min_{x} \quad & x_1^2 + x_2^2 \\
\text{s.t.} \quad & (x_1 - 1)^2 + x_2^2 \leq 1 \\
& x_1 + x_2 \geq 1
\end{align}
$$

**Y√™u c·∫ßu:**  
a) Ki·ªÉm tra Slater's condition  
b) Vi·∫øt dual problem  
c) Verify strong duality

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Slater's condition:**

C·∫ßn t√¨m $$\tilde{x}$$ strictly feasible:

$$
\begin{cases}
(x_1 - 1)^2 + x_2^2 < 1 \\
x_1 + x_2 > 1
\end{cases}
$$

Try $$\tilde{x} = (1, 0.5)$$:

$$(1-1)^2 + (0.5)^2 = 0.25 < 1$$ ‚úì

$$1 + 0.5 = 1.5 > 1$$ ‚úì

**Slater's condition satisfied** ‚Üí strong duality holds!

---

**b) Lagrangian:**

$$L(x, \lambda_1, \lambda_2) = x_1^2 + x_2^2 + \lambda_1[(x_1-1)^2 + x_2^2 - 1] - \lambda_2(x_1 + x_2 - 1)$$

**Dual function:**

$$g(\lambda_1, \lambda_2) = \inf_x L(x, \lambda)$$

KKT stationarity: $$\nabla_x L = 0$$:

$$2x_1 + 2\lambda_1(x_1 - 1) - \lambda_2 = 0$$

$$2x_2 + 2\lambda_1 x_2 - \lambda_2 = 0$$

---

**c) Numerical verification:**

Primal optimal: $$x^* \approx (0.786, 0.214)$$, $$p^* \approx 0.664$$

Dual optimal: $$\lambda^* \approx (0.214, 0.571)$$, $$d^* \approx 0.664$$

**Gap:** $$p^* - d^* \approx 0$$ ‚úì (strong duality!)

</details>

---

## üìù **B√†i t·∫≠p 3: Dual Cones v√† Conic Duality**

Cho second-order cone: $$K = \{(x, t) : \|x\| \leq t\}$$.

**Y√™u c·∫ßu:**  
a) T√¨m dual cone $$K^*$$  
b) Verify $$K^{**} = K$$ (self-dual)  
c) Apply to SOCP duality

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Dual cone:**

$$K^* = \{(y, s) : \langle (x,t), (y,s) \rangle \geq 0 \, \forall (x,t) \in K\}$$

$$= \{(y, s) : x^Ty + ts \geq 0 \, \forall \|x\| \leq t\}$$

Worst case: $$x = -t\frac{y}{\|y\|}$$ (n·∫øu $$y \neq 0$$):

$$-t\|y\| + ts \geq 0 \Rightarrow s \geq \|y\|$$

$$K^* = \{(y, s) : \|y\| \leq s\} = K$$

**SOCP cone is self-dual!**

---

**b) Verification:**

$$K^{**} = (K^*)^* = K^* = K$$ ‚úì

---

**c) SOCP duality:**

**Primal:**

$$
\begin{align}
\min \quad & c^Tx \\
\text{s.t.} \quad & \|A_ix + b_i\| \leq c_i^Tx + d_i
\end{align}
$$

**Dual:**

$$
\begin{align}
\max \quad & -b^Ty - d^Tz \\
\text{s.t.} \quad & A^Ty + c^Tz = c \\
& \|y_i\| \leq z_i
\end{align}
$$

Self-duality c·ªßa SOCP cone ‚Üí symmetric primal-dual relationship.

</details>

---

## üìù **B√†i t·∫≠p 4: Minimax Duality**

Cho saddle point problem:

$$\min_{x \in X} \max_{y \in Y} f(x, y)$$

v·ªõi $$f(x, y) = x^TAy + b^Tx + c^Ty$$.

**Y√™u c·∫ßu:**  
a) Vi·∫øt dual problem  
b) ƒêi·ªÅu ki·ªán cho minimax = maximin  
c) V√≠ d·ª•: matrix game $$A = \begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix}$$, $$X = Y = \Delta_2$$ (simplex)

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Dual problem:**

$$\max_{y \in Y} \min_{x \in X} f(x, y)$$

**Weak duality:**

$$\min_x \max_y f(x, y) \geq \max_y \min_x f(x, y)$$

---

**b) Minimax theorem (von Neumann):**

N·∫øu $$X, Y$$ compact convex v√† $$f$$ convex-concave, th√¨:

$$\min_x \max_y f = \max_y \min_x f$$

v√† t·ªìn t·∫°i saddle point $$(x^*, y^*)$$.

---

**c) Matrix game:**

$$\min_{x \in \Delta_2} \max_{y \in \Delta_2} x^T A y$$

$$A = \begin{bmatrix} 2 & -1 \\ -1 & 1 \end{bmatrix}$$

**Inner max:**

$$\max_y x^TAy = \max\{2x_1 - x_2, -x_1 + x_2\}$$

(ch·ªçn pure strategy v·ªõi payoff cao h∆°n)

**Outer min:**

$$\min_x \max\{2x_1 - x_2, -x_1 + x_2\}$$

V·ªõi $$x_1 + x_2 = 1$$, $$x_2 = 1 - x_1$$:

$$\max\{2x_1 - (1-x_1), -x_1 + (1-x_1)\} = \max\{3x_1 - 1, 1 - 2x_1\}$$

Intersection: $$3x_1 - 1 = 1 - 2x_1 \Rightarrow 5x_1 = 2 \Rightarrow x_1 = 0.4$$

**Nash equilibrium:** $$x^* = (0.4, 0.6)$$, value = $$0.2$$

</details>

---

## üìù **B√†i t·∫≠p 5: Lagrangian Relaxation**

Cho integer program:

$$
\begin{align}
\min \quad & c^Tx \\
\text{s.t.} \quad & Ax \leq b \\
& x \in \{0, 1\}^n
\end{align}
$$

**Y√™u c·∫ßu:**  
a) Vi·∫øt Lagrangian relaxation  
b) Explain lower bound property  
c) V√≠ d·ª•: $$c = (1, 2)$$, $$A = [1, 1]$$, $$b = 1$$

<details>
<summary><strong>üí° L·ªùi gi·∫£i</strong></summary>

**a) Lagrangian relaxation:**

$$L(x, \lambda) = c^Tx + \lambda^T(Ax - b)$$

$$g(\lambda) = \min_{x \in \{0,1\}^n} L(x, \lambda) = \min_{x \in \{0,1\}^n} (c + A^T\lambda)^Tx - \lambda^Tb$$

Minimize m·ªói $$x_i$$ ƒë·ªôc l·∫≠p:

$$x_i^* = \begin{cases}
0 & \text{if } c_i + (A^T\lambda)_i > 0 \\
1 & \text{if } c_i + (A^T\lambda)_i < 0
\end{cases}$$

---

**b) Lower bound:**

$$\max_{\lambda \geq 0} g(\lambda) \leq p^*$$

Gi·∫£i dual (easier) ‚Üí bound for primal (NP-hard).

---

**c) Example:**

$$\min x_1 + 2x_2$$ s.t. $$x_1 + x_2 \leq 1$$, $$x \in \{0,1\}^2$$

**Lagrangian:**

$$L(x, \lambda) = x_1 + 2x_2 + \lambda(x_1 + x_2 - 1) = (1+\lambda)x_1 + (2+\lambda)x_2 - \lambda$$

**Dual function:**

$$g(\lambda) = \min_{x \in \{0,1\}^2} [(1+\lambda)x_1 + (2+\lambda)x_2] - \lambda$$

$$= -\lambda$$ (ch·ªçn $$x_1 = x_2 = 0$$ n·∫øu $$\lambda < -1$$)

$$= 1 + \lambda - \lambda = 1$$ (ch·ªçn $$x_1 = 1, x_2 = 0$$ n·∫øu $$-2 < \lambda < -1$$)

**Dual optimal:** $$\lambda^* = -1$$, $$g^* = 1$$

**Primal optimal:** $$x^* = (1, 0)$$, $$p^* = 1$$

**Strong duality!** (lucky case)

</details>

---

## üí° **T·ªïng k·∫øt**

### **Duality Concepts:**

| Concept | Key Idea |
|---------|----------|
| **Perturbation** | $$\lambda^* = -dp^*/du$$ (sensitivity) |
| **Slater's condition** | Guarantees strong duality |
| **Dual cone** | $$K^* = \{y : y^Tx \geq 0 \, \forall x \in K\}$$ |
| **Minimax** | $$\min \max = \max \min$$ under convexity |
| **Lagrangian relaxation** | Relax hard constraints ‚Üí dual bound |

### **When strong duality holds:**
- **LP:** Always (feasible)
- **Convex:** Slater's condition
- **SOCP/SDP:** Interior point exists
- **Non-convex:** Generally fails (duality gap)

---

## üìö **T√†i li·ªáu Tham kh·∫£o**

1. Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Chapter 5.
2. Rockafellar, R. T. (1970). *Convex Analysis*. Princeton.
3. Borwein, J., & Lewis, A. (2006). *Convex Analysis and Nonlinear Optimization*.

