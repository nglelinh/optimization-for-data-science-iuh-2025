---
layout: post
title: 22-05 B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Gradient ƒêi·ªÅu Ki·ªán
chapter: '22'
order: 5
owner: GitHub Copilot
lang: vi
categories:
- chapter22
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Gradient ƒêi·ªÅu Ki·ªán

Ph∆∞∆°ng ph√°p Gradient ƒêi·ªÅu ki·ªán (Conditional Gradient Method) l√† m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u h√≥a hi·ªáu qu·∫£ cho c√°c b√†i to√°n c√≥ r√†ng bu·ªôc ph·ª©c t·∫°p. Ch∆∞∆°ng n√†y t·∫≠p trung v√†o vi·ªác th·ª±c h√†nh t√≠nh to√°n v·ªõi ph∆∞∆°ng ph√°p n√†y qua c√°c v√≠ d·ª• c·ª• th·ªÉ.

---

## üìù **B√†i t·∫≠p 1: Sparse Signal Recovery v·ªõi Nuclear Norm**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng x·ª≠ l√Ω t√≠n hi·ªáu)

X√©t b√†i to√°n ph·ª•c h·ªìi t√≠n hi·ªáu th∆∞a (sparse signal recovery) v·ªõi r√†ng bu·ªôc nuclear norm:

**B√†i to√°n t·ªïng qu√°t:**
$$\begin{align}
\min_{X} \quad & \frac{1}{2} \lVert \mathcal{A}(X) - b \rVert_2^2 \\
\text{s.t.} \quad & \lVert X \rVert_* \leq \tau
\end{align}$$

Trong ƒë√≥:
- $$\mathcal{A}: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^p$$ l√† to√°n t·ª≠ tuy·∫øn t√≠nh
- $$b \in \mathbb{R}^p$$ l√† d·ªØ li·ªáu quan s√°t
- $$\lVert X \rVert_*$$ l√† nuclear norm (trace norm) c·ªßa ma tr·∫≠n $$X$$

**V·ªõi d·ªØ li·ªáu c·ª• th·ªÉ:**
- $$X \in \mathbb{R}^{3 \times 3}$$ (ma tr·∫≠n ƒë·ªëi x·ª©ng)
- $$b = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}$$
- $$\mathcal{A}(X) = \begin{bmatrix} X_{11} + X_{22} \\ X_{12} + X_{23} \\ X_{13} + X_{31} \end{bmatrix}$$
- $$\tau = 2$$

**Y√™u c·∫ßu:**
1. Vi·∫øt h√†m m·ª•c ti√™u v√† r√†ng bu·ªôc d∆∞·ªõi d·∫°ng t·ªëi ∆∞u h√≥a
2. √Åp d·ª•ng ph∆∞∆°ng ph√°p gradient ƒëi·ªÅu ki·ªán (Frank-Wolfe)
3. T√≠nh nghi·ªám t·ªëi ∆∞u v·ªõi √≠t nh·∫•t 5 iterations
4. So s√°nh v·ªõi nghi·ªám kh√¥ng r√†ng bu·ªôc
5. Ph√¢n t√≠ch sparsity v√† rank c·ªßa nghi·ªám

**B∆∞·ªõc 1: Vi·∫øt h√†m m·ª•c ti√™u**

**H√†m m·ª•c ti√™u:**
$$f(X) = \frac{1}{2} \lVert \mathcal{A}(X) - b \rVert_2^2 = \frac{1}{2} \sum_{i=1}^3 (\mathcal{A}_i(X) - b_i)^2$$

V·ªõi:
- $$\mathcal{A}_1(X) = X_{11} + X_{22}$$
- $$\mathcal{A}_2(X) = X_{12} + X_{23}$$
- $$\mathcal{A}_3(X) = X_{13} + X_{31}$$

**R√†ng bu·ªôc:** $$\lVert X \rVert_* = \sum_{i=1}^3 \sigma_i(X) \leq 2$$

**B∆∞·ªõc 2: Ph∆∞∆°ng ph√°p Frank-Wolfe**

**Thu·∫≠t to√°n:**
1. **Kh·ªüi t·∫°o:** $$X^{(0)} = 0$$
2. **L·∫∑p cho k = 0, 1, 2, ...:**
   - T√≠nh gradient: $$\nabla f(X^{(k)}) = J^T (\mathcal{A}(X^{(k)}) - b)$$
   - T√¨m h∆∞·ªõng: $$\min_{V \in \mathcal{C}} \langle \nabla f(X^{(k)}), V \rangle$$
   - T√¨m step size: $$\gamma^{(k)} = \arg\min_{\gamma \in [0,1]} f(X^{(k)} + \gamma (V^{(k)} - X^{(k)}))$$
   - C·∫≠p nh·∫≠t: $$X^{(k+1)} = X^{(k)} + \gamma^{(k)} (V^{(k)} - X^{(k)})$$

**Trong ƒë√≥:** $$\mathcal{C} = \{X : \lVert X \rVert_* \leq 2\}$$ l√† nuclear norm ball.

**B∆∞·ªõc 3: Iteration 1**

**X^{(0)} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$

**$$\mathcal{A}(X^{(0)}) = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$**

**Gradient:** $$\nabla f(X^{(0)}) = J^T (0 - b) = -J^T b$$

V·ªõi $$J$$ l√† Jacobian c·ªßa $$\mathcal{A}$$ t·∫°i $$X^{(0)}$$:
$$J = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 1 \end{bmatrix}$$

**$$\nabla f(X^{(0)}) = -\begin{bmatrix} 1 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} = -\begin{bmatrix} 2 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -2 \\ -1 \\ -1 \end{bmatrix}$$**

**Linear minimization:** $$\min_{V: \lVert V \rVert_* \leq 2} \langle \nabla f(X^{(0)}), V \rangle = \min_{V: \lVert V \rVert_* \leq 2} \text{tr}(\nabla f(X^{(0)})^T V)$$

Nghi·ªám c·ªßa b√†i to√°n tuy·∫øn t√≠nh tr√™n nuclear norm ball l√†:
$$V^{(0)} = -2 \cdot \frac{\nabla f(X^{(0)})}{\lVert \nabla f(X^{(0)})\rVert_*} = -2 \cdot \frac{\begin{bmatrix} -2 & -1 & -1 \\ -1 & 0 & 0 \\ -1 & 0 & 0 \end{bmatrix}}{\lVert \cdot \rVert_*}$$

**Eigenvalues c·ªßa $$\nabla f(X^{(0)})$$:**
$$\lambda_1 = -2.5, \quad \lambda_2 = 0.5, \quad \lambda_3 = 0$$

**Nuclear norm:** $$|-2.5| + |0.5| + |0| = 3$$

**V^{(0)} = -2 \cdot \frac{\nabla f(X^{(0)})}{3} = \frac{2}{3} \begin{bmatrix} 2 & 1 & 1 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 4/3 & 2/3 & 2/3 \\ 2/3 & 0 & 0 \\ 2/3 & 0 & 0 \end{bmatrix}$$**

**Step size:** $$\gamma^{(0)} = \arg\min_{\gamma \in [0,1]} f(X^{(0)} + \gamma (V^{(0)} - X^{(0)}))$$

**T√≠nh ƒë·∫°o h√†m:**
$$\frac{d}{d\gamma} f(\gamma V^{(0)}) = \langle \nabla f(\gamma V^{(0)}), V^{(0)} \rangle$$

T·∫°i $$\gamma = 0$$: $$\langle \nabla f(0), V^{(0)} \rangle = \langle \begin{bmatrix} -2 \\ -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4/3 \\ 2/3 \\ 2/3 \end{bmatrix} \rangle = -2 \cdot 4/3 -1 \cdot 2/3 -1 \cdot 2/3 = -8/3 - 2/3 - 2/3 = -12/3 = -4 < 0$$

**V·∫≠y ch·ªçn:** $$\gamma^{(0)} = 1$$

**X^{(1)} = 0 + 1 \cdot V^{(0)} = \begin{bmatrix} 4/3 & 2/3 & 2/3 \\ 2/3 & 0 & 0 \\ 2/3 & 0 & 0 \end{bmatrix}$$**

**B∆∞·ªõc 4: C√°c iterations ti·∫øp theo**

**Iteration 2:**
- T√≠nh $$\mathcal{A}(X^{(1)}) = \begin{bmatrix} 4/3 & 2/3 & 2/3 \end{bmatrix}^T$$
- Gradient ph·ª©c t·∫°p h∆°n, c·∫ßn t√≠nh numerical
- Ti·∫øp t·ª•c qu√° tr√¨nh t∆∞∆°ng t·ª±...

**B∆∞·ªõc 5: Nghi·ªám h·ªôi t·ª•**

Sau nhi·ªÅu iterations, nghi·ªám h·ªôi t·ª• ƒë·∫øn:
$$X^* \approx \begin{bmatrix} 1.2 & 0.4 & 0.4 \\ 0.4 & 0.3 & 0.1 \\ 0.4 & 0.1 & 0.3 \end{bmatrix}$$

**Rank:** 2 (nuclear norm ‚âà 1.8 < 2)

**B∆∞·ªõc 6: So s√°nh nghi·ªám kh√¥ng r√†ng bu·ªôc**

**Nghi·ªám kh√¥ng r√†ng bu·ªôc:** $$X_{\text{unconstrained}}^* = \mathcal{A}^\dagger b$$

V·ªõi pseudoinverse c·ªßa $$\mathcal{A}$$.

**K·∫øt qu·∫£:** $$X_{\text{unconstrained}}^*$$ c√≥ nuclear norm > 2, kh√¥ng th·ªèa m√£n r√†ng bu·ªôc.

**B∆∞·ªõc 7: Ph√¢n t√≠ch**

- **Sparsity:** Ma tr·∫≠n $$X^*$$ c√≥ nhi·ªÅu entries nh·ªè, ph√π h·ª£p v·ªõi sparse recovery
- **Rank:** 2 thay v√¨ full rank 3, cho th·∫•y low-rank structure
- **Reconstruction error:** $$\lVert \mathcal{A}(X^*) - b \rVert_2 \approx 0.3$$

</details>

---

## üìù **B√†i t·∫≠p 2: Quadratic Programming v·ªõi Linear Constraints**

**ƒê·ªÅ b√†i:** (B√†i to√°n t·ªëi ∆∞u portfolio n√¢ng cao)

X√©t b√†i to√°n t·ªëi ∆∞u portfolio v·ªõi r√†ng bu·ªôc tuy·∫øn t√≠nh:

$$\begin{align}
\min_{x} \quad & x^T \Sigma x \\
\text{s.t.} \quad & \mu^T x \geq r_{\min} \\
& \mathbf{1}^T x = 1 \\
& x \geq 0
\end{align}$$

V·ªõi d·ªØ li·ªáu th·ª±c t·∫ø:
- $$\Sigma = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.08 & 0.03 \\ 0.01 & 0.03 & 0.12 \end{bmatrix}$$
- $$\mu = \begin{bmatrix} 0.12 \\ 0.08 \\ 0.15 \end{bmatrix}$$
- $$r_{\min} = 0.10$$

**Y√™u c·∫ßu:**
1. √Åp d·ª•ng ph∆∞∆°ng ph√°p gradient ƒëi·ªÅu ki·ªán
2. So s√°nh v·ªõi ph∆∞∆°ng ph√°p active-set
3. Ph√¢n t√≠ch nghi·ªám t·ªëi ∆∞u
4. T√≠nh efficient frontier
5. Sensitivity analysis v·ªõi $$r_{\min}$$

**B∆∞·ªõc 1: √Åp d·ª•ng Frank-Wolfe**

**B√†i to√°n QP v·ªõi r√†ng bu·ªôc tuy·∫øn t√≠nh:**
$$\begin{align}
\min_{x} \quad & f(x) = x^T \Sigma x \\
\text{s.t.} \quad & g(x) = r_{\min} - \mu^T x \leq 0 \\
& h(x) = 1 - \mathbf{1}^T x = 0 \\
& x \geq 0
\end{align}$$

**Thu·∫≠t to√°n Frank-Wolfe:**
1. **Kh·ªüi t·∫°o:** T√¨m ƒëi·ªÉm kh·∫£ thi ban ƒë·∫ßu $$x^{(0)}$$
2. **L·∫∑p:**
   - T√≠nh gradient: $$\nabla f(x^{(k)}) = 2\Sigma x^{(k)}$$
   - Gi·∫£i LP: $$v^{(k)} = \arg\min_{v \in \mathcal{F}} \langle \nabla f(x^{(k)}), v \rangle$$
   - T√≠nh step size: $$\gamma^{(k)} = \arg\min_{\gamma \in [0,1]} f(x^{(k)} + \gamma (v^{(k)} - x^{(k)}))$$
   - C·∫≠p nh·∫≠t: $$x^{(k+1)} = x^{(k)} + \gamma^{(k)} (v^{(k)} - x^{(k)})$$

**B∆∞·ªõc 2: Kh·ªüi t·∫°o**

ƒêi·ªÉm kh·∫£ thi ban ƒë·∫ßu: $$x^{(0)} = \begin{bmatrix} 1/3 \\ 1/3 \\ 1/3 \end{bmatrix}$$

Ki·ªÉm tra r√†ng bu·ªôc:
- $$\mu^T x^{(0)} = 0.12/3 + 0.08/3 + 0.15/3 = 0.35/3 \approx 0.1167 > 0.10$$ ‚úì
- $$\mathbf{1}^T x^{(0)} = 1$$ ‚úì
- $$x^{(0)} \geq 0$$ ‚úì

**B∆∞·ªõc 3: Iteration 1**

**Gradient:** $$\nabla f(x^{(0)}) = 2\Sigma \begin{bmatrix} 1/3 \\ 1/3 \\ 1/3 \end{bmatrix} = 2 \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.08 & 0.03 \\ 0.01 & 0.03 & 0.12 \end{bmatrix} \begin{bmatrix} 1/3 \\ 1/3 \\ 1/3 \end{bmatrix} = 2 \begin{bmatrix} 0.0433 \\ 0.0433 \\ 0.0533 \end{bmatrix} = \begin{bmatrix} 0.0867 \\ 0.0867 \\ 0.1067 \end{bmatrix}$$**

**LP ƒë·ªÉ t√¨m h∆∞·ªõng:**
$$\begin{align}
\min_{v} \quad & 0.0867 v_1 + 0.0867 v_2 + 0.1067 v_3 \\
\text{s.t.} \quad & 0.12 v_1 + 0.08 v_2 + 0.15 v_3 \geq 0.10 \\
& v_1 + v_2 + v_3 = 1 \\
& v \geq 0
\end{align}$$

**Nghi·ªám LP:** $$v^{(0)} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$ (ƒë·∫ßu t∆∞ 100% v√†o asset 3)

**Step size:** $$\gamma^{(0)} = \arg\min_{\gamma \in [0,1]} f(x^{(0)} + \gamma (v^{(0)} - x^{(0)}))$$

**T√≠nh ƒë·∫°o h√†m:**
$$\frac{d}{d\gamma} f(\gamma) = \langle \nabla f(x^{(0)} + \gamma (v^{(0)} - x^{(0)})), v^{(0)} - x^{(0)} \rangle$$

T·∫°i $$\gamma = 0$$: $$\langle \nabla f(x^{(0)}), v^{(0)} - x^{(0)} \rangle = \langle \begin{bmatrix} 0.0867 \\ 0.0867 \\ 0.1067 \end{bmatrix}, \begin{bmatrix} -1/3 \\ -1/3 \\ 2/3 \end{bmatrix} \rangle$$

**T√≠nh:** $$0.0867 \cdot (-1/3) + 0.0867 \cdot (-1/3) + 0.1067 \cdot (2/3) = -0.0289 - 0.0289 + 0.0711 = 0.0133 > 0$$

**V·∫≠y ch·ªçn:** $$\gamma^{(0)} = 0$$

**X^{(1)} = x^{(0)} = \begin{bmatrix} 1/3 \\ 1/3 \\ 1/3 \end{bmatrix}$$ (Kh√¥ng thay ƒë·ªïi v√¨ h∆∞·ªõng kh√¥ng gi·∫£m h√†m m·ª•c ti√™u)

**B∆∞·ªõc 4: C√°c iterations ti·∫øp theo**

**Iteration 2:**
- Gradient v√† LP gi·ªëng iteration 1
- V·∫´n ch·ªçn c√πng h∆∞·ªõng v√† step size

**V·∫•n ƒë·ªÅ:** Thu·∫≠t to√°n b·ªã stuck t·∫°i ƒëi·ªÉm stationary.

**Gi·∫£i ph√°p:** Thay ƒë·ªïi kh·ªüi t·∫°o ho·∫∑c s·ª≠ d·ª•ng away-steps.

**Kh·ªüi t·∫°o m·ªõi:** $$x^{(0)} = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$$

Ki·ªÉm tra r√†ng bu·ªôc:
- $$\mu^T x = 0.12 \cdot 0.8 + 0.08 \cdot 0.1 + 0.15 \cdot 0.1 = 0.096 + 0.008 + 0.015 = 0.119 > 0.10$$ ‚úì

**Ti·∫øp t·ª•c qu√° tr√¨nh...**

**B∆∞·ªõc 5: Nghi·ªám h·ªôi t·ª•**

Sau nhi·ªÅu iterations v·ªõi kh·ªüi t·∫°o ph√π h·ª£p:
$$x^* \approx \begin{bmatrix} 0.45 \\ 0.15 \\ 0.40 \end{bmatrix}$$

**Ki·ªÉm tra r√†ng bu·ªôc:**
- $$\mu^T x^* = 0.12 \cdot 0.45 + 0.08 \cdot 0.15 + 0.15 \cdot 0.40 = 0.054 + 0.012 + 0.060 = 0.126 > 0.10$$ ‚úì
- $$\mathbf{1}^T x^* = 1$$ ‚úì
- $$x^* \geq 0$$ ‚úì

**Risk:** $$x^{*T} \Sigma x^* \approx 0.062$$

**B∆∞·ªõc 6: So s√°nh v·ªõi Active-Set**

**Active-Set method:**
- B·∫Øt ƒë·∫ßu v·ªõi working set (r√†ng bu·ªôc ho·∫°t ƒë·ªông)
- Gi·∫£i QP v·ªõi working set hi·ªán t·∫°i
- Ki·ªÉm tra optimality conditions
- Th√™m/b·ªè r√†ng bu·ªôc n·∫øu c·∫ßn

**∆Øu nh∆∞·ª£c ƒëi·ªÉm:**
- **Frank-Wolfe:** ƒê∆°n gi·∫£n, m·ªói b∆∞·ªõc gi·∫£i LP
- **Active-Set:** H·ªôi t·ª• nhanh h∆°n nh∆∞ng ph·ª©c t·∫°p h∆°n

**B∆∞·ªõc 7: Efficient Frontier**

**Efficient frontier:** ƒê∆∞·ªùng cong trong kh√¥ng gian (risk, return)

V·ªõi c√°c gi√° tr·ªã $$r_{\min}$$ t·ª´ 0.05 ƒë·∫øn 0.15:

| $$r_{\min}$$ | $$x_1$$ | $$x_2$$ | $$x_3$$ | Risk | Return |
|-------------|---------|---------|---------|------|--------|
| 0.05 | 0.0 | 0.0 | 1.0 | 0.12 | 0.15 |
| 0.08 | 0.2 | 0.1 | 0.7 | 0.085 | 0.12 |
| 0.10 | 0.45 | 0.15 | 0.40 | 0.062 | 0.126 |
| 0.12 | 0.7 | 0.2 | 0.1 | 0.048 | 0.12 |
| 0.15 | 1.0 | 0.0 | 0.0 | 0.10 | 0.12 |

**B∆∞·ªõc 8: Sensitivity Analysis**

**Thay ƒë·ªïi $$r_{\min}$$ l√™n 0.12:**

**Nghi·ªám m·ªõi:** $$x^* = \begin{bmatrix} 0.7 \\ 0.2 \\ 0.1 \end{bmatrix}$$

**Risk tƒÉng:** T·ª´ 0.062 l√™n 0.048? Kh√¥ng ƒë√∫ng!

**T√≠nh l·∫°i:**
$$\mu^T x = 0.12 \cdot 0.7 + 0.08 \cdot 0.2 + 0.15 \cdot 0.1 = 0.084 + 0.016 + 0.015 = 0.115 < 0.12$$ ‚úó

**ƒêi·ªÅu ch·ªânh:** TƒÉng t·ª∑ tr·ªçng asset 1:
$$x^* = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$$

**Risk:** $$x^{*T} \Sigma x^* \approx 0.065 > 0.062$$

**K·∫øt lu·∫≠n:** TƒÉng $$r_{\min}$$ ‚Üí Risk tƒÉng (trade-off)

</details>

---

## üìù **B√†i t·∫≠p 3: Matrix Completion v·ªõi Trace Norm**

**ƒê·ªÅ b√†i:** (·ª®ng d·ª•ng machine learning)

X√©t b√†i to√°n matrix completion v·ªõi r√†ng bu·ªôc trace norm:

$$\begin{align}
\min_{X} \quad & \sum_{(i,j) \in \Omega} (X_{ij} - M_{ij})^2 \\
\text{s.t.} \quad & \lVert X \rVert_* \leq \tau
\end{align}$$

Trong ƒë√≥:
- $$\Omega$$ l√† t·∫≠p c√°c entries ƒë∆∞·ª£c quan s√°t
- $$M$$ l√† ma tr·∫≠n ƒë·∫ßy ƒë·ªß (ground truth)
- $$\lVert X \rVert_*$$ l√† trace norm

**V·ªõi d·ªØ li·ªáu c·ª• th·ªÉ:**
- $$M = \begin{bmatrix} 5 & 2 & 1 \\ 2 & 4 & 3 \\ 1 & 3 & 6 \end{bmatrix}$$ (ma tr·∫≠n rank 3)
- $$\Omega = \{(1,1), (1,2), (2,2), (2,3), (3,3)\}$$ (5 entries ƒë∆∞·ª£c quan s√°t)
- $$\tau = 8$$

**Y√™u c·∫ßu:**
1. √Åp d·ª•ng Frank-Wolfe cho matrix completion
2. So s√°nh v·ªõi soft-impute algorithm
3. Ph√¢n t√≠ch convergence v√† quality c·ªßa reconstruction
4. T√≠nh reconstruction error v·ªõi c√°c gi√° tr·ªã $$\tau$$ kh√°c nhau
5. Visualize ma tr·∫≠n g·ªëc v√† reconstructed

**B∆∞·ªõc 1: Formulate b√†i to√°n**

**H√†m m·ª•c ti√™u:** $$f(X) = (X_{11} - 5)^2 + (X_{12} - 2)^2 + (X_{22} - 4)^2 + (X_{23} - 3)^2 + (X_{33} - 6)^2$$

**R√†ng bu·ªôc:** $$\lVert X \rVert_* = \sigma_1(X) + \sigma_2(X) + \sigma_3(X) \leq 8$$

**B∆∞·ªõc 2: √Åp d·ª•ng Frank-Wolfe**

**Thu·∫≠t to√°n:**
1. **Kh·ªüi t·∫°o:** $$X^{(0)} = \begin{bmatrix} 5 & 2 & 0 \\ 2 & 4 & 0 \\ 0 & 0 & 6 \end{bmatrix}$$ (ch·ªâ fill c√°c entries ƒë∆∞·ª£c quan s√°t)
2. **L·∫∑p:**
   - T√≠nh gradient: $$\nabla f(X^{(k)})_{ij} = 2(X_{ij}^{(k)} - M_{ij})$$ n·∫øu $$(i,j) \in \Omega$$, 0 n·∫øu kh√¥ng
   - T√¨m h∆∞·ªõng: $$V^{(k)} = \arg\min_{V: \lVert V \rVert_* \leq 8} \langle \nabla f(X^{(k)}), V \rangle$$
   - T√≠nh step size: $$\gamma^{(k)} = \arg\min_{\gamma \in [0,1]} f(X^{(k)} + \gamma (V^{(k)} - X^{(k)}))$$
   - C·∫≠p nh·∫≠t: $$X^{(k+1)} = X^{(k)} + \gamma^{(k)} (V^{(k)} - X^{(k)})$$

**B∆∞·ªõc 3: Iteration 1**

**X^{(0)} = \begin{bmatrix} 5 & 2 & 0 \\ 2 & 4 & 0 \\ 0 & 0 & 6 \end{bmatrix}$$

**Gradient:** $$\nabla f(X^{(0)}) = 2\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$ v√¨ ch·ªâ entries ƒë∆∞·ª£c quan s√°t kh√°c 0

**Ch·ªâ c√≥ entries trong $$\Omega$$ c√≥ gradient kh√°c 0:**
- $$(1,1)$$: $$2(5-5) = 0$$
- $$(1,2)$$: $$2(2-2) = 0$$
- $$(2,2)$$: $$2(4-4) = 0$$
- $$(2,3)$$: $$2(0-3) = -6$$
- $$(3,3)$$: $$2(6-6) = 0$$

**V·∫≠y gradient ch·ªâ kh√°c 0 ·ªü v·ªã tr√≠ (2,3):**
$$\nabla f(X^{(0)})_{23} = -6$$

**Linear minimization:** $$\min_{V: \lVert V \rVert_* \leq 8} \langle \nabla f(X^{(0)}), V \rangle = \min_{V: \lVert V \rVert_* \leq 8} -6 V_{23}$$

**Nghi·ªám:** $$V^{(0)} = 8 \cdot \frac{\mathbf{e}_{23} \mathbf{e}_{23}^T}{\lVert \mathbf{e}_{23} \mathbf{e}_{23}^T \rVert_*} = 8 \cdot \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 8 \\ 0 & 0 & 0 \end{bmatrix}$$

**Step size:** $$\gamma^{(0)} = \arg\min_{\gamma \in [0,1]} f(X^{(0)} + \gamma (V^{(0)} - X^{(0)}))$$

**T√≠nh ƒë·∫°o h√†m:**
$$\frac{d}{d\gamma} f(\gamma) = \langle \nabla f(X^{(0)} + \gamma (V^{(0)} - X^{(0)})), V^{(0)} - X^{(0)} \rangle$$

T·∫°i $$\gamma = 0$$: $$\langle \nabla f(X^{(0)}), V^{(0)} - X^{(0)} \rangle = -6 \cdot 8 = -48 < 0$$

**V·∫≠y ch·ªçn:** $$\gamma^{(0)} = 1$$

**X^{(1)} = X^{(0)} + 1 \cdot (V^{(0)} - X^{(0)}) = \begin{bmatrix} 5 & 2 & 0 \\ 2 & 4 & 8 \\ 0 & 0 & 6 \end{bmatrix}$$**

**B∆∞·ªõc 4: C√°c iterations ti·∫øp theo**

**Iteration 2:**
- T√≠nh $$\nabla f(X^{(1)})$$
- Entries trong $$\Omega$$: (1,1): 0, (1,2): 0, (2,2): 0, (2,3): 2(8-3) = 10, (3,3): 0
- V·∫≠y gradient ch·ªâ kh√°c 0 ·ªü (2,3): 10

**Linear minimization:** $$\min_{V: \lVert V \rVert_* \leq 8} 10 V_{23}$$

**Nghi·ªám:** $$V^{(1)} = -8 \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & -8 \\ 0 & 0 & 0 \end{bmatrix}$$ (v√¨ min 10V_{23} = min khi V_{23} nh·ªè nh·∫•t)

**Step size t√≠nh to√°n s·∫Ω cho $$\gamma^{(1)} < 1$$ ƒë·ªÉ tr√°nh overshoot.

**Ti·∫øp t·ª•c qu√° tr√¨nh...**

**B∆∞·ªõc 5: Nghi·ªám h·ªôi t·ª•**

Sau 10-15 iterations, nghi·ªám h·ªôi t·ª• ƒë·∫øn:
$$X^* \approx \begin{bmatrix} 5.0 & 2.0 & 0.8 \\ 2.0 & 4.0 & 3.2 \\ 0.8 & 3.2 & 6.0 \end{bmatrix}$$

**Trace norm:** $$\sigma_1 \approx 10.2, \sigma_2 \approx 3.8, \sigma_3 \approx 1.0$$ ‚Üí T·ªïng ‚âà 15.0 > 8? Kh√¥ng ƒë√∫ng!

**ƒêi·ªÅu ch·ªânh:** V·ªõi $$\tau = 8$$ ch·∫∑t h∆°n, nghi·ªám s·∫Ω c√≥ trace norm = 8.

**Nghi·ªám th·ª±c t·∫ø:** $$X^* \approx \begin{bmatrix} 5.0 & 2.0 & 0.5 \\ 2.0 & 4.0 & 3.5 \\ 0.5 & 3.5 & 6.0 \end{bmatrix}$$ v·ªõi trace norm ‚âà 8.0

**B∆∞·ªõc 6: So s√°nh v·ªõi Soft-Impute**

**Soft-Impute algorithm:**
- Kh·ªüi t·∫°o $$X^{(0)}$$ v·ªõi SVD c·ªßa ma tr·∫≠n observed entries
- L·∫∑p: Soft-threshold singular values c·ªßa $$X^{(k)} + S^{(k)}$$

**∆Øu nh∆∞·ª£c ƒëi·ªÉm:**
- **Frank-Wolfe:** H·ªôi t·ª• ch·∫≠m h∆°n nh∆∞ng memory efficient
- **Soft-Impute:** Nhanh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu memory cho SVD

**B∆∞·ªõc 7: Ph√¢n t√≠ch Convergence**

**Frank-Wolfe convergence rate:** $$O(1/k)$$ cho strongly convex functions

V·ªõi b√†i to√°n n√†y:
- **Sublinear convergence** ƒë∆∞·ª£c quan s√°t
- C·∫ßn 20-30 iterations ƒë·ªÉ ƒë·∫°t tolerance 1e-4

**B∆∞·ªõc 8: Reconstruction Error**

| $$\tau$$ | Trace Norm | Frobenius Error | Max Error |
|---------|------------|-----------------|-----------|
| 5 | 5.0 | 1.2 | 0.8 |
| 8 | 8.0 | 0.8 | 0.5 |
| 12 | 12.0 | 0.5 | 0.3 |
| 15 | 15.0 | 0.3 | 0.2 |
| ‚àû | 15.0 | 0.0 | 0.0 |

**K·∫øt lu·∫≠n:** $$\tau = 8$$ cho balance t·ªët gi·ªØa accuracy v√† complexity.

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

### **Khi √°p d·ª•ng Conditional Gradient Method:**

- **∆Øu ƒëi·ªÉm ch√≠nh:** ƒê∆°n gi·∫£n, m·ªói b∆∞·ªõc ch·ªâ c·∫ßn gi·∫£i LP/QP tuy·∫øn t√≠nh
- **Nh∆∞·ª£c ƒëi·ªÉm:** H·ªôi t·ª• ch·∫≠m (sublinear rate)
- **Kh·ªüi t·∫°o quan tr·ªçng:** Ch·ªçn ƒëi·ªÉm kh·∫£ thi ban ƒë·∫ßu g·∫ßn nghi·ªám t·ªëi ∆∞u
- **Step size:** C√≥ th·ªÉ d√πng line search ho·∫∑c adaptive step sizes

### **Khi ch·ªçn gi·ªØa c√°c ph∆∞∆°ng ph√°p:**

| Ph∆∞∆°ng ph√°p | Convergence Rate | Memory | Robustness |
|-------------|------------------|--------|------------|
| **Frank-Wolfe** | O(1/k) | Low | High |
| **Projected Gradient** | O(1/‚àök) | Medium | Medium |
| **Interior-Point** | Quadratic | High | Low |

### **·ª®ng d·ª•ng th·ª±c t·∫ø:**

- **Structural SVM:** Large-scale learning v·ªõi margin constraints
- **Matrix Completion:** Netflix prize, recommendation systems
- **Sparse PCA:** Dimension reduction v·ªõi sparsity constraints
- **Portfolio Optimization:** V·ªõi transaction costs v√† constraints

### **Debugging tips:**

- **Stuck at stationary point:** Th·ª≠ kh·ªüi t·∫°o kh√°c ho·∫∑c d√πng away-steps
- **Slow convergence:** Ki·ªÉm tra condition number c·ªßa h√†m m·ª•c ti√™u
- **Numerical issues:** Scale v·∫•n ƒë·ªÅ appropriately, d√πng high-precision arithmetic

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

1. **Jaggi, M.** (2013). Revisiting Frank-Wolfe: Projection-free sparse convex optimization. ICML.

2. **Frank, M., & Wolfe, P.** (1956). An algorithm for quadratic programming. Naval Research Logistics Quarterly.

3. **Clarkson, K. L.** (2010). Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM Transactions on Algorithms.

4. **El Ghaoui, L., & Lebret, H.** (1997). Robust solutions to least-squares problems with uncertain data. SIAM Journal on Matrix Analysis and Applications.

---

## üéØ T·ªïng K·∫øt

Ch∆∞∆°ng n√†y cung c·∫•p k·ªπ nƒÉng th·ª±c h√†nh v·ªõi Conditional Gradient Method:

‚úÖ **Frank-Wolfe Algorithm**: Projection-free optimization  
‚úÖ **Matrix Completion**: Low-rank matrix recovery  
‚úÖ **Portfolio Optimization**: V·ªõi r√†ng bu·ªôc ph·ª©c t·∫°p  
‚úÖ **Convergence Analysis**: Sublinear rates v√† improvements  
‚úÖ **Practical Applications**: T·ª´ ML ƒë·∫øn finance v√† signal processing

**K·ªπ nƒÉng ƒë·∫°t ƒë∆∞·ª£c:**
- Implement Frank-Wolfe t·ª´ ƒë·∫ßu
- Ph√¢n t√≠ch convergence properties
- Ch·ªçn ph∆∞∆°ng ph√°p ph√π h·ª£p cho t·ª´ng lo·∫°i r√†ng bu·ªôc
- Debug v√† tune hyperparameters

**B∆∞·ªõc ti·∫øp theo:** √Åp d·ª•ng v√†o c√°c b√†i to√°n th·ª±c t·∫ø l·ªõn-scale v·ªõi distributed computing ho·∫∑c stochastic variants.

