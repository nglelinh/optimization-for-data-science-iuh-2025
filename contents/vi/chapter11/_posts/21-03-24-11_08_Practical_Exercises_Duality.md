---
layout: post
title: 11-8 BÃ i Táº­p Thá»±c HÃ nh - TÃ­nh Äá»‘i Ngáº«u Tá»•ng QuÃ¡t
chapter: '11'
order: 9
owner: GitHub Copilot
lang: vi
categories:
- chapter11
lesson_type: required
---

# BÃ i Táº­p Thá»±c HÃ nh - TÃ­nh Äá»‘i Ngáº«u Tá»•ng QuÃ¡t

## ğŸ“ **BÃ i táº­p 1: Quadratic Programming Ä‘Æ¡n giáº£n**

**Äá» bÃ i:** Cho bÃ i toÃ¡n
$$\min_x \frac{1}{2}x^2 + x \quad \text{s.t.} \quad x \geq 1$$

**YÃªu cáº§u:**
1. Viáº¿t láº¡i bÃ i toÃ¡n á»Ÿ dáº¡ng chuáº©n
2. XÃ¢y dá»±ng hÃ m Lagrangian
3. TÃ­nh hÃ m Ä‘á»‘i ngáº«u $$g(u)$$
4. Giáº£i bÃ i toÃ¡n Ä‘á»‘i ngáº«u
5. So sÃ¡nh nghiá»‡m primal vÃ  dual

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Viáº¿t láº¡i bÃ i toÃ¡n**
$$\min_x \frac{1}{2}x^2 + x \quad \text{s.t.} \quad -x + 1 \leq 0$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(x,u) = \frac{1}{2}x^2 + x + u(-x + 1) = \frac{1}{2}x^2 + (1-u)x + u$$

**BÆ°á»›c 3: TÃ­nh hÃ m Ä‘á»‘i ngáº«u**
$$g(u) = \inf_x L(x,u) = \inf_x \left\{\frac{1}{2}x^2 + (1-u)x + u\right\}$$

Äáº¡o hÃ m theo $$x$$: $$\frac{\partial L}{\partial x} = x + (1-u) = 0$$

Suy ra: $$x^* = u - 1$$

Thay vÃ o: $$g(u) = \frac{1}{2}(u-1)^2 + (1-u)(u-1) + u = -\frac{1}{2}(u-1)^2 + u$$

**BÆ°á»›c 4: BÃ i toÃ¡n Ä‘á»‘i ngáº«u**
$$\max_u g(u) = \max_u \left\{-\frac{1}{2}(u-1)^2 + u\right\} \quad \text{s.t.} \quad u \geq 0$$

Äáº¡o hÃ m: $$g'(u) = -(u-1) + 1 = 2 - u = 0 \Rightarrow u^* = 2$$

**BÆ°á»›c 5: So sÃ¡nh nghiá»‡m**
- **Dual optimal:** $$u^* = 2$$, $$g^* = g(2) = -\frac{1}{2} + 2 = \frac{3}{2}$$
- **Primal optimal:** $$x^* = u^* - 1 = 1$$, $$f^* = \frac{1}{2} + 1 = \frac{3}{2}$$
- **Strong duality:** $$f^* = g^* = \frac{3}{2}$$ âœ“

</details>

---

## ğŸ“ **BÃ i táº­p 2: BÃ i toÃ¡n vá»›i nhiá»u rÃ ng buá»™c**

**Äá» bÃ i:** 
$$\min_{x,y} x^2 + y^2 \quad \text{s.t.} \quad x + y \geq 2, \quad x \geq 0, \quad y \geq 0$$

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng hÃ m Lagrangian Ä‘áº§y Ä‘á»§
2. TÃ­nh hÃ m Ä‘á»‘i ngáº«u $$g(u_1, u_2, u_3)$$
3. TÃ¬m Ä‘iá»u kiá»‡n Ä‘á»ƒ $$g(u_1, u_2, u_3) > -\infty$$
4. Giáº£i bÃ i toÃ¡n Ä‘á»‘i ngáº«u

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Viáº¿t láº¡i bÃ i toÃ¡n**
$$\min_{x,y} x^2 + y^2 \quad \text{s.t.} \quad -x - y + 2 \leq 0, \quad -x \leq 0, \quad -y \leq 0$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(x,y,u_1,u_2,u_3) = x^2 + y^2 + u_1(-x - y + 2) + u_2(-x) + u_3(-y)$$
$$= x^2 + y^2 - (u_1 + u_2)x - (u_1 + u_3)y + 2u_1$$

**BÆ°á»›c 3: TÃ­nh hÃ m Ä‘á»‘i ngáº«u**
$$g(u_1,u_2,u_3) = \inf_{x,y} L(x,y,u_1,u_2,u_3)$$

Äiá»u kiá»‡n tá»‘i Æ°u:
- $$\frac{\partial L}{\partial x} = 2x - (u_1 + u_2) = 0 \Rightarrow x^* = \frac{u_1 + u_2}{2}$$
- $$\frac{\partial L}{\partial y} = 2y - (u_1 + u_3) = 0 \Rightarrow y^* = \frac{u_1 + u_3}{2}$$

Thay vÃ o:
$$g(u_1,u_2,u_3) = -\frac{(u_1 + u_2)^2}{4} - \frac{(u_1 + u_3)^2}{4} + 2u_1$$

**BÆ°á»›c 4: Äiá»u kiá»‡n há»¯u háº¡n**
$$g(u_1,u_2,u_3) > -\infty$$ luÃ´n Ä‘Ãºng vÃ¬ lÃ  hÃ m báº­c hai.

**BÆ°á»›c 5: BÃ i toÃ¡n Ä‘á»‘i ngáº«u**
$$\max_{u_1,u_2,u_3} g(u_1,u_2,u_3) \quad \text{s.t.} \quad u_1,u_2,u_3 \geq 0$$

Sá»­ dá»¥ng Ä‘iá»u kiá»‡n KKT Ä‘á»ƒ tÃ¬m nghiá»‡m tá»‘i Æ°u.

</details>

---

## ğŸ“ **BÃ i táº­p 3: Linear Programming vá»›i rÃ ng buá»™c Ä‘áº³ng thá»©c**

**Äá» bÃ i:** (BÃ i 5.1 tá»« Boyd & Vandenberghe)
$$\min_x c^T x \quad \text{s.t.} \quad Ax = b$$

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng hÃ m Lagrangian
2. TÃ­nh hÃ m Ä‘á»‘i ngáº«u $$g(\nu)$$
3. Chá»©ng minh strong duality
4. TÃ¬m Ä‘iá»u kiá»‡n tá»‘i Æ°u

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: HÃ m Lagrangian**
$$L(x,\nu) = c^T x + \nu^T (Ax - b)$$

**BÆ°á»›c 2: HÃ m Ä‘á»‘i ngáº«u**
$$g(\nu) = \inf_x L(x,\nu) = \inf_x \{c^T x + \nu^T Ax\} - \nu^T b$$
$$= \inf_x (c + A^T \nu)^T x - \nu^T b$$

Náº¿u $$c + A^T \nu \neq 0$$, thÃ¬ $$g(\nu) = -\infty$$

Náº¿u $$c + A^T \nu = 0$$, thÃ¬ $$g(\nu) = -\nu^T b$$

**BÆ°á»›c 3: BÃ i toÃ¡n Ä‘á»‘i ngáº«u**
$$\max_\nu \{-\nu^T b\} \quad \text{s.t.} \quad A^T \nu + c = 0$$

TÆ°Æ¡ng Ä‘Æ°Æ¡ng:
$$\min_\nu \nu^T b \quad \text{s.t.} \quad A^T \nu = -c$$

**BÆ°á»›c 4: Strong duality**
VÃ¬ bÃ i toÃ¡n primal lÃ  LP vÃ  feasible (giáº£ sá»­), nÃªn cÃ³ strong duality.

</details>

---

## ğŸ“ **BÃ i táº­p 4: Quadratic Programming vá»›i rÃ ng buá»™c báº¥t Ä‘áº³ng thá»©c**

**Äá» bÃ i:** (BÃ i 5.2 tá»« Boyd & Vandenberghe)
$$\min_x \frac{1}{2}x^T P x + q^T x + r \quad \text{s.t.} \quad Gx \leq h$$

vá»›i $$P \succ 0$$.

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng hÃ m Lagrangian
2. TÃ­nh hÃ m Ä‘á»‘i ngáº«u
3. TÃ¬m Ä‘iá»u kiá»‡n KKT
4. Giáº£i trÆ°á»ng há»£p cá»¥ thá»ƒ

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: HÃ m Lagrangian**
$$L(x,\lambda) = \frac{1}{2}x^T P x + q^T x + r + \lambda^T (Gx - h)$$

**BÆ°á»›c 2: TÃ­nh hÃ m Ä‘á»‘i ngáº«u**
$$g(\lambda) = \inf_x L(x,\lambda)$$

Äiá»u kiá»‡n tá»‘i Æ°u: $$\nabla_x L = Px + q + G^T \lambda = 0$$

Suy ra: $$x^*(\lambda) = -P^{-1}(q + G^T \lambda)$$

Thay vÃ o:
$$g(\lambda) = -\frac{1}{2}(q + G^T \lambda)^T P^{-1} (q + G^T \lambda) + r - \lambda^T h$$

**BÆ°á»›c 3: BÃ i toÃ¡n Ä‘á»‘i ngáº«u**
$$\max_\lambda g(\lambda) \quad \text{s.t.} \quad \lambda \geq 0$$

**BÆ°á»›c 4: Äiá»u kiá»‡n KKT**
- Stationarity: $$Px^* + q + G^T \lambda^* = 0$$
- Primal feasibility: $$Gx^* \leq h$$
- Dual feasibility: $$\lambda^* \geq 0$$
- Complementary slackness: $$\lambda_i^* (G_i x^* - h_i) = 0$$

</details>

---

## ğŸ“ **BÃ i táº­p 5: Norm approximation**

**Äá» bÃ i:** (BÃ i 5.3 tá»« Boyd & Vandenberghe)
$$\min_x \lVert Ax - b \rVert$$

vá»›i norm lÃ  $$\ell_1, \ell_2,$$ hoáº·c $$\ell_\infty$$.

**YÃªu cáº§u:**
1. Reformulate thÃ nh bÃ i toÃ¡n cÃ³ rÃ ng buá»™c
2. XÃ¢y dá»±ng dual cho má»—i trÆ°á»ng há»£p
3. So sÃ¡nh cÃ¡c dual problems

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**TrÆ°á»ng há»£p 1: $$\ell_1$$ norm**
$$\min_{x,t} \mathbf{1}^T t \quad \text{s.t.} \quad -t \leq Ax - b \leq t$$

Dual:
$$\max_{\lambda,\mu} -b^T(\lambda - \mu) \quad \text{s.t.} \quad A^T(\lambda - \mu) = 0, \quad \lambda + \mu = \mathbf{1}, \quad \lambda,\mu \geq 0$$

**TrÆ°á»ng há»£p 2: $$\ell_2$$ norm**
$$\min_{x,t} t \quad \text{s.t.} \quad \lVert Ax - b \rVert_2 \leq t$$

Dual:
$$\max_{\lambda,\nu} -b^T \nu - \lambda \quad \text{s.t.} \quad A^T \nu = 0, \quad \lVert \nu \rVert_2 \leq \lambda, \quad \lambda \geq 0$$

**TrÆ°á»ng há»£p 3: $$\ell_\infty$$ norm**
$$\min_{x,t} t \quad \text{s.t.} \quad -t\mathbf{1} \leq Ax - b \leq t\mathbf{1}$$

Dual:
$$\max_{\lambda,\mu} -b^T(\lambda - \mu) \quad \text{s.t.} \quad A^T(\lambda - \mu) = 0, \quad \lVert \lambda - \mu \rVert_1 \leq 1$$

</details>

---

## ğŸ“ **BÃ i táº­p 6: Entropy maximization**

**Äá» bÃ i:** (BÃ i 5.4 tá»« Boyd & Vandenberghe)
$$\max_x \sum_{i=1}^n x_i \log x_i \quad \text{s.t.} \quad Ax = b, \quad x \geq 0$$

**YÃªu cáº§u:**
1. Chuyá»ƒn vá» bÃ i toÃ¡n minimization
2. XÃ¢y dá»±ng hÃ m Lagrangian
3. TÃ­nh hÃ m Ä‘á»‘i ngáº«u
4. TÃ¬m nghiá»‡m tá»‘i Æ°u

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Chuyá»ƒn Ä‘á»•i**
$$\min_x -\sum_{i=1}^n x_i \log x_i \quad \text{s.t.} \quad Ax = b, \quad x \geq 0$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(x,\nu,\lambda) = -\sum_{i=1}^n x_i \log x_i + \nu^T(Ax - b) - \lambda^T x$$

**BÆ°á»›c 3: Äiá»u kiá»‡n tá»‘i Æ°u**
$$\frac{\partial L}{\partial x_i} = -(\log x_i + 1) + (A^T \nu)_i - \lambda_i = 0$$

Suy ra: $$x_i = e^{(A^T \nu)_i - \lambda_i - 1}$$

**BÆ°á»›c 4: Complementary slackness**
Náº¿u $$x_i > 0$$, thÃ¬ $$\lambda_i = 0$$, suy ra:
$$x_i = e^{(A^T \nu)_i - 1}$$

**BÆ°á»›c 5: HÃ m Ä‘á»‘i ngáº«u**
$$g(\nu,\lambda) = -\sum_{i=1}^n e^{(A^T \nu)_i - \lambda_i - 1} - \nu^T b$$

</details>

---

## ğŸ“ **BÃ i táº­p 7: Minimum volume ellipsoid**

**Äá» bÃ i:** (BÃ i 5.5 tá»« Boyd & Vandenberghe)
TÃ¬m ellipsoid cÃ³ thá»ƒ tÃ­ch nhá» nháº¥t chá»©a cÃ¡c Ä‘iá»ƒm $$x_1, \ldots, x_m \in \mathbb{R}^n$$:

$$\min_{A,b} \log \det A^{-1} \quad \text{s.t.} \quad \lVert A x_i + b \rVert_2 \leq 1, \quad i = 1,\ldots,m$$

**YÃªu cáº§u:**
1. Giáº£i thÃ­ch Ã½ nghÄ©a hÃ¬nh há»c
2. XÃ¢y dá»±ng hÃ m Lagrangian
3. TÃ­nh Ä‘iá»u kiá»‡n KKT
4. PhÃ¢n tÃ­ch trÆ°á»ng há»£p 2D

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Ã nghÄ©a hÃ¬nh há»c**
Ellipsoid: $$\mathcal{E} = \{A^{-1}(u - b) : \lVert u \rVert_2 \leq 1\}$$
Thá»ƒ tÃ­ch: $$\text{vol}(\mathcal{E}) = \frac{\pi^{n/2}}{\Gamma(n/2 + 1)} \det A^{-1}$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(A,b,\lambda) = \log \det A^{-1} + \sum_{i=1}^m \lambda_i (\lVert A x_i + b \rVert_2^2 - 1)$$

**BÆ°á»›c 3: Äiá»u kiá»‡n KKT**
- $$\frac{\partial L}{\partial b} = 2\sum_{i=1}^m \lambda_i (A x_i + b) = 0$$
- $$\frac{\partial L}{\partial A} = -A^{-T} + 2\sum_{i=1}^m \lambda_i (A x_i + b) x_i^T = 0$$

**BÆ°á»›c 4: Tá»« Ä‘iá»u kiá»‡n Ä‘áº§u tiÃªn:**
$$\sum_{i=1}^m \lambda_i (A x_i + b) = 0$$

**BÆ°á»›c 5: Tá»« Ä‘iá»u kiá»‡n thá»© hai:**
$$A^{-T} = 2\sum_{i=1}^m \lambda_i (A x_i + b) x_i^T$$

</details>

---

## ğŸ“ **BÃ i táº­p 8: Chebyshev center**

**Äá» bÃ i:** (BÃ i 5.6 tá»« Boyd & Vandenberghe)
TÃ¬m tÃ¢m vÃ  bÃ¡n kÃ­nh cá»§a hÃ¬nh cáº§u lá»›n nháº¥t náº±m trong polyhedron:
$$P = \{x : a_i^T x \leq b_i, \quad i = 1,\ldots,m\}$$

$$\max_{x_c,r} r \quad \text{s.t.} \quad a_i^T x_c + r \lVert a_i \rVert_2 \leq b_i, \quad i = 1,\ldots,m$$

**YÃªu cáº§u:**
1. Giáº£i thÃ­ch Ã½ nghÄ©a hÃ¬nh há»c
2. XÃ¢y dá»±ng dual problem
3. PhÃ¢n tÃ­ch Ä‘iá»u kiá»‡n strong duality
4. TÃ­nh toÃ¡n cho trÆ°á»ng há»£p cá»¥ thá»ƒ

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Ã nghÄ©a hÃ¬nh há»c**
HÃ¬nh cáº§u $$\{x : \lVert x - x_c \rVert_2 \leq r\}$$ náº±m trong $$P$$ khi vÃ  chá»‰ khi:
$$\max_{\lVert u \rVert_2 \leq 1} a_i^T(x_c + ru) \leq b_i$$
$$\Leftrightarrow a_i^T x_c + r \lVert a_i \rVert_2 \leq b_i$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(x_c,r,\lambda) = -r + \sum_{i=1}^m \lambda_i (a_i^T x_c + r \lVert a_i \rVert_2 - b_i)$$

**BÆ°á»›c 3: Äiá»u kiá»‡n tá»‘i Æ°u**
- $$\frac{\partial L}{\partial x_c} = \sum_{i=1}^m \lambda_i a_i = 0$$
- $$\frac{\partial L}{\partial r} = -1 + \sum_{i=1}^m \lambda_i \lVert a_i \rVert_2 = 0$$

**BÆ°á»›c 4: HÃ m Ä‘á»‘i ngáº«u**
Tá»« Ä‘iá»u kiá»‡n trÃªn: $$\sum_{i=1}^m \lambda_i \lVert a_i \rVert_2 = 1$$

$$g(\lambda) = -\sum_{i=1}^m \lambda_i b_i$$

**BÆ°á»›c 5: BÃ i toÃ¡n Ä‘á»‘i ngáº«u**
$$\min_\lambda \sum_{i=1}^m \lambda_i b_i \quad \text{s.t.} \quad \sum_{i=1}^m \lambda_i a_i = 0, \quad \sum_{i=1}^m \lambda_i \lVert a_i \rVert_2 = 1, \quad \lambda \geq 0$$

</details>

---

## ğŸ“ **BÃ i táº­p 9: Geometric programming**

**Äá» bÃ i:** (BÃ i 5.7 tá»« Boyd & Vandenberghe)
BÃ i toÃ¡n geometric programming:
$$\min_x f_0(x) \quad \text{s.t.} \quad f_i(x) \leq 1, \quad i = 1,\ldots,m$$

vá»›i $$f_i(x) = \sum_{k=1}^{K_i} c_{ik} x_1^{a_{ik1}} \cdots x_n^{a_{ikn}}$$, $$c_{ik} > 0$$.

**YÃªu cáº§u:**
1. Chuyá»ƒn vá» convex form báº±ng log transformation
2. XÃ¢y dá»±ng dual cá»§a convex form
3. PhÃ¢n tÃ­ch complementary slackness
4. Giáº£i bÃ i toÃ¡n cá»¥ thá»ƒ

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Log transformation**
Äáº·t $$y_j = \log x_j$$, ta cÃ³:
$$f_i(x) = \sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}$$

BÃ i toÃ¡n trá»Ÿ thÃ nh:
$$\min_y \log\left(\sum_{k=1}^{K_0} c_{0k} e^{a_{0k}^T y}\right) \quad \text{s.t.} \quad \log\left(\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}\right) \leq 0$$

**BÆ°á»›c 2: HÃ m Lagrangian**
$$L(y,\lambda) = \log\left(\sum_{k=1}^{K_0} c_{0k} e^{a_{0k}^T y}\right) + \sum_{i=1}^m \lambda_i \log\left(\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}\right)$$

**BÆ°á»›c 3: Äiá»u kiá»‡n tá»‘i Æ°u**
$$\frac{\partial L}{\partial y_j} = \sum_{i=0}^m \lambda_i \frac{\sum_{k=1}^{K_i} c_{ik} a_{ikj} e^{a_{ik}^T y}}{\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}} = 0$$

vá»›i $$\lambda_0 = 1$$.

**BÆ°á»›c 4: Complementary slackness**
Náº¿u $$\lambda_i > 0$$, thÃ¬ $$f_i(x^*) = 1$$ (rÃ ng buá»™c active).

</details>

---

## ğŸ“ **BÃ i táº­p 10: Water-filling**

**Äá» bÃ i:** (BÃ i 5.8 tá»« Boyd & Vandenberghe)
BÃ i toÃ¡n phÃ¢n bá»• cÃ´ng suáº¥t trong thÃ´ng tin:
$$\max_x \sum_{i=1}^n \log(\alpha_i + x_i) \quad \text{s.t.} \quad \sum_{i=1}^n x_i \leq P, \quad x_i \geq 0$$

vá»›i $$\alpha_i > 0$$, $$P > 0$$.

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng Ä‘iá»u kiá»‡n KKT
2. TÃ¬m nghiá»‡m water-filling
3. PhÃ¢n tÃ­ch thuáº­t toÃ¡n
4. á»¨ng dá»¥ng vÃ o trÆ°á»ng há»£p cá»¥ thá»ƒ

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: HÃ m Lagrangian**
$$L(x,\nu,\lambda) = -\sum_{i=1}^n \log(\alpha_i + x_i) + \nu\left(\sum_{i=1}^n x_i - P\right) - \sum_{i=1}^n \lambda_i x_i$$

**BÆ°á»›c 2: Äiá»u kiá»‡n KKT**
- Stationarity: $$-\frac{1}{\alpha_i + x_i} + \nu - \lambda_i = 0$$
- Complementary slackness: $$\lambda_i x_i = 0$$, $$\nu\left(\sum_{i=1}^n x_i - P\right) = 0$$

**BÆ°á»›c 3: PhÃ¢n tÃ­ch**
Tá»« stationarity: $$\frac{1}{\alpha_i + x_i} = \nu - \lambda_i$$

Náº¿u $$x_i > 0$$, thÃ¬ $$\lambda_i = 0$$, suy ra:
$$\alpha_i + x_i = \frac{1}{\nu} \Rightarrow x_i = \frac{1}{\nu} - \alpha_i$$

Náº¿u $$x_i = 0$$, thÃ¬ $$\lambda_i \geq 0$$, suy ra:
$$\nu \leq \frac{1}{\alpha_i}$$

**BÆ°á»›c 4: Water-filling solution**
$$x_i^* = \max\left\{0, \frac{1}{\nu^*} - \alpha_i\right\}$$

vá»›i $$\nu^*$$ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»«:
$$\sum_{i=1}^n \max\left\{0, \frac{1}{\nu^*} - \alpha_i\right\} = P$$

**BÆ°á»›c 5: Thuáº­t toÃ¡n**
1. Sáº¯p xáº¿p $$\alpha_i$$ theo thá»© tá»± tÄƒng dáº§n
2. TÃ¬m $$\nu^*$$ sao cho constraint Ä‘Æ°á»£c thá»a mÃ£n
3. TÃ­nh $$x_i^*$$ theo cÃ´ng thá»©c water-filling

</details>


---

## ğŸ“ **BÃ i táº­p 11: Semidefinite programming relaxation**

**Äá» bÃ i:** (BÃ i 5.9 tá»« Boyd & Vandenberghe)
Cho bÃ i toÃ¡n QCQP (Quadratically Constrained Quadratic Program):
$$\min_x x^T P_0 x + 2q_0^T x + r_0$$
$$\text{s.t.} \quad x^T P_i x + 2q_i^T x + r_i \leq 0, \quad i = 1,\ldots,m$$

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng SDP relaxation
2. TÃ­nh dual cá»§a SDP relaxation
3. PhÃ¢n tÃ­ch Ä‘iá»u kiá»‡n strong duality
4. So sÃ¡nh bound vá»›i original problem

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: SDP relaxation**
Äáº·t $$X = xx^T$$, bÃ i toÃ¡n trá»Ÿ thÃ nh:
$$\min_{X,x} \text{tr}(P_0 X) + 2q_0^T x + r_0$$
$$\text{s.t.} \quad \text{tr}(P_i X) + 2q_i^T x + r_i \leq 0, \quad X \succeq 0, \quad X = xx^T$$

Bá» constraint $$X = xx^T$$:
$$\min_{X,x} \text{tr}(P_0 X) + 2q_0^T x + r_0$$
$$\text{s.t.} \quad \text{tr}(P_i X) + 2q_i^T x + r_i \leq 0, \quad X \succeq 0$$

**BÆ°á»›c 2: Dual cá»§a SDP**
$$\max_{\lambda,Z} -\sum_{i=1}^m \lambda_i r_i + r_0$$
$$\text{s.t.} \quad \sum_{i=1}^m \lambda_i P_i - P_0 + Z = 0, \quad \sum_{i=1}^m \lambda_i q_i = q_0, \quad \lambda \geq 0, \quad Z \succeq 0$$

**BÆ°á»›c 3: Strong duality**
SDP cÃ³ strong duality khi Slater's condition thá»a mÃ£n: tá»“n táº¡i $$(X,x)$$ vá»›i $$X \succ 0$$ vÃ  constraints nghiÃªm ngáº·t.

</details>

---

## ğŸ“ **BÃ i táº­p 12: Robust linear programming**

**Äá» bÃ i:** (BÃ i 5.10 tá»« Boyd & Vandenberghe)
$$\min_x c^T x \quad \text{s.t.} \quad a_i^T x \leq b_i \quad \forall a_i \in \mathcal{E}_i, \quad i = 1,\ldots,m$$

vá»›i $$\mathcal{E}_i = \{a_i^{nom} + u : \lVert u \rVert_2 \leq \rho_i\}$$.

**YÃªu cáº§u:**
1. Reformulate thÃ nh deterministic problem
2. XÃ¢y dá»±ng dual problem
3. PhÃ¢n tÃ­ch worst-case scenarios
4. TÃ­nh price of robustness

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Deterministic reformulation**
$$\max_{a_i \in \mathcal{E}_i} a_i^T x = (a_i^{nom})^T x + \rho_i \lVert x \rVert_2$$

BÃ i toÃ¡n trá»Ÿ thÃ nh:
$$\min_x c^T x \quad \text{s.t.} \quad (a_i^{nom})^T x + \rho_i \lVert x \rVert_2 \leq b_i$$

**BÆ°á»›c 2: Second-order cone constraints**
$$\min_{x,t_i} c^T x \quad \text{s.t.} \quad (a_i^{nom})^T x + \rho_i t_i \leq b_i, \quad \lVert x \rVert_2 \leq t_i$$

**BÆ°á»›c 3: Dual problem**
$$\max_{\lambda,\mu_i,v_i} \sum_{i=1}^m \lambda_i b_i$$
$$\text{s.t.} \quad \sum_{i=1}^m \lambda_i a_i^{nom} + \sum_{i=1}^m \mu_i v_i = c, \quad \lVert v_i \rVert_2 \leq \mu_i, \quad \lambda_i \rho_i = \mu_i$$

</details>

---

## ğŸ“ **BÃ i táº­p 13: Maximum likelihood estimation**

**Äá» bÃ i:** (BÃ i 5.11 tá»« Boyd & Vandenberghe)
Cho $$n$$ observations $$y_1, \ldots, y_n$$ tá»« distribution vá»›i density:
$$p(y|\theta) = \exp(\theta^T \phi(y) - g(\theta))$$

**YÃªu cáº§u:**
1. Formulate MLE problem
2. Derive dual problem
3. PhÃ¢n tÃ­ch convexity
4. TÃ¬m Ä‘iá»u kiá»‡n optimality

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: MLE formulation**
$$\max_\theta \sum_{i=1}^n \log p(y_i|\theta) = \max_\theta \sum_{i=1}^n (\theta^T \phi(y_i) - g(\theta))$$

TÆ°Æ¡ng Ä‘Æ°Æ¡ng:
$$\min_\theta ng(\theta) - \theta^T \sum_{i=1}^n \phi(y_i)$$

**BÆ°á»›c 2: Convexity**
VÃ¬ $$g(\theta)$$ lÃ  log-partition function nÃªn convex. Do Ä‘Ã³ bÃ i toÃ¡n lÃ  convex.

**BÆ°á»›c 3: Optimality condition**
$$\nabla g(\theta^*) = \frac{1}{n}\sum_{i=1}^n \phi(y_i)$$

**BÆ°á»›c 4: Dual interpretation**
Dual variables tÆ°Æ¡ng á»©ng vá»›i moment constraints trong exponential family.

</details>

---

## ğŸ“ **BÃ i táº­p 14: Analytic centering**

**Äá» bÃ i:** (BÃ i 5.12 tá»« Boyd & Vandenberghe)
$$\min_x -\sum_{i=1}^m \log(b_i - a_i^T x)$$

vá»›i $$\{x : a_i^T x < b_i, i = 1,\ldots,m\} \neq \emptyset$$.

**YÃªu cáº§u:**
1. Chá»©ng minh strict convexity
2. TÃ¬m Ä‘iá»u kiá»‡n optimality
3. PhÃ¢n tÃ­ch geometric interpretation
4. XÃ¢y dá»±ng Newton method

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Strict convexity**
$$\nabla^2 f(x) = \sum_{i=1}^m \frac{a_i a_i^T}{(b_i - a_i^T x)^2} \succ 0$$

**BÆ°á»›c 2: Optimality condition**
$$\sum_{i=1}^m \frac{a_i}{b_i - a_i^T x^*} = 0$$

**BÆ°á»›c 3: Geometric interpretation**
$$x^*$$ lÃ  center cá»§a polytope $$\{x : a_i^T x \leq b_i\}$$ theo nghÄ©a minimize tÃ­ch cÃ¡c khoáº£ng cÃ¡ch Ä‘áº¿n boundaries.

**BÆ°á»›c 4: Newton step**
$$\Delta x = -(\nabla^2 f(x))^{-1} \nabla f(x)$$

vá»›i backtracking line search Ä‘á»ƒ Ä‘áº£m báº£o feasibility.

</details>

---

## ğŸ“ **BÃ i táº­p 15: Minimum fuel optimal control**

**Äá» bÃ i:** (BÃ i 5.13 tá»« Boyd & Vandenberghe)
$$\min_{u,x} \sum_{t=0}^{T-1} \lVert u_t \rVert_1$$
$$\text{s.t.} \quad x_{t+1} = Ax_t + Bu_t, \quad x_0 = x^{init}, \quad x_T = x^{final}$$

**YÃªu cáº§u:**
1. Reformulate thÃ nh LP
2. XÃ¢y dá»±ng dual problem
3. PhÃ¢n tÃ­ch bang-bang control
4. TÃ¬m switching times

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: LP formulation**
Äáº·t $$u_t = u_t^+ - u_t^-$$ vá»›i $$u_t^+, u_t^- \geq 0$$:
$$\min \sum_{t=0}^{T-1} \mathbf{1}^T (u_t^+ + u_t^-)$$
$$\text{s.t.} \quad x_{t+1} = Ax_t + B(u_t^+ - u_t^-), \quad x_0 = x^{init}, \quad x_T = x^{final}$$

**BÆ°á»›c 2: Dual variables**
- $$\lambda_t$$: dual cho dynamics constraints
- $$\mu_t^+, \mu_t^-$$: dual cho control bounds

**BÆ°á»›c 3: Bang-bang property**
Optimal control cÃ³ dáº¡ng $$u_t^* \in \{-u^{max}, 0, u^{max}\}$$ cho má»—i component.

**BÆ°á»›c 4: Switching condition**
$$u_t^*$$ switches khi $$B^T \lambda_t$$ Ä‘á»•i dáº¥u.

</details>

---

## ğŸ“ **BÃ i táº­p 16: Sparse signal recovery**

**Äá» bÃ i:** (BÃ i 5.14 tá»« Boyd & Vandenberghe)
$$\min_x \lVert x \rVert_1 \quad \text{s.t.} \quad Ax = b$$

vá»›i $$A \in \mathbb{R}^{m \times n}$$, $$m < n$$.

**YÃªu cáº§u:**
1. Reformulate thÃ nh LP
2. Derive dual problem
3. PhÃ¢n tÃ­ch uniqueness conditions
4. So sÃ¡nh vá»›i $$\ell_0$$ minimization

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: LP formulation**
$$\min_{x,t} \mathbf{1}^T t \quad \text{s.t.} \quad Ax = b, \quad -t \leq x \leq t$$

**BÆ°á»›c 2: Dual problem**
$$\max_{\nu,\lambda,\mu} b^T \nu$$
$$\text{s.t.} \quad A^T \nu + \lambda - \mu = 0, \quad \lambda + \mu = \mathbf{1}, \quad \lambda,\mu \geq 0$$

TÆ°Æ¡ng Ä‘Æ°Æ¡ng:
$$\max_\nu b^T \nu \quad \text{s.t.} \quad \lVert A^T \nu \rVert_\infty \leq 1$$

**BÆ°á»›c 3: Complementary slackness**
Náº¿u $$|x_i^*| < t_i^*$$, thÃ¬ $$(A^T \nu^*)_i = 0$$.
Náº¿u $$x_i^* = t_i^*$$, thÃ¬ $$(A^T \nu^*)_i = 1$$.
Náº¿u $$x_i^* = -t_i^*$$, thÃ¬ $$(A^T \nu^*)_i = -1$$.

**BÆ°á»›c 4: Uniqueness**
Nghiá»‡m unique khi restricted isometry property (RIP) thá»a mÃ£n.

</details>

---

## ğŸ“ **BÃ i táº­p 17: Matrix completion**

**Äá» bÃ i:** (BÃ i 5.15 tá»« Boyd & Vandenberghe)
$$\min_X \text{tr}(X) \quad \text{s.t.} \quad X_{ij} = M_{ij}, \quad (i,j) \in \Omega, \quad X \succeq 0$$

vá»›i $$\Omega$$ lÃ  táº­p cÃ¡c entries Ä‘Ã£ biáº¿t.

**YÃªu cáº§u:**
1. XÃ¢y dá»±ng dual problem
2. PhÃ¢n tÃ­ch rank minimization connection
3. TÃ¬m Ä‘iá»u kiá»‡n recoverability
4. Implement alternating minimization

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: Dual problem**
$$\max_Y \sum_{(i,j) \in \Omega} M_{ij} Y_{ij}$$
$$\text{s.t.} \quad \mathcal{P}_\Omega(Y) + Z = I, \quad Z \succeq 0$$

vá»›i $$\mathcal{P}_\Omega(Y)_{ij} = Y_{ij}$$ náº¿u $$(i,j) \in \Omega$$, ngÆ°á»£c láº¡i $$= 0$$.

**BÆ°á»›c 2: Rank connection**
Nuclear norm $$\lVert X \rVert_* = \text{tr}(X)$$ lÃ  convex relaxation cá»§a $$\text{rank}(X)$$.

**BÆ°á»›c 3: Complementary slackness**
$$X^* Z^* = 0$$, nghÄ©a lÃ  $$\text{range}(X^*) \perp \text{range}(Z^*)$$.

**BÆ°á»›c 4: Recoverability condition**
Incoherence condition vÃ  sufficient sampling trong $$\Omega$$.

</details>

---

## ğŸ“ **BÃ i táº­p 18: Total variation denoising**

**Äá» bÃ i:** (BÃ i 5.16 tá»« Boyd & Vandenberghe)
$$\min_x \frac{1}{2}\lVert x - y \rVert_2^2 + \lambda \sum_{i=1}^{n-1} |x_{i+1} - x_i|$$

**YÃªu cáº§u:**
1. Reformulate thÃ nh QP
2. Derive dual problem
3. PhÃ¢n tÃ­ch piecewise linear solution
4. Implement ADMM algorithm

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: QP formulation**
Äáº·t $$z_i = x_{i+1} - x_i$$:
$$\min_{x,z} \frac{1}{2}\lVert x - y \rVert_2^2 + \lambda \lVert z \rVert_1$$
$$\text{s.t.} \quad Dx = z$$

vá»›i $$D$$ lÃ  difference matrix.

**BÆ°á»›c 2: Dual problem**
$$\max_\nu -\frac{1}{2}\lVert y - D^T \nu \rVert_2^2 + y^T D^T \nu$$
$$\text{s.t.} \quad \lVert \nu \rVert_\infty \leq \lambda$$

**BÆ°á»›c 3: Piecewise linear**
Solution $$x^*$$ lÃ  piecewise linear vá»›i breakpoints táº¡i cÃ¡c Ä‘iá»ƒm mÃ  $$|\nu_i^*| = \lambda$$.

**BÆ°á»›c 4: ADMM**
Alternating minimization giá»¯a $$x$$ vÃ  $$z$$ vá»›i augmented Lagrangian.

</details>

---

## ğŸ“ **BÃ i táº­p 19: Facility location**

**Äá» bÃ i:** (BÃ i 5.17 tá»« Boyd & Vandenberghe)
$$\min_{x,y} \sum_{i=1}^m f_i y_i + \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij}$$
$$\text{s.t.} \quad \sum_{j=1}^n x_{ij} \leq M_i y_i, \quad \sum_{i=1}^m x_{ij} = d_j, \quad x_{ij} \geq 0, \quad y_i \in \{0,1\}$$

**YÃªu cáº§u:**
1. LP relaxation analysis
2. Lagrangian dual formulation
3. Subgradient method implementation
4. Branch-and-bound algorithm

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: LP relaxation**
Relax $$y_i \in [0,1]$$:
$$\min_{x,y} \sum_{i=1}^m f_i y_i + \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij}$$
$$\text{s.t.} \quad \sum_{j=1}^n x_{ij} \leq M_i y_i, \quad \sum_{i=1}^m x_{ij} = d_j, \quad x,y \geq 0, \quad y \leq 1$$

**BÆ°á»›c 2: Lagrangian dual**
Dualize demand constraints:
$$L(x,y,\lambda) = \sum_{i=1}^m f_i y_i + \sum_{i,j} c_{ij} x_{ij} + \sum_j \lambda_j (d_j - \sum_i x_{ij})$$

**BÆ°á»›c 3: Dual function**
$$g(\lambda) = \sum_j \lambda_j d_j + \sum_i \min\{0, f_i + \sum_j (c_{ij} - \lambda_j) M_i / M_i\}$$

**BÆ°á»›c 4: Subgradient**
$$\partial g(\lambda) = d - \sum_i x_i^*(\lambda)$$

</details>

---

## ğŸ“ **BÃ i táº­p 20: Optimal power flow**

**Äá» bÃ i:** (BÃ i 5.18 tá»« Boyd & Vandenberghe)
$$\min_p \sum_{i=1}^n C_i(p_i)$$
$$\text{s.t.} \quad \sum_{i=1}^n p_i = \sum_{j=1}^m d_j, \quad |f_k| \leq f_k^{max}, \quad p_i^{min} \leq p_i \leq p_i^{max}$$

vá»›i $$f_k$$ lÃ  power flow trÃªn line $$k$$.

**YÃªu cáº§u:**
1. DC power flow approximation
2. Lagrangian dual formulation
3. Economic dispatch interpretation
4. Locational marginal pricing

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i chi tiáº¿t</strong></summary>

**BÆ°á»›c 1: DC approximation**
$$f_k = \sum_{i=1}^n H_{ki} p_i$$

vá»›i $$H$$ lÃ  power transfer distribution factor matrix.

**BÆ°á»›c 2: Reformulated problem**
$$\min_p \sum_{i=1}^n C_i(p_i)$$
$$\text{s.t.} \quad \mathbf{1}^T p = \mathbf{1}^T d, \quad -f^{max} \leq Hp \leq f^{max}, \quad p^{min} \leq p \leq p^{max}$$

**BÆ°á»›c 3: Lagrangian**
$$L(p,\lambda,\mu,\nu) = \sum_i C_i(p_i) + \lambda(\mathbf{1}^T d - \mathbf{1}^T p) + \mu^T(Hp - f^{max}) + \nu^T(-f^{max} - Hp)$$

**BÆ°á»›c 4: Optimality**
$$C_i'(p_i^*) = \lambda^* - (H^T(\mu^* - \nu^*))_i + \text{dual variables cho bounds}$$

$$\lambda^*$$ lÃ  system marginal price, $$(H^T(\mu^* - \nu^*))_i$$ lÃ  congestion component.

</details>

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 5: Duality
   - Exercises 5.1-5.39

2. **Rockafellar, R. T.** (1970). *Convex Analysis*. Princeton University Press.

3. **Bertsekas, D. P.** (1999). *Nonlinear Programming*. Athena Scientific.

---

## ğŸ’¡ Máº¹o Thá»±c HÃ nh

#### **Khi xÃ¢y dá»±ng bÃ i toÃ¡n Ä‘á»‘i ngáº«u:**
- LuÃ´n kiá»ƒm tra tÃ­nh convex cá»§a bÃ i toÃ¡n nguyÃªn thá»§y
- XÃ¡c Ä‘á»‹nh rÃµ cÃ¡c rÃ ng buá»™c báº¥t Ä‘áº³ng thá»©c vÃ  Ä‘áº³ng thá»©c
- ChÃº Ã½ dáº¥u cá»§a dual variables: $$u \geq 0$$ cho báº¥t Ä‘áº³ng thá»©c, $$v$$ free cho Ä‘áº³ng thá»©c

#### **Khi tÃ­nh hÃ m Ä‘á»‘i ngáº«u:**
- Sá»­ dá»¥ng Ä‘iá»u kiá»‡n $$\nabla_x L(x,u,v) = 0$$ Ä‘á»ƒ tÃ¬m $$x^*(u,v)$$
- Thay ngÆ°á»£c láº¡i vÃ o Lagrangian Ä‘á»ƒ Ä‘Æ°á»£c $$g(u,v)$$
- Kiá»ƒm tra Ä‘iá»u kiá»‡n Ä‘á»ƒ $$g(u,v) > -\infty$$

#### **Khi phÃ¢n tÃ­ch KKT conditions:**
- Complementary slackness: $$u_i h_i(x^*) = 0$$
- Dual feasibility: $$u \geq 0$$
- Primal feasibility: $$h_i(x^*) \leq 0$$, $$l_j(x^*) = 0$$

