---
layout: post
title: 11-9 B√†i T·∫≠p Th·ª±c H√†nh N√¢ng Cao - T√≠nh ƒê·ªëi Ng·∫´u T·ªïng Qu√°t
chapter: '11'
order: 10
owner: GitHub Copilot
lang: vi
categories:
- chapter11
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh N√¢ng Cao - T√≠nh ƒê·ªëi Ng·∫´u T·ªïng Qu√°t

## üìù **B√†i t·∫≠p 1: Lagrangian Construction v√† Dual Function**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Chapter 11)
L·∫≠p tr√¨nh ph∆∞∆°ng ph√°p c√≥ h·ªá th·ªëng ƒë·ªÉ x√¢y d·ª±ng Lagrangian v√† t√≠nh h√†m ƒë·ªëi ng·∫´u:

a) **QP ƒë∆°n gi·∫£n:** $$\min_x \frac{1}{2}x^T Q x + c^T x$$ s.t. $$Ax \leq b$$, $$x \geq 0$$
b) **T·ªëi ∆∞u danh m·ª•c ƒë·∫ßu t∆∞:** $$\min_x \frac{1}{2}x^T \Sigma x$$ s.t. $$\mu^T x \geq r$$, $$\mathbf{1}^T x = 1$$, $$x \geq 0$$
c) **X√¢y d·ª±ng ƒë·ªëi ng·∫´u SVM:** $$\min_w \frac{1}{2}\|w\|^2$$ s.t. $$y_i(w^T x_i + b) \geq 1$$

**Y√™u c·∫ßu:**
1. X√¢y d·ª±ng Lagrangian c√≥ h·ªá th·ªëng
2. T√≠nh to√°n h√†m ƒë·ªëi ng·∫´u
3. Ki·ªÉm ch·ª©ng t√≠nh ƒë·ªëi ng·∫´u m·∫°nh
4. L·∫≠p tr√¨nh s·ªë v√† tr·ª±c quan h√≥a

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Framework cho Lagrangian construction**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize, linprog
import cvxpy as cp

class DualityAnalyzer:
    def __init__(self, problem_name):
        self.problem_name = problem_name
        self.primal_optimal = None
        self.dual_optimal = None
        self.duality_gap = None
        
    def construct_lagrangian(self, objective, eq_constraints=None, ineq_constraints=None):
        """
        Construct Lagrangian symbolically
        """
        print(f"Lagrangian Construction for {self.problem_name}")
        print("=" * 50)
        
        print("L(x, ŒΩ, Œª) = f(x)", end="")
        
        if eq_constraints:
            print(" + Œ£ ŒΩ‚±º l‚±º(x)", end="")
            
        if ineq_constraints:
            print(" + Œ£ Œª·µ¢ h·µ¢(x)", end="")
            
        print()
        print("where:")
        print("- ŒΩ ‚àà ‚Ñù ≥ (unrestricted for equality constraints)")
        print("- Œª ‚àà ‚Ñù·µê, Œª ‚â• 0 (non-negative for inequality constraints)")
        
    def solve_simple_qp(self):
        """Solve simple QP and its dual"""
        
        # Problem: min (1/2)x^T Q x + c^T x s.t. Ax ‚â§ b, x ‚â• 0
        Q = np.array([[2, 1], [1, 2]])
        c = np.array([1, 1])
        A = np.array([[1, 1], [1, -1]])
        b = np.array([1, 0])
        
        print("\nSimple QP Problem:")
        print("min (1/2)x^T Q x + c^T x")
        print("s.t. Ax ‚â§ b, x ‚â• 0")
        print(f"Q = \n{Q}")
        print(f"c = {c}")
        print(f"A = \n{A}")
        print(f"b = {b}")
        
        # Solve primal using CVXPY
        x = cp.Variable(2)
        objective = cp.Minimize(0.5 * cp.quad_form(x, Q) + c.T @ x)
        constraints = [A @ x <= b, x >= 0]
        
        prob = cp.Problem(objective, constraints)
        prob.solve()
        
        x_primal = x.value
        f_primal = prob.value
        
        print(f"\nPrimal Solution:")
        print(f"x* = {x_primal}")
        print(f"f* = {f_primal:.6f}")
        
        # Construct dual manually
        print(f"\nDual Construction:")
        print("L(x,Œª,Œº) = (1/2)x^T Q x + c^T x + Œª^T(Ax - b) - Œº^T x")
        print("g(Œª,Œº) = min_x L(x,Œª,Œº)")
        
        # For QP: ‚àá_x L = Qx + c + A^T Œª - Œº = 0
        # So x*(Œª,Œº) = Q^(-1)(Œº - c - A^T Œª)
        
        def dual_function(lam, mu):
            """Compute dual function value"""
            if np.any(lam < 0) or np.any(mu < 0):
                return -np.inf
                
            # x*(Œª,Œº) = Q^(-1)(Œº - c - A^T Œª)
            Q_inv = np.linalg.inv(Q)
            x_star = Q_inv @ (mu - c - A.T @ lam)
            
            # g(Œª,Œº) = L(x*(Œª,Œº), Œª, Œº)
            g_val = (0.5 * x_star.T @ Q @ x_star + c.T @ x_star + 
                    lam.T @ (A @ x_star - b) - mu.T @ x_star)
            
            return g_val
        
        # Solve dual problem: max g(Œª,Œº) s.t. Œª,Œº ‚â• 0
        def neg_dual_objective(vars):
            lam = vars[:2]
            mu = vars[2:]
            return -dual_function(lam, mu)
        
        # Initial guess
        x0 = np.array([0.1, 0.1, 0.1, 0.1])
        bounds = [(0, None)] * 4
        
        result_dual = minimize(neg_dual_objective, x0, bounds=bounds, method='L-BFGS-B')
        
        if result_dual.success:
            lam_dual = result_dual.x[:2]
            mu_dual = result_dual.x[2:]
            g_dual = -result_dual.fun
            
            print(f"\nDual Solution:")
            print(f"Œª* = {lam_dual}")
            print(f"Œº* = {mu_dual}")
            print(f"g* = {g_dual:.6f}")
            print(f"Duality gap = {f_primal - g_dual:.8f}")
        
        return x_primal, f_primal, lam_dual, mu_dual, g_dual

# Run QP example
analyzer = DualityAnalyzer("Simple QP")
analyzer.construct_lagrangian("(1/2)x^T Q x + c^T x", 
                             ineq_constraints=["Ax - b ‚â§ 0", "-x ‚â§ 0"])
x_opt, f_opt, lam_opt, mu_opt, g_opt = analyzer.solve_simple_qp()
```

**B∆∞·ªõc 2: Portfolio optimization**

```python
def solve_portfolio_optimization():
    """Solve portfolio optimization and analyze duality"""
    
    # Generate synthetic data
    np.random.seed(42)
    n_assets = 4
    
    # Expected returns
    mu = np.array([0.1, 0.12, 0.14, 0.08])
    
    # Covariance matrix (positive definite)
    A = np.random.randn(n_assets, n_assets)
    Sigma = A.T @ A + 0.1 * np.eye(n_assets)
    
    # Target return
    r_target = 0.11
    
    print("Portfolio Optimization Problem:")
    print("min (1/2) x^T Œ£ x")
    print("s.t. Œº^T x ‚â• r, 1^T x = 1, x ‚â• 0")
    print(f"Œº = {mu}")
    print(f"r_target = {r_target}")
    
    # Solve primal
    x = cp.Variable(n_assets)
    objective = cp.Minimize(0.5 * cp.quad_form(x, Sigma))
    constraints = [
        mu.T @ x >= r_target,  # Return constraint
        np.ones(n_assets).T @ x == 1,  # Budget constraint
        x >= 0  # Long-only
    ]
    
    prob = cp.Problem(objective, constraints)
    prob.solve()
    
    x_portfolio = x.value
    risk_primal = prob.value
    
    print(f"\nPrimal Solution:")
    print(f"Optimal portfolio: {x_portfolio}")
    print(f"Portfolio risk: {risk_primal:.6f}")
    print(f"Portfolio return: {mu.T @ x_portfolio:.6f}")
    
    # Analyze dual
    print(f"\nDual Analysis:")
    print("L(x,Œª,ŒΩ,Œº) = (1/2)x^T Œ£ x - Œª(Œº^T x - r) + ŒΩ(1^T x - 1) - Œº^T x")
    print("Optimality: Œ£x* - ŒªŒº + ŒΩ1 - Œº = 0")
    print("So: x* = Œ£^(-1)(ŒªŒº - ŒΩ1 + Œº)")
    
    # Get dual variables from CVXPY
    lambda_dual = constraints[0].dual_value  # Return constraint
    nu_dual = constraints[1].dual_value      # Budget constraint
    mu_dual = constraints[2].dual_value      # Non-negativity
    
    print(f"Œª* (return price) = {lambda_dual:.6f}")
    print(f"ŒΩ* (budget price) = {nu_dual:.6f}")
    print(f"Œº* (non-negativity) = {mu_dual}")
    
    # Economic interpretation
    print(f"\nEconomic Interpretation:")
    print(f"- Œª* = {lambda_dual:.6f}: Marginal cost of increasing return requirement")
    print(f"- ŒΩ* = {nu_dual:.6f}: Marginal cost of budget constraint")
    
    return x_portfolio, risk_primal, lambda_dual, nu_dual

# Run portfolio example
portfolio_result = solve_portfolio_optimization()
```

**B∆∞·ªõc 3: SVM dual derivation**

```python
def derive_svm_dual():
    """Derive and solve SVM dual problem"""
    
    # Generate synthetic 2D data
    np.random.seed(42)
    n_samples = 20
    
    # Class 1: centered at (-1, -1)
    X1 = np.random.randn(n_samples//2, 2) * 0.5 + np.array([-1, -1])
    y1 = np.ones(n_samples//2)
    
    # Class 2: centered at (1, 1)  
    X2 = np.random.randn(n_samples//2, 2) * 0.5 + np.array([1, 1])
    y2 = -np.ones(n_samples//2)
    
    X = np.vstack([X1, X2])
    y = np.hstack([y1, y2])
    
    print("SVM Problem:")
    print("Primal: min (1/2)||w||¬≤ s.t. y·µ¢(w^T x·µ¢ + b) ‚â• 1")
    print("Dual: max Œ£Œ±·µ¢ - (1/2)Œ£Œ£Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºx·µ¢^T x‚±º s.t. Œ£Œ±·µ¢y·µ¢ = 0, Œ±·µ¢ ‚â• 0")
    
    # Solve using CVXPY
    # Primal formulation
    w = cp.Variable(2)
    b = cp.Variable()
    
    objective = cp.Minimize(0.5 * cp.sum_squares(w))
    constraints = [cp.multiply(y, X @ w + b) >= 1]
    
    prob_primal = cp.Problem(objective, constraints)
    prob_primal.solve()
    
    w_primal = w.value
    b_primal = b.value
    primal_obj = prob_primal.value
    
    print(f"\nPrimal Solution:")
    print(f"w* = {w_primal}")
    print(f"b* = {b_primal:.6f}")
    print(f"Primal objective = {primal_obj:.6f}")
    
    # Dual formulation
    alpha = cp.Variable(n_samples)
    
    # Gram matrix
    K = X @ X.T
    
    dual_obj = cp.Maximize(cp.sum(alpha) - 0.5 * cp.quad_form(cp.multiply(y, alpha), K))
    dual_constraints = [
        cp.sum(cp.multiply(y, alpha)) == 0,
        alpha >= 0
    ]
    
    prob_dual = cp.Problem(dual_obj, dual_constraints)
    prob_dual.solve()
    
    alpha_dual = alpha.value
    dual_objective = prob_dual.value
    
    print(f"\nDual Solution:")
    print(f"Œ±* = {alpha_dual}")
    print(f"Dual objective = {dual_objective:.6f}")
    print(f"Duality gap = {abs(primal_obj - dual_objective):.8f}")
    
    # Recover primal variables from dual
    w_recovered = np.sum((alpha_dual * y)[:, np.newaxis] * X, axis=0)
    
    # Find support vectors (Œ± > 0)
    support_vectors = alpha_dual > 1e-6
    sv_indices = np.where(support_vectors)[0]
    
    print(f"\nSupport Vectors:")
    print(f"Number of SVs: {len(sv_indices)}")
    print(f"SV indices: {sv_indices}")
    print(f"Œ± values for SVs: {alpha_dual[support_vectors]}")
    
    # Visualization
    plt.figure(figsize=(12, 5))
    
    # Plot 1: Data and decision boundary
    plt.subplot(1, 2, 1)
    
    # Plot data points
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='o', label='Class +1')
    plt.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', marker='s', label='Class -1')
    
    # Highlight support vectors
    plt.scatter(X[sv_indices, 0], X[sv_indices, 1], 
               s=200, facecolors='none', edgecolors='black', linewidth=2, label='Support Vectors')
    
    # Plot decision boundary
    if w_primal is not None:
        x_min, x_max = plt.xlim()
        y_min, y_max = plt.ylim()
        
        xx = np.linspace(x_min, x_max, 100)
        yy = -(w_primal[0] * xx + b_primal) / w_primal[1]
        
        plt.plot(xx, yy, 'k-', linewidth=2, label='Decision Boundary')
        
        # Plot margins
        yy_margin_pos = -(w_primal[0] * xx + b_primal - 1) / w_primal[1]
        yy_margin_neg = -(w_primal[0] * xx + b_primal + 1) / w_primal[1]
        
        plt.plot(xx, yy_margin_pos, 'k--', alpha=0.5, label='Margin')
        plt.plot(xx, yy_margin_neg, 'k--', alpha=0.5)
    
    plt.xlabel('x‚ÇÅ')
    plt.ylabel('x‚ÇÇ')
    plt.title('SVM Classification')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Dual variables
    plt.subplot(1, 2, 2)
    plt.bar(range(n_samples), alpha_dual, alpha=0.7)
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    plt.xlabel('Sample Index')
    plt.ylabel('Œ± value')
    plt.title('Dual Variables (Lagrange Multipliers)')
    plt.grid(True, alpha=0.3)
    
    # Highlight support vectors
    for idx in sv_indices:
        plt.bar(idx, alpha_dual[idx], color='red', alpha=0.8)
    
    plt.tight_layout()
    plt.show()
    
    return w_primal, b_primal, alpha_dual, sv_indices

# Run SVM example
svm_result = derive_svm_dual()
```

**B∆∞·ªõc 4: General framework cho duality analysis**

```python
class GeneralDualityFramework:
    def __init__(self):
        self.problems = {}
        
    def add_problem(self, name, primal_solver, dual_solver=None):
        """Add a problem to analyze"""
        self.problems[name] = {
            'primal': primal_solver,
            'dual': dual_solver
        }
    
    def analyze_strong_duality(self, problem_name):
        """Check strong duality conditions"""
        print(f"\nStrong Duality Analysis for {problem_name}:")
        print("=" * 40)
        
        print("Conditions for Strong Duality:")
        print("1. Primal problem is convex")
        print("2. Slater's condition: ‚àÉ x such that h·µ¢(x) < 0 ‚àÄi, l‚±º(x) = 0 ‚àÄj")
        print("3. Alternative: Linear constraints (LICQ)")
        print("4. Alternative: Quadratic with positive definite Hessian")
        
    def visualize_duality_gap(self, primal_values, dual_values, iterations):
        """Visualize convergence of duality gap"""
        
        gaps = np.array(primal_values) - np.array(dual_values)
        
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 3, 1)
        plt.semilogy(iterations, primal_values, 'b-', label='Primal', linewidth=2)
        plt.semilogy(iterations, dual_values, 'r-', label='Dual', linewidth=2)
        plt.xlabel('Iteration')
        plt.ylabel('Objective Value')
        plt.title('Primal-Dual Convergence')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 3, 2)
        plt.semilogy(iterations, gaps, 'g-', linewidth=2)
        plt.xlabel('Iteration')
        plt.ylabel('Duality Gap')
        plt.title('Duality Gap Convergence')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 3, 3)
        plt.loglog(iterations[1:], gaps[1:], 'purple', linewidth=2)
        plt.xlabel('Iteration')
        plt.ylabel('Duality Gap')
        plt.title('Convergence Rate (Log-Log)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # Estimate convergence rate
        if len(gaps) > 10:
            log_gaps = np.log(gaps[-10:])
            log_iters = np.log(iterations[-10:])
            rate = np.polyfit(log_iters, log_gaps, 1)[0]
            print(f"Estimated convergence rate: O(k^{rate:.2f})")

# Create framework and analyze
framework = GeneralDualityFramework()
framework.analyze_strong_duality("Quadratic Programming")

# D·ªØ li·ªáu h·ªôi t·ª• m·∫´u
iterations = np.arange(1, 101)
primal_vals = 10 * np.exp(-0.1 * iterations) + 5
dual_vals = 5 - 8 * np.exp(-0.15 * iterations)

framework.visualize_duality_gap(primal_vals, dual_vals, iterations)
```

</details>

---

## üìù **B√†i t·∫≠p 2: Strong Duality v√† Slater's Condition**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Section 11.4)
Ph√¢n t√≠ch ƒëi·ªÅu ki·ªán ƒë·ªëi ng·∫´u m·∫°nh v√† l·∫≠p tr√¨nh ki·ªÉm ch·ª©ng:

a) **Ki·ªÉm ch·ª©ng ƒëi·ªÅu ki·ªán Slater** cho c√°c b√†i to√°n kh√°c nhau
b) **V√≠ d·ª• ph·∫£n ch·ª©ng** khi ƒë·ªëi ng·∫´u m·∫°nh kh√¥ng th·ªèa m√£n
c) **Ph√¢n t√≠ch ƒëi·ªÅu ki·ªán ƒë·ªß r√†ng bu·ªôc**
d) **Gi·∫£i th√≠ch h√¨nh h·ªçc** c·ªßa khe ƒë·ªëi ng·∫´u

**Y√™u c·∫ßu:**
1. Ki·ªÉm tra ƒëi·ªÅu ki·ªán Slater c√≥ h·ªá th·ªëng
2. X√¢y d·ª±ng c√°c v√≠ d·ª• ph·∫£n ch·ª©ng
3. Tr·ª±c quan h√≥a h√¨nh h·ªçc
4. H√†m √Ω th·ª±c ti·ªÖn

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Slater's condition verification framework**

```python
class SlaterConditionChecker:
    def __init__(self):
        self.problems = []
        
    def check_slater_condition(self, ineq_constraints, eq_constraints=None, 
                              domain=None, problem_name="Unknown"):
        """
        Check Slater's condition for a given problem
        """
        print(f"Slater's Condition Analysis: {problem_name}")
        print("=" * 50)
        
        print("Slater's Condition Requirements:")
        print("1. Problem must be convex")
        print("2. ‚àÉ x ‚àà relint(dom f) such that:")
        print("   - h·µ¢(x) < 0 for all i (strict inequality)")
        print("   - l‚±º(x) = 0 for all j (equality constraints)")
        
        # Try to find a Slater point
        return self._find_slater_point(ineq_constraints, eq_constraints, domain)
    
    def _find_slater_point(self, ineq_constraints, eq_constraints, domain):
        """Attempt to find a point satisfying Slater's condition"""
        
        # This is a simplified implementation
        # In practice, this would involve solving a feasibility problem
        
        print("\nSearching for Slater point...")
        
        # V√≠ d·ª•: th·ª≠ c√°c ƒëi·ªÉm ng·∫´u nhi√™n trong mi·ªÅn
        n_trials = 1000
        found_slater = False
        
        for trial in range(n_trials):
            # Generate random point (problem-specific)
            if domain == "R2":
                x = np.random.randn(2) * 2
            elif domain == "R+":
                x = np.random.rand(2) * 5
            else:
                x = np.random.randn(2)
            
            # Check constraints
            ineq_satisfied = True
            eq_satisfied = True
            
            # Check inequality constraints (need strict satisfaction)
            for constraint in ineq_constraints:
                if constraint(x) >= 0:  # Should be < 0
                    ineq_satisfied = False
                    break
            
            # Check equality constraints
            if eq_constraints:
                for constraint in eq_constraints:
                    if abs(constraint(x)) > 1e-6:  # Should be = 0
                        eq_satisfied = False
                        break
            
            if ineq_satisfied and eq_satisfied:
                print(f"‚úì Slater point found: x = {x}")
                print(f"  Constraint values:")
                for i, constraint in enumerate(ineq_constraints):
                    print(f"    h_{i}(x) = {constraint(x):.6f} < 0")
                found_slater = True
                break
        
        if not found_slater:
            print("‚úó No Slater point found in random search")
            print("  This suggests Slater's condition may not hold")
        
        return found_slater

# V√≠ d·ª• 1: QP ƒë∆°n gi·∫£n (ƒëi·ªÅu ki·ªán Slater th·ªèa m√£n)
def example_slater_holds():
    """V√≠ d·ª• khi ƒëi·ªÅu ki·ªán Slater th·ªèa m√£n"""
    
    print("Example 1: QP with Slater's Condition")
    print("min (1/2)(x‚ÇÅ¬≤ + x‚ÇÇ¬≤) s.t. x‚ÇÅ + x‚ÇÇ ‚â§ 1, x‚ÇÅ ‚â• 0, x‚ÇÇ ‚â• 0")
    
    # Define constraints: h·µ¢(x) ‚â§ 0
    def h1(x): return x[0] + x[1] - 1      # x‚ÇÅ + x‚ÇÇ ‚â§ 1
    def h2(x): return -x[0]                # x‚ÇÅ ‚â• 0
    def h3(x): return -x[1]                # x‚ÇÇ ‚â• 0
    
    constraints = [h1, h2, h3]
    
    checker = SlaterConditionChecker()
    slater_holds = checker.check_slater_condition(constraints, domain="R+", 
                                                 problem_name="Simple QP")
    
    # Solve the problem to verify strong duality
    x = cp.Variable(2)
    objective = cp.Minimize(0.5 * cp.sum_squares(x))
    cvx_constraints = [x[0] + x[1] <= 1, x >= 0]
    
    prob = cp.Problem(objective, cvx_constraints)
    prob.solve()
    
    primal_opt = prob.value
    
    # Check duality gap
    dual_vars = [c.dual_value for c in cvx_constraints]
    print(f"\nPrimal optimal: {primal_opt:.6f}")
    print(f"Dual variables: {dual_vars}")
    print(f"Strong duality verified: {prob.status == 'optimal'}")
    
    return slater_holds

# Example 2: Problem where Slater's condition fails
def example_slater_fails():
    """Example where Slater's condition fails"""
    
    print("\n" + "="*60)
    print("Example 2: Problem without Slater's Condition")
    print("min x s.t. x¬≤ ‚â§ 0")
    print("(Only feasible point is x = 0, no strict feasibility)")
    
    def h1(x): return x[0]**2  # x¬≤ ‚â§ 0
    
    constraints = [h1]
    
    checker = SlaterConditionChecker()
    slater_holds = checker.check_slater_condition(constraints, domain="R", 
                                                 problem_name="Degenerate Problem")
    
    # This problem has duality gap
    print("\nAnalysis:")
    print("- Primal optimal: x* = 0, f* = 0")
    print("- Dual problem: max g(Œª) = min_x {x + Œªx¬≤}")
    print("- If Œª > 0: g(Œª) = -‚àû (unbounded below)")
    print("- If Œª = 0: g(Œª) = -‚àû (unbounded below)")  
    print("- If Œª < 0: g(Œª) = -1/(4Œª)")
    print("- Dual optimal: g* = 0 (taking Œª ‚Üí 0‚Åª)")
    print("- Duality gap: f* - g* = 0 (actually no gap in this case)")
    
    return slater_holds

# Example 3: Geometric interpretation
def geometric_duality_interpretation():
    """Visualize geometric interpretation of duality"""
    
    print("\n" + "="*60)
    print("Geometric Interpretation of Duality")
    
    # Problem: min x‚ÇÅ s.t. x‚ÇÅ¬≤ + x‚ÇÇ¬≤ ‚â§ 1
    
    # Create visualization
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Plot 1: Feasible region and optimal point
    ax1 = axes[0]
    theta = np.linspace(0, 2*np.pi, 100)
    x1_circle = np.cos(theta)
    x2_circle = np.sin(theta)
    
    ax1.fill(x1_circle, x2_circle, alpha=0.3, color='lightblue', label='Feasible region')
    ax1.plot(x1_circle, x2_circle, 'b-', linewidth=2)
    
    # Optimal point
    ax1.plot(-1, 0, 'ro', markersize=10, label='Optimal point')
    
    # Objective function contours
    x1_range = np.linspace(-1.5, 1.5, 100)
    for c in [-1, -0.5, 0, 0.5]:
        ax1.axvline(x=c, color='green', alpha=0.5, linestyle='--')
    
    ax1.set_xlim(-1.5, 1.5)
    ax1.set_ylim(-1.5, 1.5)
    ax1.set_xlabel('x‚ÇÅ')
    ax1.set_ylabel('x‚ÇÇ')
    ax1.set_title('Primal Problem')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_aspect('equal')
    
    # Plot 2: Dual function
    ax2 = axes[1]
    lambda_vals = np.linspace(0.01, 2, 100)
    
    # Dual function: g(Œª) = min_x {x‚ÇÅ + Œª(x‚ÇÅ¬≤ + x‚ÇÇ¬≤ - 1)}
    # = min_x {x‚ÇÅ + Œªx‚ÇÅ¬≤ + Œªx‚ÇÇ¬≤ - Œª}
    # Minimizing over x‚ÇÇ: optimal x‚ÇÇ = 0
    # Minimizing over x‚ÇÅ: optimal x‚ÇÅ = -1/(2Œª)
    # g(Œª) = -1/(2Œª) + Œª(-1/(2Œª))¬≤ - Œª = -1/(2Œª) - 1/(4Œª) - Œª = -3/(4Œª) - Œª
    
    g_vals = -3/(4*lambda_vals) - lambda_vals
    
    ax2.plot(lambda_vals, g_vals, 'r-', linewidth=2, label='g(Œª)')
    ax2.axhline(y=-1, color='blue', linestyle='--', label='Primal optimal')
    
    # Find dual optimal
    optimal_idx = np.argmax(g_vals)
    ax2.plot(lambda_vals[optimal_idx], g_vals[optimal_idx], 'ro', markersize=8, label='Dual optimal')
    
    ax2.set_xlabel('Œª')
    ax2.set_ylabel('g(Œª)')
    ax2.set_title('Dual Function')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Duality gap evolution
    ax3 = axes[2]
    
    # Show how duality gap closes as we approach optimal Œª
    lambda_range = np.linspace(0.1, 2, 50)
    primal_val = -1  # Known optimal value
    dual_vals = -3/(4*lambda_range) - lambda_range
    gaps = primal_val - dual_vals
    
    ax3.semilogy(lambda_range, np.maximum(gaps, 1e-10), 'purple', linewidth=2)
    ax3.set_xlabel('Œª')
    ax3.set_ylabel('Duality Gap')
    ax3.set_title('Duality Gap vs Œª')
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Analytical solution
    optimal_lambda = np.sqrt(3/4)
    optimal_dual = -3/(4*optimal_lambda) - optimal_lambda
    
    print(f"Analytical Results:")
    print(f"Optimal Œª* = ‚àö(3/4) = {optimal_lambda:.6f}")
    print(f"Dual optimal g* = {optimal_dual:.6f}")
    print(f"Primal optimal f* = -1")
    print(f"Duality gap = {-1 - optimal_dual:.8f}")

# Run examples
example1_result = example_slater_holds()
example2_result = example_slater_fails()
geometric_duality_interpretation()
```

**B∆∞·ªõc 2: Constraint qualifications analysis**

```python
def analyze_constraint_qualifications():
    """Analyze different constraint qualifications"""
    
    print("Constraint Qualifications Analysis")
    print("=" * 40)
    
    qualifications = {
        "Slater's Condition": {
            "description": "‚àÉ x: h·µ¢(x) < 0 ‚àÄi, l‚±º(x) = 0 ‚àÄj",
            "applies_to": "Convex problems",
            "strength": "Sufficient for strong duality"
        },
        
        "LICQ (Linear Independence CQ)": {
            "description": "‚àáh·µ¢(x*), ‚àál‚±º(x*) are linearly independent",
            "applies_to": "Smooth problems",
            "strength": "Necessary for KKT conditions"
        },
        
        "MFCQ (Mangasarian-Fromovitz CQ)": {
            "description": "‚àál‚±º(x*) lin. indep., ‚àÉd: ‚àáh·µ¢(x*)·µÄd < 0",
            "applies_to": "Smooth problems",
            "strength": "Weaker than LICQ"
        },
        
        "CPLD (Constant Positive Linear Dependence)": {
            "description": "Technical condition on constraint gradients",
            "applies_to": "General smooth problems", 
            "strength": "Very weak"
        }
    }
    
    for name, info in qualifications.items():
        print(f"\n{name}:")
        print(f"  Description: {info['description']}")
        print(f"  Applies to: {info['applies_to']}")
        print(f"  Strength: {info['strength']}")
    
    # Example: LICQ failure
    print(f"\nExample: LICQ Failure")
    print("Problem: min x‚ÇÅ s.t. (x‚ÇÅ - 1)¬≥ ‚â§ 0, x‚ÇÇ = 0")
    print("At x* = (1, 0):")
    print("‚àáh(x*) = ‚àá((x‚ÇÅ-1)¬≥) = (0, 0) - not linearly independent!")
    print("‚àál(x*) = ‚àá(x‚ÇÇ) = (0, 1)")
    print("LICQ fails, but KKT conditions may still hold")

analyze_constraint_qualifications()
```

**B∆∞·ªõc 3: Practical implications**

```python
def practical_duality_implications():
    """Demonstrate practical implications of duality theory"""
    
    print("Practical Implications of Duality Theory")
    print("=" * 45)
    
    implications = [
        {
            "area": "Algorithm Design",
            "examples": [
                "Primal-dual interior point methods",
                "Dual decomposition for distributed optimization",
                "Augmented Lagrangian methods"
            ]
        },
        {
            "area": "Sensitivity Analysis", 
            "examples": [
                "Shadow prices from dual variables",
                "Perturbation analysis",
                "Parametric optimization"
            ]
        },
        {
            "area": "Problem Reformulation",
            "examples": [
                "Dual problem may be easier to solve",
                "Better numerical properties",
                "Parallel computation opportunities"
            ]
        },
        {
            "area": "Bounds and Approximation",
            "examples": [
                "Dual provides lower bounds",
                "Early stopping criteria",
                "Approximation quality assessment"
            ]
        }
    ]
    
    for impl in implications:
        print(f"\n{impl['area']}:")
        for example in impl['examples']:
            print(f"  ‚Ä¢ {example}")
    
    # Demonstrate sensitivity analysis
    print(f"\nSensitivity Analysis Example:")
    print("Consider: min x‚ÇÅ¬≤ + x‚ÇÇ¬≤ s.t. x‚ÇÅ + x‚ÇÇ ‚â• c")
    
    c_values = np.linspace(0, 2, 21)
    optimal_values = []
    dual_variables = []
    
    for c in c_values:
        x = cp.Variable(2)
        objective = cp.Minimize(cp.sum_squares(x))
        constraint = [x[0] + x[1] >= c]
        
        prob = cp.Problem(objective, constraint)
        prob.solve()
        
        if prob.status == 'optimal':
            optimal_values.append(prob.value)
            dual_variables.append(constraint[0].dual_value)
        else:
            optimal_values.append(np.inf)
            dual_variables.append(0)
    
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 3, 1)
    plt.plot(c_values, optimal_values, 'b-', linewidth=2)
    plt.xlabel('c (RHS parameter)')
    plt.ylabel('Optimal Value')
    plt.title('Optimal Value vs Parameter')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 3, 2)
    plt.plot(c_values, dual_variables, 'r-', linewidth=2)
    plt.xlabel('c (RHS parameter)')
    plt.ylabel('Dual Variable (Shadow Price)')
    plt.title('Shadow Price vs Parameter')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 3, 3)
    # Numerical derivative of optimal value
    numerical_derivative = np.gradient(optimal_values, c_values)
    plt.plot(c_values, numerical_derivative, 'g-', linewidth=2, label='‚àÇf*/‚àÇc')
    plt.plot(c_values, dual_variables, 'r--', linewidth=2, label='Œª* (dual var)')
    plt.xlabel('c (RHS parameter)')
    plt.ylabel('Derivative / Shadow Price')
    plt.title('Envelope Theorem Verification')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("Envelope Theorem: ‚àÇf*/‚àÇc = Œª* (shadow price)")
    print("The dual variable gives the sensitivity of optimal value to parameter changes")

practical_duality_implications()
```

</details>

---

## üìù **B√†i t·∫≠p 3: Primal-Dual Algorithms**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd & Vandenberghe, Applications)
L·∫≠p tr√¨nh c√°c thu·∫≠t to√°n primal-dual v√† ph√¢n t√≠ch h·ªôi t·ª•:

a) **Ph∆∞∆°ng ph√°p ƒëi·ªÉm trong primal-dual**
b) **ADMM (Ph∆∞∆°ng ph√°p nh√¢n t·ª≠ h∆∞·ªõng xen k·∫Ω)**
c) **Ph√¢n r√£ ƒë·ªëi ng·∫´u** cho t·ªëi ∆∞u ph√¢n t√°n
d) **Ph√¢n t√≠ch h·ªôi t·ª•** v√† so s√°nh

**Y√™u c·∫ßu:**
1. L·∫≠p tr√¨nh thu·∫≠t to√°n
2. Ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª•
3. Th√≠ nghi·ªám s·ªë
4. ·ª®ng d·ª•ng th·ª±c ti·ªÖn

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Primal-dual interior point method**

```python
class PrimalDualInteriorPoint:
    def __init__(self, tolerance=1e-6, max_iterations=100):
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'primal_obj': [],
            'dual_obj': [],
            'duality_gap': [],
            'primal_residual': [],
            'dual_residual': []
        }
    
    def solve_qp(self, Q, c, A, b):
        """
        Solve QP: min (1/2)x^T Q x + c^T x s.t. Ax ‚â§ b
        using primal-dual interior point method
        """
        
        n = len(c)  # Number of variables
        m = len(b)  # Number of constraints
        
        # Initialize variables
        x = np.ones(n)
        s = np.ones(m)  # Slack variables
        y = np.ones(m)  # Dual variables
        
        # Ensure initial point is feasible
        residual = A @ x + s - b
        if np.any(residual > 0):
            s = s + np.maximum(0, residual) + 0.1
        
        mu = 0.1  # Barrier parameter
        sigma = 0.1  # Centering parameter
        
        print("Primal-Dual Interior Point Method")
        print("=" * 40)
        print(f"{'Iter':<4} {'Primal Obj':<12} {'Dual Obj':<12} {'Gap':<12} {'Œº':<10}")
        print("-" * 60)
        
        for iteration in range(self.max_iter):
            # Compute residuals
            rd = Q @ x + c - A.T @ y  # Dual residual
            rp = A @ x + s - b        # Primal residual
            rc = s * y                # Complementarity residual
            
            # Compute objectives
            primal_obj = 0.5 * x.T @ Q @ x + c.T @ x
            dual_obj = -0.5 * x.T @ Q @ x + b.T @ y
            gap = primal_obj - dual_obj
            
            # Store history
            self.history['primal_obj'].append(primal_obj)
            self.history['dual_obj'].append(dual_obj)
            self.history['duality_gap'].append(gap)
            self.history['primal_residual'].append(np.linalg.norm(rp))
            self.history['dual_residual'].append(np.linalg.norm(rd))
            
            print(f"{iteration:<4} {primal_obj:<12.6f} {dual_obj:<12.6f} {gap:<12.6f} {mu:<10.6f}")
            
            # Check convergence
            if (np.linalg.norm(rd) < self.tol and 
                np.linalg.norm(rp) < self.tol and 
                gap < self.tol):
                print(f"Converged in {iteration} iterations")
                break
            
            # Update barrier parameter
            mu = sigma * np.dot(s, y) / m
            
            # Solve Newton system
            # [Q   -A^T  0 ] [Œîx]   [-rd]
            # [A    0    I ] [Œîy] = [-rp]
            # [0    S    Y ] [Œîs]   [-rc + œÉŒºe]
            
            S = np.diag(s)
            Y = np.diag(y)
            
            # Form KKT matrix
            KKT = np.block([
                [Q, -A.T, np.zeros((n, m))],
                [A, np.zeros((m, m)), np.eye(m)],
                [np.zeros((m, n)), S, Y]
            ])
            
            rhs = np.concatenate([
                -rd,
                -rp, 
                -rc + sigma * mu * np.ones(m)
            ])
            
            # Solve system
            try:
                delta = np.linalg.solve(KKT, rhs)
                dx = delta[:n]
                dy = delta[n:n+m]
                ds = delta[n+m:]
                
                # Line search
                alpha_primal = self._line_search_primal(s, ds)
                alpha_dual = self._line_search_dual(y, dy)
                alpha = 0.99 * min(alpha_primal, alpha_dual)
                
                # Update variables
                x += alpha * dx
                y += alpha * dy
                s += alpha * ds
                
            except np.linalg.LinAlgError:
                print("Singular KKT matrix, stopping")
                break
        
        return x, y, s
    
    def _line_search_primal(self, s, ds):
        """Find maximum step size maintaining s > 0"""
        negative_indices = ds < 0
        if not np.any(negative_indices):
            return 1.0
        return min(1.0, np.min(-s[negative_indices] / ds[negative_indices]))
    
    def _line_search_dual(self, y, dy):
        """Find maximum step size maintaining y > 0"""
        negative_indices = dy < 0
        if not np.any(negative_indices):
            return 1.0
        return min(1.0, np.min(-y[negative_indices] / dy[negative_indices]))
    
    def plot_convergence(self):
        """Plot convergence history"""
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        iterations = range(len(self.history['primal_obj']))
        
        # Objective values
        axes[0, 0].semilogy(iterations, self.history['primal_obj'], 'b-', label='Primal')
        axes[0, 0].semilogy(iterations, self.history['dual_obj'], 'r-', label='Dual')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('Objective Value')
        axes[0, 0].set_title('Objective Convergence')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Duality gap
        axes[0, 1].semilogy(iterations, self.history['duality_gap'], 'g-')
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('Duality Gap')
        axes[0, 1].set_title('Duality Gap Convergence')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Primal residual
        axes[1, 0].semilogy(iterations, self.history['primal_residual'], 'purple')
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Primal Residual')
        axes[1, 0].set_title('Primal Feasibility')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Dual residual
        axes[1, 1].semilogy(iterations, self.history['dual_residual'], 'orange')
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylabel('Dual Residual')
        axes[1, 1].set_title('Dual Feasibility')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Test primal-dual interior point
def test_interior_point():
    """Test interior point method on QP"""
    
    # Problem: min (1/2)(x‚ÇÅ¬≤ + x‚ÇÇ¬≤) + x‚ÇÅ + x‚ÇÇ s.t. x‚ÇÅ + x‚ÇÇ ‚â§ 1, x‚ÇÅ,x‚ÇÇ ‚â• 0
    Q = np.array([[1, 0], [0, 1]])
    c = np.array([1, 1])
    A = np.array([
        [1, 1],   # x‚ÇÅ + x‚ÇÇ ‚â§ 1
        [-1, 0],  # -x‚ÇÅ ‚â§ 0 (x‚ÇÅ ‚â• 0)
        [0, -1]   # -x‚ÇÇ ‚â§ 0 (x‚ÇÇ ‚â• 0)
    ])
    b = np.array([1, 0, 0])
    
    solver = PrimalDualInteriorPoint()
    x_opt, y_opt, s_opt = solver.solve_qp(Q, c, A, b)
    
    print(f"\nOptimal solution: x* = {x_opt}")
    print(f"Optimal dual variables: y* = {y_opt}")
    print(f"Optimal slack variables: s* = {s_opt}")
    
    solver.plot_convergence()
    
    return x_opt, y_opt

# Run test
x_optimal, y_optimal = test_interior_point()
```

**B∆∞·ªõc 2: ADMM implementation**

```python
class ADMM:
    def __init__(self, rho=1.0, tolerance=1e-6, max_iterations=1000):
        self.rho = rho
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'primal_residual': [],
            'dual_residual': [],
            'objective': []
        }
    
    def solve_lasso(self, A, b, lambda_reg):
        """
        Solve LASSO: min (1/2)||Ax - b||¬≤ + Œª||x||‚ÇÅ
        using ADMM with splitting x = z
        """
        
        m, n = A.shape
        
        # Initialize variables
        x = np.zeros(n)
        z = np.zeros(n)
        u = np.zeros(n)  # Scaled dual variable
        
        # Precompute for x-update
        AtA = A.T @ A
        Atb = A.T @ b
        L = AtA + self.rho * np.eye(n)
        L_inv = np.linalg.inv(L)
        
        print("ADMM for LASSO")
        print("=" * 30)
        print(f"{'Iter':<4} {'Objective':<12} {'Primal Res':<12} {'Dual Res':<12}")
        print("-" * 50)
        
        for iteration in range(self.max_iter):
            # x-update: minimize (1/2)||Ax - b||¬≤ + (œÅ/2)||x - z + u||¬≤
            x = L_inv @ (Atb + self.rho * (z - u))
            
            # z-update: minimize Œª||z||‚ÇÅ + (œÅ/2)||x - z + u||¬≤
            z_old = z.copy()
            z = self._soft_threshold(x + u, lambda_reg / self.rho)
            
            # u-update: u := u + x - z
            u = u + x - z
            
            # Compute residuals
            primal_residual = np.linalg.norm(x - z)
            dual_residual = self.rho * np.linalg.norm(z - z_old)
            
            # Compute objective
            objective = 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_reg * np.linalg.norm(z, 1)
            
            # Store history
            self.history['primal_residual'].append(primal_residual)
            self.history['dual_residual'].append(dual_residual)
            self.history['objective'].append(objective)
            
            if iteration % 10 == 0:
                print(f"{iteration:<4} {objective:<12.6f} {primal_residual:<12.6f} {dual_residual:<12.6f}")
            
            # Check convergence
            if primal_residual < self.tol and dual_residual < self.tol:
                print(f"Converged in {iteration} iterations")
                break
        
        return x, z
    
    def _soft_threshold(self, x, threshold):
        """Soft thresholding operator"""
        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)
    
    def solve_consensus_optimization(self, local_objectives, A_global, b_global):
        """
        Solve consensus optimization:
        min Œ£·µ¢ f·µ¢(x·µ¢) s.t. x·µ¢ = z ‚àÄi
        """
        
        N = len(local_objectives)  # Number of agents
        n = len(A_global[0])       # Dimension
        
        # Initialize variables
        x = [np.zeros(n) for _ in range(N)]
        z = np.zeros(n)
        u = [np.zeros(n) for _ in range(N)]
        
        print("ADMM Consensus Optimization")
        print("=" * 35)
        
        for iteration in range(self.max_iter):
            # x-updates (can be done in parallel)
            for i in range(N):
                x[i] = self._solve_local_subproblem(i, z, u[i], local_objectives[i])
            
            # z-update: z := (1/N) Œ£·µ¢ (x·µ¢ + u·µ¢)
            z = np.mean([x[i] + u[i] for i in range(N)], axis=0)
            
            # u-updates
            for i in range(N):
                u[i] = u[i] + x[i] - z
            
            # Check convergence
            primal_residual = np.sqrt(sum(np.linalg.norm(x[i] - z)**2 for i in range(N)))
            
            if iteration % 50 == 0:
                print(f"Iteration {iteration}: Primal residual = {primal_residual:.6f}")
            
            if primal_residual < self.tol:
                print(f"Converged in {iteration} iterations")
                break
        
        return x, z
    
    def _solve_local_subproblem(self, agent_id, z, u, local_objective):
        """Solve local subproblem for agent i"""
        # This is problem-specific and would be implemented based on f·µ¢
        # For quadratic: min (1/2)x·µ¢·µÄ Q·µ¢ x·µ¢ + c·µ¢·µÄ x·µ¢ + (œÅ/2)||x·µ¢ - z + u·µ¢||¬≤
        
        # Simplified implementation for quadratic case
        Q_i = local_objective['Q']
        c_i = local_objective['c']
        
        # Solve: (Q·µ¢ + œÅI) x·µ¢ = -c·µ¢ + œÅ(z - u·µ¢)
        L = Q_i + self.rho * np.eye(len(c_i))
        rhs = -c_i + self.rho * (z - u)
        
        return np.linalg.solve(L, rhs)
    
    def plot_convergence(self):
        """Plot ADMM convergence"""
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        iterations = range(len(self.history['objective']))
        
        # Objective
        axes[0].semilogy(iterations, self.history['objective'], 'b-', linewidth=2)
        axes[0].set_xlabel('Iteration')
        axes[0].set_ylabel('Objective Value')
        axes[0].set_title('Objective Convergence')
        axes[0].grid(True, alpha=0.3)
        
        # Primal residual
        axes[1].semilogy(iterations, self.history['primal_residual'], 'r-', linewidth=2)
        axes[1].set_xlabel('Iteration')
        axes[1].set_ylabel('Primal Residual')
        axes[1].set_title('Primal Residual')
        axes[1].grid(True, alpha=0.3)
        
        # Dual residual
        axes[2].semilogy(iterations, self.history['dual_residual'], 'g-', linewidth=2)
        axes[2].set_xlabel('Iteration')
        axes[2].set_ylabel('Dual Residual')
        axes[2].set_title('Dual Residual')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Test ADMM on LASSO
def test_admm_lasso():
    """Test ADMM on LASSO problem"""
    
    # Generate synthetic data
    np.random.seed(42)
    m, n = 100, 50
    A = np.random.randn(m, n)
    x_true = np.zeros(n)
    x_true[:5] = np.random.randn(5)  # Sparse true solution
    b = A @ x_true + 0.1 * np.random.randn(m)
    
    lambda_reg = 0.1
    
    # Solve with ADMM
    admm = ADMM(rho=1.0)
    x_admm, z_admm = admm.solve_lasso(A, b, lambda_reg)
    
    print(f"\nTrue solution (first 10): {x_true[:10]}")
    print(f"ADMM solution (first 10): {x_admm[:10]}")
    print(f"Recovery error: {np.linalg.norm(x_admm - x_true):.6f}")
    
    admm.plot_convergence()
    
    # Compare with CVXPY
    x_cvx = cp.Variable(n)
    objective = cp.Minimize(0.5 * cp.sum_squares(A @ x_cvx - b) + lambda_reg * cp.norm(x_cvx, 1))
    prob = cp.Problem(objective)
    prob.solve()
    
    print(f"CVXPY solution (first 10): {x_cvx.value[:10]}")
    print(f"ADMM vs CVXPY error: {np.linalg.norm(x_admm - x_cvx.value):.6f}")
    
    return x_admm, x_cvx.value

# Run ADMM test
x_admm_result, x_cvx_result = test_admm_lasso()
```

**B∆∞·ªõc 3: Dual decomposition**

```python
class DualDecomposition:
    def __init__(self, step_size=0.01, max_iterations=1000):
        self.step_size = step_size
        self.max_iter = max_iterations
        self.history = {
            'dual_objective': [],
            'primal_objective': [],
            'constraint_violation': []
        }
    
    def solve_resource_allocation(self, utilities, capacities, demands):
        """
        Solve resource allocation problem:
        max Œ£·µ¢ U·µ¢(x·µ¢) s.t. Œ£·µ¢ x·µ¢ ‚â§ C, x·µ¢ ‚â• 0
        
        Using dual decomposition on coupling constraint
        """
        
        N = len(utilities)  # Number of agents
        
        # Initialize dual variable (price)
        price = 1.0
        
        print("Dual Decomposition for Resource Allocation")
        print("=" * 45)
        print(f"{'Iter':<4} {'Price':<10} {'Dual Obj':<12} {'Constraint':<12}")
        print("-" * 50)
        
        for iteration in range(self.max_iter):
            # Solve local problems (can be done in parallel)
            local_solutions = []
            total_demand = 0
            
            for i in range(N):
                # Solve: max U·µ¢(x·µ¢) - price √ó x·µ¢
                x_i = self._solve_local_utility(utilities[i], price, demands[i])
                local_solutions.append(x_i)
                total_demand += x_i
            
            # Compute dual objective
            dual_obj = sum(utilities[i](local_solutions[i]) - price * local_solutions[i] 
                          for i in range(N)) + price * capacities
            
            # Compute primal objective (if feasible)
            if total_demand <= capacities:
                primal_obj = sum(utilities[i](local_solutions[i]) for i in range(N))
            else:
                primal_obj = -np.inf  # Infeasible
            
            # Constraint violation
            constraint_viol = max(0, total_demand - capacities)
            
            # Store history
            self.history['dual_objective'].append(dual_obj)
            self.history['primal_objective'].append(primal_obj if primal_obj > -np.inf else np.nan)
            self.history['constraint_violation'].append(constraint_viol)
            
            if iteration % 50 == 0:
                print(f"{iteration:<4} {price:<10.4f} {dual_obj:<12.4f} {constraint_viol:<12.4f}")
            
            # Update price (subgradient step)
            subgradient = capacities - total_demand
            price = max(0, price - self.step_size * subgradient)
            
            # Check convergence
            if constraint_viol < 1e-6:
                print(f"Converged in {iteration} iterations")
                break
        
        return local_solutions, price
    
    def _solve_local_utility(self, utility_func, price, max_demand):
        """Solve local utility maximization problem"""
        
        # For quadratic utility: U(x) = ax - (b/2)x¬≤
        # max ax - (b/2)x¬≤ - price √ó x
        # Optimal: x* = (a - price) / b, clipped to [0, max_demand]
        
        if hasattr(utility_func, 'a') and hasattr(utility_func, 'b'):
            # Quadratic utility
            x_unconstrained = (utility_func.a - price) / utility_func.b
            return np.clip(x_unconstrained, 0, max_demand)
        else:
            # General case: use numerical optimization
            from scipy.optimize import minimize_scalar
            
            def neg_utility(x):
                return -(utility_func(x) - price * x)
            
            result = minimize_scalar(neg_utility, bounds=(0, max_demand), method='bounded')
            return result.x
    
    def plot_convergence(self):
        """Plot dual decomposition convergence"""
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        iterations = range(len(self.history['dual_objective']))
        
        # Dual objective
        axes[0].plot(iterations, self.history['dual_objective'], 'b-', linewidth=2)
        axes[0].set_xlabel('Iteration')
        axes[0].set_ylabel('Dual Objective')
        axes[0].set_title('Dual Objective')
        axes[0].grid(True, alpha=0.3)
        
        # Primal objective (when feasible)
        primal_vals = [val for val in self.history['primal_objective'] if not np.isnan(val)]
        if primal_vals:
            axes[1].plot(range(len(primal_vals)), primal_vals, 'r-', linewidth=2)
        axes[1].set_xlabel('Iteration')
        axes[1].set_ylabel('Primal Objective')
        axes[1].set_title('Primal Objective (when feasible)')
        axes[1].grid(True, alpha=0.3)
        
        # Constraint violation
        axes[2].semilogy(iterations, np.maximum(self.history['constraint_violation'], 1e-10), 'g-', linewidth=2)
        axes[2].set_xlabel('Iteration')
        axes[2].set_ylabel('Constraint Violation')
        axes[2].set_title('Constraint Violation')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Test dual decomposition
def test_dual_decomposition():
    """Test dual decomposition on resource allocation"""
    
    # Define quadratic utilities: U·µ¢(x) = a·µ¢x - (b·µ¢/2)x¬≤
    class QuadraticUtility:
        def __init__(self, a, b):
            self.a = a
            self.b = b
        
        def __call__(self, x):
            return self.a * x - 0.5 * self.b * x**2
    
    # Create utilities for 5 agents
    utilities = [
        QuadraticUtility(10, 1),   # High value, low curvature
        QuadraticUtility(8, 1.5),  # Medium value, medium curvature
        QuadraticUtility(12, 2),   # High value, high curvature
        QuadraticUtility(6, 0.8),  # Low value, low curvature
        QuadraticUtility(9, 1.2)   # Medium value, medium curvature
    ]
    
    capacities = 25  # Total resource capacity
    demands = [15, 12, 10, 18, 14]  # Maximum demand for each agent
    
    # Solve using dual decomposition
    decomp = DualDecomposition(step_size=0.1)
    solutions, final_price = decomp.solve_resource_allocation(utilities, capacities, demands)
    
    print(f"\nFinal allocations: {solutions}")
    print(f"Total allocation: {sum(solutions):.4f}")
    print(f"Capacity: {capacities}")
    print(f"Final price: {final_price:.4f}")
    
    decomp.plot_convergence()
    
    # Verify optimality conditions
    print(f"\nOptimality Verification:")
    for i, (sol, util) in enumerate(zip(solutions, utilities)):
        marginal_utility = util.a - util.b * sol
        print(f"Agent {i}: allocation = {sol:.4f}, marginal utility = {marginal_utility:.4f}")
    
    print(f"All marginal utilities should equal the price: {final_price:.4f}")
    
    return solutions, final_price

# Run dual decomposition test
solutions_result, price_result = test_dual_decomposition()
```

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi l·∫≠p tr√¨nh Lagrangian:**
- Theo d√µi c·∫©n th·∫≠n d·∫•u r√†ng bu·ªôc (‚â§ 0 cho b·∫•t ƒë·∫≥ng th·ª©c)
- Ki·ªÉm ch·ª©ng ƒëi·ªÅu ki·ªán kh√¥ng √¢m c·ªßa bi·∫øn ƒë·ªëi ng·∫´u
- S·ª≠ d·ª•ng t√≠nh to√°n k√Ω hi·ªáu cho ƒë·∫°o h√†m ph·ª©c t·∫°p
- Ki·ªÉm tra ƒëi·ªÅu ki·ªán b·∫≠c hai cho t√≠nh l·ªìi

#### **Khi ph√¢n t√≠ch ƒë·ªëi ng·∫´u m·∫°nh:**
- Lu√¥n ki·ªÉm tra ƒëi·ªÅu ki·ªán Slater tr∆∞·ªõc
- T√¨m ki·∫øm c√°c ƒëi·ªÅu ki·ªán ƒë·ªß r√†ng bu·ªôc
- Ki·ªÉm ch·ª©ng t√≠nh l·ªìi c·ªßa b√†i to√°n primal
- Xem x√©t gi·∫£i th√≠ch h√¨nh h·ªçc

#### **Khi l·∫≠p tr√¨nh thu·∫≠t to√°n primal-dual:**
- Gi√°m s√°t c·∫£ ph·∫ßn d∆∞ primal v√† dual
- S·ª≠ d·ª•ng k√≠ch th∆∞·ªõc b∆∞·ªõc v√† tham s·ªë r√†o c·∫£n ph√π h·ª£p
- L·∫≠p tr√¨nh c√°c ph√©p to√°n ƒë·∫°i s·ªë tuy·∫øn t√≠nh hi·ªáu qu·∫£
- Th√™m ki·ªÉm tra ·ªïn ƒë·ªãnh s·ªë

#### **Khi √°p d·ª•ng v√†o b√†i to√°n th·ª±c t·∫ø:**
- Khai th√°c c·∫•u tr√∫c b√†i to√°n ƒë·ªÉ tƒÉng hi·ªáu qu·∫£
- Xem x√©t l·∫≠p tr√¨nh ph√¢n t√°n
- S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c kh·ªüi ƒë·ªông ·∫•m
- Ki·ªÉm ch·ª©ng k·∫øt qu·∫£ v·ªõi nghi·ªám ƒë√£ bi·∫øt

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press.
   - Chapter 11: General Duality Theory

2. **Bertsekas, D. P.** (1999). *Nonlinear Programming*. Athena Scientific.

3. **Boyd, S., et al.** (2011). *Distributed Optimization and Statistical Learning via ADMM*. Foundations and Trends in Machine Learning.
