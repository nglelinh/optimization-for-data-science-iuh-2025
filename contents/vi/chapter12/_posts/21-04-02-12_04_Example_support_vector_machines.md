---
layout: post
title: 12-04 VÃ­ dá»¥ MÃ¡y Vector Há»— Trá»£ (Support Vector Machines - SVM)
chapter: '12'
order: 5
owner: Wontak Ryu
categories:
- chapter12
lang: vi
lesson_type: required
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

# MÃ¡y Vector Há»— Trá»£ (Support Vector Machines - SVM)

## 1. Giá»›i thiá»‡u

**Support Vector Machine (SVM)** lÃ  má»™t thuáº­t toÃ¡n phÃ¢n loáº¡i máº¡nh máº½, tÃ¬m siÃªu pháº³ng tá»‘i Æ°u Ä‘á»ƒ phÃ¢n tÃ¡ch cÃ¡c lá»›p dá»¯ liá»‡u vá»›i **margin lá»›n nháº¥t**.

### Ã tÆ°á»Ÿng cá»‘t lÃµi

- **SiÃªu pháº³ng**: $\beta^T x + \beta_0 = 0$ phÃ¢n tÃ¡ch hai lá»›p
- **Margin**: Khoáº£ng cÃ¡ch tá»« siÃªu pháº³ng Ä‘áº¿n Ä‘iá»ƒm gáº§n nháº¥t = $\frac{2}{\|\beta\|}$
- **Má»¥c tiÃªu**: Maximize margin $\Leftrightarrow$ Minimize $\|\beta\|$

## 2. Hard-Margin SVM (Dá»¯ liá»‡u tÃ¡ch Ä‘Æ°á»£c)

Khi dá»¯ liá»‡u **tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh**:

$$
\begin{align}
&\min_{\beta, \beta_0} &&{\frac{1}{2} \|\beta\|_2^2} \\
&\text{s.t.} && y_i (\beta^T x_i + \beta_0) \geq 1, \quad i = 1, \ldots, n
\end{align}
$$

**Giáº£i thÃ­ch:**
- HÃ m má»¥c tiÃªu: Minimize $\|\beta\|$ Ä‘á»ƒ maximize margin
- RÃ ng buá»™c: Má»i Ä‘iá»ƒm pháº£i cÃ¡ch siÃªu pháº³ng Ã­t nháº¥t 1 Ä‘Æ¡n vá»‹ (theo metric)

## 3. Soft-Margin SVM (Dá»¯ liá»‡u khÃ´ng tÃ¡ch Ä‘Æ°á»£c)

Trong thá»±c táº¿, dá»¯ liá»‡u thÆ°á»ng **khÃ´ng tÃ¡ch Ä‘Æ°á»£c hoÃ n toÃ n**. Giá»›i thiá»‡u **slack variables** $\xi_i \geq 0$:

$$
\begin{align}
&\min_{\beta, \beta_0, \xi} &&{\frac{1}{2} \|\beta\|_2^2 + C\sum_{i=1}^n \xi_i} \\
&\text{s.t.} &&{\xi_i \geq 0, \quad i = 1, \ldots, n}\\
& && y_i (\beta^T x_i + \beta_0) \geq 1 - \xi_i, \quad i = 1, \ldots, n
\end{align}
$$

Vá»›i $y \in \{-1, 1\}^n$ vÃ  $X \in \mathbb{R}^{n \times p}$.

### Giáº£i thÃ­ch cÃ¡c thÃ nh pháº§n

**1) Slack variable $\xi_i$:**
- $\xi_i = 0$: Äiá»ƒm phÃ¢n loáº¡i Ä‘Ãºng, ngoÃ i margin
- $0 < \xi_i < 1$: Äiá»ƒm trong margin zone, váº«n Ä‘Ãºng phÃ­a
- $\xi_i = 1$: Äiá»ƒm náº±m trÃªn siÃªu pháº³ng
- $\xi_i > 1$: Äiá»ƒm bá»‹ phÃ¢n loáº¡i sai

**2) Tham sá»‘ $C > 0$:**
- $C$ lá»›n: Pháº¡t náº·ng vi pháº¡m â†’ margin nhá», Ã­t misclassify
- $C$ nhá»: Cho phÃ©p nhiá»u vi pháº¡m â†’ margin lá»›n
- **Trade-off**: Margin width vs. Training error

**3) HÃ m má»¥c tiÃªu:**
- $\frac{1}{2}\|\beta\|_2^2$: Maximize margin
- $C\sum \xi_i$: Minimize total violation

## 4. Äiá»u kiá»‡n KKT cho SVM

### 4.1. HÃ m Lagrangian

Vá»›i nhÃ¢n tá»­ Lagrange $v \geq 0$ (cho $\xi_i \geq 0$) vÃ  $w \geq 0$ (cho rÃ ng buá»™c margin):

$$
\begin{align}
L(\beta, \beta_0, \xi, v, w) = &\frac{1}{2} \|\beta\|_2^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n v_i \xi_i \\
&+ \sum_{i=1}^n w_i (1 - \xi_i - y_i (\beta^T x_i + \beta_0))
\end{align}
$$

### 4.2. Äiá»u kiá»‡n Stationarity

**1) Äáº¡o hÃ m theo $\beta$:**
$$
\frac{\partial L}{\partial \beta} = \beta - \sum_{i=1}^n w_i y_i x_i = 0
$$

$$
\Rightarrow \boxed{\beta^\star = \sum_{i=1}^n w_i^\star y_i x_i}
$$

**Ã nghÄ©a quan trá»ng:** Nghiá»‡m tá»‘i Æ°u lÃ  tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u!

**2) Äáº¡o hÃ m theo $\beta_0$:**
$$
\boxed{\sum_{i=1}^n w_i^\star y_i = 0}
$$

**3) Äáº¡o hÃ m theo $\xi_i$:**
$$
C - v_i - w_i = 0 \Rightarrow \boxed{v_i^\star = C - w_i^\star}
$$

**Suy ra:** $0 \leq w_i^\star \leq C$ (vÃ¬ $v_i^\star \geq 0$)

### 4.3. Complementary Slackness

**1) Cho $\xi_i \geq 0$:**
$$
v_i^\star \xi_i^\star = 0, \quad i = 1, \ldots, n
$$

**2) Cho rÃ ng buá»™c margin:**
$$
w_i^\star (1 - \xi_i^\star - y_i (\beta^{\star T} x_i + \beta_0^\star)) = 0, \quad i = 1, \ldots, n
$$

### 4.4. PhÃ¢n tÃ­ch cÃ¡c trÆ°á»ng há»£p

#### **TrÆ°á»ng há»£p 1: $w_i^\star = 0$ (Non-support vector)**

- Tá»« $v_i^\star = C > 0$ â†’ $\xi_i^\star = 0$ (CS 1)
- Tá»« CS 2: RÃ ng buá»™c margin khÃ´ng cháº·t
- **Káº¿t luáº­n:** $y_i(\beta^{\star T} x_i + \beta_0^\star) > 1$ 
- Äiá»ƒm náº±m **ngoÃ i margin**, phÃ¢n loáº¡i Ä‘Ãºng, khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n nghiá»‡m

#### **TrÆ°á»ng há»£p 2: $0 < w_i^\star < C$ (Support vector trÃªn margin)**

- Cáº£ $v_i^\star$ vÃ  $w_i^\star$ Ä‘á»u dÆ°Æ¡ng
- Tá»« CS: $\xi_i^\star = 0$ vÃ  $y_i(\beta^{\star T} x_i + \beta_0^\star) = 1$
- **Káº¿t luáº­n:** Äiá»ƒm náº±m **Ä‘Ãºng trÃªn margin boundary**
- Quan trá»ng nháº¥t trong xÃ¡c Ä‘á»‹nh siÃªu pháº³ng!

#### **TrÆ°á»ng há»£p 3: $w_i^\star = C$ (Support vector vi pháº¡m margin)**

- Tá»« $v_i^\star = 0$: khÃ´ng rÃ ng buá»™c $\xi_i^\star$
- Tá»« CS 2: $y_i(\beta^{\star T} x_i + \beta_0^\star) = 1 - \xi_i^\star$

**PhÃ¢n loáº¡i con:**
- $\xi_i^\star = 0$: TrÃªn margin (giá»‘ng trÆ°á»ng há»£p 2)
- $0 < \xi_i^\star < 1$: Trong margin zone, phÃ¢n loáº¡i Ä‘Ãºng
- $\xi_i^\star \geq 1$: Bá»‹ phÃ¢n loáº¡i sai (misclassified)

## 5. Support Vectors

### Äá»‹nh nghÄ©a

**Support vectors** lÃ  cÃ¡c Ä‘iá»ƒm cÃ³ $w_i^\star > 0$.

Tá»« Ä‘iá»u kiá»‡n stationarity:
$$
\beta^\star = \sum_{i: w_i^\star > 0} w_i^\star y_i x_i
$$

â†’ **Chá»‰ support vectors quyáº¿t Ä‘á»‹nh siÃªu pháº³ng!**

### PhÃ¢n loáº¡i Support Vectors

<figure class="image" style="align: center;">
<p align="center">
 <img src="{{ site.baseurl }}/img/chapter_img/chapter12/svm.png" alt="" width="70%" height="70%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 1] Minh há»a cÃ¡c support vectors vá»›i } \xi^\star \neq 0 \text{ [3]}$$ </figcaption>
</p>
</figure>

**1) Support vectors trÃªn margin** ($\xi_i^\star = 0$, $0 < w_i^\star \leq C$):
- Náº±m Ä‘Ãºng trÃªn Ä‘Æ°á»ng biÃªn margin
- $y_i(\beta^{\star T} x_i + \beta_0^\star) = 1$
- Quan trá»ng nháº¥t trong xÃ¡c Ä‘á»‹nh siÃªu pháº³ng

**2) Support vectors trong margin** ($0 < \xi_i^\star < 1$, $w_i^\star = C$):
- Náº±m giá»¯a margin vÃ  siÃªu pháº³ng
- Váº«n phÃ¢n loáº¡i Ä‘Ãºng nhÆ°ng vi pháº¡m margin
- ThÆ°á»ng lÃ  outliers hoáº·c Ä‘iá»ƒm gáº§n biÃªn

**3) Support vectors bá»‹ misclassify** ($\xi_i^\star \geq 1$, $w_i^\star = C$):
- Náº±m sai phÃ­a siÃªu pháº³ng
- Bá»‹ phÃ¢n loáº¡i sai
- ÄÃ³ng gÃ³p nhiá»u vÃ o penalty $C\sum \xi_i$

### Ã nghÄ©a thá»±c tiá»…n

1. **Hiá»‡u quáº£ tÃ­nh toÃ¡n:**
   - Chá»‰ cáº§n lÆ°u trá»¯ $s$ support vectors (thÆ°á»ng $s \ll n$)
   - Prediction: $\hat{y} = \text{sign}(\sum_{SV} w_i y_i \langle x_i, x \rangle + \beta_0)$

2. **á»”n Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh:**
   - Chá»‰ phá»¥ thuá»™c support vectors
   - CÃ¡c Ä‘iá»ƒm xa margin khÃ´ng áº£nh hÆ°á»Ÿng
   - Robust vá»›i outliers (náº¿u $C$ phÃ¹ há»£p)

3. **Sparsity:**
   - Nhiá»u $w_i = 0$ (non-support vectors)
   - MÃ´ hÃ¬nh "sparse" vÃ  dá»… giáº£i thÃ­ch

## 6. VÃ­ dá»¥ Chi tiáº¿t

### VÃ­ dá»¥ 1D: Dá»¯ liá»‡u Ä‘Æ¡n giáº£n

**Dá»¯ liá»‡u:**
- Lá»›p $+1$: $x_1 = 3, x_2 = 4$
- Lá»›p $-1$: $x_3 = 1, x_4 = 2$

**BÃ i toÃ¡n hard-margin:**
$$
\begin{align}
\min_{\beta, \beta_0} \quad &\frac{1}{2}\beta^2 \\
\text{s.t.} \quad &3\beta + \beta_0 \geq 1, \quad 4\beta + \beta_0 \geq 1 \\
&-\beta - \beta_0 \geq 1, \quad -2\beta - \beta_0 \geq 1
\end{align}
$$

**Giáº£i phÃ¡p hÃ¬nh há»c:**
- SiÃªu pháº³ng tá»‘i Æ°u náº±m giá»¯a $x = 2$ vÃ  $x = 3$
- Äiá»ƒm gáº§n nháº¥t: $x_2 = 4$ vÃ  $x_4 = 2$

**Äiá»u kiá»‡n margin cháº·t:**
$$
\begin{cases}
4\beta + \beta_0 = 1 \\
2\beta + \beta_0 = -1
\end{cases}
\Rightarrow \beta = 1, \quad \beta_0 = -3
$$

**Nghiá»‡m:**
- SiÃªu pháº³ng: $x - 3 = 0$ hay $x = 3$ (sai! pháº£i lÃ  $x=2.5$)
- HÃ£y kiá»ƒm tra láº¡i...

**Sá»­a láº¡i:** Äiá»ƒm gáº§n nháº¥t pháº£i lÃ  $x = 3$ (lá»›p +1) vÃ  $x = 2$ (lá»›p -1):
$$
\begin{cases}
3\beta + \beta_0 = 1 \\
2\beta + \beta_0 = -1
\end{cases}
\Rightarrow \beta = 2, \quad \beta_0 = -5
$$

- SiÃªu pháº³ng: $2x - 5 = 0$ hay $x = 2.5$ âœ“
- Margin: $\frac{2}{|\beta|} = \frac{2}{2} = 1$
- Support vectors: $x = 3$ vÃ  $x = 2$

## 7. BÃ i toÃ¡n Äá»‘i ngáº«u

### Dual Problem

Tháº¿ cÃ¡c Ä‘iá»u kiá»‡n stationarity vÃ o Lagrangian:

$$
\begin{align}
\max_{w} \quad &\sum_{i=1}^n w_i - \frac{1}{2} \sum_{i,j} w_i w_j y_i y_j \langle x_i, x_j \rangle \\
\text{s.t.} \quad &\sum_{i=1}^n w_i y_i = 0 \\
&0 \leq w_i \leq C, \quad i = 1, \ldots, n
\end{align}
$$

### Æ¯u Ä‘iá»ƒm

1. **Kernel trick**: Chá»‰ phá»¥ thuá»™c vÃ o $\langle x_i, x_j \rangle$ â†’ cÃ³ thá»ƒ dÃ¹ng kernel
2. **Ãt rÃ ng buá»™c hÆ¡n**: Chá»‰ 1 rÃ ng buá»™c Ä‘áº³ng thá»©c + box constraints
3. **Solver hiá»‡u quáº£**: SMO, LIBSVM

## 8. á»¨ng dá»¥ng KKT trong SVM

### 8.1. Kiá»ƒm tra tÃ­nh tá»‘i Æ°u

DÃ¹ng Ä‘iá»u kiá»‡n KKT Ä‘á»ƒ verify nghiá»‡m:
1. Primal/Dual feasibility
2. Stationarity
3. Complementary slackness

### 8.2. TÃ­nh $\beta_0$

Tá»« support vectors trÃªn margin ($0 < w_i < C$):
$$
\beta_0 = y_i - \beta^T x_i = y_i - \sum_{j} w_j y_j \langle x_j, x_i \rangle
$$

**Thá»±c táº¿:** Láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ SV trÃªn margin:
$$
\beta_0 = \frac{1}{|S|} \sum_{i \in S} \left( y_i - \sum_{j} w_j y_j \langle x_j, x_i \rangle \right)
$$

### 8.3. Lá»c Support Vectors

TrÆ°á»›c khi giáº£i, loáº¡i bá» Ä‘iá»ƒm cháº¯c cháº¯n khÃ´ng pháº£i SV:
$$
y_i(\beta_{\text{init}}^T x_i + \beta_{0,\text{init}}) \gg 1
$$

â†’ TÄƒng hiá»‡u quáº£ cho datasets lá»›n

## 9. BÃ i táº­p Thá»±c hÃ nh

### BÃ i táº­p 1: PhÃ¢n tÃ­ch Support Vectors

Cho nghiá»‡m SVM vá»›i $C = 1$:
```
w = (0, 0.5, 0.3, 1, 0, 1, 0.2)
Î¾ = (0, 0, 0.2, 0.8, 0, 1.5, 0)
```

**YÃªu cáº§u:**
a) XÃ¡c Ä‘á»‹nh support vectors  
b) PhÃ¢n loáº¡i theo vá»‹ trÃ­ (trÃªn margin, trong margin, misclassified)  
c) Giáº£i thÃ­ch $w_1 = 0$ nhÆ°ng $\xi_1 = 0$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**a) Support vectors:** $i \in \{2, 3, 4, 6, 7\}$ (vÃ¬ $w_i > 0$)

**b) PhÃ¢n loáº¡i:**
- **TrÃªn margin**: $i = 2, 7$ ($\xi_i = 0$, $0 < w_i < C$)
- **Trong margin**: $i = 3, 4$ ($0 < \xi_i < 1$, $w_i = C$)
- **Misclassified**: $i = 6$ ($\xi_6 = 1.5 > 1$, $w_6 = C$)

**c)** $w_1 = 0$ nghÄ©a lÃ  khÃ´ng pháº£i SV. Tá»« CS: $v_1 = C - 0 = 1 > 0$ â†’ $\xi_1 = 0$ âœ“  
Äiá»ƒm $x_1$ náº±m xa margin, phÃ¢n loáº¡i Ä‘Ãºng.

</details>

---

### BÃ i táº­p 2: TÃ­nh toÃ¡n tá»« KKT

Cho $C = 2$, $n = 3$, $p = 2$:
```
xâ‚ = (0, 1), yâ‚ = +1, wâ‚ = 0.5
xâ‚‚ = (1, 0), yâ‚‚ = +1, wâ‚‚ = 0.5  
xâ‚ƒ = (0, 0), yâ‚ƒ = -1, wâ‚ƒ = 1
```
$\xi_1 = \xi_2 = 0$, $\xi_3 = 0.2$.

**YÃªu cáº§u:**
a) TÃ­nh $\beta$ tá»« stationarity  
b) TÃ­nh $\beta_0$ tá»« SV trÃªn margin  
c) Kiá»ƒm tra CS cho $i=3$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**a)** $\beta = \sum w_i y_i x_i = 0.5(0,1) + 0.5(1,0) + 1 \cdot (-1)(0,0) = (0.5, 0.5)$

**b)** Tá»« $i=1$: $y_1(\beta^T x_1 + \beta_0) = 1$  
$1 \cdot (0.5 + \beta_0) = 1$ â†’ $\beta_0 = 0.5$

**c)** CS: $w_3(1 - \xi_3 - y_3(\beta^T x_3 + \beta_0)) = 0$  
$y_3(\beta^T x_3 + \beta_0) = -1(0.5) = -0.5$  
$1(1 - 0.2 - (-0.5)) = 1.3 \neq 0$ âœ—  

**Káº¿t luáº­n:** CS vi pháº¡m! CÃ³ lá»—i trong dá»¯ liá»‡u.

</details>

---

## 10. TÃ³m táº¯t

### CÃ¡c Ä‘iá»ƒm chÃ­nh

1. **SVM tÃ¬m siÃªu pháº³ng** vá»›i margin lá»›n nháº¥t
2. **Soft-margin** cho phÃ©p vi pháº¡m qua $\xi_i$, Ä‘iá»u khiá»ƒn bá»Ÿi $C$
3. **Äiá»u kiá»‡n KKT** cho:
   - $\beta = \sum w_i y_i x_i$ (chá»‰ phá»¥ thuá»™c support vectors)
   - PhÃ¢n loáº¡i Ä‘iá»ƒm thÃ nh SV vÃ  non-SV
4. **Support vectors** quyáº¿t Ä‘á»‹nh siÃªu pháº³ng, thÆ°á»ng $s \ll n$
5. **Complementary slackness** giáº£i thÃ­ch khi nÃ o $w_i = 0$ hay $w_i > 0$

### Ã nghÄ©a thá»±c tiá»…n

Äiá»u kiá»‡n KKT khÃ´ng dÃ¹ng trá»±c tiáº¿p Ä‘á»ƒ giáº£i SVM (dÃ¹ng SMO, LIBSVM), nhÆ°ng giÃºp:
- Kiá»ƒm chá»©ng tÃ­nh tá»‘i Æ°u
- Hiá»ƒu cáº¥u trÃºc nghiá»‡m (sparsity, support vectors)
- PhÃ¡t triá»ƒn thuáº­t toÃ¡n hiá»‡u quáº£
- PhÃ¢n tÃ­ch Ä‘á»™ nháº¡y cá»§a mÃ´ hÃ¬nh

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

1. **Vapnik, V. N.** (1995). *The Nature of Statistical Learning Theory*. Springer.
2. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Chapter 12.
3. **Hastie, T., et al.** (2009). *The Elements of Statistical Learning* (2nd ed.). Springer. Chapter 12.
4. **Cristianini, N., & Shawe-Taylor, J.** (2000). *An Introduction to Support Vector Machines*. Cambridge University Press.
