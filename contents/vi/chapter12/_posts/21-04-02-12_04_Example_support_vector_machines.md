---
layout: post
title: 12-04 VÃ­ dá»¥ MÃ¡y Vector Há»— Trá»£ (Support Vector Machines - SVM)
chapter: '12'
order: 5
owner: Wontak Ryu
categories:
- chapter12
lang: vi
lesson_type: required
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

## 1. Giá»›i thiá»‡u

**Support Vector Machine (SVM)** lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n mÃ¡y há»c cá»• Ä‘iá»ƒn vÃ  máº¡nh máº½ nháº¥t cho bÃ i toÃ¡n phÃ¢n loáº¡i. ÄÆ°á»£c phÃ¡t triá»ƒn bá»Ÿi Vladimir Vapnik vÃ  cá»™ng sá»± vÃ o nhá»¯ng nÄƒm 1990, SVM Ä‘Ã£ táº¡o nÃªn má»™t cuá»™c cÃ¡ch máº¡ng trong lÄ©nh vá»±c machine learning nhá» ná»n táº£ng toÃ¡n há»c vá»¯ng cháº¯c vÃ  kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u chiá»u cao má»™t cÃ¡ch hiá»‡u quáº£.

**Ã tÆ°á»Ÿng cá»‘t lÃµi:** HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n cáº§n váº½ má»™t Ä‘Æ°á»ng tháº³ng Ä‘á»ƒ phÃ¢n chia hai nhÃ³m Ä‘iá»ƒm trÃªn máº·t pháº³ng. CÃ³ vÃ´ sá»‘ cÃ¡ch váº½, nhÆ°ng Ä‘Æ°á»ng nÃ o lÃ  "tá»‘t nháº¥t"? SVM tráº£ lá»i cÃ¢u há»i nÃ y báº±ng cÃ¡ch tÃ¬m **Ä‘Æ°á»ng phÃ¢n chia cÃ³ khoáº£ng cÃ¡ch lá»›n nháº¥t** Ä‘áº¿n cÃ¡c Ä‘iá»ƒm gáº§n nháº¥t cá»§a cáº£ hai nhÃ³m. Khoáº£ng cÃ¡ch nÃ y Ä‘Æ°á»£c gá»i lÃ  **margin (biÃªn Ä‘á»™)**. Margin cÃ ng lá»›n, mÃ´ hÃ¬nh cÃ ng "tá»± tin" trong viá»‡c phÃ¢n loáº¡i vÃ  cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n trÃªn dá»¯ liá»‡u má»›i.

**Lá»™ trÃ¬nh bÃ i há»c:** ChÃºng ta sáº½ cÃ¹ng nhau Ä‘i tá»« trá»±c giÃ¡c hÃ¬nh há»c Ä‘áº¿n cÃ´ng thá»©c toÃ¡n há»c má»™t cÃ¡ch tá»± nhiÃªn:
- Báº¯t Ä‘áº§u vá»›i **trá»±c giÃ¡c hÃ¬nh há»c**: Táº¡i sao margin lá»›n láº¡i quan trá»ng?
- Chuyá»ƒn sang **cÃ´ng thá»©c toÃ¡n há»c**: Biá»ƒu diá»…n bÃ i toÃ¡n dÆ°á»›i dáº¡ng tá»‘i Æ°u lá»“i (Quadratic Programming)
- KhÃ¡m phÃ¡ **support vectors**: Nhá»¯ng Ä‘iá»ƒm dá»¯ liá»‡u "quyáº¿t Ä‘á»‹nh" siÃªu pháº³ng phÃ¢n cÃ¡ch
- Má»Ÿ rá»™ng sang **Soft-Margin SVM**: Xá»­ lÃ½ dá»¯ liá»‡u thá»±c táº¿ cÃ³ nhiá»…u vÃ  khÃ´ng tÃ¡ch Ä‘Æ°á»£c hoÃ n toÃ n
- NghiÃªn cá»©u **bÃ i toÃ¡n Ä‘á»‘i ngáº«u**: CÃ¡nh cá»­a dáº«n Ä‘áº¿n kernel trick vÃ  cÃ¡c á»©ng dá»¥ng nÃ¢ng cao

### 1.1. Kiáº¿n thá»©c ná»n táº£ng: Khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm tá»›i siÃªu pháº³ng

TrÆ°á»›c khi báº¯t Ä‘áº§u, hÃ£y Ã´n láº¡i má»™t cÃ´ng thá»©c quan trá»ng tá»« hÃ¬nh há»c giáº£i tÃ­ch:

**KhÃ´ng gian 2 chiá»u:**  
Khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm $$(x_0, y_0)$$ tá»›i Ä‘Æ°á»ng tháº³ng $$w_1x + w_2y + b = 0$$ lÃ :
$$
\frac{\lvert w_1x_0 + w_2y_0 + b\rvert}{\sqrt{w_1^2 + w_2^2}}
$$

**KhÃ´ng gian 3 chiá»u:**  
Khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm $$(x_0, y_0, z_0)$$ tá»›i máº·t pháº³ng $$w_1x + w_2y + w_3z + b = 0$$ lÃ :
$$
\frac{\lvert w_1x_0 + w_2y_0 + w_3z_0 + b\rvert}{\sqrt{w_1^2 + w_2^2 + w_3^2}}
$$

**Tá»•ng quÃ¡t - KhÃ´ng gian nhiá»u chiá»u:**  
Khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm (vector) $$\mathbf{x}_0$$ tá»›i **siÃªu máº·t pháº³ng** $$\mathbf{w}^T\mathbf{x} + b = 0$$ lÃ :
$$
\frac{\lvert\mathbf{w}^T\mathbf{x}_0 + b\rvert}{\|\mathbf{w}\|_2}
$$

**LÆ°u Ã½ quan trá»ng:**  
- Náº¿u bá» dáº¥u giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i, ta xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c Ä‘iá»ƒm náº±m vá» phÃ­a nÃ o cá»§a siÃªu pháº³ng
- Biá»ƒu thá»©c dÆ°Æ¡ng: Ä‘iá»ƒm á»Ÿ **phÃ­a dÆ°Æ¡ng** cá»§a siÃªu pháº³ng
- Biá»ƒu thá»©c Ã¢m: Ä‘iá»ƒm á»Ÿ **phÃ­a Ã¢m** cá»§a siÃªu pháº³ng
- Biá»ƒu thá»©c báº±ng 0: Ä‘iá»ƒm náº±m trÃªn siÃªu pháº³ng

### 1.2. Ã tÆ°á»Ÿng cá»‘t lÃµi - Ba khÃ¡i niá»‡m then chá»‘t

TrÆ°á»›c khi Ä‘i vÃ o chi tiáº¿t, hÃ£y náº¯m vá»¯ng ba khÃ¡i niá»‡m ná»n táº£ng:

- **SiÃªu pháº³ng phÃ¢n cÃ¡ch**: Trong khÃ´ng gian nhiá»u chiá»u, siÃªu pháº³ng $$\beta^T x + \beta_0 = 0$$ Ä‘Ã³ng vai trÃ² nhÆ° má»™t "bá»©c tÆ°á»ng" chia khÃ´ng gian thÃ nh hai vÃ¹ng, má»—i vÃ¹ng chá»©a má»™t lá»›p dá»¯ liá»‡u.

- **Margin (BiÃªn Ä‘á»™)**: ÄÃ¢y lÃ  khoáº£ng cÃ¡ch tá»« siÃªu pháº³ng Ä‘áº¿n Ä‘iá»ƒm gáº§n nháº¥t trong má»—i lá»›p. Khi siÃªu pháº³ng náº±m á»Ÿ vá»‹ trÃ­ tá»‘i Æ°u, margin báº±ng $$\frac{2}{\|\beta\|}$$ (chÃºng ta sáº½ chá»©ng minh Ä‘iá»u nÃ y sau).

- **Má»¥c tiÃªu tá»‘i Æ°u**: Maximize margin $$\Leftrightarrow$$ Minimize $$\|\beta\|$$. Táº¡i sao láº¡i tÆ°Æ¡ng Ä‘Æ°Æ¡ng? VÃ¬ margin tá»‰ lá»‡ nghá»‹ch vá»›i $$\|\beta\|$$, nÃªn margin lá»›n nháº¥t khi $$\|\beta\|$$ nhá» nháº¥t.

### 1.3. Minh há»a trá»±c quan: Khoáº£ng cÃ¡ch vÃ  phÃ¢n loáº¡i

<div id="distance-demo" style="margin: 20px 0;">
    <canvas id="distanceCanvas" width="600" height="400" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    <div style="text-align: center; margin-top: 15px;">
        <p style="font-size: 14px; color: #666;">
            <strong>HÆ°á»›ng dáº«n:</strong> Di chuyá»ƒn chuá»™t trÃªn canvas Ä‘á»ƒ xem khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm Ä‘áº¿n Ä‘Æ°á»ng tháº³ng
        </p>
        <p id="distanceInfo" style="font-size: 16px; font-weight: bold; color: #333;">
            Khoáº£ng cÃ¡ch: 0
        </p>
    </div>
</div>

<script>
(function() {
    const canvas = document.getElementById('distanceCanvas');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const width = canvas.width;
    const height = canvas.height;
    
    // Tham sá»‘ Ä‘Æ°á»ng tháº³ng: w1*x + w2*y + b = 0
    // Chá»n Ä‘Æ°á»ng tháº³ng Ä‘i qua giá»¯a canvas vá»›i gÃ³c nghiÃªng
    const w1 = 1;
    const w2 = 0.5;
    const b = -200;
    const norm = Math.sqrt(w1*w1 + w2*w2);
    
    let mouseX = width / 2;
    let mouseY = height / 2;
    
    function draw() {
        // Clear canvas
        ctx.clearRect(0, 0, width, height);
        
        // Váº½ lÆ°á»›i ná»n
        ctx.strokeStyle = '#f0f0f0';
        ctx.lineWidth = 1;
        for (let x = 0; x < width; x += 50) {
            ctx.beginPath();
            ctx.moveTo(x, 0);
            ctx.lineTo(x, height);
            ctx.stroke();
        }
        for (let y = 0; y < height; y += 50) {
            ctx.beginPath();
            ctx.moveTo(0, y);
            ctx.lineTo(width, y);
            ctx.stroke();
        }
        
        // Váº½ Ä‘Æ°á»ng tháº³ng w1*x + w2*y + b = 0
        ctx.strokeStyle = '#2c3e50';
        ctx.lineWidth = 3;
        ctx.beginPath();
        
        // TÃ­nh y cho x = 0 vÃ  x = width
        const y0 = -(w1 * 0 + b) / w2;
        const y1 = -(w1 * width + b) / w2;
        
        ctx.moveTo(0, y0);
        ctx.lineTo(width, y1);
        ctx.stroke();
        
        // Váº½ label cho Ä‘Æ°á»ng tháº³ng
        ctx.fillStyle = '#2c3e50';
        ctx.font = '14px Arial';
        ctx.fillText('wâ‚x + wâ‚‚y + b = 0', width - 150, y1 + 20);
        
        // TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm chuá»™t Ä‘áº¿n Ä‘Æ°á»ng tháº³ng
        const distance = Math.abs(w1 * mouseX + w2 * mouseY + b) / norm;
        const signedDist = (w1 * mouseX + w2 * mouseY + b) / norm;
        
        // TÃ¬m Ä‘iá»ƒm chiáº¿u vuÃ´ng gÃ³c
        const t = -(w1 * mouseX + w2 * mouseY + b) / (w1*w1 + w2*w2);
        const projX = mouseX + t * w1;
        const projY = mouseY + t * w2;
        
        // Váº½ Ä‘Æ°á»ng vuÃ´ng gÃ³c
        ctx.strokeStyle = '#e74c3c';
        ctx.lineWidth = 2;
        ctx.setLineDash([5, 5]);
        ctx.beginPath();
        ctx.moveTo(mouseX, mouseY);
        ctx.lineTo(projX, projY);
        ctx.stroke();
        ctx.setLineDash([]);
        
        // Váº½ Ä‘iá»ƒm chuá»™t
        ctx.fillStyle = signedDist > 0 ? '#3498db' : '#e74c3c';
        ctx.beginPath();
        ctx.arc(mouseX, mouseY, 8, 0, 2 * Math.PI);
        ctx.fill();
        
        // Váº½ Ä‘iá»ƒm chiáº¿u
        ctx.fillStyle = '#95a5a6';
        ctx.beginPath();
        ctx.arc(projX, projY, 5, 0, 2 * Math.PI);
        ctx.fill();
        
        // Hiá»ƒn thá»‹ khoáº£ng cÃ¡ch
        const distInfo = document.getElementById('distanceInfo');
        if (distInfo) {
            const side = signedDist > 0 ? '(PhÃ­a dÆ°Æ¡ng)' : '(PhÃ­a Ã¢m)';
            distInfo.innerHTML = `Khoáº£ng cÃ¡ch: <span style="color: ${signedDist > 0 ? '#3498db' : '#e74c3c'}">${distance.toFixed(2)}</span> ${side}`;
        }
        
        // Váº½ text thÃ´ng tin
        ctx.fillStyle = '#34495e';
        ctx.font = '12px Arial';
        ctx.fillText(`Äiá»ƒm: (${mouseX.toFixed(0)}, ${mouseY.toFixed(0)})`, mouseX + 15, mouseY - 15);
    }
    
    // Event listeners
    canvas.addEventListener('mousemove', function(e) {
        const rect = canvas.getBoundingClientRect();
        mouseX = e.clientX - rect.left;
        mouseY = e.clientY - rect.top;
        draw();
    });
    
    // Initial draw
    draw();
})();
</script>

---

## 2. BÃ i toÃ¡n phÃ¢n chia hai lá»›p dá»¯ liá»‡u

HÃ£y báº¯t Ä‘áº§u vá»›i má»™t tÃ¬nh huá»‘ng Ä‘Æ¡n giáº£n nhÆ°ng Ä‘áº§y thÃ¡ch thá»©c: chÃºng ta cÃ³ hai nhÃ³m (lá»›p) Ä‘iá»ƒm dá»¯ liá»‡u trong khÃ´ng gian nhiá»u chiá»u, vÃ  hai nhÃ³m nÃ y **tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh** (linearly separable). Äiá»u nÃ y cÃ³ nghÄ©a lÃ  tá»“n táº¡i Ã­t nháº¥t má»™t siÃªu pháº³ng cÃ³ thá»ƒ phÃ¢n chia hoÃ n toÃ n hai nhÃ³m - táº¥t cáº£ Ä‘iá»ƒm cá»§a nhÃ³m A náº±m vá» má»™t phÃ­a, táº¥t cáº£ Ä‘iá»ƒm cá»§a nhÃ³m B náº±m vá» phÃ­a Ä‘á»‘i diá»‡n.

**Nhiá»‡m vá»¥ cá»§a chÃºng ta:** TÃ¬m má»™t siÃªu pháº³ng phÃ¢n chia sao cho khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o "Ä‘i láº¡c" - má»—i Ä‘iá»ƒm pháº£i náº±m Ä‘Ãºng phÃ­a cá»§a siÃªu pháº³ng tÆ°Æ¡ng á»©ng vá»›i nhÃ£n cá»§a nÃ³.

**ThÃ¡ch thá»©c Ä‘áº§u tiÃªn:** Náº¿u dá»¯ liá»‡u tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh, chÃºng ta cÃ³ **vÃ´ sá»‘ cÃ¡ch chá»n** siÃªu pháº³ng phÃ¢n chia! Minh há»a dÆ°á»›i Ä‘Ã¢y cho tháº¥y ba Ä‘Æ°á»ng tháº³ng khÃ¡c nhau Ä‘á»u phÃ¢n chia chÃ­nh xÃ¡c hai lá»›p:

<figure class="image" style="align: center;">
<p align="center">
 <img src="https://machinelearningcoban.com/assets/19_svm/svm1.png" alt="" width="60%" height="60%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 2] CÃ¡c máº·t phÃ¢n cÃ¡ch kháº£ dÄ© cho hai classes linearly separable}$$ </figcaption>
</p>
</figure>

**CÃ¢u há»i then chá»‘t:** Trong vÃ´ sá»‘ cÃ¡c máº·t phÃ¢n chia Ä‘á»u "Ä‘Ãºng" vá» máº·t phÃ¢n loáº¡i, **Ä‘Ã¢u lÃ  máº·t phÃ¢n chia Tá»T NHáº¤T**? 

HÃ£y quan sÃ¡t ká»¹ hÃ¬nh minh há»a phÃ­a trÃªn. Báº¡n cÃ³ nháº­n tháº¥y gÃ¬ khÃ´ng? Hai Ä‘Æ°á»ng tháº³ng Ä‘áº§u tiÃªn "lá»‡ch" khÃ¡ nhiá»u vá» phÃ­a cÃ¡c Ä‘iá»ƒm hÃ¬nh trÃ²n Ä‘á» - chÃºng dÆ°á»ng nhÆ° "sá»£" cÃ¡c Ä‘iá»ƒm hÃ¬nh vuÃ´ng xanh vÃ  nÃ© trÃ¡nh chÃºng. Trong khi Ä‘Ã³, Ä‘Æ°á»ng tháº³ng thá»© ba náº±m gáº§n nhÆ° chÃ­nh giá»¯a hai nhÃ³m, táº¡o ra má»™t "vÃ¹ng an toÃ n" xung quanh nÃ³.

Trá»±c giÃ¡c cho chÃºng ta biáº¿t: Ä‘Æ°á»ng phÃ¢n chia á»Ÿ giá»¯a sáº½ "tá»± tin" vÃ  "robust" hÆ¡n khi gáº·p dá»¯ liá»‡u má»›i. NhÆ°ng lÃ m tháº¿ nÃ o Ä‘á»ƒ biáº¿n trá»±c giÃ¡c nÃ y thÃ nh má»™t cÃ´ng thá»©c toÃ¡n há»c cá»¥ thá»ƒ?

### 2.1. TiÃªu chuáº©n: Maximum Margin - Tá»« trá»±c giÃ¡c Ä‘áº¿n Ä‘á»‹nh lÆ°á»£ng

SVM giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng má»™t Ã½ tÆ°á»Ÿng cá»±c ká»³ tinh táº¿: sá»­ dá»¥ng khÃ¡i niá»‡m **margin** (lá», biÃªn Ä‘á»™) Ä‘á»ƒ Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ "an toÃ n" cá»§a má»™t Ä‘Æ°á»ng phÃ¢n chia.

<figure class="image" style="align: center;">
<p align="center">
 <img src="https://machinelearningcoban.com/assets/19_svm/svm2.png" alt="" width="70%" height="70%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 3] So sÃ¡nh cÃ¡c máº·t phÃ¢n cÃ¡ch vá»›i margin khÃ¡c nhau}$$ </figcaption>
</p>
</figure>

**Margin lÃ  gÃ¬?** ÄÆ¡n giáº£n nháº¥t, margin lÃ  khoáº£ng cÃ¡ch tá»« siÃªu pháº³ng phÃ¢n cÃ¡ch Ä‘áº¿n **Ä‘iá»ƒm gáº§n nháº¥t** cá»§a má»—i lá»›p dá»¯ liá»‡u. HÃ£y nghÄ© vá» nÃ³ nhÆ° "vÃ¹ng Ä‘á»‡m an toÃ n" xung quanh Ä‘Æ°á»ng phÃ¢n chia.

**SiÃªu pháº³ng tá»‘i Æ°u pháº£i thá»a mÃ£n hai nguyÃªn táº¯c vÃ ng:**

1. **CÃ´ng báº±ng (Äá»‘i xá»©ng)**: Khoáº£ng cÃ¡ch tá»« siÃªu pháº³ng Ä‘áº¿n Ä‘iá»ƒm gáº§n nháº¥t cá»§a lá»›p A pháº£i **báº±ng** khoáº£ng cÃ¡ch Ä‘áº¿n Ä‘iá»ƒm gáº§n nháº¥t cá»§a lá»›p B. KhÃ´ng thiÃªn vá»‹ lá»›p nÃ o cáº£!

2. **Tá»‘i Ä‘a hÃ³a an toÃ n**: Trong táº¥t cáº£ cÃ¡c siÃªu pháº³ng cÃ´ng báº±ng, chá»n cÃ¡i cÃ³ margin **lá»›n nháº¥t** - tá»©c "vÃ¹ng Ä‘á»‡m" rá»™ng nháº¥t.

**Táº¡i sao margin lá»›n láº¡i quan trá»ng?** Ba lÃ½ do thá»±c tiá»…n:

- **Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a**: Margin rá»™ng giá»‘ng nhÆ° cÃ³ "vÃ¹ng dung sai" lá»›n. Khi cÃ³ dá»¯ liá»‡u má»›i hÆ¡i lá»‡ch so vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, mÃ´ hÃ¬nh váº«n phÃ¢n loáº¡i Ä‘Ãºng.
  
- **Äá»™ tin cáº­y**: Äiá»ƒm dá»¯ liá»‡u cÃ ng xa siÃªu pháº³ng, quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i cÃ ng "tá»± tin". Margin lá»›n nghÄ©a lÃ  háº§u háº¿t Ä‘iá»ƒm Ä‘á»u Ä‘Æ°á»£c phÃ¢n loáº¡i vá»›i Ä‘á»™ tin cáº­y cao.
  
- **Chá»‘ng nhiá»…u**: Náº¿u cÃ³ vÃ i Ä‘iá»ƒm bá»‹ nhiá»…u (Ä‘o Ä‘áº¡c sai, outliers), margin lá»›n giÃºp mÃ´ hÃ¬nh Ã­t bá»‹ áº£nh hÆ°á»Ÿng.

**Káº¿t luáº­n quan trá»ng**: BÃ i toÃ¡n SVM chÃ­nh lÃ  bÃ i toÃ¡n tÃ¬m máº·t phÃ¢n cÃ¡ch cÃ³ **margin lá»›n nháº¥t**. VÃ¬ váº­y, SVM cÃ²n Ä‘Æ°á»£c gá»i lÃ  **Maximum Margin Classifier** - bá»™ phÃ¢n loáº¡i margin cá»±c Ä‘áº¡i.

---

## 3. XÃ¢y dá»±ng bÃ i toÃ¡n tá»‘i Æ°u cho SVM

ChÃºng ta Ä‘Ã£ cÃ³ trá»±c giÃ¡c rÃµ rÃ ng: cáº§n tÃ¬m siÃªu pháº³ng cÃ³ margin lá»›n nháº¥t. Giá» lÃ  lÃºc biáº¿n trá»±c giÃ¡c nÃ y thÃ nh **ngÃ´n ngá»¯ toÃ¡n há»c chÃ­nh xÃ¡c** - má»™t bÃ i toÃ¡n tá»‘i Æ°u mÃ  mÃ¡y tÃ­nh cÃ³ thá»ƒ giáº£i Ä‘Æ°á»£c. HÃ nh trÃ¬nh nÃ y sáº½ Ä‘Æ°a chÃºng ta tá»« hÃ¬nh há»c trá»±c quan Ä‘áº¿n má»™t bÃ i toÃ¡n Quadratic Programming chuáº©n.

### 3.1. Thiáº¿t láº­p kÃ½ hiá»‡u

**Giáº£ sá»­**: CÃ¡c cáº·p dá»¯ liá»‡u cá»§a training set lÃ  $$(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)$$ vá»›i:
- Vector $$x_i \in \mathbb{R}^d$$: Ä‘áº§u vÃ o cá»§a má»™t Ä‘iá»ƒm dá»¯ liá»‡u
- $$y_i \in \{-1, +1\}$$: nhÃ£n cá»§a Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã³
- $$d$$: sá»‘ chiá»u cá»§a dá»¯ liá»‡u
- $$N$$: sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u

**LÆ°u Ã½**: Äá»ƒ dá»… hÃ¬nh dung, chÃºng ta xÃ©t trÆ°á»ng há»£p khÃ´ng gian hai chiá»u. CÃ¡c phÃ©p toÃ¡n hoÃ n toÃ n cÃ³ thá»ƒ Ä‘Æ°á»£c tá»•ng quÃ¡t lÃªn khÃ´ng gian nhiá»u chiá»u.

<figure class="image" style="align: center;">
<p align="center">
 <img src="https://machinelearningcoban.com/assets/19_svm/svm6.png" alt="" width="70%" height="70%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 4] PhÃ¢n tÃ­ch bÃ i toÃ¡n SVM}$$ </figcaption>
</p>
</figure>

**Giáº£ sá»­**:
- CÃ¡c Ä‘iá»ƒm vuÃ´ng xanh thuá»™c class +1
- CÃ¡c Ä‘iá»ƒm trÃ²n Ä‘á» thuá»™c class -1
- Máº·t $$w^T x + b = w_1 x_1 + w_2 x_2 + b = 0$$ lÃ  máº·t phÃ¢n chia
- Class +1 náº±m vá» **phÃ­a dÆ°Æ¡ng**, class -1 náº±m vá» **phÃ­a Ã¢m**

### 3.2. CÃ´ng thá»©c khoáº£ng cÃ¡ch - Má»™t máº¹o toÃ¡n há»c tinh táº¿

Vá»›i cáº·p dá»¯ liá»‡u $$(x_n, y_n)$$ báº¥t ká»³, khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm $$x_n$$ tá»›i siÃªu pháº³ng phÃ¢n chia Ä‘Æ°á»£c tÃ­nh bá»Ÿi:

$$
\frac{y_n(w^T x_n + b)}{\|w\|_2}
$$

**Táº¡i sao láº¡i cÃ³ $$y_n$$ á»Ÿ tá»­ sá»‘?** ÄÃ¢y lÃ  má»™t máº¹o thÃ´ng minh! Nhá»› láº¡i ráº±ng:
- Náº¿u $$x_n$$ thuá»™c lá»›p +1, nÃ³ náº±m á»Ÿ phÃ­a dÆ°Æ¡ng cá»§a siÃªu pháº³ng â†’ $$w^T x_n + b > 0$$
- Náº¿u $$x_n$$ thuá»™c lá»›p -1, nÃ³ náº±m á»Ÿ phÃ­a Ã¢m cá»§a siÃªu pháº³ng â†’ $$w^T x_n + b < 0$$

VÃ¬ theo giáº£ sá»­ mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘Ãºng, $$y_n$$ luÃ´n **cÃ¹ng dáº¥u** vá»›i $$(w^T x_n + b)$$. Do Ä‘Ã³:
$$
y_n(w^T x_n + b) > 0 \quad \text{(luÃ´n dÆ°Æ¡ng!)}
$$

NhÃ¢n vá»›i $$y_n$$ giÃºp ta **loáº¡i bá» dáº¥u giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i** trong cÃ´ng thá»©c khoáº£ng cÃ¡ch, lÃ m cho cÃ¡c phÃ©p toÃ¡n sau nÃ y Ä‘Æ¡n giáº£n hÆ¡n ráº¥t nhiá»u!

### 3.3. Äá»‹nh nghÄ©a Margin

Vá»›i máº·t phÃ¢n chia nhÆ° trÃªn, **margin** Ä‘Æ°á»£c tÃ­nh lÃ  khoáº£ng cÃ¡ch gáº§n nháº¥t tá»« má»™t Ä‘iá»ƒm tá»›i máº·t Ä‘Ã³ (báº¥t ká»ƒ Ä‘iá»ƒm nÃ o trong hai classes):

$$
\text{margin} = \min_n \frac{y_n(w^T x_n + b)}{\|w\|_2}
$$

### 3.4. BÃ i toÃ¡n tá»‘i Æ°u ban Ä‘áº§u

BÃ i toÃ¡n tá»‘i Æ°u trong SVM chÃ­nh lÃ  bÃ i toÃ¡n tÃ¬m $$w$$ vÃ  $$b$$ sao cho margin nÃ y Ä‘áº¡t giÃ¡ trá»‹ **lá»›n nháº¥t**:

$$
\begin{align}
(w, b) &= \arg\max_{w,b} \left\{ \min_n \frac{y_n(w^T x_n + b)}{\|w\|_2} \right\} \\
&= \arg\max_{w,b} \left\{ \frac{1}{\|w\|_2} \min_n y_n(w^T x_n + b) \right\} \qquad (1)
\end{align}
$$

**Váº¥n Ä‘á»**: Viá»‡c giáº£i trá»±c tiáº¿p bÃ i toÃ¡n nÃ y sáº½ ráº¥t phá»©c táº¡p.

### 3.5. Biáº¿n Ä‘á»•i vá» bÃ i toÃ¡n Ä‘Æ¡n giáº£n - Thá»§ thuáº­t chuáº©n hÃ³a

TrÆ°á»›c khi tiáº¿p tá»¥c, chÃºng ta cáº§n má»™t **quan sÃ¡t quan trá»ng** giÃºp Ä‘Æ¡n giáº£n hÃ³a bÃ i toÃ¡n Ä‘Ã¡ng ká»ƒ.

**TÃ­nh báº¥t Ä‘á»‹nh vá» tá»· lá»‡:** Giáº£ sá»­ ta cÃ³ má»™t siÃªu pháº³ng $$w^T x + b = 0$$. Náº¿u nhÃ¢n cáº£ $$w$$ vÃ  $$b$$ vá»›i cÃ¹ng má»™t háº±ng sá»‘ dÆ°Æ¡ng $$k > 0$$, ta Ä‘Æ°á»£c siÃªu pháº³ng má»›i $$(kw)^T x + kb = 0$$. Äiá»u ká»³ diá»‡u lÃ :
- Máº·t phÃ¢n chia **váº«n lÃ  cÃ¹ng má»™t máº·t** (vÃ¬ phÆ°Æ¡ng trÃ¬nh váº«n cho cÃ¹ng táº­p há»£p cÃ¡c Ä‘iá»ƒm thá»a mÃ£n)
- Khoáº£ng cÃ¡ch tá»« má»—i Ä‘iá»ƒm Ä‘áº¿n máº·t phÃ¢n chia **khÃ´ng Ä‘á»•i** (phÃ©p chia cho $$\|kw\| = k\|w\|$$ triá»‡t tiÃªu há»‡ sá»‘ $$k$$)
- Do Ä‘Ã³, margin cÅ©ng **khÃ´ng Ä‘á»•i**!

**Thá»§ thuáº­t chuáº©n hÃ³a thÃ´ng minh:** VÃ¬ tá»· lá»‡ cá»§a $$w$$ vÃ  $$b$$ khÃ´ng quan trá»ng, ta cÃ³ thá»ƒ tá»± do chá»n tá»· lá»‡ sao cho tiá»‡n tÃ­nh toÃ¡n nháº¥t. Cá»¥ thá»ƒ, ta sáº½ **chuáº©n hÃ³a** sao cho vá»›i nhá»¯ng Ä‘iá»ƒm **gáº§n siÃªu pháº³ng nháº¥t** (sau nÃ y gá»i lÃ  support vectors), ta cÃ³:

$$
y_n(w^T x_n + b) = 1
$$

ÄÃ¢y lÃ  má»™t quy Æ°á»›c tuyá»‡t vá»i vÃ¬ nÃ³ biáº¿n margin thÃ nh cÃ´ng thá»©c ráº¥t Ä‘áº¹p: $$\text{margin} = \frac{1}{\|w\|}$$.

<figure class="image" style="align: center;">
<p align="center">
 <img src="https://machinelearningcoban.com/assets/19_svm/svm3.png" alt="" width="60%" height="60%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 5] CÃ¡c Ä‘iá»ƒm gáº§n máº·t phÃ¢n cÃ¡ch nháº¥t Ä‘Æ°á»£c khoanh trÃ²n}$$ </figcaption>
</p>
</figure>

**Há»‡ quáº£**: Vá»›i má»i $$n$$, ta cÃ³:

$$
y_n(w^T x_n + b) \geq 1
$$

### 3.6. BÃ i toÃ¡n tá»‘i Æ°u cÃ³ rÃ ng buá»™c

Váº­y bÃ i toÃ¡n tá»‘i Æ°u $$(1)$$ cÃ³ thá»ƒ Ä‘Æ°a vá»:

$$
\begin{align}
(w, b) &= \arg\max_{w,b} \frac{1}{\|w\|_2} \\
\text{subject to: } & y_n(w^T x_n + b) \geq 1, \quad \forall n = 1,2,\ldots,N \qquad (2)
\end{align}
$$

### 3.7. BÃ i toÃ¡n tá»‘i Æ°u chuáº©n

Báº±ng biáº¿n Ä‘á»•i Ä‘Æ¡n giáº£n, ta cÃ³ thá»ƒ Ä‘Æ°a bÃ i toÃ¡n vá» dáº¡ng:

$$
\begin{align}
(w, b) &= \arg\min_{w,b} \frac{1}{2}\|w\|_2^2 \\
\text{subject to: } & 1 - y_n(w^T x_n + b) \leq 0, \quad \forall n = 1,2,\ldots,N \qquad (3)
\end{align}
$$

**Giáº£i thÃ­ch cÃ¡c biáº¿n Ä‘á»•i**:
1. Láº¥y **nghá»‹ch Ä‘áº£o** hÃ m má»¥c tiÃªu (max â†’ min)
2. **BÃ¬nh phÆ°Æ¡ng** Ä‘á»ƒ Ä‘Æ°á»£c hÃ m kháº£ vi
3. NhÃ¢n vá»›i $$\frac{1}{2}$$ Ä‘á»ƒ biá»ƒu thá»©c Ä‘áº¡o hÃ m Ä‘áº¹p hÆ¡n
4. Äá»•i dáº¥u rÃ ng buá»™c

### 3.8. TÃ­nh cháº¥t cá»§a bÃ i toÃ¡n

**Quan sÃ¡t quan trá»ng**:

1. **HÃ m má»¥c tiÃªu**: $$\frac{1}{2}\|w\|_2^2$$ lÃ  má»™t hÃ m **lá»“i** (norm bÃ¬nh phÆ°Æ¡ng)

2. **HÃ m rÃ ng buá»™c**: $$1 - y_n(w^T x_n + b)$$ lÃ  hÃ m **tuyáº¿n tÃ­nh** theo $$w$$ vÃ  $$b$$, nÃªn lÃ  hÃ m **lá»“i**

3. **Káº¿t luáº­n**: BÃ i toÃ¡n $$(3)$$ lÃ  má»™t **bÃ i toÃ¡n lá»“i** (convex optimization problem)

4. **HÆ¡n ná»¯a**: ÄÃ¢y lÃ  má»™t **Quadratic Programming (QP)** problem

5. **Strictly convex**: HÃ m má»¥c tiÃªu lÃ  strictly convex vÃ¬ $$\|w\|_2^2 = w^T I w$$ vÃ  $$I$$ lÃ  ma tráº­n Ä‘Æ¡n vá»‹ - má»™t ma tráº­n **xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng**

6. **Nghiá»‡m duy nháº¥t**: Tá»« tÃ­nh strictly convex, suy ra nghiá»‡m cho SVM lÃ  **duy nháº¥t**

### 3.9. XÃ¡c Ä‘á»‹nh class cho Ä‘iá»ƒm má»›i

Sau khi tÃ¬m Ä‘Æ°á»£c máº·t phÃ¢n cÃ¡ch $$w^T x + b = 0$$, class cá»§a báº¥t ká»³ má»™t Ä‘iá»ƒm nÃ o sáº½ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh Ä‘Æ¡n giáº£n báº±ng:

$$
\text{class}(x) = \text{sgn}(w^T x + b)
$$

Trong Ä‘Ã³ hÃ m $$\text{sgn}$$ lÃ  hÃ m xÃ¡c Ä‘á»‹nh dáº¥u:
- Nháº­n giÃ¡ trá»‹ **+1** náº¿u Ä‘á»‘i sá»‘ lÃ  khÃ´ng Ã¢m
- Nháº­n giÃ¡ trá»‹ **-1** náº¿u ngÆ°á»£c láº¡i

---

## 4. Hard-Margin SVM (Dá»¯ liá»‡u tÃ¡ch Ä‘Æ°á»£c)

Khi dá»¯ liá»‡u **tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh**, bÃ i toÃ¡n SVM cÃ³ dáº¡ng:

$$
\begin{align}
&\min_{\beta, \beta_0} &&{\frac{1}{2} \|\beta\|_2^2} \\
&\text{s.t.} && y_i (\beta^T x_i + \beta_0) \geq 1, \quad i = 1, \ldots, n
\end{align}
$$

**Giáº£i thÃ­ch:**
- HÃ m má»¥c tiÃªu: Minimize $$\|\beta\|$$ Ä‘á»ƒ maximize margin
- RÃ ng buá»™c: Má»i Ä‘iá»ƒm pháº£i cÃ¡ch siÃªu pháº³ng Ã­t nháº¥t 1 Ä‘Æ¡n vá»‹ (theo metric)

### 4.1. Minh há»a tÆ°Æ¡ng tÃ¡c: Hard-Margin SVM

<div id="hardmargin-demo" style="margin: 20px 0;">
    <canvas id="hardMarginCanvas" width="700" height="500" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    <div style="text-align: center; margin-top: 15px;">
        <button id="resetData" style="padding: 10px 20px; font-size: 14px; cursor: pointer; background: #3498db; color: white; border: none; border-radius: 5px; margin: 5px;">
            ğŸ”„ Táº¡o dá»¯ liá»‡u má»›i
        </button>
        <button id="addPositive" style="padding: 10px 20px; font-size: 14px; cursor: pointer; background: #e74c3c; color: white; border: none; border-radius: 5px; margin: 5px;">
            â• ThÃªm Ä‘iá»ƒm dÆ°Æ¡ng (+1)
        </button>
        <button id="addNegative" style="padding: 10px 20px; font-size: 14px; cursor: pointer; background: #2ecc71; color: white; border: none; border-radius: 5px; margin: 5px;">
            â• ThÃªm Ä‘iá»ƒm Ã¢m (-1)
        </button>
        <p style="font-size: 14px; color: #666; margin-top: 10px;">
            <strong>HÆ°á»›ng dáº«n:</strong> Click vÃ o canvas Ä‘á»ƒ thÃªm Ä‘iá»ƒm má»›i. ÄÆ°á»ng Ä‘en Ä‘áº­m lÃ  siÃªu pháº³ng phÃ¢n cÃ¡ch, Ä‘Æ°á»ng Ä‘á»©t nÃ©t lÃ  margin boundaries.
        </p>
    </div>
</div>

<script>
(function() {
    const canvas = document.getElementById('hardMarginCanvas');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const width = canvas.width;
    const height = canvas.height;
    
    let addMode = 0; // 0: none, 1: positive, -1: negative
    
    // Dá»¯ liá»‡u Ä‘iá»ƒm
    let points = [
        // Class +1 (Ä‘á»)
        {x: 450, y: 150, label: 1},
        {x: 500, y: 100, label: 1},
        {x: 550, y: 200, label: 1},
        {x: 480, y: 250, label: 1},
        {x: 520, y: 180, label: 1},
        // Class -1 (xanh)
        {x: 150, y: 250, label: -1},
        {x: 200, y: 300, label: -1},
        {x: 250, y: 350, label: -1},
        {x: 180, y: 400, label: -1},
        {x: 220, y: 280, label: -1}
    ];
    
    function simpleSVM() {
        // Thuáº­t toÃ¡n Ä‘Æ¡n giáº£n Ä‘á»ƒ tÃ¬m hyperplane
        // TÃ¬m trung Ä‘iá»ƒm giá»¯a 2 classes
        let posPoints = points.filter(p => p.label === 1);
        let negPoints = points.filter(p => p.label === -1);
        
        if (posPoints.length === 0 || negPoints.length === 0) return null;
        
        // TÃ­nh centroid má»—i class
        let posCenter = {
            x: posPoints.reduce((sum, p) => sum + p.x, 0) / posPoints.length,
            y: posPoints.reduce((sum, p) => sum + p.y, 0) / posPoints.length
        };
        
        let negCenter = {
            x: negPoints.reduce((sum, p) => sum + p.x, 0) / negPoints.length,
            y: negPoints.reduce((sum, p) => sum + p.y, 0) / negPoints.length
        };
        
        // Vector káº¿t ná»‘i 2 centroids
        let dx = posCenter.x - negCenter.x;
        let dy = posCenter.y - negCenter.y;
        
        // Hyperplane vuÃ´ng gÃ³c vá»›i vector nÃ y, Ä‘i qua trung Ä‘iá»ƒm
        let midX = (posCenter.x + negCenter.x) / 2;
        let midY = (posCenter.y + negCenter.y) / 2;
        
        // w = (dx, dy), bias tÃ­nh tá»« Ä‘iá»ƒm mid
        let w = {x: dx, y: dy};
        let b = -(w.x * midX + w.y * midY);
        let norm = Math.sqrt(w.x * w.x + w.y * w.y);
        
        // Normalize
        w.x /= norm;
        w.y /= norm;
        b /= norm;
        
        // TÃ¬m margin: khoáº£ng cÃ¡ch gáº§n nháº¥t tá»« Ä‘iá»ƒm Ä‘áº¿n hyperplane
        let minDist = Infinity;
        points.forEach(p => {
            let dist = Math.abs(w.x * p.x + w.y * p.y + b);
            if (dist < minDist) minDist = dist;
        });
        
        // TÃ¬m support vectors
        let supportVectors = points.filter(p => {
            let dist = Math.abs(w.x * p.x + w.y * p.y + b);
            return dist < minDist + 5; // tolerance
        });
        
        return {w, b, margin: minDist, supportVectors};
    }
    
    function draw() {
        // Clear canvas
        ctx.clearRect(0, 0, width, height);
        
        // Background
        ctx.fillStyle = '#fafafa';
        ctx.fillRect(0, 0, width, height);
        
        // TÃ¬m SVM solution
        let svm = simpleSVM();
        
        if (svm) {
            // Váº½ margin boundaries (Ä‘Æ°á»ng Ä‘á»©t nÃ©t)
            ctx.strokeStyle = '#95a5a6';
            ctx.lineWidth = 2;
            ctx.setLineDash([5, 5]);
            
            // Margin boundary 1
            let b1 = svm.b + svm.margin;
            drawLine(svm.w, b1);
            
            // Margin boundary 2
            let b2 = svm.b - svm.margin;
            drawLine(svm.w, b2);
            
            ctx.setLineDash([]);
            
            // Váº½ hyperplane chÃ­nh (Ä‘Æ°á»ng Ä‘áº­m)
            ctx.strokeStyle = '#2c3e50';
            ctx.lineWidth = 3;
            drawLine(svm.w, svm.b);
            
            // Highlight support vectors
            ctx.strokeStyle = '#f39c12';
            ctx.lineWidth = 3;
            svm.supportVectors.forEach(sv => {
                ctx.beginPath();
                ctx.arc(sv.x, sv.y, 15, 0, 2 * Math.PI);
                ctx.stroke();
            });
            
            // Hiá»ƒn thá»‹ thÃ´ng tin
            ctx.fillStyle = '#2c3e50';
            ctx.font = 'bold 16px Arial';
            ctx.fillText(`Margin: ${(svm.margin * 2).toFixed(1)}`, 10, 30);
            ctx.fillText(`Support Vectors: ${svm.supportVectors.length}`, 10, 55);
        }
        
        // Váº½ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u
        points.forEach(p => {
            ctx.fillStyle = p.label === 1 ? '#e74c3c' : '#2ecc71';
            ctx.beginPath();
            ctx.arc(p.x, p.y, 8, 0, 2 * Math.PI);
            ctx.fill();
            
            // Viá»n
            ctx.strokeStyle = '#34495e';
            ctx.lineWidth = 2;
            ctx.stroke();
        });
        
        // Legend
        ctx.fillStyle = '#e74c3c';
        ctx.beginPath();
        ctx.arc(630, 30, 8, 0, 2 * Math.PI);
        ctx.fill();
        ctx.fillStyle = '#34495e';
        ctx.font = '14px Arial';
        ctx.fillText('Class +1', 645, 35);
        
        ctx.fillStyle = '#2ecc71';
        ctx.beginPath();
        ctx.arc(630, 55, 8, 0, 2 * Math.PI);
        ctx.fill();
        ctx.fillStyle = '#34495e';
        ctx.fillText('Class -1', 645, 60);
        
        // Support vector legend
        ctx.strokeStyle = '#f39c12';
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.arc(630, 80, 8, 0, 2 * Math.PI);
        ctx.stroke();
        ctx.fillStyle = '#34495e';
        ctx.fillText('Support Vector', 645, 85);
    }
    
    function drawLine(w, b) {
        // Váº½ Ä‘Æ°á»ng tháº³ng w.x * x + w.y * y + b = 0
        ctx.beginPath();
        
        if (Math.abs(w.y) > 0.01) {
            let y0 = -(w.x * 0 + b) / w.y;
            let y1 = -(w.x * width + b) / w.y;
            ctx.moveTo(0, y0);
            ctx.lineTo(width, y1);
        } else {
            let x = -b / w.x;
            ctx.moveTo(x, 0);
            ctx.lineTo(x, height);
        }
        
        ctx.stroke();
    }
    
    function generateRandomData() {
        points = [];
        // Class +1
        for (let i = 0; i < 5; i++) {
            points.push({
                x: 450 + Math.random() * 150,
                y: 100 + Math.random() * 200,
                label: 1
            });
        }
        // Class -1
        for (let i = 0; i < 5; i++) {
            points.push({
                x: 150 + Math.random() * 150,
                y: 250 + Math.random() * 200,
                label: -1
            });
        }
        draw();
    }
    
    // Event listeners
    canvas.addEventListener('click', function(e) {
        if (addMode !== 0) {
            const rect = canvas.getBoundingClientRect();
            const x = e.clientX - rect.left;
            const y = e.clientY - rect.top;
            points.push({x, y, label: addMode});
            draw();
            addMode = 0;
            document.getElementById('addPositive').style.opacity = '1';
            document.getElementById('addNegative').style.opacity = '1';
        }
    });
    
    document.getElementById('resetData').addEventListener('click', generateRandomData);
    
    document.getElementById('addPositive').addEventListener('click', function() {
        addMode = 1;
        this.style.opacity = '0.6';
        document.getElementById('addNegative').style.opacity = '1';
    });
    
    document.getElementById('addNegative').addEventListener('click', function() {
        addMode = -1;
        this.style.opacity = '0.6';
        document.getElementById('addPositive').style.opacity = '1';
    });
    
    // Initial draw
    draw();
})();
</script>

---

## 5. Soft-Margin SVM: Äá»‘i máº·t vá»›i dá»¯ liá»‡u thá»±c táº¿

Cho Ä‘áº¿n nay, chÃºng ta Ä‘Ã£ lÃ m viá»‡c vá»›i má»™t giáº£ thiáº¿t lÃ½ tÆ°á»Ÿng: dá»¯ liá»‡u **hoÃ n toÃ n tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh**, khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o vi pháº¡m margin. ÄÃ¢y lÃ  **Hard-Margin SVM** - má»™t mÃ´ hÃ¬nh "cá»©ng nháº¯c" khÃ´ng cho phÃ©p báº¥t ká»³ ngoáº¡i lá»‡ nÃ o.

NhÆ°ng thá»±c táº¿ khÃ´ng hoÃ n háº£o nhÆ° váº­y!

### 5.0. Khi lÃ½ thuyáº¿t gáº·p thá»±c tiá»…n: Táº¡i sao cáº§n Slack Variables?

**YÃªu cáº§u kháº¯t khe cá»§a Hard-Margin SVM:**

Má»i Ä‘iá»ƒm dá»¯ liá»‡u pháº£i thá»a mÃ£n:
$$
y_i (\beta^T x_i + \beta_0) \geq 1, \quad \forall i = 1,\ldots,n
$$

Äiá»u nÃ y nghÄ©a lÃ  **Táº¤T Cáº¢** cÃ¡c Ä‘iá»ƒm - khÃ´ng ngoáº¡i lá»‡ - pháº£i:
1. ÄÆ°á»£c phÃ¢n loáº¡i **hoÃ n toÃ n Ä‘Ãºng** (náº±m Ä‘Ãºng phÃ­a cá»§a siÃªu pháº³ng)
2. Náº±m **ngoÃ i vÃ¹ng margin** (cÃ¡ch siÃªu pháº³ng Ã­t nháº¥t 1 Ä‘Æ¡n vá»‹ chuáº©n hÃ³a)

**Ba tÃ¬nh huá»‘ng phá»• biáº¿n khiáº¿n Hard-Margin SVM "bÃ³ tay":**

**1) Dá»¯ liá»‡u khÃ´ng tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh (Linearly Non-separable):**
```
Class +1:  â—‹ â—‹ â—‹ â— â—‹     â† CÃ³ 1 Ä‘iá»ƒm class -1 lá»t vÃ o
                          
Class -1:  â— â— â— â—‹ â—     â† CÃ³ 1 Ä‘iá»ƒm class +1 lá»t vÃ o
```
â†’ **KHÃ”NG Tá»’N Táº I** siÃªu pháº³ng nÃ o tÃ¡ch Ä‘Æ°á»£c hoÃ n toÃ n!

**2) Outliers (Äiá»ƒm nhiá»…u):**
```
Class +1:  â—‹ â—‹ â—‹ â—‹
                      â—  â† Outlier duy nháº¥t
Class -1:  â— â— â— â—
```
â†’ Outlier cÃ³ thá»ƒ **kÃ©o lá»‡ch** siÃªu pháº³ng tá»‘i Æ°u, lÃ m giáº£m margin!

**3) Overlap tá»± nhiÃªn:**
- Trong nhiá»u bÃ i toÃ¡n thá»±c táº¿ (y táº¿, tÃ i chÃ­nh), hai class cÃ³ **vÃ¹ng chá»“ng láº¥n** tá»± nhiÃªn
- KhÃ´ng cÃ³ ranh giá»›i rÃµ rÃ ng 100%

**Giáº£i phÃ¡p tinh táº¿: Slack Variables (Biáº¿n ná»›i lá»ng) $$\xi_i$$**

Thay vÃ¬ báº¯t buá»™c táº¥t cáº£ Ä‘iá»ƒm pháº£i tuÃ¢n thá»§ nghiÃªm ngáº·t, chÃºng ta cho phÃ©p **ná»›i lá»ng cÃ³ kiá»ƒm soÃ¡t**:

$$
y_i (\beta^T x_i + \beta_0) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

**Diá»…n giáº£i báº±ng ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng:**

- Má»—i Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ má»™t **"quyá»n vi pháº¡m"** riÃªng, Ä‘Æ°á»£c Ä‘o báº±ng $$\xi_i$$
- Náº¿u Ä‘iá»ƒm $$i$$ lÃ  "outlier" hoáº·c gáº§n biÃªn quyáº¿t Ä‘á»‹nh, nÃ³ cÃ³ thá»ƒ sá»­ dá»¥ng $$\xi_i > 0$$ Ä‘á»ƒ "xin phÃ©p" vi pháº¡m margin
- NhÆ°ng vi pháº¡m **khÃ´ng miá»…n phÃ­**! Äiá»ƒm Ä‘Ã³ pháº£i "tráº£ giÃ¡" báº±ng cÃ¡ch Ä‘Ã³ng gÃ³p $$C \cdot \xi_i$$ vÃ o hÃ m má»¥c tiÃªu
- Tham sá»‘ $$C$$ quyáº¿t Ä‘á»‹nh "má»©c pháº¡t": $$C$$ lá»›n = pháº¡t náº·ng = Ã­t vi pháº¡m Ä‘Æ°á»£c cháº¥p nháº­n

**Táº¡i sao Ä‘Ã¢y lÃ  Ã½ tÆ°á»Ÿng xuáº¥t sáº¯c:**

- âœ… **LuÃ´n cÃ³ nghiá»‡m**: BÃ i toÃ¡n khÃ´ng bao giá» vÃ´ nghiá»‡m (infeasible), vÃ¬ ta luÃ´n cÃ³ thá»ƒ chá»n $$\xi_i$$ Ä‘á»§ lá»›n
- âœ… **Robust vá»›i outliers**: VÃ i Ä‘iá»ƒm láº¡c khÃ´ng lÃ m sá»¥p Ä‘á»• toÃ n bá»™ mÃ´ hÃ¬nh
- âœ… **Linh hoáº¡t**: Xá»­ lÃ½ Ä‘Æ°á»£c cáº£ dá»¯ liá»‡u tÃ¡ch Ä‘Æ°á»£c vÃ  khÃ´ng tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh
- âœ… **CÃ¢n báº±ng thÃ´ng minh**: Trade-off giá»¯a "margin rá»™ng" (tá»•ng quÃ¡t hÃ³a tá»‘t) vÃ  "Ã­t lá»—i huáº¥n luyá»‡n" (Ä‘á»™ chÃ­nh xÃ¡c cao)

### 5.1. CÃ´ng thá»©c toÃ¡n há»c cá»§a Soft-Margin SVM

Dá»±a trÃªn Ã½ tÆ°á»Ÿng slack variable á»Ÿ trÃªn, ta cÃ³ bÃ i toÃ¡n tá»‘i Æ°u hoÃ n chá»‰nh:

$$
\begin{align}
&\min_{\beta, \beta_0, \xi} &&{\frac{1}{2} \|\beta\|_2^2 + C\sum_{i=1}^n \xi_i} \\
&\text{s.t.} &&{\xi_i \geq 0, \quad i = 1, \ldots, n}\\
& && y_i (\beta^T x_i + \beta_0) \geq 1 - \xi_i, \quad i = 1, \ldots, n
\end{align}
$$

trong Ä‘Ã³:
- $$\beta \in \mathbb{R}^p$$: vector trá»ng sá»‘ cá»§a siÃªu pháº³ng
- $$\beta_0 \in \mathbb{R}$$: bias term
- $$\xi = (\xi_1, \ldots, \xi_n) \in \mathbb{R}^n$$: vector slack variables
- $$y \in \{-1, +1\}^n$$: vector nhÃ£n
- $$X \in \mathbb{R}^{n \times p}$$: ma tráº­n dá»¯ liá»‡u

### Giáº£i thÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n

**1) Slack variable $$\xi_i$$ - Biáº¿n ná»›i lá»ng:**

Slack variable Ä‘o lÆ°á»ng **má»©c Ä‘á»™ vi pháº¡m** cá»§a tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u. GiÃ¡ trá»‹ cá»§a nÃ³ cho biáº¿t Ä‘iá»ƒm Ä‘Ã³ náº±m á»Ÿ Ä‘Ã¢u:

- **$$\xi_i = 0$$**: Äiá»ƒm **lÃ½ tÆ°á»Ÿng** 
  - PhÃ¢n loáº¡i Ä‘Ãºng
  - Náº±m ngoÃ i margin (á»Ÿ phÃ­a Ä‘Ãºng, cÃ¡ch siÃªu pháº³ng $$\geq 1$$ Ä‘Æ¡n vá»‹)
  - KhÃ´ng vi pháº¡m gÃ¬ cáº£
  - KhÃ´ng bá»‹ pháº¡t trong hÃ m má»¥c tiÃªu

- **$$0 < \xi_i < 1$$**: Äiá»ƒm **vi pháº¡m margin nháº¹**
  - Váº«n phÃ¢n loáº¡i **Ä‘Ãºng** (náº±m Ä‘Ãºng phÃ­a cá»§a siÃªu pháº³ng)
  - NhÆ°ng náº±m **trong vÃ¹ng margin** (quÃ¡ gáº§n siÃªu pháº³ng)
  - Vi pháº¡m margin boundary
  - ThÆ°á»ng lÃ  outliers hoáº·c Ä‘iá»ƒm gáº§n biÃªn quyáº¿t Ä‘á»‹nh

- **$$\xi_i = 1$$**: Äiá»ƒm **trÃªn siÃªu pháº³ng**
  - Náº±m chÃ­nh xÃ¡c trÃªn Ä‘Æ°á»ng phÃ¢n cÃ¡ch $$\beta^T x_i + \beta_0 = 0$$
  - KhÃ´ng thá»ƒ quyáº¿t Ä‘á»‹nh phÃ¢n loáº¡i
  - Äiá»ƒm giá»›i háº¡n giá»¯a Ä‘Ãºng vÃ  sai

- **$$\xi_i > 1$$**: Äiá»ƒm **bá»‹ phÃ¢n loáº¡i sai**
  - Náº±m **sai phÃ­a** siÃªu pháº³ng
  - Misclassified point
  - Bá»‹ pháº¡t náº·ng trong hÃ m má»¥c tiÃªu
  - GiÃ¡ trá»‹ $$\xi_i$$ cÃ ng lá»›n, sai cÃ ng nghiÃªm trá»ng

**2) Tham sá»‘ $$C > 0$$ - Háº±ng sá»‘ Ä‘iá»u khiá»ƒn (Regularization parameter):**

$$C$$ quyáº¿t Ä‘á»‹nh **cÃ¢n báº±ng** giá»¯a hai má»¥c tiÃªu mÃ¢u thuáº«n:

- **Vai trÃ²**: Trá»ng sá»‘ pháº¡t cho vi pháº¡m margin
- **Ã nghÄ©a**: "GiÃ¡" pháº£i tráº£ khi vi pháº¡m

**áº¢nh hÆ°á»Ÿng cá»§a C:**

| GiÃ¡ trá»‹ C | Margin | Vi pháº¡m Ä‘Æ°á»£c cháº¥p nháº­n | Kháº£ nÄƒng tá»•ng quÃ¡t | Nguy cÆ¡ |
|-----------|--------|------------------------|-------------------|---------|
| **$$C$$ ráº¥t nhá»** (â†’ 0) | Rá»™ng | Nhiá»u | Tá»‘t (generalize tá»‘t) | Underfitting |
| **$$C$$ trung bÃ¬nh** | CÃ¢n báº±ng | Vá»«a pháº£i | Tá»‘t nháº¥t | - |
| **$$C$$ ráº¥t lá»›n** (â†’ âˆ) | Háº¹p | Ráº¥t Ã­t | KÃ©m (fit data) | Overfitting |

**Giáº£i thÃ­ch chi tiáº¿t:**

- **$$C$$ nhá»** (vÃ­ dá»¥: 0.01):
  - Pháº¡t nháº¹ cho vi pháº¡m â†’ cháº¥p nháº­n nhiá»u $$\xi_i > 0$$
  - Æ¯u tiÃªn **margin rá»™ng** hÆ¡n lÃ  Ä‘á»™ chÃ­nh xÃ¡c training
  - MÃ´ hÃ¬nh **má»m dáº»o**, robust vá»›i outliers
  - CÃ³ thá»ƒ bá»‹ underfitting

- **$$C$$ lá»›n** (vÃ­ dá»¥: 100):
  - Pháº¡t náº·ng cho vi pháº¡m â†’ cá»‘ gáº¯ng $$\xi_i \approx 0$$
  - Æ¯u tiÃªn **Ä‘á»™ chÃ­nh xÃ¡c training** hÆ¡n lÃ  margin rá»™ng
  - Gáº§n vá»›i Hard-Margin SVM
  - CÃ³ thá»ƒ bá»‹ overfitting, nháº¡y cáº£m vá»›i outliers

**CÃ´ng thá»©c toÃ¡n há»c:**

Khi $$C \to \infty$$:
$$
\min \frac{1}{2}\|\beta\|^2 + \underbrace{C\sum \xi_i}_{\text{pháº£i } \approx 0} 
\Rightarrow \xi_i \to 0, \, \forall i
$$
â†’ Trá»Ÿ vá» Hard-Margin SVM!

**3) HÃ m má»¥c tiÃªu - Hai thÃ nh pháº§n:**

$$
\underbrace{\frac{1}{2}\|\beta\|_2^2}_{\text{(1) Maximize margin}} + \underbrace{C\sum_{i=1}^n \xi_i}_{\text{(2) Minimize violations}}
$$

- **(1) Margin term**: $$\frac{1}{2}\|\beta\|_2^2$$
  - Minimize $$\|\beta\|$$ âŸº Maximize margin width $$\frac{2}{\|\beta\|}$$
  - Muá»‘n margin **cÃ ng rá»™ng cÃ ng tá»‘t**

- **(2) Loss term**: $$C\sum_{i=1}^n \xi_i$$
  - Tá»•ng má»©c Ä‘á»™ vi pháº¡m cá»§a táº¥t cáº£ cÃ¡c Ä‘iá»ƒm
  - Muá»‘n vi pháº¡m **cÃ ng Ã­t cÃ ng tá»‘t**
  - $$C$$ lÃ  trá»ng sá»‘ cÃ¢n báº±ng hai má»¥c tiÃªu

### 5.2. VÃ­ dá»¥ sá»‘ cá»¥ thá»ƒ vá» Slack Variable

Äá»ƒ hiá»ƒu rÃµ hÆ¡n cÃ¡ch slack variable hoáº¡t Ä‘á»™ng, xÃ©t má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n:

**Giáº£ sá»­**: Ta cÃ³ siÃªu pháº³ng Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c: $$\beta = 1, \beta_0 = -2.5$$, tá»©c $$x - 2.5 = 0$$ hay $$x = 2.5$$

**Dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch:**

| Äiá»ƒm | $$x_i$$ | $$y_i$$ | $$y_i(\beta^T x_i + \beta_0)$$ | RÃ ng buá»™c | $$\xi_i$$ | PhÃ¢n loáº¡i | Ghi chÃº |
|------|---------|---------|-------------------------------|-----------|-----------|-----------|---------|
| A | 4.0 | +1 | $$1(4-2.5)=1.5$$ | $$1.5 \geq 1$$ âœ“ | **0** | ÄÃºng | NgoÃ i margin, lÃ½ tÆ°á»Ÿng |
| B | 3.0 | +1 | $$1(3-2.5)=0.5$$ | $$0.5 \geq 1$$ âœ— | **0.5** | ÄÃºng | Trong margin zone |
| C | 2.5 | +1 | $$1(2.5-2.5)=0$$ | $$0 \geq 1$$ âœ— | **1.0** | BiÃªn | TrÃªn siÃªu pháº³ng |
| D | 2.0 | +1 | $$1(2-2.5)=-0.5$$ | $$-0.5 \geq 1$$ âœ— | **1.5** | Sai | Bá»‹ misclassified |
| E | 1.0 | -1 | $$-1(1-2.5)=1.5$$ | $$1.5 \geq 1$$ âœ“ | **0** | ÄÃºng | NgoÃ i margin, lÃ½ tÆ°á»Ÿng |
| F | 2.0 | -1 | $$-1(2-2.5)=0.5$$ | $$0.5 \geq 1$$ âœ— | **0.5** | ÄÃºng | Trong margin zone |

**Giáº£i thÃ­ch tá»«ng trÆ°á»ng há»£p:**

1. **Äiá»ƒm A** ($$x=4, y=+1$$):
   - $$y(\beta^T x + \beta_0) = 1.5 \geq 1$$ â†’ Thá»a rÃ ng buá»™c!
   - $$\xi_A = 0$$ (khÃ´ng cáº§n ná»›i lá»ng)
   - Äiá»ƒm **lÃ½ tÆ°á»Ÿng**, Ä‘Ãºng phÃ­a vÃ  xa siÃªu pháº³ng

2. **Äiá»ƒm B** ($$x=3, y=+1$$):
   - $$y(\beta^T x + \beta_0) = 0.5 < 1$$ â†’ Vi pháº¡m!
   - Cáº§n $$\xi_B = 0.5$$ Ä‘á»ƒ: $$0.5 \geq 1 - 0.5$$ âœ“
   - Váº«n phÃ¢n loáº¡i Ä‘Ãºng nhÆ°ng quÃ¡ gáº§n siÃªu pháº³ng

3. **Äiá»ƒm C** ($$x=2.5, y=+1$$):
   - Náº±m chÃ­nh xÃ¡c trÃªn siÃªu pháº³ng
   - $$\xi_C = 1.0$$ (ngÆ°á»¡ng giá»›i háº¡n)

4. **Äiá»ƒm D** ($$x=2, y=+1$$):
   - Náº±m sai phÃ­a (bÃªn Ã¢m) máº·c dÃ¹ nhÃ£n lÃ  +1
   - $$\xi_D = 1.5 > 1$$ â†’ Bá»‹ phÃ¢n loáº¡i sai!
   - ÄÃ³ng gÃ³p lá»›n vÃ o penalty $$C \cdot \xi_D$$

**Tá»•ng penalty:**
$$
\sum_{i=1}^n \xi_i = 0 + 0.5 + 1.0 + 1.5 + 0 + 0.5 = 3.5
$$

**HÃ m má»¥c tiÃªu:**
$$
\frac{1}{2}(1)^2 + C \cdot 3.5 = 0.5 + 3.5C
$$

- Náº¿u $$C = 1$$: Objective = $$0.5 + 3.5 = 4.0$$
- Náº¿u $$C = 10$$: Objective = $$0.5 + 35 = 35.5$$ (pháº¡t náº·ng!)
- Náº¿u $$C = 0.1$$: Objective = $$0.5 + 0.35 = 0.85$$ (Ã­t quan tÃ¢m Ä‘áº¿n vi pháº¡m)

### 5.3. Minh há»a tÆ°Æ¡ng tÃ¡c: áº¢nh hÆ°á»Ÿng cá»§a tham sá»‘ C

<div id="softmargin-demo" style="margin: 20px 0;">
    <canvas id="softMarginCanvas" width="700" height="500" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    <div style="text-align: center; margin-top: 15px;">
        <label for="cSlider" style="font-size: 16px; font-weight: bold;">Tham sá»‘ C: <span id="cValue">1.0</span></label><br>
        <input type="range" id="cSlider" min="0.01" max="10" step="0.1" value="1" style="width: 400px; margin: 10px;">
        <p style="font-size: 14px; color: #666; margin-top: 10px;">
            <strong>C nhá» (â†)</strong>: Margin rá»™ng, nhiá»u vi pháº¡m Ä‘Æ°á»£c cháº¥p nháº­n<br>
            <strong>C lá»›n (â†’)</strong>: Margin háº¹p, Ã­t vi pháº¡m, pháº¡t náº·ng
        </p>
        <div id="svmStats" style="margin-top: 10px; font-size: 14px;">
            <span style="color: #e74c3c; font-weight: bold;">â— Vi pháº¡m margin: 0</span> | 
            <span style="color: #f39c12; font-weight: bold;">â— Support vectors: 0</span>
        </div>
    </div>
</div>

<script>
(function() {
    const canvas = document.getElementById('softMarginCanvas');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const width = canvas.width;
    const height = canvas.height;
    
    // Dá»¯ liá»‡u vá»›i má»™t sá»‘ Ä‘iá»ƒm gÃ¢y overlap
    const points = [
        // Class +1
        {x: 480, y: 150, label: 1},
        {x: 520, y: 100, label: 1},
        {x: 550, y: 200, label: 1},
        {x: 500, y: 250, label: 1},
        {x: 530, y: 180, label: 1},
        // Outliers class +1
        {x: 350, y: 280, label: 1}, // Vi pháº¡m
        {x: 380, y: 320, label: 1}, // Vi pháº¡m
        // Class -1
        {x: 180, y: 250, label: -1},
        {x: 220, y: 300, label: -1},
        {x: 260, y: 350, label: -1},
        {x: 200, y: 400, label: -1},
        {x: 240, y: 280, label: -1},
        // Outliers class -1
        {x: 420, y: 180, label: -1}, // Vi pháº¡m
        {x: 400, y: 220, label: -1}  // Vi pháº¡m
    ];
    
    let C = 1.0;
    
    function softMarginSVM() {
        // Simplified soft-margin SVM
        let posPoints = points.filter(p => p.label === 1);
        let negPoints = points.filter(p => p.label === -1);
        
        if (posPoints.length === 0 || negPoints.length === 0) return null;
        
        // TÃ­nh centroid vá»›i trá»ng sá»‘ dá»±a trÃªn C
        // C lá»›n -> Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers
        // C nhá» -> cháº¥p nháº­n outliers nhiá»u hÆ¡n
        
        let posCenter = {
            x: posPoints.reduce((sum, p) => sum + p.x, 0) / posPoints.length,
            y: posPoints.reduce((sum, p) => sum + p.y, 0) / posPoints.length
        };
        
        let negCenter = {
            x: negPoints.reduce((sum, p) => sum + p.x, 0) / negPoints.length,
            y: negPoints.reduce((sum, p) => sum + p.y, 0) / negPoints.length
        };
        
        // Äiá»u chá»‰nh hyperplane dá»±a trÃªn C
        let dx = posCenter.x - negCenter.x;
        let dy = posCenter.y - negCenter.y;
        
        // Vá»›i C nhá», cho phÃ©p hyperplane linh hoáº¡t hÆ¡n
        let flexibility = Math.max(0.1, 1 / C);
        
        let midX = (posCenter.x + negCenter.x) / 2;
        let midY = (posCenter.y + negCenter.y) / 2;
        
        let w = {x: dx, y: dy};
        let b = -(w.x * midX + w.y * midY);
        let norm = Math.sqrt(w.x * w.x + w.y * w.y);
        
        w.x /= norm;
        w.y /= norm;
        b /= norm;
        
        // TÃ­nh margin base
        let distances = points.map(p => ({
            point: p,
            dist: Math.abs(w.x * p.x + w.y * p.y + b),
            signedDist: (w.x * p.x + w.y * p.y + b) * p.label
        }));
        
        // Soft margin: margin Ä‘iá»u chá»‰nh theo C
        let baseMargin = 50;
        let margin = baseMargin / (1 + C * 0.5);
        
        // TÃ¬m violations vÃ  support vectors
        let violations = distances.filter(d => d.signedDist < margin);
        let supportVectors = distances.filter(d => 
            Math.abs(d.dist - margin) < 20 || d.signedDist < margin
        );
        
        return {
            w, b, margin,
            violations: violations.map(v => v.point),
            supportVectors: supportVectors.map(v => v.point),
            distances
        };
    }
    
    function draw() {
        ctx.clearRect(0, 0, width, height);
        
        // Background
        ctx.fillStyle = '#fafafa';
        ctx.fillRect(0, 0, width, height);
        
        let svm = softMarginSVM();
        
        if (svm) {
            // Váº½ margin region (vÃ¹ng mÃ u nháº¡t)
            ctx.fillStyle = 'rgba(52, 152, 219, 0.1)';
            ctx.fillRect(0, 0, width, height);
            
            // Váº½ margin boundaries
            ctx.strokeStyle = '#95a5a6';
            ctx.lineWidth = 2;
            ctx.setLineDash([5, 5]);
            
            let b1 = svm.b + svm.margin;
            let b2 = svm.b - svm.margin;
            drawLine(svm.w, b1);
            drawLine(svm.w, b2);
            
            ctx.setLineDash([]);
            
            // Váº½ hyperplane
            ctx.strokeStyle = '#2c3e50';
            ctx.lineWidth = 3;
            drawLine(svm.w, svm.b);
            
            // Highlight violations (slack variables)
            svm.violations.forEach(v => {
                ctx.strokeStyle = 'rgba(231, 76, 60, 0.5)';
                ctx.lineWidth = 20;
                ctx.beginPath();
                ctx.arc(v.x, v.y, 12, 0, 2 * Math.PI);
                ctx.stroke();
            });
            
            // Highlight support vectors
            svm.supportVectors.forEach(sv => {
                ctx.strokeStyle = '#f39c12';
                ctx.lineWidth = 3;
                ctx.beginPath();
                ctx.arc(sv.x, sv.y, 15, 0, 2 * Math.PI);
                ctx.stroke();
            });
            
            // Update stats
            const statsDiv = document.getElementById('svmStats');
            if (statsDiv) {
                statsDiv.innerHTML = `
                    <span style="color: #e74c3c; font-weight: bold;">â— Vi pháº¡m margin: ${svm.violations.length}</span> | 
                    <span style="color: #f39c12; font-weight: bold;">â— Support vectors: ${svm.supportVectors.length}</span> | 
                    <span style="color: #3498db; font-weight: bold;">Margin width: ${(svm.margin * 2).toFixed(1)}</span>
                `;
            }
        }
        
        // Váº½ Ä‘iá»ƒm dá»¯ liá»‡u
        points.forEach(p => {
            ctx.fillStyle = p.label === 1 ? '#e74c3c' : '#2ecc71';
            ctx.beginPath();
            ctx.arc(p.x, p.y, 8, 0, 2 * Math.PI);
            ctx.fill();
            
            ctx.strokeStyle = '#34495e';
            ctx.lineWidth = 2;
            ctx.stroke();
        });
        
        // Legend
        ctx.fillStyle = '#34495e';
        ctx.font = 'bold 14px Arial';
        ctx.fillText(`C = ${C.toFixed(2)}`, 10, 30);
        
        // Giáº£i thÃ­ch C
        if (C < 0.5) {
            ctx.fillText('Soft margin: Cháº¥p nháº­n nhiá»u vi pháº¡m', 10, 55);
        } else if (C > 5) {
            ctx.fillText('Hard margin: Pháº¡t náº·ng vi pháº¡m', 10, 55);
        } else {
            ctx.fillText('Balanced: Trade-off margin vs error', 10, 55);
        }
    }
    
    function drawLine(w, b) {
        ctx.beginPath();
        if (Math.abs(w.y) > 0.01) {
            let y0 = -(w.x * 0 + b) / w.y;
            let y1 = -(w.x * width + b) / w.y;
            ctx.moveTo(0, y0);
            ctx.lineTo(width, y1);
        } else {
            let x = -b / w.x;
            ctx.moveTo(x, 0);
            ctx.lineTo(x, height);
        }
        ctx.stroke();
    }
    
    // Slider event
    const slider = document.getElementById('cSlider');
    const cValue = document.getElementById('cValue');
    
    if (slider && cValue) {
        slider.addEventListener('input', function() {
            C = parseFloat(this.value);
            cValue.textContent = C.toFixed(2);
            draw();
        });
    }
    
    // Initial draw
    draw();
})();
</script>

---

## 5A. Hinge Loss: Loss Function cá»§a SVM

Sau khi hiá»ƒu bÃ i toÃ¡n tá»‘i Æ°u cá»§a SVM, chÃºng ta sáº½ tÃ¬m hiá»ƒu má»™t **gÃ³c nhÃ¬n khÃ¡c** - nhÃ¬n SVM nhÆ° má»™t bÃ i toÃ¡n **minimizing loss function**, tÆ°Æ¡ng tá»± nhÆ° cÃ¡c thuáº­t toÃ¡n machine learning khÃ¡c.

### 5A.1. Tá»« rÃ ng buá»™c Ä‘áº¿n Loss Function

**NhÃ¬n láº¡i Soft-Margin SVM:**

$$
\begin{align}
\min_{\beta, \beta_0, \xi} \quad &\frac{1}{2}\|\beta\|^2 + C\sum_{i=1}^n \xi_i \\
\text{s.t.} \quad &y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i \\
&\xi_i \geq 0, \quad i = 1, \ldots, n
\end{align}
$$

**CÃ¢u há»i tá»± nhiÃªn:** CÃ³ thá»ƒ loáº¡i bá» biáº¿n phá»¥ $$\xi_i$$ vÃ  rÃ ng buá»™c khÃ´ng?

**Tráº£ lá»i:** CÃ³! Tá»« rÃ ng buá»™c $$\xi_i \geq 0$$ vÃ  $$y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i$$, ta cÃ³:

$$
\xi_i \geq \max\{0, 1 - y_i(\beta^T x_i + \beta_0)\}
$$

Äá»ƒ minimize hÃ m má»¥c tiÃªu, ta chá»n $$\xi_i$$ nhá» nháº¥t cÃ³ thá»ƒ:

$$
\xi_i = \max\{0, 1 - y_i(\beta^T x_i + \beta_0)\}
$$

**Káº¿t quáº£:** BÃ i toÃ¡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i:

$$
\min_{\beta, \beta_0} \quad \frac{1}{2}\|\beta\|^2 + C\sum_{i=1}^n \underbrace{\max\{0, 1 - y_i(\beta^T x_i + \beta_0)\}}_{\text{Hinge Loss } \ell(y_i, f(x_i))}
$$

ÄÃ¢y chÃ­nh lÃ  **formulation vá»›i hinge loss**!

### 5A.2. Äá»‹nh nghÄ©a Hinge Loss

**Hinge Loss** cho má»™t Ä‘iá»ƒm dá»¯ liá»‡u $$(x_i, y_i)$$ vá»›i dá»± Ä‘oÃ¡n $$f(x_i) = \beta^T x_i + \beta_0$$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

$$
\ell_{\text{hinge}}(y_i, f(x_i)) = \max\{0, 1 - y_i f(x_i)\}
$$

hoáº·c viáº¿t gá»n hÆ¡n:

$$
\ell_{\text{hinge}}(y, f) = [1 - yf]_+ = \begin{cases}
0 & \text{náº¿u } yf \geq 1 \\
1 - yf & \text{náº¿u } yf < 1
\end{cases}
$$

vá»›i kÃ½ hiá»‡u $$[\cdot]_+ = \max\{0, \cdot\}$$ (pháº§n dÆ°Æ¡ng).

**Ã nghÄ©a cá»§a tá»«ng trÆ°á»ng há»£p:**

| $$yf(x)$$ | Diá»…n giáº£i | Hinge Loss | Ã nghÄ©a |
|-----------|-----------|------------|---------|
| $$> 1$$ | PhÃ¢n loáº¡i Ä‘Ãºng, xa margin | $$0$$ | KhÃ´ng bá»‹ pháº¡t, Ä‘iá»ƒm lÃ½ tÆ°á»Ÿng |
| $$= 1$$ | PhÃ¢n loáº¡i Ä‘Ãºng, trÃªn margin | $$0$$ | BiÃªn giá»›i, khÃ´ng pháº¡t |
| $$0 < yf < 1$$ | PhÃ¢n loáº¡i Ä‘Ãºng, trong margin | $$1 - yf > 0$$ | Bá»‹ pháº¡t vÃ¬ vi pháº¡m margin |
| $$yf = 0$$ | TrÃªn siÃªu pháº³ng quyáº¿t Ä‘á»‹nh | $$1$$ | Pháº¡t má»©c trung bÃ¬nh |
| $$yf < 0$$ | **PhÃ¢n loáº¡i sai** | $$1 - yf > 1$$ | Pháº¡t náº·ng, sai cÃ ng nhiá»u pháº¡t cÃ ng náº·ng |

**Äá»“ thá»‹ Hinge Loss:**

```
Loss
  |
4 |           /
  |          /
3 |         /  
  |        /   
2 |       /    
  |      /     
1 |_____/      
  |            
0 |____________
  -2  -1  0  1  2  yf(x)
      â†‘     â†‘
    sai   Ä‘Ãºng
```

### 5A.3. So sÃ¡nh vá»›i cÃ¡c Loss Function khÃ¡c

**1) Zero-One Loss (0-1 Loss):**

$$
\ell_{0-1}(y, f) = \mathbb{I}[yf < 0] = \begin{cases}
0 & \text{náº¿u phÃ¢n loáº¡i Ä‘Ãºng} \\
1 & \text{náº¿u phÃ¢n loáº¡i sai}
\end{cases}
$$

- **Ã nghÄ©a**: Äáº¿m sá»‘ lá»—i phÃ¢n loáº¡i, Ä‘Ã¢y lÃ  má»¥c tiÃªu thá»±c sá»± trong classification
- **Váº¥n Ä‘á»**: KhÃ´ng kháº£ vi, khÃ´ng lá»“i, NP-hard Ä‘á»ƒ tá»‘i Æ°u!

**2) Logistic Loss (Log Loss):**

$$
\ell_{\text{logistic}}(y, f) = \log(1 + e^{-yf})
$$

- **DÃ¹ng trong**: Logistic Regression
- **Äáº·c Ä‘iá»ƒm**: Smooth (kháº£ vi má»i nÆ¡i), lá»“i
- **So vá»›i hinge**: KhÃ´ng bao giá» báº±ng 0, luÃ´n pháº¡t (nháº¹) ngay cáº£ khi Ä‘Ãºng

**3) Exponential Loss:**

$$
\ell_{\text{exp}}(y, f) = e^{-yf}
$$

- **DÃ¹ng trong**: AdaBoost
- **Äáº·c Ä‘iá»ƒm**: Pháº¡t ráº¥t náº·ng vá»›i outliers

**4) Squared Hinge Loss:**

$$
\ell_{\text{squared}}(y, f) = [1 - yf]_+^2
$$

- **Äáº·c Ä‘iá»ƒm**: Kháº£ vi táº¡i $$yf = 1$$, pháº¡t náº·ng hÆ¡n hinge loss chuáº©n

**Báº£ng so sÃ¡nh trá»±c quan:**

| Loss Function | CÃ´ng thá»©c | Lá»“i | Kháº£ vi | Robust vá»›i outliers | Sparse solution |
|---------------|-----------|-----|--------|---------------------|-----------------|
| **0-1 Loss** | $$\mathbb{I}[yf<0]$$ | âŒ | âŒ | âœ… | - |
| **Hinge Loss** | $$[1-yf]_+$$ | âœ… | âš ï¸ (subgradient) | âœ… | âœ… |
| **Logistic Loss** | $$\log(1+e^{-yf})$$ | âœ… | âœ… | âš ï¸ | âŒ |
| **Exponential Loss** | $$e^{-yf}$$ | âœ… | âœ… | âŒ | âŒ |
| **Squared Hinge** | $$[1-yf]_+^2$$ | âœ… | âœ… | âš ï¸ | âŒ |

**Táº¡i sao Hinge Loss phÃ¹ há»£p vá»›i SVM?**

1. **Lá»“i**: Äáº£m báº£o bÃ i toÃ¡n tá»‘i Æ°u cÃ³ nghiá»‡m duy nháº¥t
2. **Margin-based**: Pháº¡t cáº£ Ä‘iá»ƒm Ä‘Ãºng nhÆ°ng gáº§n margin (khÃ´ng nhÆ° logistic chá»‰ quan tÃ¢m Ä‘Ãºng/sai)
3. **Sparse**: Loss báº±ng 0 khi $$yf \geq 1$$ â†’ Chá»‰ support vectors Ä‘Ã³ng gÃ³p
4. **Robust**: Pháº¡t tuyáº¿n tÃ­nh vá»›i outliers (khÃ´ng nhÆ° exponential pháº¡t quÃ¡ náº·ng)

### 5A.4. Hinge Loss vÃ  Regularization

**BÃ i toÃ¡n SVM vá»›i Hinge Loss:**

$$
\min_{\beta, \beta_0} \quad \underbrace{\frac{1}{2}\|\beta\|^2}_{\text{Regularization}} + C\sum_{i=1}^n \underbrace{[1 - y_i(\beta^T x_i + \beta_0)]_+}_{\text{Hinge Loss}}
$$

ÄÃ¢y lÃ  dáº¡ng **Regularized Empirical Risk Minimization**:

$$
\min_{\beta, \beta_0} \quad \underbrace{\frac{\lambda}{2}\|\beta\|^2}_{\text{Regularization}} + \underbrace{\frac{1}{n}\sum_{i=1}^n \ell(y_i, f(x_i))}_{\text{Empirical Risk}}
$$

vá»›i $$\lambda = \frac{1}{C \cdot n}$$.

**Hai vai trÃ²:**

1. **$$\frac{1}{2}\|\beta\|^2$$**: 
   - Regularization term (Ridge regularization)
   - Prevent overfitting
   - Encourage large margin
   - Smooth solution

2. **$$\sum [1 - y_i f(x_i)]_+$$**:
   - Data fitting term
   - Minimize training error
   - Ensure correct classification

**Trade-off Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi $$C$$:**

$$
\begin{cases}
C \to 0: & \text{Æ¯u tiÃªn regularization} \to \text{Large margin, simple model} \\
C \to \infty: & \text{Æ¯u tiÃªn data fitting} \to \text{Small margin, fit data closely}
\end{cases}
$$

### 5A.5. Hinge Loss trong Deep Learning

Hinge loss khÃ´ng chá»‰ dÃ¹ng cho SVM, nÃ³ cÃ²n xuáº¥t hiá»‡n trong:

**1) Multi-class Hinge Loss:**

$$
\ell_{\text{multiclass}}(y, f) = \sum_{j \neq y} [1 + f_j - f_y]_+
$$

- DÃ¹ng trong multi-class SVM
- $$f_j$$: score cho class $$j$$
- $$y$$: true class

**2) Triplet Loss (trong Metric Learning):**

$$
\ell_{\text{triplet}} = [\|f(a) - f(p)\|^2 - \|f(a) - f(n)\|^2 + \text{margin}]_+
$$

- $$a$$: anchor, $$p$$: positive, $$n$$: negative
- DÃ¹ng trong face recognition, siamese networks

**3) Ranking Loss:**

$$
\ell_{\text{rank}} = [1 - (f(x^+) - f(x^-))]_+
$$

- DÃ¹ng trong learning to rank, recommendation systems

### 5A.6. Subgradient vÃ  Optimization

**Váº¥n Ä‘á»**: Hinge loss **khÃ´ng kháº£ vi** táº¡i $$yf = 1$$!

**Giáº£i phÃ¡p**: DÃ¹ng **subgradient**

Táº¡i Ä‘iá»ƒm $$yf = 1$$, subgradient cá»§a $$[1 - yf]_+$$ lÃ  báº¥t ká»³ giÃ¡ trá»‹ trong $$[-1, 0]$$.

**Subgradient cá»§a hinge loss:**

$$
\partial \ell_{\text{hinge}}(y, f) = \begin{cases}
\{0\} & \text{náº¿u } yf > 1 \\
[-1, 0] & \text{náº¿u } yf = 1 \\
\{-y\} & \text{náº¿u } yf < 1
\end{cases}
$$

**Gradient cá»§a bÃ i toÃ¡n SVM:**

$$
\nabla_\beta J(\beta) = \beta + C\sum_{i=1}^n \partial_\beta [1 - y_i(\beta^T x_i + \beta_0)]_+
$$

$$
= \beta - C\sum_{i: y_i f(x_i) < 1} y_i x_i
$$

ÄÃ¢y lÃ  cÆ¡ sá»Ÿ cho cÃ¡c thuáº­t toÃ¡n nhÆ° **Subgradient Descent** vÃ  **Stochastic Gradient Descent** Ä‘á»ƒ train SVM.

### 5A.7. Káº¿t ná»‘i vá»›i Soft-Margin SVM

**Ba formulation tÆ°Æ¡ng Ä‘Æ°Æ¡ng:**

**1) Constrained form (ban Ä‘áº§u):**
$$
\min \frac{1}{2}\|\beta\|^2 + C\sum \xi_i \quad \text{s.t. } y_i f(x_i) \geq 1 - \xi_i, \xi_i \geq 0
$$

**2) Unconstrained with Hinge Loss:**
$$
\min \frac{1}{2}\|\beta\|^2 + C\sum [1 - y_i f(x_i)]_+
$$

**3) Dual form:**
$$
\max \sum w_i - \frac{1}{2}\sum_{i,j} w_i w_j y_i y_j \langle x_i, x_j \rangle \quad \text{s.t. } 0 \leq w_i \leq C, \sum w_i y_i = 0
$$

Ba formulation nÃ y **hoÃ n toÃ n tÆ°Æ¡ng Ä‘Æ°Æ¡ng**, cho cÃ¹ng má»™t nghiá»‡m tá»‘i Æ°u!

**Khi nÃ o dÃ¹ng formulation nÃ o?**

| Formulation | Khi nÃ o dÃ¹ng | Solver | Æ¯u Ä‘iá»ƒm |
|-------------|--------------|--------|---------|
| **Constrained** | LÃ½ thuyáº¿t, phÃ¢n tÃ­ch KKT | QP solver | Dá»… phÃ¢n tÃ­ch, rÃµ rÃ ng vá» rÃ ng buá»™c |
| **Hinge Loss** | Training vá»›i SGD | Subgradient descent | Nhanh, scalable, dá»… implement |
| **Dual** | Kernel methods | SMO, LIBSVM | Kernel trick, sparse solution |

---

## 6. BÃ i toÃ¡n Äá»‘i ngáº«u (Dual Problem)

### 6.1. Táº¡i sao cáº§n Dual Problem? - CÃ¡nh cá»­a dáº«n Ä‘áº¿n sá»©c máº¡nh thá»±c sá»± cá»§a SVM

CÃ³ váº» nhÆ° chÃºng ta Ä‘Ã£ cÃ³ bÃ i toÃ¡n Soft-Margin SVM hoÃ n chá»‰nh á»Ÿ dáº¡ng **primal** (gá»‘c). Táº¡i sao láº¡i cáº§n má»™t "phiÃªn báº£n Ä‘á»‘i ngáº«u" (dual)?

CÃ¢u tráº£ lá»i lÃ : **BÃ i toÃ¡n Ä‘á»‘i ngáº«u má»Ÿ ra nhá»¯ng kháº£ nÄƒng mÃ  bÃ i toÃ¡n gá»‘c khÃ´ng thá»ƒ lÃ m Ä‘Æ°á»£c!**

**Ba lÃ½ do then chá»‘t khiáº¿n Dual Problem trá»Ÿ thÃ nh "ngÃ´i sao" trong thá»±c táº¿:**

1. **Kernel Trick - PhÃ©p mÃ u cá»§a SVM**: 
   - Dual form chá»‰ phá»¥ thuá»™c vÃ o **tÃ­ch vÃ´ hÆ°á»›ng** $$\langle x_i, x_j \rangle$$ giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u
   - Ta cÃ³ thá»ƒ thay tháº¿ báº±ng **kernel function** $$K(x_i, x_j)$$ Ä‘á»ƒ Ã¡nh xáº¡ ngáº§m lÃªn khÃ´ng gian vÃ´ háº¡n chiá»u
   - Äiá»u nÃ y cho phÃ©p SVM xá»­ lÃ½ dá»¯ liá»‡u **phi tuyáº¿n phá»©c táº¡p** mÃ  khÃ´ng tÄƒng Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n!

2. **Hiá»‡u quáº£ khi sá»‘ chiá»u lá»›n**:
   - Primal: $$p + 1 + n$$ biáº¿n ($$p$$ cÃ³ thá»ƒ ráº¥t lá»›n, tháº­m chÃ­ vÃ´ háº¡n vá»›i kernel)
   - Dual: chá»‰ $$n$$ biáº¿n (sá»‘ máº«u, thÆ°á»ng nhá» hÆ¡n nhiá»u)
   - Khi $$n \ll p$$ (vÃ­ dá»¥: phÃ¢n loáº¡i vÄƒn báº£n, gene expression), dual nhanh hÆ¡n Ä‘Ã¡ng ká»ƒ

3. **Sparsity tá»± nhiÃªn**:
   - Nghiá»‡m dual cÃ³ tÃ­nh sparse: chá»‰ **support vectors** cÃ³ $$w_i > 0$$
   - Pháº§n lá»›n $$w_i = 0$$ â†’ dá»± Ä‘oÃ¡n nhanh, tiáº¿t kiá»‡m bá»™ nhá»›
   - Dá»… hiá»ƒu: "Nhá»¯ng Ä‘iá»ƒm nÃ o quyáº¿t Ä‘á»‹nh siÃªu pháº³ng?"

### 6.2. XÃ¢y dá»±ng Dual Problem

**BÃ i toÃ¡n Primal:**
$$
\begin{align}
\min_{\beta, \beta_0, \xi} \quad &\frac{1}{2}\|\beta\|^2 + C\sum_{i=1}^n \xi_i \\
\text{s.t.} \quad &y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i \\
&\xi_i \geq 0
\end{align}
$$

**Lagrangian:**
$$
L(\beta, \beta_0, \xi, w, v) = \frac{1}{2}\|\beta\|^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n w_i[y_i(\beta^T x_i + \beta_0) - 1 + \xi_i] - \sum_{i=1}^n v_i \xi_i
$$

trong Ä‘Ã³ $$w_i \geq 0, v_i \geq 0$$ lÃ  cÃ¡c Lagrange multipliers.

**Äiá»u kiá»‡n KKT - Stationarity:**

Láº¥y Ä‘áº¡o hÃ m theo cÃ¡c biáº¿n primal vÃ  cho báº±ng 0:

1. $$\frac{\partial L}{\partial \beta} = 0 \Rightarrow \beta = \sum_{i=1}^n w_i y_i x_i$$

2. $$\frac{\partial L}{\partial \beta_0} = 0 \Rightarrow \sum_{i=1}^n w_i y_i = 0$$

3. $$\frac{\partial L}{\partial \xi_i} = 0 \Rightarrow C - w_i - v_i = 0 \Rightarrow w_i \leq C$$

**Tháº¿ vÃ o Lagrangian**, ta Ä‘Æ°á»£c **Dual Problem:**

$$
\begin{align}
\max_{w} \quad &\sum_{i=1}^n w_i - \frac{1}{2} \sum_{i,j=1}^n w_i w_j y_i y_j \langle x_i, x_j \rangle \\
\text{s.t.} \quad &\sum_{i=1}^n w_i y_i = 0 \\
&0 \leq w_i \leq C, \quad i = 1, \ldots, n
\end{align}
$$

**Hoáº·c dáº¡ng minimization:**
$$
\begin{align}
\min_{w} \quad &\frac{1}{2} \sum_{i,j=1}^n w_i w_j y_i y_j \langle x_i, x_j \rangle - \sum_{i=1}^n w_i \\
\text{s.t.} \quad &\sum_{i=1}^n w_i y_i = 0 \\
&0 \leq w_i \leq C, \quad i = 1, \ldots, n
\end{align}
$$

### 6.3. Ã nghÄ©a sÃ¢u sáº¯c cá»§a Dual Variables - "GiÃ¡ trá»‹" cá»§a má»—i Ä‘iá»ƒm dá»¯ liá»‡u

**Biáº¿n Ä‘á»‘i ngáº«u $$w_i$$** (Lagrange multiplier) khÃ´ng chá»‰ lÃ  má»™t con sá»‘ trá»«u tÆ°á»£ng - nÃ³ **"Ä‘á»‹nh giÃ¡"** táº§m quan trá»ng cá»§a má»—i Ä‘iá»ƒm dá»¯ liá»‡u trong viá»‡c quyáº¿t Ä‘á»‹nh siÃªu pháº³ng!

HÃ£y "Ä‘á»c" Ã½ nghÄ©a cá»§a $$w_i$$:

- **$$w_i = 0$$**: Äiá»ƒm $$x_i$$ lÃ  **"ngÆ°á»i ngoÃ i cuá»™c"**
  - Náº±m xa margin, phÃ¢n loáº¡i Ä‘Ãºng vÃ  an toÃ n
  - **KhÃ´ng Ä‘Ã³ng gÃ³p gÃ¬** vÃ o siÃªu pháº³ng: cÃ³ thá»ƒ xÃ³a Ä‘iá»ƒm nÃ y mÃ  mÃ´ hÃ¬nh khÃ´ng Ä‘á»•i!
  - Chiáº¿m Ä‘a sá»‘ cÃ¡c Ä‘iá»ƒm trong dataset

- **$$0 < w_i < C$$**: Äiá»ƒm $$x_i$$ lÃ  **"support vector lÃ½ tÆ°á»Ÿng"**
  - Náº±m **chÃ­nh xÃ¡c trÃªn margin boundary** ($$\xi_i = 0$$)
  - PhÃ¢n loáº¡i Ä‘Ãºng, nhÆ°ng á»Ÿ vá»‹ trÃ­ "nguy hiá»ƒm" nháº¥t
  - ÄÃ¢y lÃ  nhá»¯ng Ä‘iá»ƒm **quan trá»ng nháº¥t**, quyáº¿t Ä‘á»‹nh Ä‘á»™ nghiÃªng vÃ  vá»‹ trÃ­ cá»§a siÃªu pháº³ng
  - ThÆ°á»ng ráº¥t Ã­t: chá»‰ 5-20% sá»‘ Ä‘iá»ƒm

- **$$w_i = C$$**: Äiá»ƒm $$x_i$$ lÃ  **"support vector váº¥n Ä‘á»"**
  - Vi pháº¡m margin ($$\xi_i > 0$$): hoáº·c náº±m trong margin, hoáº·c bá»‹ phÃ¢n loáº¡i sai
  - "ÄÃ³ng gÃ³p" vÃ o siÃªu pháº³ng nhÆ°ng cÅ©ng **bá»‹ pháº¡t** trong hÃ m má»¥c tiÃªu
  - ThÆ°á»ng lÃ  outliers, Ä‘iá»ƒm nhiá»…u, hoáº·c Ä‘iá»ƒm thuá»™c vÃ¹ng chá»“ng láº¥n giá»¯a hai lá»›p

### 6.4. TÃ­nh $$\beta_0$$ tá»« nghiá»‡m Dual

Sau khi giáº£i dual problem vÃ  cÃ³ $$w^\star$$, ta tÃ­nh:

1. **$$\beta^\star$$** tá»« cÃ´ng thá»©c: $$\beta^\star = \sum_{i=1}^n w_i^\star y_i x_i$$

2. **$$\beta_0^\star$$** tá»« support vector trÃªn margin ($$0 < w_i^\star < C$$, $$\xi_i^\star = 0$$):
   $$
   \beta_0^\star = y_i - \beta^{\star T} x_i
   $$
   
   Trong thá»±c táº¿, Ä‘á»ƒ robust hÆ¡n, ta láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ SV trÃªn margin:
   $$
   \beta_0^\star = \frac{1}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} (y_i - \beta^{\star T} x_i)
   $$
   vá»›i $$\mathcal{S} = \{i: 0 < w_i^\star < C\}$$

### 6.5. Æ¯u Ä‘iá»ƒm cá»§a Dual Formulation

1. **Kernel trick**: Chá»‰ phá»¥ thuá»™c vÃ o $$\langle x_i, x_j \rangle$$ â†’ cÃ³ thá»ƒ thay báº±ng $$K(x_i, x_j)$$
2. **Ãt rÃ ng buá»™c**: Chá»‰ 1 rÃ ng buá»™c Ä‘áº³ng thá»©c + box constraints
3. **Solver hiá»‡u quáº£**: SMO (Sequential Minimal Optimization), LIBSVM
4. **Sparsity tá»± nhiÃªn**: Chá»‰ support vectors cÃ³ $$w_i > 0$$

---

## 7. Support Vectors: Nhá»¯ng "trá»¥ cá»™t" cá»§a mÃ´ hÃ¬nh

Giá» Ä‘Ã¢y chÃºng ta Ä‘áº¿n vá»›i **khÃ¡i niá»‡m trung tÃ¢m** mÃ  SVM Ä‘Æ°á»£c Ä‘áº·t tÃªn theo: **Support Vectors** (cÃ¡c vector há»— trá»£).

### Äá»‹nh nghÄ©a chÃ­nh xÃ¡c

**Support vectors** lÃ  táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ há»‡ sá»‘ Ä‘á»‘i ngáº«u khÃ¡c khÃ´ng:
$$
\text{Support vectors} = \{x_i : w_i^\star > 0\}
$$

**Táº¡i sao chÃºng quan trá»ng Ä‘áº¿n váº­y?** Nhá»› láº¡i cÃ´ng thá»©c tá»« Ä‘iá»u kiá»‡n KKT stationarity:
$$
\beta^\star = \sum_{i: w_i^\star > 0} w_i^\star y_i x_i = \sum_{\text{support vectors}} w_i^\star y_i x_i
$$

**PhÃ¡t hiá»‡n sá»­ng sá»‘t:** SiÃªu pháº³ng tá»‘i Æ°u **CHá»ˆ phá»¥ thuá»™c vÃ o support vectors!**

- Táº¥t cáº£ Ä‘iá»ƒm khÃ¡c ($$w_i = 0$$) cÃ³ thá»ƒ bá» Ä‘i mÃ  mÃ´ hÃ¬nh khÃ´ng thay Ä‘á»•i
- SiÃªu pháº³ng "Ä‘á»©ng trÃªn vai" cá»§a má»™t sá»‘ Ã­t Ä‘iá»ƒm quan trá»ng
- ÄÃ¢y lÃ  lÃ½ do SVM cÃ³ tÃ­nh **sparse** tá»± nhiÃªn

### Ba loáº¡i Support Vectors - Má»—i loáº¡i má»™t vai trÃ²

<figure class="image" style="align: center;">
<p align="center">
 <img src="{{ site.baseurl }}/img/chapter_img/chapter12/svm.png" alt="" width="70%" height="70%">
 <figcaption style="text-align: center;">$$ \text{[HÃ¬nh 1] Minh há»a cÃ¡c support vectors vá»›i } \xi^\star \neq 0 \text{ [3]}$$ </figcaption>
</p>
</figure>

**1) Support vectors trÃªn margin** ($$\xi_i^\star = 0$$, $$0 < w_i^\star \leq C$$):
- Náº±m Ä‘Ãºng trÃªn Ä‘Æ°á»ng biÃªn margin
- $$y_i(\beta^{\star T} x_i + \beta_0^\star) = 1$$
- Quan trá»ng nháº¥t trong xÃ¡c Ä‘á»‹nh siÃªu pháº³ng

**2) Support vectors trong margin** ($$0 < \xi_i^\star < 1$$, $$w_i^\star = C$$):
- Náº±m giá»¯a margin vÃ  siÃªu pháº³ng
- Váº«n phÃ¢n loáº¡i Ä‘Ãºng nhÆ°ng vi pháº¡m margin
- ThÆ°á»ng lÃ  outliers hoáº·c Ä‘iá»ƒm gáº§n biÃªn

**3) Support vectors bá»‹ misclassify** ($$\xi_i^\star \geq 1$$, $$w_i^\star = C$$):
- Náº±m sai phÃ­a siÃªu pháº³ng
- Bá»‹ phÃ¢n loáº¡i sai
- ÄÃ³ng gÃ³p nhiá»u vÃ o penalty $$C\sum \xi_i$$

---

## 8. Kernel Methods: Tá»« tuyáº¿n tÃ­nh Ä‘áº¿n phi tuyáº¿n

Cho Ä‘áº¿n giá», chÃºng ta má»›i chá»‰ lÃ m viá»‡c vá»›i **siÃªu pháº³ng tuyáº¿n tÃ­nh** - má»™t "Ä‘Æ°á»ng tháº³ng" (hoáº·c máº·t pháº³ng trong khÃ´ng gian nhiá»u chiá»u) Ä‘á»ƒ phÃ¢n chia dá»¯ liá»‡u. NhÆ°ng dá»¯ liá»‡u thá»±c táº¿ thÆ°á»ng cÃ³ **ranh giá»›i phá»©c táº¡p, phi tuyáº¿n**. VÃ­ dá»¥: hai lá»›p dá»¯ liá»‡u phÃ¢n bá»‘ theo hÃ¬nh trÃ²n Ä‘á»“ng tÃ¢m - khÃ´ng cÃ³ Ä‘Æ°á»ng tháº³ng nÃ o tÃ¡ch Ä‘Æ°á»£c chÃºng!

ÄÃ¢y chÃ­nh lÃ  lÃºc **Kernel Trick** - má»™t trong nhá»¯ng Ã½ tÆ°á»Ÿng tinh táº¿ vÃ  máº¡nh máº½ nháº¥t trong machine learning - phÃ¡t huy tÃ¡c dá»¥ng.

### 8.0. NhÃ¬n láº¡i: Dual Problem vÃ  vai trÃ² cá»§a tÃ­ch vÃ´ hÆ°á»›ng

TrÆ°á»›c khi Ä‘i vÃ o kernel, hÃ£y nhÃ¬n láº¡i **bÃ i toÃ¡n Ä‘á»‘i ngáº«u** Ä‘Ã£ há»c á»Ÿ Section 6:

$$
\min_{w} \frac{1}{2} \sum_{i,j=1}^n w_i w_j y_i y_j \underbrace{\langle x_i, x_j \rangle}_{\text{TÃ­ch vÃ´ hÆ°á»›ng!}} - \sum_{i=1}^n w_i
$$

**Quan sÃ¡t then chá»‘t:** BÃ i toÃ¡n dual **CHá»ˆ phá»¥ thuá»™c** vÃ o tÃ­ch vÃ´ hÆ°á»›ng $$\langle x_i, x_j \rangle$$ giá»¯a cÃ¡c cáº·p Ä‘iá»ƒm dá»¯ liá»‡u, KHÃ”NG phá»¥ thuá»™c trá»±c tiáº¿p vÃ o $$x_i$$ hay $$x_j$$ riÃªng láº»!

**Há»‡ quáº£ quan trá»ng:** Náº¿u ta thay tÃ­ch vÃ´ hÆ°á»›ng $$\langle x_i, x_j \rangle$$ báº±ng má»™t hÃ m khÃ¡c $$K(x_i, x_j)$$, toÃ n bá»™ bÃ i toÃ¡n váº«n giáº£i Ä‘Æ°á»£c, miá»…n lÃ  $$K$$ thá»a mÃ£n má»™t sá»‘ Ä‘iá»u kiá»‡n nháº¥t Ä‘á»‹nh.

ÄÃ¢y chÃ­nh lÃ  **cá»­a ngÃµ** dáº«n Ä‘áº¿n kernel methods!

### 8.1. Linear Kernel: Äiá»ƒm khá»Ÿi Ä‘áº§u Ä‘Æ¡n giáº£n nháº¥t

TrÆ°á»›c khi nghÄ© Ä‘áº¿n nhá»¯ng kernel phá»©c táº¡p, hÃ£y báº¯t Ä‘áº§u vá»›i trÆ°á»ng há»£p **Ä‘Æ¡n giáº£n nháº¥t**: chÃ­nh tÃ­ch vÃ´ hÆ°á»›ng thÃ´ng thÆ°á»ng!

**Äá»‹nh nghÄ©a Linear Kernel:**

$$
K_{\text{linear}}(x, z) = \langle x, z \rangle = x^T z = \sum_{j=1}^d x_j z_j
$$

**CÃ³ váº» quÃ¡ Ä‘Æ¡n giáº£n pháº£i khÃ´ng?** NhÆ°ng Ä‘Ã¢y chÃ­nh lÃ  vÃ­ dá»¥ HOÃ€N Háº¢O Ä‘á»ƒ hiá»ƒu kernel!

**Táº¡i sao Linear Kernel lÃ  má»™t kernel?**

CÃ¢u há»i: CÃ³ tá»“n táº¡i Ã¡nh xáº¡ $$\phi$$ sao cho $$K_{\text{linear}}(x, z) = \langle \phi(x), \phi(z) \rangle$$?

CÃ¢u tráº£ lá»i: **CÃ“!** Chá»n Ã¡nh xáº¡ Ä‘Æ¡n giáº£n nháº¥t:

$$
\phi(x) = x \quad \text{(Ã¡nh xáº¡ Ä‘á»“ng nháº¥t - identity map)}
$$

Khi Ä‘Ã³:

$$
K_{\text{linear}}(x, z) = \langle \phi(x), \phi(z) \rangle = \langle x, z \rangle = x^T z \quad \checkmark
$$

**Ã nghÄ©a:**

- Linear kernel = "KHÃ”NG Ã¡nh xáº¡ gÃ¬ cáº£", dá»¯ liá»‡u váº«n á»Ÿ khÃ´ng gian gá»‘c
- SiÃªu pháº³ng phÃ¢n chia váº«n lÃ  **tuyáº¿n tÃ­nh** trong khÃ´ng gian gá»‘c
- ÄÃ¢y chÃ­nh lÃ  SVM thÃ´ng thÆ°á»ng mÃ  chÃºng ta Ä‘Ã£ há»c!

**Táº¡i sao váº«n gá»i nÃ³ lÃ  "kernel"?**

VÃ¬ nÃ³ thá»a mÃ£n **Ä‘iá»u kiá»‡n Mercer** (sáº½ há»c sau):
- **Äá»‘i xá»©ng**: $$K(x, z) = x^T z = z^T x = K(z, x)$$ âœ“
- **Positive semi-definite**: Ma tráº­n Gram $$\mathbf{K}_{ij} = x_i^T x_j$$ luÃ´n PSD âœ“

**Khi nÃ o dÃ¹ng Linear Kernel?**

1. **Baseline Ä‘áº§u tiÃªn**: LuÃ´n thá»­ linear kernel trÆ°á»›c!
2. **Dá»¯ liá»‡u chiá»u cao** ($$p \gg n$$): NhÆ° phÃ¢n loáº¡i vÄƒn báº£n vá»›i hÃ ng nghÃ¬n tá»«
3. **Dá»¯ liá»‡u Ä‘Ã£ tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh**: KhÃ´ng cáº§n phi tuyáº¿n hÃ³a
4. **Cáº§n tá»‘c Ä‘á»™**: Nhanh nháº¥t, Ã­t tham sá»‘ nháº¥t

**VÃ­ dá»¥ á»©ng dá»¥ng - Text Classification:**

Trong phÃ¢n loáº¡i email spam/ham:
- Dá»¯ liá»‡u Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng TF-IDF vá»›i 10,000 tá»« vá»±ng â†’ khÃ´ng gian 10,000 chiá»u
- Linear kernel thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t vÃ¬ dá»¯ liá»‡u vÄƒn báº£n thÆ°á»ng tÃ¡ch Ä‘Æ°á»£c trong khÃ´ng gian TF-IDF
- Nhanh vÃ  hiá»‡u quáº£, khÃ´ng cáº§n phi tuyáº¿n hÃ³a

**BÃ i toÃ¡n dual vá»›i Linear Kernel:**

$$
\min_{w} \frac{1}{2} \sum_{i,j} w_i w_j y_i y_j \underbrace{K_{\text{linear}}(x_i, x_j)}_{= x_i^T x_j} - \sum_i w_i
$$

Giá»‘ng há»‡t bÃ i toÃ¡n dual ban Ä‘áº§u! Linear kernel chá»‰ lÃ  "viáº¿t láº¡i" tÃ­ch vÃ´ hÆ°á»›ng dÆ°á»›i dáº¡ng kernel.

**Káº¿t luáº­n quan trá»ng:**

> Linear kernel lÃ  **ná»n táº£ng** Ä‘á»ƒ hiá»ƒu cÃ¡c kernel khÃ¡c. NÃ³ cho tháº¥y:
> 1. Kernel lÃ  má»™t hÃ m $$K(x, z)$$ tÃ­nh "Ä‘á»™ giá»‘ng nhau" giá»¯a $$x$$ vÃ  $$z$$
> 2. Vá»›i linear kernel, "Ä‘á»™ giá»‘ng nhau" = tÃ­ch vÃ´ hÆ°á»›ng thÃ´ng thÆ°á»ng
> 3. CÃ¡c kernel khÃ¡c sáº½ Ä‘á»‹nh nghÄ©a "Ä‘á»™ giá»‘ng nhau" theo cÃ¡ch khÃ¡c, phá»©c táº¡p hÆ¡n

Giá» chÃºng ta Ä‘Ã£ hiá»ƒu linear kernel, hÃ£y xem Ä‘iá»u gÃ¬ xáº£y ra khi dá»¯ liá»‡u **khÃ´ng tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh**!

---

### 8.2. Äá»™ng lá»±c: Khi Linear Kernel khÃ´ng Ä‘á»§

**Váº¥n Ä‘á» cÆ¡ báº£n:** Nhiá»u bá»™ dá»¯ liá»‡u thá»±c táº¿ **khÃ´ng tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh** trong khÃ´ng gian gá»‘c.

**VÃ­ dá»¥ kinh Ä‘iá»ƒn - Dá»¯ liá»‡u XOR:**

```
Class +1:  (+1, +1)  vÃ   (-1, -1)  (hai gÃ³c chÃ©o)
Class -1:  (+1, -1)  vÃ   (-1, +1)  (hai gÃ³c chÃ©o cÃ²n láº¡i)
```

KhÃ´ng cÃ³ Ä‘Æ°á»ng tháº³ng nÃ o cÃ³ thá»ƒ phÃ¢n chia hai lá»›p nÃ y trong khÃ´ng gian 2D!

**Ã tÆ°á»Ÿng Ä‘á»™t phÃ¡:** Náº¿u khÃ´ng tÃ¡ch Ä‘Æ°á»£c á»Ÿ khÃ´ng gian hiá»‡n táº¡i, hÃ£y **Ã¡nh xáº¡ lÃªn khÃ´ng gian chiá»u cao hÆ¡n** - nÆ¡i dá»¯ liá»‡u cÃ³ thá»ƒ tÃ¡ch Ä‘Æ°á»£c!

### 8.3. Feature Mapping: Tá»« Linear Ä‘áº¿n Polynomial

**Báº¯t Ä‘áº§u vá»›i vÃ­ dá»¥ Ä‘Æ¡n giáº£n - Dá»¯ liá»‡u 1 chiá»u:**

Giáº£ sá»­ dá»¯ liá»‡u trÃªn trá»¥c sá»‘:
```
Class +1: x = -2, x = +2  (hai Ä‘áº§u)
Class -1: x = 0           (á»Ÿ giá»¯a)
```

KhÃ´ng cÃ³ Ä‘iá»ƒm nÃ o trÃªn trá»¥c sá»‘ (threshold) tÃ¡ch Ä‘Æ°á»£c hai class nÃ y!

**Giáº£i phÃ¡p: Ãnh xáº¡ lÃªn khÃ´ng gian 2 chiá»u**

Äá»‹nh nghÄ©a Ã¡nh xáº¡:
$$
\phi(x) = (x, x^2) \in \mathbb{R}^2
$$

Dá»¯ liá»‡u sau khi Ã¡nh xáº¡:
```
Class +1: Ï†(-2) = (-2, 4),  Ï†(+2) = (+2, 4)
Class -1: Ï†(0) = (0, 0)
```

BÃ¢y giá» trong khÃ´ng gian 2D $$(x, x^2)$$, ta cÃ³ thá»ƒ dÃ¹ng Ä‘Æ°á»ng tháº³ng $$x^2 = 2$$ Ä‘á»ƒ tÃ¡ch hai class!

**Kernel tÆ°Æ¡ng á»©ng:**

Vá»›i Ã¡nh xáº¡ $$\phi(x) = (x, x^2)$$, kernel lÃ :

$$
\begin{align}
K(x, z) &= \langle \phi(x), \phi(z) \rangle \\
&= \langle (x, x^2), (z, z^2) \rangle \\
&= xz + x^2z^2
\end{align}
$$

**Má»Ÿ rá»™ng: Polynomial Kernel tá»•ng quÃ¡t**

### 8.4. Feature Mapping tá»•ng quÃ¡t: Ãnh xáº¡ lÃªn khÃ´ng gian má»›i

**Äá»‹nh nghÄ©a:** Ãnh xáº¡ Ä‘áº·c trÆ°ng (feature map) lÃ  má»™t hÃ m $$\phi$$ biáº¿n Ä‘á»•i dá»¯ liá»‡u tá»« khÃ´ng gian gá»‘c sang khÃ´ng gian má»›i:

$$
\phi: \mathbb{R}^d \to \mathcal{H}
$$

trong Ä‘Ã³ $$\mathcal{H}$$ lÃ  **khÃ´ng gian Ä‘áº·c trÆ°ng** (feature space), thÆ°á»ng cÃ³ sá»‘ chiá»u cao hÆ¡n (tháº­m chÃ­ vÃ´ háº¡n!).

**VÃ­ dá»¥ cá»¥ thá»ƒ - Ãnh xáº¡ báº­c 2 cho $$\mathbb{R}^2$$:**

Vá»›i $$x = (x_1, x_2) \in \mathbb{R}^2$$, Ä‘á»‹nh nghÄ©a:

$$
\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2, \sqrt{2}x_1, \sqrt{2}x_2, 1) \in \mathbb{R}^6
$$

BÃ¢y giá» $$x$$ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng **6 chiá»u** thay vÃ¬ 2 chiá»u! Trong khÃ´ng gian má»›i nÃ y, nhiá»u dá»¯ liá»‡u phi tuyáº¿n trá»Ÿ nÃªn tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh.

**BÃ i toÃ¡n SVM trong khÃ´ng gian má»›i:**

Thay vÃ¬ giáº£i:
$$
\min_{w} \frac{1}{2} \sum_{i,j} w_i w_j y_i y_j \langle x_i, x_j \rangle - \sum_i w_i
$$

Ta giáº£i trong khÃ´ng gian Ä‘áº·c trÆ°ng:
$$
\min_{w} \frac{1}{2} \sum_{i,j} w_i w_j y_i y_j \langle \phi(x_i), \phi(x_j) \rangle - \sum_i w_i
$$

**Váº¥n Ä‘á» lá»›n:** Náº¿u $$\phi(x)$$ cÃ³ sá»‘ chiá»u ráº¥t cao (vÃ­ dá»¥: 1000 chiá»u), viá»‡c tÃ­nh toÃ¡n $$\langle \phi(x_i), \phi(x_j) \rangle$$ sáº½ **cá»±c ká»³ tá»‘n kÃ©m**!

### 8.5. Kernel Trick: PhÃ©p mÃ u khÃ´ng cáº§n Ã¡nh xáº¡ tÆ°á»ng minh

ÄÃ¢y lÃ  **bÆ°á»›c Ä‘á»™t phÃ¡** biáº¿n kernel methods tá»« Ã½ tÆ°á»Ÿng Ä‘áº¹p thÃ nh cÃ´ng cá»¥ thá»±c tiá»…n!

**Ã tÆ°á»Ÿng thiÃªn tÃ i:** Thay vÃ¬ tÃ­nh $$\phi(x)$$ rá»“i tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng, ta tÃ¬m má»™t **hÃ m kernel** $$K$$ sao cho:

$$
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
$$

mÃ  **KHÃ”NG cáº§n tÃ­nh $$\phi(x)$$ tÆ°á»ng minh**!

**Táº¡i sao Ä‘Ã¢y lÃ  phÃ©p mÃ u?**

- TÃ­nh $$K(x_i, x_j)$$ trá»±c tiáº¿p trong khÃ´ng gian gá»‘c: **$$O(d)$$ - ráº¥t nhanh!**
- TrÃ¡nh tÃ­nh $$\phi(x_i)$$ vÃ  $$\phi(x_j)$$ trong khÃ´ng gian chiá»u cao: tiáº¿t kiá»‡m tÃ i nguyÃªn khá»•ng lá»“
- Tháº­m chÃ­ Ã¡nh xáº¡ lÃªn **khÃ´ng gian vÃ´ háº¡n chiá»u** mÃ  váº«n tÃ­nh toÃ¡n Ä‘Æ°á»£c!

**VÃ­ dá»¥ minh há»a - Polynomial Kernel báº­c 2:**

Vá»›i Ã¡nh xáº¡ $$\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2, \sqrt{2}x_1, \sqrt{2}x_2, 1)$$, ta cÃ³:

$$
\begin{align}
\langle \phi(x), \phi(z) \rangle &= x_1^2 z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2 + 2x_1z_1 + 2x_2z_2 + 1 \\
&= (x_1z_1 + x_2z_2)^2 + 2(x_1z_1 + x_2z_2) + 1 \\
&= (\langle x, z \rangle + 1)^2
\end{align}
$$

Váº­y thay vÃ¬ tÃ­nh 6 phÃ©p nhÃ¢n trong $$\mathbb{R}^6$$, ta chá»‰ cáº§n:

$$
K(x, z) = (\langle x, z \rangle + 1)^2
$$

**2 phÃ©p tÃ­nh trong $$\mathbb{R}^2$$** - Nhanh gáº¥p nhiá»u láº§n!

**So sÃ¡nh chi phÃ­ tÃ­nh toÃ¡n:**

| PhÆ°Æ¡ng phÃ¡p | CÃ¡c bÆ°á»›c | Äá»™ phá»©c táº¡p |
|-------------|----------|-------------|
| **CÃ¡ch 1: TÃ­nh tÆ°á»ng minh** | 1. TÃ­nh $$\phi(x)$$ (6 chiá»u)<br>2. TÃ­nh $$\phi(z)$$ (6 chiá»u)<br>3. TÃ­nh tÃ­ch vÃ´ hÆ°á»›ng (6 phÃ©p nhÃ¢n) | $$O(6 + 6 + 6) = O(18)$$ |
| **CÃ¡ch 2: Kernel trick** | 1. TÃ­nh $$\langle x, z \rangle$$ (2 phÃ©p nhÃ¢n)<br>2. TÃ­nh $$(\cdot + 1)^2$$ (1 phÃ©p cá»™ng, 1 phÃ©p mÅ©) | $$O(2 + 2) = O(4)$$ |

Nhanh hÆ¡n **4.5 láº§n**! VÃ  Ä‘Ã¢y chá»‰ vá»›i Ã¡nh xáº¡ báº­c 2 trong $$\mathbb{R}^2$$. Vá»›i dá»¯ liá»‡u chiá»u cao vÃ  báº­c lá»›n hÆ¡n, lá»£i Ã­ch cÃ²n lá»›n hÆ¡n ráº¥t nhiá»u!

**VÃ­ dá»¥ thá»±c táº¿ vá»›i dá»¯ liá»‡u chiá»u cao:**

Vá»›i polynomial kernel báº­c 3 trong khÃ´ng gian 100 chiá»u:
- **Sá»‘ chiá»u sau Ã¡nh xáº¡ tÆ°á»ng minh**: $$C(100+3, 3) \approx 176,851$$ chiá»u!
- **Chi phÃ­ tÃ­nh toÃ¡n tÆ°á»ng minh**: Pháº£i tÃ­nh vÃ  lÆ°u trá»¯ vector 176,851 chiá»u
- **Chi phÃ­ vá»›i kernel trick**: Chá»‰ cáº§n tÃ­nh $$(\langle x, z \rangle + 1)^3$$ - vÃ i phÃ©p toÃ¡n Ä‘Æ¡n giáº£n
- **Káº¿t quáº£**: Kernel trick nhanh hÆ¡n **hÃ ng trÄƒm láº§n**!

**Káº¿t luáº­n vá» Kernel Trick:**

1. **Tiáº¿t kiá»‡m tÃ­nh toÃ¡n khá»•ng lá»“**: KhÃ´ng cáº§n tÃ­nh $$\phi(x)$$ tÆ°á»ng minh
2. **Cho phÃ©p khÃ´ng gian vÃ´ háº¡n chiá»u**: RBF kernel Ã¡nh xáº¡ lÃªn $$\mathbb{R}^\infty$$ nhÆ°ng váº«n tÃ­nh Ä‘Æ°á»£c!
3. **Elegant vÃ  powerful**: ÄÆ¡n giáº£n nhÆ°ng máº¡nh máº½

> **Kernel trick** lÃ  lÃ½ do SVM cÃ³ thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u phi tuyáº¿n phá»©c táº¡p mÃ  váº«n hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n!

### 8.6. CÃ¡c Kernel phá»• biáº¿n vÃ  á»©ng dá»¥ng

Giá» chÃºng ta Ä‘Ã£ hiá»ƒu kernel trick, hÃ£y xem cÃ¡c kernel phá»• biáº¿n nháº¥t trong thá»±c táº¿:

**1) Linear Kernel (Kernel tuyáº¿n tÃ­nh):**

$$
K(x, z) = \langle x, z \rangle = x^T z
$$

- **Khi nÃ o dÃ¹ng:** Dá»¯ liá»‡u Ä‘Ã£ tÃ¡ch Ä‘Æ°á»£c tuyáº¿n tÃ­nh, hoáº·c sá»‘ chiá»u ráº¥t cao ($$p \gg n$$)
- **Æ¯u Ä‘iá»ƒm:** Nhanh nháº¥t, Ã­t tham sá»‘, trÃ¡nh overfitting
- **VÃ­ dá»¥:** PhÃ¢n loáº¡i vÄƒn báº£n (bag-of-words vá»›i hÃ ng nghÃ¬n tá»« vá»±ng)

**2) Polynomial Kernel (Kernel Ä‘a thá»©c):**

$$
K(x, z) = (\langle x, z \rangle + c)^q
$$

vá»›i $$q$$ lÃ  báº­c Ä‘a thá»©c, $$c \geq 0$$ lÃ  háº±ng sá»‘.

- **Khi nÃ o dÃ¹ng:** Ranh giá»›i quyáº¿t Ä‘á»‹nh lÃ  Ä‘a thá»©c báº­c tháº¥p ($$q = 2, 3$$)
- **Æ¯u Ä‘iá»ƒm:** Linh hoáº¡t, mÃ´ hÃ¬nh hÃ³a tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c features
- **NhÆ°á»£c Ä‘iá»ƒm:** KhÃ³ tune tham sá»‘ $$q$$ vÃ  $$c$$, dá»… overfitting vá»›i $$q$$ lá»›n
- **VÃ­ dá»¥:** Nháº­n dáº¡ng chá»¯ viáº¿t tay, xá»­ lÃ½ áº£nh

**3) RBF (Radial Basis Function) Kernel - Kernel Gauss:**

$$
K(x, z) = \exp\left(-\frac{\|x - z\|^2}{2\sigma^2}\right) = \exp(-\gamma \|x - z\|^2)
$$

vá»›i $$\gamma = \frac{1}{2\sigma^2} > 0$$ lÃ  tham sá»‘ bandwidth.

- **Khi nÃ o dÃ¹ng:** **Default choice** cho háº§u háº¿t bÃ i toÃ¡n - ráº¥t linh hoáº¡t!
- **Æ¯u Ä‘iá»ƒm:** 
  - Ãnh xáº¡ lÃªn **khÃ´ng gian vÃ´ háº¡n chiá»u**
  - CÃ³ thá»ƒ xáº¥p xá»‰ má»i hÃ m liÃªn tá»¥c (universal approximator)
  - Chá»‰ cÃ³ 1 tham sá»‘ $$\gamma$$ cáº§n tune
- **NhÆ°á»£c Ä‘iá»ƒm:** Dá»… overfit náº¿u $$\gamma$$ quÃ¡ lá»›n
- **VÃ­ dá»¥:** Nháº­n dáº¡ng khuÃ´n máº·t, phÃ¢n loáº¡i gene expression

**4) Sigmoid Kernel:**

$$
K(x, z) = \tanh(\alpha \langle x, z \rangle + c)
$$

- **Khi nÃ o dÃ¹ng:** MÃ´ phá»ng neural network vá»›i má»™t hidden layer
- **LÆ°u Ã½:** KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng positive definite (khÃ´ng Ä‘áº£m báº£o há»™i tá»¥)
- **VÃ­ dá»¥:** Má»™t sá»‘ bÃ i toÃ¡n nhá»‹ phÃ¢n Ä‘Æ¡n giáº£n

**Tá»•ng káº¿t so sÃ¡nh 4 kernel:**

| Kernel | Linear | Polynomial | RBF | Sigmoid |
|--------|--------|------------|-----|---------|
| **CÃ´ng thá»©c** | $$x^Tz$$ | $$(\langle x,z\rangle + c)^q$$ | $$\exp(-\gamma\\|x-z\\|^2)$$ | $$\tanh(\alpha\langle x,z\rangle+c)$$ |
| **Tham sá»‘** | 0 | 2 ($$q, c$$) | 1 ($$\gamma$$) | 2 ($$\alpha, c$$) |
| **KhÃ´ng gian** | $$\mathbb{R}^d$$ | $$\mathbb{R}^{C(d+q,q)}$$ | $$\mathbb{R}^\infty$$ | - |
| **Dá»… tune** | âœ… Ráº¥t dá»… | âš ï¸ KhÃ³ | âœ… Dá»… | âŒ Ráº¥t khÃ³ |
| **Tá»‘c Ä‘á»™** | âš¡ Nhanh nháº¥t | âš¡ Nhanh | ğŸŒ Trung bÃ¬nh | ğŸŒ Trung bÃ¬nh |
| **Khi nÃ o dÃ¹ng** | Baseline | TÆ°Æ¡ng tÃ¡c báº­c tháº¥p | **Default** | Hiáº¿m khi |

### 8.7. Äiá»u kiá»‡n há»£p lá»‡ cá»§a Kernel - Mercer's Theorem

**CÃ¢u há»i quan trá»ng:** KhÃ´ng pháº£i hÃ m nÃ o cÅ©ng cÃ³ thá»ƒ lÃ m kernel! Äiá»u kiá»‡n nÃ o Ä‘áº£m báº£o $$K$$ lÃ  má»™t kernel há»£p lá»‡?

**Äá»‹nh lÃ½ Mercer (Mercer's Theorem):**

Má»™t hÃ m $$K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$$ lÃ  má»™t kernel há»£p lá»‡ (valid kernel) náº¿u vÃ  chá»‰ náº¿u nÃ³ lÃ  **symmetric positive semi-definite**, tá»©c lÃ :

1. **Äá»‘i xá»©ng:** $$K(x, z) = K(z, x), \quad \forall x, z$$

2. **Positive semi-definite:** Vá»›i má»i $$n$$ Ä‘iá»ƒm $$\{x_1, \ldots, x_n\}$$ vÃ  má»i $$a_1, \ldots, a_n \in \mathbb{R}$$:
   $$
   \sum_{i=1}^n \sum_{j=1}^n a_i a_j K(x_i, x_j) \geq 0
   $$
   
   Hay tÆ°Æ¡ng Ä‘Æ°Æ¡ng: Ma tráº­n Gram $$\mathbf{K}$$ vá»›i $$K_{ij} = K(x_i, x_j)$$ lÃ  **positive semi-definite**.

**Ã nghÄ©a thá»±c tiá»…n:**

- Náº¿u $$K$$ thá»a Ä‘iá»u kiá»‡n Mercer â†’ Tá»“n táº¡i $$\phi$$ sao cho $$K(x, z) = \langle \phi(x), \phi(z) \rangle$$
- BÃ i toÃ¡n tá»‘i Æ°u SVM lÃ  **convex** vÃ  cÃ³ nghiá»‡m duy nháº¥t
- Náº¿u $$K$$ vi pháº¡m Ä‘iá»u kiá»‡n â†’ BÃ i toÃ¡n cÃ³ thá»ƒ khÃ´ng há»™i tá»¥!

**Kiá»ƒm tra trong thá»±c táº¿:**

- CÃ¡c kernel chuáº©n (linear, polynomial, RBF) Ä‘á»u thá»a Mercer
- Náº¿u tá»± Ä‘á»‹nh nghÄ©a kernel má»›i, cáº§n kiá»ƒm tra tÃ­nh positive definite cá»§a ma tráº­n $$\mathbf{K}$$

### 8.8. XÃ¢y dá»±ng Kernel má»›i tá»« Kernel cÅ©

**TÃ­nh cháº¥t Ä‘Ã³ng (Closure properties):** Náº¿u $$K_1$$ vÃ  $$K_2$$ lÃ  kernel há»£p lá»‡, thÃ¬ cÃ¡c phÃ©p toÃ¡n sau cÅ©ng cho kernel há»£p lá»‡:

1. **Tá»•ng:** $$K(x, z) = K_1(x, z) + K_2(x, z)$$

2. **NhÃ¢n vá»›i háº±ng sá»‘ dÆ°Æ¡ng:** $$K(x, z) = c \cdot K_1(x, z), \quad c > 0$$

3. **TÃ­ch:** $$K(x, z) = K_1(x, z) \cdot K_2(x, z)$$

4. **HÃ m cá»§a kernel:** $$K(x, z) = f(K_1(x, z))$$ vá»›i $$f$$ lÃ  hÃ m positive-preserving

5. **Tensor product:** $$K(x, z) = K_1(x_1, z_1) \cdot K_2(x_2, z_2)$$ vá»›i $$x = (x_1, x_2)$$

**VÃ­ dá»¥ á»©ng dá»¥ng:**

Káº¿t há»£p RBF vá»›i Linear Ä‘á»ƒ cÃ³ cáº£ tÃ­nh tuyáº¿n tÃ­nh vÃ  phi tuyáº¿n:

$$
K(x, z) = \underbrace{\alpha \langle x, z \rangle}_{\text{Linear}} + \underbrace{\beta \exp(-\gamma \|x - z\|^2)}_{\text{RBF}}, \quad \alpha, \beta > 0
$$

### 8.9. Kernel SVM trong thá»±c táº¿

**CÃ´ng thá»©c dá»± Ä‘oÃ¡n vá»›i Kernel:**

Sau khi huáº¥n luyá»‡n, dá»± Ä‘oÃ¡n cho Ä‘iá»ƒm má»›i $$x$$:

$$
f(x) = \text{sign}\left(\sum_{i \in SV} w_i^\star y_i K(x, x_i) + \beta_0^\star\right)
$$

**LÆ°u Ã½ quan trá»ng:**

- Chá»‰ cáº§n tÃ­nh $$K(x, x_i)$$ vá»›i **support vectors** (ráº¥t Ã­t so vá»›i toÃ n bá»™ dá»¯ liá»‡u)
- KhÃ´ng cáº§n biáº¿t $$\phi(x)$$ lÃ  gÃ¬ - chá»‰ cáº§n $$K$$!
- Tá»‘c Ä‘á»™ dá»± Ä‘oÃ¡n: $$O(\#SV \cdot d)$$ vá»›i $$d$$ lÃ  chiá»u dá»¯ liá»‡u gá»‘c

**Lá»±a chá»n kernel trong thá»±c táº¿:**

| TÃ¬nh huá»‘ng | Kernel Ä‘á» xuáº¥t | LÃ½ do |
|------------|----------------|-------|
| **Baseline / khÃ´ng biáº¿t gÃ¬** | Linear | Nhanh, Ã­t overfit, baseline tá»‘t |
| **Dá»¯ liá»‡u phi tuyáº¿n** | RBF | Linh hoáº¡t, universal approximator |
| **Sá»‘ chiá»u cao ($$p \gg n$$)** | Linear | TrÃ¡nh overfit trong khÃ´ng gian chiá»u cao |
| **Feature engineering sáºµn** | Linear | ÄÃ£ cÃ³ features tá»‘t rá»“i |
| **TÆ°Æ¡ng tÃ¡c báº­c tháº¥p** | Polynomial ($$q=2,3$$) | Model hÃ³a tÆ°Æ¡ng tÃ¡c rÃµ rÃ ng |
| **Computational budget tháº¥p** | Linear | Nhanh nháº¥t |

**Quy trÃ¬nh thá»±c hÃ nh:**

1. **Báº¯t Ä‘áº§u vá»›i Linear SVM**: Baseline nhanh
2. **Thá»­ RBF SVM**: Náº¿u Linear khÃ´ng tá»‘t
3. **Grid search cho $$C$$ vÃ  $$\gamma$$**: Tune hyperparameters
4. **Cross-validation**: ÄÃ¡nh giÃ¡ Ä‘á»™ tá»•ng quÃ¡t

### 8.10. Minh há»a tÆ°Æ¡ng tÃ¡c: So sÃ¡nh cÃ¡c Kernel

<div id="kernel-demo" style="margin: 20px 0;">
    <canvas id="kernelCanvas" width="800" height="500" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    <div style="text-align: center; margin-top: 15px;">
        <label for="kernelSelect" style="font-size: 16px; font-weight: bold; margin-right: 10px;">Chá»n Kernel:</label>
        <select id="kernelSelect" style="padding: 8px; font-size: 14px; border-radius: 5px; border: 1px solid #ccc;">
            <option value="linear">Linear Kernel</option>
            <option value="poly2">Polynomial (degree 2)</option>
            <option value="poly3">Polynomial (degree 3)</option>
            <option value="rbf" selected>RBF/Gaussian</option>
        </select>
        <div style="margin-top: 15px;" id="rbfGammaControl">
            <label for="gammaSlider" style="font-size: 14px;">Gamma (RBF): <span id="gammaValue">0.5</span></label><br>
            <input type="range" id="gammaSlider" min="0.1" max="5" step="0.1" value="0.5" style="width: 300px; margin-top: 5px;">
        </div>
        <p style="font-size: 14px; color: #666; margin-top: 10px;">
            Dá»¯ liá»‡u phi tuyáº¿n (XOR-like pattern). Quan sÃ¡t ranh giá»›i quyáº¿t Ä‘á»‹nh vá»›i tá»«ng kernel.
        </p>
    </div>
</div>

<script>
(function() {
    const canvas = document.getElementById('kernelCanvas');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const width = canvas.width;
    const height = canvas.height;
    
    // Dá»¯ liá»‡u XOR-like pattern
    const points = [
        // Class +1 (Ä‘á») - hai gÃ³c Ä‘á»‘i diá»‡n
        {x: 200, y: 150, label: 1}, {x: 220, y: 130, label: 1}, {x: 180, y: 170, label: 1},
        {x: 600, y: 350, label: 1}, {x: 620, y: 330, label: 1}, {x: 580, y: 370, label: 1},
        {x: 210, y: 160, label: 1}, {x: 590, y: 340, label: 1},
        
        // Class -1 (xanh) - hai gÃ³c Ä‘á»‘i diá»‡n cÃ²n láº¡i
        {x: 600, y: 150, label: -1}, {x: 620, y: 130, label: -1}, {x: 580, y: 170, label: -1},
        {x: 200, y: 350, label: -1}, {x: 220, y: 330, label: -1}, {x: 180, y: 370, label: -1},
        {x: 610, y: 160, label: -1}, {x: 190, y: 340, label: -1}
    ];
    
    let currentKernel = 'rbf';
    let gamma = 0.5;
    
    // Kernel functions
    function kernel(x1, x2, type) {
        const dx = x1.x - x2.x;
        const dy = x1.y - x2.y;
        const dot = x1.x * x2.x + x1.y * x2.y;
        
        switch(type) {
            case 'linear':
                return dot;
            case 'poly2':
                return Math.pow(dot / 10000 + 1, 2);
            case 'poly3':
                return Math.pow(dot / 10000 + 1, 3);
            case 'rbf':
                return Math.exp(-gamma * (dx*dx + dy*dy) / 10000);
            default:
                return dot;
        }
    }
    
    // Simplified prediction
    function predict(testPoint) {
        let sum = 0;
        points.forEach(p => {
            const k = kernel(testPoint, p, currentKernel);
            sum += p.label * k;
        });
        return sum;
    }
    
    function draw() {
        ctx.clearRect(0, 0, width, height);
        
        // Váº½ background vá»›i decision boundary
        const gridSize = 5;
        for (let x = 0; x < width; x += gridSize) {
            for (let y = 0; y < height; y += gridSize) {
                const pred = predict({x, y});
                const alpha = Math.min(Math.abs(pred) * 0.02, 0.3);
                ctx.fillStyle = pred > 0 ? `rgba(231, 76, 60, ${alpha})` : `rgba(46, 204, 113, ${alpha})`;
                ctx.fillRect(x, y, gridSize, gridSize);
            }
        }
        
        // Váº½ decision boundary (Ä‘Æ°á»ng contour pred = 0)
        ctx.strokeStyle = '#2c3e50';
        ctx.lineWidth = 3;
        ctx.beginPath();
        let firstPoint = true;
        
        for (let y = 50; y < height - 50; y += 2) {
            for (let x = 50; x < width - 50; x += 2) {
                const pred = predict({x, y});
                const predRight = predict({x: x+2, y});
                const predDown = predict({x, y: y+2});
                
                // TÃ¬m zero crossing
                if (Math.sign(pred) !== Math.sign(predRight) || Math.sign(pred) !== Math.sign(predDown)) {
                    if (firstPoint) {
                        ctx.moveTo(x, y);
                        firstPoint = false;
                    } else {
                        ctx.lineTo(x, y);
                    }
                }
            }
        }
        ctx.stroke();
        
        // Váº½ data points
        points.forEach(p => {
            ctx.fillStyle = p.label === 1 ? '#e74c3c' : '#2ecc71';
            ctx.beginPath();
            ctx.arc(p.x, p.y, 8, 0, 2 * Math.PI);
            ctx.fill();
            
            ctx.strokeStyle = '#34495e';
            ctx.lineWidth = 2;
            ctx.stroke();
        });
        
        // Legend
        ctx.fillStyle = '#34495e';
        ctx.font = 'bold 16px Arial';
        ctx.fillText(`Kernel: ${currentKernel.toUpperCase()}`, 20, 30);
        
        if (currentKernel === 'rbf') {
            ctx.fillText(`Î³ = ${gamma.toFixed(2)}`, 20, 55);
        }
    }
    
    // Event listeners
    const kernelSelect = document.getElementById('kernelSelect');
    const gammaSlider = document.getElementById('gammaSlider');
    const gammaValue = document.getElementById('gammaValue');
    const gammaControl = document.getElementById('rbfGammaControl');
    
    if (kernelSelect) {
        kernelSelect.addEventListener('change', function() {
            currentKernel = this.value;
            gammaControl.style.display = currentKernel === 'rbf' ? 'block' : 'none';
            draw();
        });
    }
    
    if (gammaSlider && gammaValue) {
        gammaSlider.addEventListener('input', function() {
            gamma = parseFloat(this.value);
            gammaValue.textContent = gamma.toFixed(2);
            draw();
        });
    }
    
    // Initial draw
    draw();
})();
</script>

### 8.11. Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a Kernel Methods

**âœ… Æ¯u Ä‘iá»ƒm vÆ°á»£t trá»™i:**

1. **KhÃ´ng gian vÃ´ háº¡n chiá»u**: RBF kernel Ã¡nh xáº¡ lÃªn khÃ´ng gian vÃ´ háº¡n chiá»u mÃ  váº«n tÃ­nh toÃ¡n nhanh
2. **KhÃ´ng cáº§n feature engineering**: Kernel tá»± Ä‘á»™ng táº¡o features phi tuyáº¿n
3. **LÃ½ thuyáº¿t vá»¯ng cháº¯c**: Mercer's theorem Ä‘áº£m báº£o tÃ­nh cháº¥t toÃ¡n há»c
4. **Linh hoáº¡t**: CÃ³ thá»ƒ thiáº¿t káº¿ kernel riÃªng cho tá»«ng domain (string kernel, graph kernel, ...)
5. **Hiá»‡u quáº£ vá»›i dá»¯ liá»‡u nhá»/trung bÃ¬nh**: SVM+kernel hoáº¡t Ä‘á»™ng tá»‘t vá»›i $$n < 10^5$$

**âŒ NhÆ°á»£c Ä‘iá»ƒm cáº§n lÆ°u Ã½:**

1. **KhÃ³ tune hyperparameters**: $$C$$, $$\gamma$$, ... cáº§n grid search tá»‘n thá»i gian
2. **Training cháº­m vá»›i dá»¯ liá»‡u lá»›n**: Äá»™ phá»©c táº¡p $$O(n^2)$$ Ä‘áº¿n $$O(n^3)$$ cho $$n$$ lá»›n
3. **LÆ°u trá»¯ support vectors**: Cáº§n lÆ°u táº¥t cáº£ SV Ä‘á»ƒ dá»± Ä‘oÃ¡n (cÃ³ thá»ƒ nhiá»u vá»›i dá»¯ liá»‡u phá»©c táº¡p)
4. **Black box**: KhÃ³ interpret hÆ¡n linear model
5. **KhÃ´ng cÃ³ probability estimate tá»± nhiÃªn**: Cáº§n Platt scaling Ä‘á»ƒ cÃ³ xÃ¡c suáº¥t

**So sÃ¡nh vá»›i Neural Networks:**

| TiÃªu chÃ­ | Kernel SVM | Neural Networks |
|----------|------------|-----------------|
| **Dá»¯ liá»‡u nhá» ($$n < 10^4$$)** | âœ… Tá»‘t hÆ¡n | CÃ³ thá»ƒ overfit |
| **Dá»¯ liá»‡u lá»›n ($$n > 10^6$$)** | âŒ Cháº­m | âœ… Tá»‘t hÆ¡n |
| **Interpretability** | Trung bÃ¬nh (SV) | âŒ KhÃ³ |
| **Hyperparameter tuning** | âŒ KhÃ³ | âŒ Ráº¥t khÃ³ |
| **Training time** | $$O(n^2)$$-$$O(n^3)$$ | Phá»¥ thuá»™c kiáº¿n trÃºc |
| **LÃ½ thuyáº¿t** | âœ… Vá»¯ng cháº¯c | Äang phÃ¡t triá»ƒn |

---

## 9. VÃ­ dá»¥ Chi tiáº¿t

### VÃ­ dá»¥ 1D: Dá»¯ liá»‡u Ä‘Æ¡n giáº£n

**Dá»¯ liá»‡u:**
- Lá»›p $$+1$$: $$x_1 = 3, x_2 = 4$$
- Lá»›p $$-1$$: $$x_3 = 1, x_4 = 2$$

<div id="svm1d-demo" style="margin: 20px 0;">
    <canvas id="svm1dCanvas" width="700" height="300" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    <div style="text-align: center; margin-top: 15px;">
        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; display: inline-block;">
            <p style="margin: 5px; font-size: 14px;"><strong>Nghiá»‡m tá»‘i Æ°u:</strong></p>
            <p style="margin: 5px; font-size: 14px;">SiÃªu pháº³ng: $$x = 2.5$$</p>
            <p style="margin: 5px; font-size: 14px;">$$\beta = 2, \quad \beta_0 = -5$$</p>
            <p style="margin: 5px; font-size: 14px;">Margin width: $$\frac{2}{\lvert\beta\rvert} = 1$$</p>
            <p style="margin: 5px; font-size: 14px; color: #f39c12;"><strong>Support Vectors:</strong> $$x = 2$$ vÃ  $$x = 3$$</p>
        </div>
    </div>
</div>

<script>
(function() {
    const canvas = document.getElementById('svm1dCanvas');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const width = canvas.width;
    const height = canvas.height;
    
    // Dá»¯ liá»‡u
    const points = [
        {x: 3, label: 1, name: 'xâ‚'},
        {x: 4, label: 1, name: 'xâ‚‚'},
        {x: 1, label: -1, name: 'xâ‚ƒ'},
        {x: 2, label: -1, name: 'xâ‚„'}
    ];
    
    // Nghiá»‡m: Î² = 2, Î²â‚€ = -5
    // Hyperplane: 2x - 5 = 0 => x = 2.5
    const hyperplane = 2.5;
    const margin = 0.5; // margin = 1/Î² = 1/2
    
    // Scaling cho visualization
    const scale = 100;
    const offsetX = 150;
    const centerY = height / 2;
    
    function xToScreen(x) {
        return offsetX + x * scale;
    }
    
    function draw() {
        // Clear
        ctx.clearRect(0, 0, width, height);
        
        // Background
        ctx.fillStyle = '#fafafa';
        ctx.fillRect(0, 0, width, height);
        
        // Váº½ trá»¥c x
        ctx.strokeStyle = '#34495e';
        ctx.lineWidth = 2;
        ctx.beginPath();
        ctx.moveTo(50, centerY);
        ctx.lineTo(width - 50, centerY);
        ctx.stroke();
        
        // Váº½ mÅ©i tÃªn trá»¥c x
        ctx.beginPath();
        ctx.moveTo(width - 50, centerY);
        ctx.lineTo(width - 60, centerY - 5);
        ctx.lineTo(width - 60, centerY + 5);
        ctx.closePath();
        ctx.fillStyle = '#34495e';
        ctx.fill();
        
        // Váº½ cÃ¡c Ä‘iá»ƒm trÃªn trá»¥c
        for (let i = 0; i <= 5; i++) {
            const x = xToScreen(i);
            ctx.strokeStyle = '#95a5a6';
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(x, centerY - 5);
            ctx.lineTo(x, centerY + 5);
            ctx.stroke();
            
            ctx.fillStyle = '#7f8c8d';
            ctx.font = '12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText(i.toString(), x, centerY + 20);
        }
        
        // Váº½ margin boundaries (Ä‘Æ°á»ng Ä‘á»©t nÃ©t)
        ctx.strokeStyle = '#95a5a6';
        ctx.lineWidth = 2;
        ctx.setLineDash([5, 5]);
        
        // Left margin boundary (x = 2)
        const leftMargin = xToScreen(hyperplane - margin);
        ctx.beginPath();
        ctx.moveTo(leftMargin, 50);
        ctx.lineTo(leftMargin, height - 50);
        ctx.stroke();
        
        // Right margin boundary (x = 3)
        const rightMargin = xToScreen(hyperplane + margin);
        ctx.beginPath();
        ctx.moveTo(rightMargin, 50);
        ctx.lineTo(rightMargin, height - 50);
        ctx.stroke();
        
        ctx.setLineDash([]);
        
        // Váº½ margin zone (vÃ¹ng mÃ u nháº¡t)
        ctx.fillStyle = 'rgba(52, 152, 219, 0.1)';
        ctx.fillRect(leftMargin, 50, rightMargin - leftMargin, height - 100);
        
        // Váº½ hyperplane (Ä‘Æ°á»ng Ä‘áº­m)
        const hyperX = xToScreen(hyperplane);
        ctx.strokeStyle = '#2c3e50';
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.moveTo(hyperX, 50);
        ctx.lineTo(hyperX, height - 50);
        ctx.stroke();
        
        // Label cho hyperplane
        ctx.fillStyle = '#2c3e50';
        ctx.font = 'bold 14px Arial';
        ctx.textAlign = 'center';
        ctx.fillText('x = 2.5', hyperX, 40);
        ctx.font = '12px Arial';
        ctx.fillText('SiÃªu pháº³ng', hyperX, 25);
        
        // Labels cho margins
        ctx.fillStyle = '#7f8c8d';
        ctx.font = '12px Arial';
        ctx.fillText('x = 2', leftMargin, height - 35);
        ctx.fillText('x = 3', rightMargin, height - 35);
        
        // Váº½ data points
        points.forEach(p => {
            const x = xToScreen(p.x);
            
            // Äiá»ƒm
            ctx.fillStyle = p.label === 1 ? '#e74c3c' : '#2ecc71';
            ctx.beginPath();
            ctx.arc(x, centerY, 12, 0, 2 * Math.PI);
            ctx.fill();
            
            // Viá»n
            ctx.strokeStyle = '#34495e';
            ctx.lineWidth = 2;
            ctx.stroke();
            
            // Highlight support vectors
            if (p.x === 2 || p.x === 3) {
                ctx.strokeStyle = '#f39c12';
                ctx.lineWidth = 4;
                ctx.beginPath();
                ctx.arc(x, centerY, 18, 0, 2 * Math.PI);
                ctx.stroke();
            }
            
            // Label Ä‘iá»ƒm
            ctx.fillStyle = '#34495e';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText(p.name, x, centerY - 30);
            
            // Hiá»ƒn thá»‹ class
            ctx.font = '12px Arial';
            const classLabel = p.label === 1 ? 'y = +1' : 'y = -1';
            ctx.fillStyle = p.label === 1 ? '#e74c3c' : '#2ecc71';
            ctx.fillText(classLabel, x, centerY + 40);
        });
        
        // Váº½ khoáº£ng cÃ¡ch margin (mÅ©i tÃªn hai chiá»u)
        const arrowY = 80;
        ctx.strokeStyle = '#3498db';
        ctx.lineWidth = 2;
        
        // ÄÆ°á»ng káº»
        ctx.beginPath();
        ctx.moveTo(leftMargin, arrowY);
        ctx.lineTo(hyperX, arrowY);
        ctx.stroke();
        
        ctx.beginPath();
        ctx.moveTo(hyperX, arrowY);
        ctx.lineTo(rightMargin, arrowY);
        ctx.stroke();
        
        // MÅ©i tÃªn trÃ¡i
        ctx.beginPath();
        ctx.moveTo(leftMargin + 5, arrowY - 5);
        ctx.lineTo(leftMargin, arrowY);
        ctx.lineTo(leftMargin + 5, arrowY + 5);
        ctx.stroke();
        
        // MÅ©i tÃªn pháº£i
        ctx.beginPath();
        ctx.moveTo(rightMargin - 5, arrowY - 5);
        ctx.lineTo(rightMargin, arrowY);
        ctx.lineTo(rightMargin - 5, arrowY + 5);
        ctx.stroke();
        
        // Label margin
        ctx.fillStyle = '#3498db';
        ctx.font = 'bold 12px Arial';
        ctx.textAlign = 'center';
        ctx.fillText('margin = 0.5', (leftMargin + hyperX) / 2, arrowY - 10);
        ctx.fillText('margin = 0.5', (hyperX + rightMargin) / 2, arrowY - 10);
        
        // Legend
        const legendX = width - 180;
        const legendY = 100;
        
        // Class +1
        ctx.fillStyle = '#e74c3c';
        ctx.beginPath();
        ctx.arc(legendX, legendY, 8, 0, 2 * Math.PI);
        ctx.fill();
        ctx.strokeStyle = '#34495e';
        ctx.lineWidth = 2;
        ctx.stroke();
        ctx.fillStyle = '#34495e';
        ctx.font = '12px Arial';
        ctx.textAlign = 'left';
        ctx.fillText('Class +1', legendX + 15, legendY + 4);
        
        // Class -1
        ctx.fillStyle = '#2ecc71';
        ctx.beginPath();
        ctx.arc(legendX, legendY + 25, 8, 0, 2 * Math.PI);
        ctx.fill();
        ctx.strokeStyle = '#34495e';
        ctx.lineWidth = 2;
        ctx.stroke();
        ctx.fillStyle = '#34495e';
        ctx.fillText('Class -1', legendX + 15, legendY + 29);
        
        // Support Vector
        ctx.strokeStyle = '#f39c12';
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.arc(legendX, legendY + 50, 8, 0, 2 * Math.PI);
        ctx.stroke();
        ctx.fillStyle = '#34495e';
        ctx.fillText('Support Vector', legendX + 15, legendY + 54);
    }
    
    // Initial draw
    draw();
})();
</script>

**BÃ i toÃ¡n hard-margin:**
$$
\begin{align}
\min_{\beta, \beta_0} \quad &\frac{1}{2}\beta^2 \\
\text{s.t.} \quad &3\beta + \beta_0 \geq 1, \quad 4\beta + \beta_0 \geq 1 \\
&-\beta - \beta_0 \geq 1, \quad -2\beta - \beta_0 \geq 1
\end{align}
$$

**PhÃ¢n tÃ­ch nghiá»‡m:**

Support vectors lÃ  cÃ¡c Ä‘iá»ƒm gáº§n nháº¥t vá»›i siÃªu pháº³ng: $$x = 3$$ (lá»›p +1) vÃ  $$x = 2$$ (lá»›p -1).

**Äiá»u kiá»‡n margin cháº·t:**
$$
\begin{cases}
3\beta + \beta_0 = 1 \\
2\beta + \beta_0 = -1
\end{cases}
\Rightarrow \beta = 2, \quad \beta_0 = -5
$$

**Káº¿t quáº£:**
- SiÃªu pháº³ng: $$2x - 5 = 0$$ hay $$x = 2.5$$ âœ“
- Margin width: $$\frac{2}{\lvert\beta\rvert} = \frac{2}{2} = 1$$
- Support vectors: $$x = 2$$ vÃ  $$x = 3$$
- CÃ¡c Ä‘iá»ƒm $$x = 1$$ vÃ  $$x = 4$$ khÃ´ng pháº£i support vectors vÃ¬ chÃºng náº±m xa margin hÆ¡n

---

## 9. BÃ i táº­p Thá»±c hÃ nh

### BÃ i táº­p 1: PhÃ¢n tÃ­ch Support Vectors

Cho nghiá»‡m SVM vá»›i $$C = 1$$:
```
w = (0, 0.5, 0.3, 1, 0, 1, 0.2)
Î¾ = (0, 0, 0.2, 0.8, 0, 1.5, 0)
```

**YÃªu cáº§u:**
a) XÃ¡c Ä‘á»‹nh support vectors  
b) PhÃ¢n loáº¡i theo vá»‹ trÃ­ (trÃªn margin, trong margin, misclassified)  
c) Giáº£i thÃ­ch $$w_1 = 0$$ nhÆ°ng $$\xi_1 = 0$$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**a) Support vectors:** $$i \in \{2, 3, 4, 6, 7\}$$ (vÃ¬ $$w_i > 0$$)

**b) PhÃ¢n loáº¡i:**
- **TrÃªn margin**: $$i = 2, 7$$ ($$\xi_i = 0$$, $$0 < w_i < C$$)
- **Trong margin**: $$i = 3, 4$$ ($$0 < \xi_i < 1$$, $$w_i = C$$)
- **Misclassified**: $$i = 6$$ ($$\xi_6 = 1.5 > 1$$, $$w_6 = C$$)

**c)** $$w_1 = 0$$ nghÄ©a lÃ  khÃ´ng pháº£i SV. Tá»« CS: $$v_1 = C - 0 = 1 > 0$$ â†’ $$\xi_1 = 0$$ âœ“  
Äiá»ƒm $$x_1$$ náº±m xa margin, phÃ¢n loáº¡i Ä‘Ãºng.

</details>

---

### BÃ i táº­p 2: TÃ­nh toÃ¡n tá»« KKT

Cho $$C = 2$$, $$n = 3$$, $$p = 2$$:
```
xâ‚ = (0, 1), yâ‚ = +1, wâ‚ = 0.5
xâ‚‚ = (1, 0), yâ‚‚ = +1, wâ‚‚ = 0.5  
xâ‚ƒ = (0, 0), yâ‚ƒ = -1, wâ‚ƒ = 1
```
$$\xi_1 = \xi_2 = 0$$, $$\xi_3 = 0.2$$.

**YÃªu cáº§u:**
a) TÃ­nh $$\beta$$ tá»« stationarity  
b) TÃ­nh $$\beta_0$$ tá»« SV trÃªn margin  
c) Kiá»ƒm tra CS cho $$i=3$$

<details>
<summary><strong>ğŸ’¡ Lá»i giáº£i</strong></summary>

**a)** $$\beta = \sum w_i y_i x_i = 0.5(0,1) + 0.5(1,0) + 1 \cdot (-1)(0,0) = (0.5, 0.5)$$

**b)** Tá»« $$i=1$$: $$y_1(\beta^T x_1 + \beta_0) = 1$$  
$$1 \cdot (0.5 + \beta_0) = 1$$ â†’ $$\beta_0 = 0.5$$

**c)** CS: $$w_3(1 - \xi_3 - y_3(\beta^T x_3 + \beta_0)) = 0$$  
$$y_3(\beta^T x_3 + \beta_0) = -1(0.5) = -0.5$$  
$$1(1 - 0.2 - (-0.5)) = 1.3 \neq 0$$ âœ—  

**Káº¿t luáº­n:** CS vi pháº¡m! CÃ³ lá»—i trong dá»¯ liá»‡u.

</details>

---

## 10. TÃ³m táº¯t: NhÃ¬n láº¡i hÃ nh trÃ¬nh chinh phá»¥c SVM

### 10.1. Tá»« trá»±c giÃ¡c hÃ¬nh há»c Ä‘áº¿n toÃ¡n há»c cháº·t cháº½

ChÃºng ta vá»«a hoÃ n thÃ nh má»™t hÃ nh trÃ¬nh Ä‘áº§y thÃº vá»‹, Ä‘i tá»« **cÃ¢u há»i tá»± nhiÃªn** Ä‘áº¿n **cÃ´ng cá»¥ toÃ¡n há»c máº¡nh máº½**:

**ğŸ”¹ Xuáº¥t phÃ¡t Ä‘iá»ƒm (Trá»±c giÃ¡c):** 
"Trong vÃ´ sá»‘ Ä‘Æ°á»ng phÃ¢n chia, Ä‘Æ°á»ng nÃ o lÃ  *tá»‘t nháº¥t*?" â†’ CÃ¢u há»i Ä‘Æ¡n giáº£n nhÆ°ng sÃ¢u sáº¯c

**ğŸ”¹ BÆ°á»›c 1 (Äá»‹nh lÆ°á»£ng):** 
Margin = "vÃ¹ng an toÃ n" xung quanh Ä‘Æ°á»ng phÃ¢n chia â†’ CÃ´ng cá»¥ Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng khÃ¡ch quan

**ğŸ”¹ BÆ°á»›c 2 (ToÃ¡n há»c hÃ³a):** 
Maximize margin â‡” Quadratic Programming â†’ BÃ i toÃ¡n tá»‘i Æ°u lá»“i, nghiá»‡m duy nháº¥t, giáº£i Ä‘Æ°á»£c hiá»‡u quáº£

**ğŸ”¹ BÆ°á»›c 3 (Äá»‘i máº·t thá»±c táº¿):** 
Hard-Margin â†’ Soft-Margin vá»›i slack variables â†’ Xá»­ lÃ½ nhiá»…u, outliers, dá»¯ liá»‡u khÃ´ng tÃ¡ch Ä‘Æ°á»£c

**ğŸ”¹ BÆ°á»›c 3A (GÃ³c nhÃ¬n Loss Function):**
Constrained optimization â†’ Hinge Loss â†’ Káº¿t ná»‘i vá»›i machine learning, subgradient descent

**ğŸ”¹ BÆ°á»›c 4 (Má»Ÿ rá»™ng sá»©c máº¡nh):** 
Primal â†’ Dual Problem â†’ CÃ¡nh cá»­a Ä‘áº¿n kernel trick vÃ  khÃ´ng gian vÃ´ háº¡n chiá»u

**ğŸ”¹ BÆ°á»›c 5 (Phi tuyáº¿n hÃ³a):**
Linear boundary â†’ Kernel Methods â†’ Xá»­ lÃ½ ranh giá»›i quyáº¿t Ä‘á»‹nh phá»©c táº¡p, phi tuyáº¿n

**ğŸ”¹ ÄÃ­ch Ä‘áº¿n:** 
Má»™t framework hoÃ n chá»‰nh, vá»«a cÃ³ ná»n táº£ng lÃ½ thuyáº¿t vá»¯ng cháº¯c, vá»«a máº¡nh máº½ trong thá»±c tiá»…n, tá»« tuyáº¿n tÃ­nh Ä‘áº¿n phi tuyáº¿n, tá»« optimization Ä‘áº¿n machine learning

### 10.2. CÃ¡c khÃ¡i niá»‡m then chá»‘t

| KhÃ¡i niá»‡m | CÃ´ng thá»©c | Ã nghÄ©a |
|-----------|-----------|---------|
| **Margin** | $$\frac{2}{\|\beta\|}$$ | Khoáº£ng cÃ¡ch giá»¯a 2 margin boundaries |
| **Hard-Margin** | $$y_i(\beta^T x_i + \beta_0) \geq 1$$ | Táº¥t cáº£ Ä‘iá»ƒm pháº£i phÃ¢n loáº¡i Ä‘Ãºng |
| **Soft-Margin** | $$y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i$$ | Cho phÃ©p vi pháº¡m cÃ³ kiá»ƒm soÃ¡t |
| **Support Vectors** | $$w_i > 0$$ | CÃ¡c Ä‘iá»ƒm quyáº¿t Ä‘á»‹nh siÃªu pháº³ng |
| **Tham sá»‘ C** | $$C \sum \xi_i$$ | Trá»ng sá»‘ pháº¡t vi pháº¡m margin |
| **Hinge Loss** | $$[1 - yf]_+ = \max\\{0, 1-yf\\}$$ | Loss function cá»§a SVM, lá»“i vÃ  sparse |
| **Kernel Function** | $$K(x, z) = \langle \phi(x), \phi(z) \rangle$$ | Ãnh xáº¡ ngáº§m lÃªn khÃ´ng gian Ä‘áº·c trÆ°ng |
| **RBF Kernel** | $$K(x, z) = \exp(-\gamma \\|x - z\\|^2)$$ | Kernel phá»• biáº¿n nháº¥t, linh hoáº¡t |

### 10.3. Ba biáº¿n thá»ƒ cá»§a SVM

1. **Hard-Margin SVM** (Dá»¯ liá»‡u tÃ¡ch Ä‘Æ°á»£c hoÃ n toÃ n)
   - Nghiá»‡m tá»“n táº¡i âŸº dá»¯ liá»‡u linearly separable
   - Margin lá»›n nháº¥t, khÃ´ng cho phÃ©p vi pháº¡m
   - Nháº¡y cáº£m vá»›i outliers

2. **Soft-Margin SVM** (Dá»¯ liá»‡u thá»±c táº¿)
   - LuÃ´n cÃ³ nghiá»‡m
   - Trade-off: margin rá»™ng vs. vi pháº¡m Ã­t (Ä‘iá»u khiá»ƒn bá»Ÿi $$C$$)
   - Robust vá»›i nhiá»…u

3. **Kernel SVM** (Dá»¯ liá»‡u phi tuyáº¿n)
   - Sá»­ dá»¥ng dual formulation
   - Ãnh xáº¡ ngáº§m lÃªn khÃ´ng gian chiá»u cao
   - KhÃ´ng tÄƒng Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n

### 10.4. Kernel Methods: ChÃ¬a khÃ³a cho phi tuyáº¿n

**Kernel Trick** lÃ  bÆ°á»›c Ä‘á»™t phÃ¡ quan trá»ng nháº¥t cá»§a SVM, biáº¿n nÃ³ tá»« má»™t classifier tuyáº¿n tÃ­nh thÃ nh má»™t cÃ´ng cá»¥ máº¡nh máº½ cho dá»¯ liá»‡u phi tuyáº¿n.

**Ã tÆ°á»Ÿng cá»‘t lÃµi:**

$$
K(x, z) = \langle \phi(x), \phi(z) \rangle
$$

- **KhÃ´ng cáº§n tÃ­nh $$\phi(x)$$ tÆ°á»ng minh**: TÃ­nh trá»±c tiáº¿p $$K(x, z)$$ trong khÃ´ng gian gá»‘c
- **KhÃ´ng gian vÃ´ háº¡n chiá»u**: RBF kernel Ã¡nh xáº¡ lÃªn $$\mathbb{R}^\infty$$ mÃ  váº«n tÃ­nh Ä‘Æ°á»£c!
- **Äá»™ phá»©c táº¡p khÃ´ng Ä‘á»•i**: $$O(d)$$ thay vÃ¬ $$O(\text{dim}(\phi))$$

**Bá»‘n kernel phá»• biáº¿n nháº¥t:**

| Kernel | CÃ´ng thá»©c | Khi nÃ o dÃ¹ng | Hyperparameters |
|--------|-----------|--------------|------------------|
| **Linear** | $$\langle x, z \rangle$$ | Baseline, dá»¯ liá»‡u chiá»u cao | KhÃ´ng |
| **Polynomial** | $$(\langle x, z \rangle + c)^q$$ | TÆ°Æ¡ng tÃ¡c báº­c tháº¥p | $$q, c$$ |
| **RBF/Gaussian** | $$\exp(-\gamma \\|x - z\\|^2)$$ | **Default choice**, ráº¥t linh hoáº¡t | $$\gamma$$ |
| **Sigmoid** | $$\tanh(\alpha \langle x, z \rangle + c)$$ | Giá»‘ng neural net | $$\alpha, c$$ |

**Äá»‹nh lÃ½ Mercer - Äiá»u kiá»‡n há»£p lá»‡:**

Kernel $$K$$ há»£p lá»‡ âŸº Ma tráº­n Gram $$\mathbf{K}$$ lÃ  **symmetric positive semi-definite**

- Äáº£m báº£o: BÃ i toÃ¡n SVM lÃ  convex, cÃ³ nghiá»‡m duy nháº¥t
- Táº¥t cáº£ kernel chuáº©n Ä‘á»u thá»a mÃ£n
- CÃ³ thá»ƒ xÃ¢y dá»±ng kernel má»›i tá»« kernel cÅ© (tá»•ng, tÃ­ch, ...)

**Lá»±a chá»n kernel trong thá»±c táº¿:**

1. **Báº¯t Ä‘áº§u vá»›i Linear SVM**: Nhanh, baseline tá»‘t
2. **Náº¿u khÃ´ng Ä‘á»§ â†’ RBF SVM**: Linh hoáº¡t, universal approximator
3. **Grid search $$C$$ vÃ  $$\gamma$$**: Cross-validation Ä‘á»ƒ tune
4. **Polynomial kernel**: Chá»‰ khi biáº¿t rÃµ tÆ°Æ¡ng tÃ¡c báº­c tháº¥p

### 10.5. Vai trÃ² cá»§a Support Vectors

**Support vectors** lÃ  "xÆ°Æ¡ng sá»‘ng" cá»§a SVM:

- Chá»‰ **support vectors** (Ä‘iá»ƒm cÃ³ $$w_i > 0$$) quyáº¿t Ä‘á»‹nh siÃªu pháº³ng
- ThÆ°á»ng ráº¥t Ã­t: $$|\{i: w_i > 0\}| \ll n$$
- CÃ¡c Ä‘iá»ƒm khÃ¡c cÃ³ thá»ƒ bá» Ä‘i mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n mÃ´ hÃ¬nh
- CÃ´ng thá»©c: $$\beta = \sum_{i: w_i > 0} w_i y_i x_i$$

**Ba loáº¡i support vectors:**
1. **TrÃªn margin** ($$\xi_i = 0, 0 < w_i < C$$): Äiá»ƒm lÃ½ tÆ°á»Ÿng, quan trá»ng nháº¥t
2. **Trong margin** ($$0 < \xi_i < 1, w_i = C$$): Vi pháº¡m nháº¹, váº«n Ä‘Ãºng
3. **Misclassified** ($$\xi_i > 1, w_i = C$$): Bá»‹ phÃ¢n loáº¡i sai

### 10.6. Primal vs. Dual: Khi nÃ o dÃ¹ng cÃ¡i nÃ o?

| TiÃªu chÃ­ | Primal Form | Dual Form |
|----------|-------------|-----------|
| **Sá»‘ biáº¿n** | $$p + 1 + n$$ | $$n$$ |
| **Khi nÃ o tá»‘t** | $$p \ll n$$ (Ã­t features) | $$n \ll p$$ (nhiá»u features) |
| **Kernel trick** | KhÃ´ng | CÃ³ âœ“ |
| **Solver** | CVXOPT, MOSEK | SMO, LIBSVM âœ“ |
| **Phá»• biáº¿n** | Ãt | Ráº¥t phá»• biáº¿n âœ“ |

### 10.7. Ã nghÄ©a thá»±c tiá»…n

**LÃ½ thuyáº¿t KKT** trong SVM khÃ´ng chá»‰ lÃ  cÃ´ng cá»¥ Ä‘á»ƒ giáº£i bÃ i toÃ¡n, mÃ  cÃ²n giÃºp:

1. **Hiá»ƒu cáº¥u trÃºc nghiá»‡m:**
   - Táº¡i sao nghiá»‡m sparse (chá»‰ cÃ³ support vectors)?
   - Äiá»ƒm nÃ o quan trá»ng, Ä‘iá»ƒm nÃ o khÃ´ng?

2. **PhÃ¡t triá»ƒn thuáº­t toÃ¡n:**
   - SMO (Sequential Minimal Optimization) dá»±a trÃªn Ä‘iá»u kiá»‡n KKT
   - Kiá»ƒm tra há»™i tá»¥: solution tá»‘i Æ°u âŸº thá»a KKT

3. **PhÃ¢n tÃ­ch mÃ´ hÃ¬nh:**
   - Sensitivity analysis: $$w_i$$ cho biáº¿t má»©c Ä‘á»™ "khÃ³ phÃ¢n loáº¡i" cá»§a Ä‘iá»ƒm $$i$$
   - Model interpretation: support vectors lÃ  cÃ¡c "hard cases"

4. **Debugging:**
   - Kiá»ƒm tra tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a implementation
   - PhÃ¡t hiá»‡n lá»—i numerical (complementary slackness vi pháº¡m)

### 10.8. Khi nÃ o nÃªn dÃ¹ng SVM?

**âœ… SVM phÃ¹ há»£p khi:**
- Dá»¯ liá»‡u cÃ³ **sá»‘ chiá»u cao** ($$p > n$$): text classification, bioinformatics
- Cáº§n mÃ´ hÃ¬nh **robust** vá»›i outliers
- CÃ³ **margin rÃµ rÃ ng** giá»¯a cÃ¡c classes
- Cáº§n **interpretability** (thÃ´ng qua support vectors)
- Muá»‘n dÃ¹ng **kernel trick** cho dá»¯ liá»‡u phi tuyáº¿n

**âŒ SVM khÃ´ng phÃ¹ há»£p khi:**
- Dá»¯ liá»‡u **ráº¥t lá»›n** ($$n > 10^6$$): training cháº­m
- Cáº§n **probability estimates**: SVM output lÃ  distance, khÃ´ng pháº£i xÃ¡c suáº¥t
- **Multi-class** vá»›i nhiá»u classes ($$K > 10$$): cáº§n one-vs-one hay one-vs-all
- Dá»¯ liá»‡u cÃ³ **nhiá»u noise**: neural networks cÃ³ thá»ƒ tá»‘t hÆ¡n

### 10.9. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khi nÃ o dÃ¹ng |
|-------------|---------|-----------|--------------|
| **SVM** | Margin maximization, kernel trick, robust | Training cháº­m, khÃ³ tune $$C$$ | Dá»¯ liá»‡u chiá»u cao, margin rÃµ |
| **Logistic Regression** | Nhanh, probability output, dá»… hiá»ƒu | Linear boundary, nháº¡y outliers | Baseline, cáº§n probability |
| **Decision Tree** | KhÃ´ng cáº§n normalize, dá»… visualize | Overfitting, unstable | Tabular data, cáº§n explain |
| **Neural Networks** | Flexible, powerful, scalable | Cáº§n nhiá»u data, black box | Dá»¯ liá»‡u lá»›n, pattern phá»©c táº¡p |

### 10.10. Lá»i khuyÃªn thá»±c hÃ nh

1. **Tiá»n xá»­ lÃ½ dá»¯ liá»‡u:**
   - **Normalize/standardize** features: SVM nháº¡y cáº£m vá»›i scale
   - Xá»­ lÃ½ missing values
   - Feature engineering quan trá»ng cho linear SVM

2. **Chá»n kernel:**
   - **Linear kernel**: Thá»­ Ä‘áº§u tiÃªn, nhanh, trÃ¡nh overfit
   - **RBF kernel**: Default choice cho non-linear
   - **Polynomial kernel**: Ãt dÃ¹ng, khÃ³ tune
   - **Custom kernel**: Khi cÃ³ domain knowledge

3. **Tuning hyperparameters:**
   - $$C$$: Grid search [0.001, 0.01, 0.1, 1, 10, 100, 1000]
   - $$\gamma$$ (cho RBF): Grid search [0.001, 0.01, 0.1, 1]
   - DÃ¹ng **cross-validation** (5-fold hoáº·c 10-fold)
   - **Stratified CV** náº¿u classes imbalanced

4. **ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh:**
   - KhÃ´ng chá»‰ nhÃ¬n accuracy
   - Xem **confusion matrix**, precision, recall, F1-score
   - PhÃ¢n tÃ­ch **support vectors**: Nhiá»u SV â†’ cÃ³ thá»ƒ overfitting
   - Visualize decision boundary (náº¿u 2D/3D)

5. **Xá»­ lÃ½ imbalanced data:**
   - DÃ¹ng **class_weight='balanced'** trong scikit-learn
   - Hoáº·c tune $$C_+$$ vÃ  $$C_-$$ khÃ¡c nhau cho má»—i class
   - SMOTE (Synthetic Minority Over-sampling)

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

### SÃ¡ch vÃ  giÃ¡o trÃ¬nh

1. **Vapnik, V. N.** (1995). *The Nature of Statistical Learning Theory*. Springer.
2. **Boyd, S., & Vandenberghe, L.** (2004). *Convex Optimization*. Cambridge University Press. Chapter 12.
3. **Hastie, T., et al.** (2009). *The Elements of Statistical Learning* (2nd ed.). Springer. Chapter 12.
4. **Cristianini, N., & Shawe-Taylor, J.** (2000). *An Introduction to Support Vector Machines*. Cambridge University Press.
5. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer. Chapter 7.

### BÃ i viáº¿t vÃ  tutorial

6. **Machine Learning CÆ¡ Báº£n** - [BÃ i 19: Support Vector Machine](https://machinelearningcoban.com/2017/04/09/smv/) - VÅ© Há»¯u Tiá»‡p
7. **scikit-learn Documentation** - [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)
8. **LIBSVM** - [A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
