---
layout: post
title: 18-8 B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Quasi-Newton
chapter: '18'
order: 9
owner: GitHub Copilot
lang: vi
categories:
- chapter18
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - Ph∆∞∆°ng Ph√°p Quasi-Newton

## üìù **B√†i t·∫≠p 1: Quasi-Newton Methods Implementation**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Nocedal & Wright, Chapter 6)
Implement complete quasi-Newton methods framework:

a) **SR1 update** implementation v√† analysis
b) **DFP v√† BFGS** methods comparison
c) **L-BFGS** cho large-scale problems
d) **Convergence analysis** v√† performance comparison

**Y√™u c·∫ßu:**
1. Complete quasi-Newton implementations
2. Hessian approximation updates
3. Memory-efficient variants
4. Comprehensive performance analysis

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: Quasi-Newton Framework**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize, line_search
from scipy.linalg import solve, LinAlgError
import time
from collections import deque
import warnings
warnings.filterwarnings('ignore')

class QuasiNewtonMethods:
    def __init__(self, method='BFGS', tolerance=1e-8, max_iterations=1000):
        self.method = method
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'x': [],
            'f': [],
            'grad_norm': [],
            'step_size': [],
            'hessian_condition': [],
            'hessian_approx': []
        }
    
    def solve(self, objective_func, gradient_func, x0, verbose=True):
        """
        Solve optimization problem using quasi-Newton methods
        
        Tham s·ªë:
        - objective_func: f(x) -> scalar
        - gradient_func: ‚àáf(x) -> vector
        - x0: initial point
        """
        
        x = x0.copy()
        n = len(x)
        
        # Kh·ªüi t·∫°o Hessian approximation
        if self.method in ['BFGS', 'DFP']:
            H = np.eye(n)  # Inverse Hessian approximation
        elif self.method == 'SR1':
            B = np.eye(n)  # Hessian approximation
        
        if verbose:
            print(f"{self.method} Quasi-Newton Method")
            print("=" * 35)
            print(f"{'Iter':<4} {'f(x)':<12} {'||‚àáf||':<12} {'Step':<8} {'Cond':<8}")
            print("-" * 55)
        
        # Initial evaluation
        f_val = objective_func(x)
        grad = gradient_func(x)
        
        for iteration in range(self.max_iter):
            # Store history
            self.history['x'].append(x.copy())
            self.history['f'].append(f_val)
            self.history['grad_norm'].append(np.linalg.norm(grad))
            
            # Check convergence
            grad_norm = np.linalg.norm(grad)
            if grad_norm < self.tol:
                if verbose:
                    print(f"Converged in {iteration} iterations")
                break
            
            # Compute search direction
            if self.method in ['BFGS', 'DFP']:
                p = -H @ grad
                hess_condition = np.linalg.cond(H)
            elif self.method == 'SR1':
                try:
                    p = -solve(B, grad)
                    hess_condition = np.linalg.cond(B)
                except LinAlgError:
                    if verbose:
                        print(f"Singular Hessian approximation at iteration {iteration}")
                    break
            
            self.history['hessian_condition'].append(hess_condition)
            
            # Line search
            step_size = self._line_search(objective_func, gradient_func, x, p, f_val, grad)
            self.history['step_size'].append(step_size)
            
            # Update position
            x_new = x + step_size * p
            f_new = objective_func(x_new)
            grad_new = gradient_func(x_new)
            
            if verbose:
                print(f"{iteration:<4} {f_val:<12.6f} {grad_norm:<12.6f} "
                      f"{step_size:<8.4f} {hess_condition:<8.2f}")
            
            # Update Hessian approximation
            s = x_new - x  # Step vector
            y = grad_new - grad  # Gradient change
            
            if self.method == 'BFGS':
                H = self._bfgs_update(H, s, y)
            elif self.method == 'DFP':
                H = self._dfp_update(H, s, y)
            elif self.method == 'SR1':
                B = self._sr1_update(B, s, y)
            
            # Store Hessian approximation
            if self.method in ['BFGS', 'DFP']:
                self.history['hessian_approx'].append(H.copy())
            else:
                self.history['hessian_approx'].append(B.copy())
            
            # Prepare for next iteration
            x = x_new
            f_val = f_new
            grad = grad_new
        
        return x, f_val, iteration + 1
    
    def _bfgs_update(self, H, s, y):
        """BFGS update for inverse Hessian approximation"""
        
        sy = s.T @ y
        if sy <= 1e-10:  # Skip update if curvature condition violated
            return H
        
        # BFGS update formula
        rho = 1.0 / sy
        I = np.eye(len(s))
        
        V = I - rho * np.outer(y, s)
        H_new = V.T @ H @ V + rho * np.outer(s, s)
        
        return H_new
    
    def _dfp_update(self, H, s, y):
        """DFP update for inverse Hessian approximation"""
        
        sy = s.T @ y
        if sy <= 1e-10:  # Skip update if curvature condition violated
            return H
        
        # DFP update formula
        Hy = H @ y
        yHy = y.T @ Hy
        
        H_new = H + np.outer(s, s) / sy - np.outer(Hy, Hy) / yHy
        
        return H_new
    
    def _sr1_update(self, B, s, y):
        """SR1 update for Hessian approximation"""
        
        # SR1 update formula
        r = y - B @ s
        rTs = r.T @ s
        
        # Check denominator to avoid numerical issues
        if abs(rTs) > 1e-10:
            B_new = B + np.outer(r, r) / rTs
            return B_new
        else:
            return B  # Skip update
    
    def _line_search(self, f, grad_f, x, p, f_x, grad_x, alpha=0.3, beta=0.8):
        """Backtracking line search with Armijo condition"""
        
        t = 1.0
        
        while f(x + t * p) > f_x + alpha * t * grad_x.T @ p:
            t *= beta
            if t < 1e-10:
                break
        
        return t
    
    def plot_convergence(self):
        """Plot convergence history"""
        
        if not self.history['f']:
            print("No convergence history to plot")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        iterations = range(len(self.history['f']))
        
        # Plot 1: Objective function
        axes[0, 0].semilogy(iterations, self.history['f'], 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('f(x)')
        axes[0, 0].set_title(f'{self.method} - Objective Function')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Gradient norm
        axes[0, 1].semilogy(iterations, self.history['grad_norm'], 'r-o', linewidth=2, markersize=4)
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('||‚àáf(x)||')
        axes[0, 1].set_title('Gradient Norm')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Step size
        if self.history['step_size']:
            axes[0, 2].plot(range(len(self.history['step_size'])), self.history['step_size'], 
                           'g-o', linewidth=2, markersize=4)
            axes[0, 2].set_xlabel('Iteration')
            axes[0, 2].set_ylabel('Step Size')
            axes[0, 2].set_title('Line Search Step Size')
            axes[0, 2].grid(True, alpha=0.3)
        
        # Plot 4: Hessian condition number
        if self.history['hessian_condition']:
            axes[1, 0].semilogy(range(len(self.history['hessian_condition'])), 
                               self.history['hessian_condition'], 'purple', 
                               linewidth=2, marker='s', markersize=4)
            axes[1, 0].set_xlabel('Iteration')
            axes[1, 0].set_ylabel('Condition Number')
            axes[1, 0].set_title('Hessian Approximation Condition')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 5: Optimization path (for 2D problems)
        if len(self.history['x']) > 0 and len(self.history['x'][0]) == 2:
            x_path = np.array(self.history['x'])
            axes[1, 1].plot(x_path[:, 0], x_path[:, 1], 'b-o', linewidth=2, markersize=4)
            axes[1, 1].plot(x_path[0, 0], x_path[0, 1], 'go', markersize=8, label='Start')
            axes[1, 1].plot(x_path[-1, 0], x_path[-1, 1], 'ro', markersize=8, label='End')
            axes[1, 1].set_xlabel('x‚ÇÅ')
            axes[1, 1].set_ylabel('x‚ÇÇ')
            axes[1, 1].set_title('Optimization Path')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'Optimization Path\n(2D visualization only)', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
        
        # Plot 6: Hessian approximation evolution (eigenvalues)
        if self.history['hessian_approx'] and len(self.history['hessian_approx'][0]) <= 10:
            eigenvals_history = []
            for H in self.history['hessian_approx']:
                eigenvals = np.linalg.eigvals(H)
                eigenvals_history.append(eigenvals)
            
            eigenvals_history = np.array(eigenvals_history)
            
            for i in range(min(3, eigenvals_history.shape[1])):
                axes[1, 2].semilogy(range(len(eigenvals_history)), 
                                   np.abs(eigenvals_history[:, i]), 
                                   linewidth=2, marker='o', markersize=3, 
                                   label=f'Œª_{i+1}')
            
            axes[1, 2].set_xlabel('Iteration')
            axes[1, 2].set_ylabel('|Eigenvalues|')
            axes[1, 2].set_title('Hessian Approximation Eigenvalues')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        else:
            axes[1, 2].text(0.5, 0.5, 'Eigenvalue Evolution\n(Small matrices only)', 
                           ha='center', va='center', transform=axes[1, 2].transAxes)
        
        plt.tight_layout()
        plt.show()

def example_rosenbrock_comparison():
    """Example: Compare quasi-Newton methods on Rosenbrock function"""
    
    print("Example 1: Quasi-Newton Methods on Rosenbrock Function")
    print("=" * 60)
    print("f(x) = 100(x‚ÇÇ - x‚ÇÅ¬≤)¬≤ + (1 - x‚ÇÅ)¬≤")
    print("Global minimum: x* = (1, 1), f* = 0")
    
    # Define Rosenbrock function
    def rosenbrock(x):
        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2
    
    def grad_rosenbrock(x):
        grad = np.zeros(2)
        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])
        grad[1] = 200 * (x[1] - x[0]**2)
        return grad
    
    x0 = np.array([-1.2, 1.0])  # Standard starting point
    methods = ['BFGS', 'DFP', 'SR1']
    results = {}
    
    for method in methods:
        print(f"\n{method} Method:")
        print("-" * 20)
        
        solver = QuasiNewtonMethods(method=method, tolerance=1e-8)
        start_time = time.time()
        
        x_opt, f_opt, iterations = solver.solve(rosenbrock, grad_rosenbrock, x0, verbose=False)
        
        solve_time = time.time() - start_time
        error = np.linalg.norm(x_opt - np.array([1, 1]))
        
        results[method] = {
            'x_opt': x_opt,
            'f_opt': f_opt,
            'iterations': iterations,
            'time': solve_time,
            'error': error,
            'solver': solver
        }
        
        print(f"Solution: x = [{x_opt[0]:.6f}, {x_opt[1]:.6f}]")
        print(f"Function value: f = {f_opt:.8f}")
        print(f"Iterations: {iterations}")
        print(f"Time: {solve_time:.4f} seconds")
        print(f"Error: ||x - x*|| = {error:.6f}")
    
    # Comparison summary
    print(f"\n{'='*60}")
    print("COMPARISON SUMMARY")
    print("=" * 60)
    print(f"{'Method':<8} {'Iterations':<12} {'Time (s)':<10} {'Error':<12} {'Final f(x)'}")
    print("-" * 65)
    
    for method, result in results.items():
        print(f"{method:<8} {result['iterations']:<12} {result['time']:<10.4f} "
              f"{result['error']:<12.6f} {result['f_opt']:.2e}")
    
    # Plot comparison
    plt.figure(figsize=(15, 10))
    
    # Convergence comparison
    plt.subplot(2, 2, 1)
    for method, result in results.items():
        solver = result['solver']
        if solver.history['f']:
            iterations = range(len(solver.history['f']))
            plt.semilogy(iterations, solver.history['f'], 'o-', linewidth=2, 
                        markersize=4, label=method)
    
    plt.xlabel('Iteration')
    plt.ylabel('f(x)')
    plt.title('Objective Function Convergence')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Gradient norm comparison
    plt.subplot(2, 2, 2)
    for method, result in results.items():
        solver = result['solver']
        if solver.history['grad_norm']:
            iterations = range(len(solver.history['grad_norm']))
            plt.semilogy(iterations, solver.history['grad_norm'], 'o-', linewidth=2, 
                        markersize=4, label=method)
    
    plt.xlabel('Iteration')
    plt.ylabel('||‚àáf(x)||')
    plt.title('Gradient Norm Convergence')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Optimization paths
    plt.subplot(2, 2, 3)
    
    # Contour plot
    x1 = np.linspace(-2, 2, 100)
    x2 = np.linspace(-1, 3, 100)
    X1, X2 = np.meshgrid(x1, x2)
    Z = 100 * (X2 - X1**2)**2 + (1 - X1)**2
    
    plt.contour(X1, X2, Z, levels=np.logspace(0, 3, 20), alpha=0.6)
    plt.plot(1, 1, 'r*', markersize=15, label='Global minimum')
    
    colors = ['blue', 'green', 'orange']
    for i, (method, result) in enumerate(results.items()):
        solver = result['solver']
        if solver.history['x']:
            x_path = np.array(solver.history['x'])
            plt.plot(x_path[:, 0], x_path[:, 1], 'o-', color=colors[i], 
                    linewidth=2, markersize=4, label=method)
    
    plt.xlabel('x‚ÇÅ')
    plt.ylabel('x‚ÇÇ')
    plt.title('Optimization Paths')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Performance metrics
    plt.subplot(2, 2, 4)
    
    method_names = list(results.keys())
    iterations = [results[name]['iterations'] for name in method_names]
    times = [results[name]['time'] for name in method_names]
    
    x_pos = np.arange(len(method_names))
    width = 0.35
    
    plt.bar(x_pos - width/2, iterations, width, label='Iterations', alpha=0.7)
    plt.bar(x_pos + width/2, np.array(times) * 1000, width, label='Time (ms)', alpha=0.7)
    
    plt.xlabel('Method')
    plt.ylabel('Count')
    plt.title('Performance Metrics')
    plt.xticks(x_pos, method_names)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return results

# Run Rosenbrock comparison
rosenbrock_results = example_rosenbrock_comparison()
```

**B∆∞·ªõc 2: L-BFGS Implementation**

```python
class LBFGS:
    def __init__(self, m=10, tolerance=1e-8, max_iterations=1000):
        """
        Limited-memory BFGS implementation
        
        Tham s·ªë:
        - m: number of correction pairs to store
        - tolerance: convergence tolerance
        - max_iterations: maximum number of iterations
        """
        self.m = m
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'x': [],
            'f': [],
            'grad_norm': [],
            'step_size': [],
            'memory_usage': []
        }
    
    def solve(self, objective_func, gradient_func, x0, verbose=True):
        """Solve using L-BFGS algorithm"""
        
        x = x0.copy()
        n = len(x)
        
        # Storage for correction pairs (limited memory)
        s_history = deque(maxlen=self.m)  # Steps
        y_history = deque(maxlen=self.m)  # Gradient changes
        rho_history = deque(maxlen=self.m)  # 1/(s^T y)
        
        if verbose:
            print("L-BFGS Algorithm")
            print("=" * 25)
            print(f"Memory limit: {self.m} correction pairs")
            print(f"Problem dimension: {n}")
            print()
            print(f"{'Iter':<4} {'f(x)':<12} {'||‚àáf||':<12} {'Step':<8} {'Memory':<8}")
            print("-" * 60)
        
        # Initial evaluation
        f_val = objective_func(x)
        grad = gradient_func(x)
        
        for iteration in range(self.max_iter):
            # Store history
            self.history['x'].append(x.copy())
            self.history['f'].append(f_val)
            self.history['grad_norm'].append(np.linalg.norm(grad))
            self.history['memory_usage'].append(len(s_history))
            
            # Check convergence
            grad_norm = np.linalg.norm(grad)
            if grad_norm < self.tol:
                if verbose:
                    print(f"Converged in {iteration} iterations")
                break
            
            # Compute search direction using two-loop recursion
            p = self._two_loop_recursion(grad, s_history, y_history, rho_history)
            
            # Line search
            step_size = self._line_search(objective_func, gradient_func, x, p, f_val, grad)
            self.history['step_size'].append(step_size)
            
            # Update position
            x_new = x + step_size * p
            f_new = objective_func(x_new)
            grad_new = gradient_func(x_new)
            
            if verbose:
                print(f"{iteration:<4} {f_val:<12.6f} {grad_norm:<12.6f} "
                      f"{step_size:<8.4f} {len(s_history):<8}")
            
            # Update correction pairs
            s = x_new - x
            y = grad_new - grad
            sy = s.T @ y
            
            if sy > 1e-10:  # Curvature condition
                # Add new correction pair (automatically removes oldest if full)
                s_history.append(s)
                y_history.append(y)
                rho_history.append(1.0 / sy)
            
            # Prepare for next iteration
            x = x_new
            f_val = f_new
            grad = grad_new
        
        return x, f_val, iteration + 1
    
    def _two_loop_recursion(self, grad, s_history, y_history, rho_history):
        """Two-loop recursion for L-BFGS direction computation"""
        
        if len(s_history) == 0:
            return -grad  # Steepest descent direction
        
        q = grad.copy()
        alpha_history = []
        
        # First loop (backward)
        for i in reversed(range(len(s_history))):
            alpha = rho_history[i] * s_history[i].T @ q
            alpha_history.append(alpha)
            q = q - alpha * y_history[i]
        
        # Initial Hessian approximation (scalar)
        s_last = s_history[-1]
        y_last = y_history[-1]
        gamma = (s_last.T @ y_last) / (y_last.T @ y_last)
        
        r = gamma * q
        
        # Second loop (forward)
        alpha_history.reverse()
        for i in range(len(s_history)):
            beta = rho_history[i] * y_history[i].T @ r
            r = r + (alpha_history[i] - beta) * s_history[i]
        
        return -r
    
    def _line_search(self, f, grad_f, x, p, f_x, grad_x, alpha=0.3, beta=0.8):
        """Backtracking line search"""
        
        t = 1.0
        
        while f(x + t * p) > f_x + alpha * t * grad_x.T @ p:
            t *= beta
            if t < 1e-10:
                break
        
        return t
    
    def plot_convergence(self):
        """Plot L-BFGS convergence history"""
        
        if not self.history['f']:
            print("No convergence history to plot")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        iterations = range(len(self.history['f']))
        
        # Plot 1: Objective function
        axes[0, 0].semilogy(iterations, self.history['f'], 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('f(x)')
        axes[0, 0].set_title('L-BFGS - Objective Function')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Gradient norm
        axes[0, 1].semilogy(iterations, self.history['grad_norm'], 'r-o', linewidth=2, markersize=4)
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('||‚àáf(x)||')
        axes[0, 1].set_title('Gradient Norm')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Memory usage
        axes[1, 0].plot(iterations, self.history['memory_usage'], 'g-o', linewidth=2, markersize=4)
        axes[1, 0].axhline(y=self.m, color='red', linestyle='--', alpha=0.7, label=f'Memory limit = {self.m}')
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Stored Correction Pairs')
        axes[1, 0].set_title('Memory Usage')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 4: Step sizes
        if self.history['step_size']:
            axes[1, 1].plot(range(len(self.history['step_size'])), self.history['step_size'], 
                           'purple', linewidth=2, marker='s', markersize=4)
            axes[1, 1].set_xlabel('Iteration')
            axes[1, 1].set_ylabel('Step Size')
            axes[1, 1].set_title('Line Search Step Size')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def example_large_scale_optimization():
    """Example: L-BFGS for large-scale optimization"""
    
    print("\nExample 2: L-BFGS for Large-Scale Optimization")
    print("=" * 55)
    
    # Generate large-scale quadratic problem
    np.random.seed(42)
    n = 1000  # Large dimension
    
    # Create well-conditioned positive definite matrix
    A = np.random.randn(n, n//2)
    Q = A @ A.T + 0.1 * np.eye(n)
    b = np.random.randn(n)
    c = 1.0
    
    print(f"Problem: min (1/2)x^T Q x + b^T x + c")
    print(f"Dimension: n = {n}")
    print(f"Condition number: {np.linalg.cond(Q):.2f}")
    
    def objective(x):
        return 0.5 * x.T @ Q @ x + b.T @ x + c
    
    def gradient(x):
        return Q @ x + b
    
    # Analytical solution
    x_analytical = -np.linalg.solve(Q, b)
    f_analytical = objective(x_analytical)
    
    print(f"Analytical minimum: f* = {f_analytical:.6f}")
    
    # Compare different memory sizes
    memory_sizes = [5, 10, 20, 50]
    x0 = np.random.randn(n)
    
    results_lbfgs = {}
    
    for m in memory_sizes:
        print(f"\nL-BFGS with m = {m}:")
        print("-" * 25)
        
        start_time = time.time()
        lbfgs = LBFGS(m=m, tolerance=1e-6)
        x_opt, f_opt, iterations = lbfgs.solve(objective, gradient, x0, verbose=False)
        solve_time = time.time() - start_time
        
        error = np.linalg.norm(x_opt - x_analytical)
        
        results_lbfgs[m] = {
            'x_opt': x_opt,
            'f_opt': f_opt,
            'iterations': iterations,
            'time': solve_time,
            'error': error,
            'solver': lbfgs
        }
        
        print(f"Iterations: {iterations}")
        print(f"Time: {solve_time:.4f} seconds")
        print(f"Final f(x): {f_opt:.8f}")
        print(f"Error: {error:.6f}")
        print(f"Memory efficiency: {m * n * 8 / 1024:.1f} KB vs {n*n*8/1024/1024:.1f} MB for full BFGS")
    
    # Comparison with scipy L-BFGS-B
    print(f"\nComparison with scipy L-BFGS-B:")
    print("-" * 35)
    
    start_time = time.time()
    result_scipy = minimize(objective, x0, method='L-BFGS-B', jac=gradient, 
                           options={'gtol': 1e-6, 'ftol': 1e-9})
    scipy_time = time.time() - start_time
    
    if result_scipy.success:
        scipy_error = np.linalg.norm(result_scipy.x - x_analytical)
        print(f"Scipy iterations: {result_scipy.nit}")
        print(f"Scipy time: {scipy_time:.4f} seconds")
        print(f"Scipy error: {scipy_error:.6f}")
    
    # Plot comparison
    plt.figure(figsize=(15, 10))
    
    # Convergence comparison
    plt.subplot(2, 2, 1)
    for m, result in results_lbfgs.items():
        solver = result['solver']
        if solver.history['f']:
            iterations = range(len(solver.history['f']))
            plt.semilogy(iterations, solver.history['f'], 'o-', linewidth=2, 
                        markersize=3, label=f'L-BFGS m={m}')
    
    plt.axhline(y=f_analytical, color='red', linestyle='--', alpha=0.7, label='Analytical')
    plt.xlabel('Iteration')
    plt.ylabel('f(x)')
    plt.title('Objective Function Convergence')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Memory usage comparison
    plt.subplot(2, 2, 2)
    for m, result in results_lbfgs.items():
        solver = result['solver']
        if solver.history['memory_usage']:
            iterations = range(len(solver.history['memory_usage']))
            plt.plot(iterations, solver.history['memory_usage'], 'o-', linewidth=2, 
                    markersize=3, label=f'L-BFGS m={m}')
    
    plt.xlabel('Iteration')
    plt.ylabel('Stored Correction Pairs')
    plt.title('Memory Usage Evolution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Performance summary
    plt.subplot(2, 2, 3)
    
    memory_vals = list(results_lbfgs.keys())
    iterations_vals = [results_lbfgs[m]['iterations'] for m in memory_vals]
    times_vals = [results_lbfgs[m]['time'] for m in memory_vals]
    
    plt.bar(np.arange(len(memory_vals)) - 0.2, iterations_vals, 0.4, 
           label='Iterations', alpha=0.7)
    plt.bar(np.arange(len(memory_vals)) + 0.2, np.array(times_vals) * 100, 0.4, 
           label='Time (√ó100 s)', alpha=0.7)
    
    plt.xlabel('Memory Size (m)')
    plt.ylabel('Count')
    plt.title('Performance vs Memory Size')
    plt.xticks(range(len(memory_vals)), memory_vals)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Error comparison
    plt.subplot(2, 2, 4)
    
    error_vals = [results_lbfgs[m]['error'] for m in memory_vals]
    
    plt.semilogy(memory_vals, error_vals, 'ro-', linewidth=2, markersize=6)
    plt.xlabel('Memory Size (m)')
    plt.ylabel('Final Error')
    plt.title('Accuracy vs Memory Size')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return results_lbfgs

# Run large-scale L-BFGS example
lbfgs_results = example_large_scale_optimization()
```

**B∆∞·ªõc 3: Convergence Analysis**

```python
def analyze_quasi_newton_convergence():
    """Analyze convergence properties of quasi-Newton methods"""
    
    print("\nExample 3: Convergence Analysis")
    print("=" * 40)
    
    # Test on different function types
    test_functions = {
        'Quadratic': {
            'func': lambda x: 0.5 * (x[0]**2 + 10*x[1]**2),
            'grad': lambda x: np.array([x[0], 10*x[1]]),
            'x0': np.array([5.0, 2.0]),
            'x_opt': np.array([0.0, 0.0])
        },
        
        'Rosenbrock': {
            'func': lambda x: 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2,
            'grad': lambda x: np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]), 
                                       200*(x[1] - x[0]**2)]),
            'x0': np.array([-1.2, 1.0]),
            'x_opt': np.array([1.0, 1.0])
        },
        
        'Beale': {
            'func': lambda x: (1.5 - x[0] + x[0]*x[1])**2 + (2.25 - x[0] + x[0]*x[1]**2)**2 + (2.625 - x[0] + x[0]*x[1]**3)**2,
            'grad': lambda x: np.array([
                2*(1.5 - x[0] + x[0]*x[1])*(x[1] - 1) + 2*(2.25 - x[0] + x[0]*x[1]**2)*(x[1]**2 - 1) + 2*(2.625 - x[0] + x[0]*x[1]**3)*(x[1]**3 - 1),
                2*(1.5 - x[0] + x[0]*x[1])*x[0] + 2*(2.25 - x[0] + x[0]*x[1]**2)*2*x[0]*x[1] + 2*(2.625 - x[0] + x[0]*x[1]**3)*3*x[0]*x[1]**2
            ]),
            'x0': np.array([1.0, 1.0]),
            'x_opt': np.array([3.0, 0.5])
        }
    }
    
    methods = ['BFGS', 'DFP', 'SR1']
    convergence_analysis = {}
    
    for func_name, func_info in test_functions.items():
        print(f"\nFunction: {func_name}")
        print("-" * 30)
        
        convergence_analysis[func_name] = {}
        
        for method in methods:
            solver = QuasiNewtonMethods(method=method, tolerance=1e-10, max_iterations=200)
            
            x_opt, f_opt, iterations = solver.solve(
                func_info['func'], func_info['grad'], func_info['x0'], verbose=False
            )
            
            # Analyze convergence rate
            errors = []
            for x in solver.history['x']:
                error = np.linalg.norm(x - func_info['x_opt'])
                errors.append(error)
            
            # Estimate convergence rate
            if len(errors) > 10:
                # Linear convergence: ||x_{k+1} - x*|| ‚â§ œÅ||x_k - x*||
                # Superlinear: lim ||x_{k+1} - x*|| / ||x_k - x*|| = 0
                
                ratios = []
                for i in range(5, min(len(errors)-1, 50)):
                    if errors[i] > 1e-12:
                        ratio = errors[i+1] / errors[i]
                        ratios.append(ratio)
                
                avg_ratio = np.mean(ratios) if ratios else float('inf')
                
                # Estimate order of convergence
                if len(errors) > 20:
                    log_errors = np.log(np.array(errors[10:20]) + 1e-15)
                    if len(log_errors) > 1:
                        # Fit log(error) vs iteration
                        iterations_fit = np.arange(len(log_errors))
                        slope = np.polyfit(iterations_fit, log_errors, 1)[0]
                        convergence_rate = -slope
                    else:
                        convergence_rate = 0
                else:
                    convergence_rate = 0
            else:
                avg_ratio = float('inf')
                convergence_rate = 0
            
            convergence_analysis[func_name][method] = {
                'iterations': iterations,
                'final_error': np.linalg.norm(x_opt - func_info['x_opt']),
                'avg_ratio': avg_ratio,
                'convergence_rate': convergence_rate,
                'errors': errors
            }
            
            print(f"{method:>6}: {iterations:3d} iter, error: {convergence_analysis[func_name][method]['final_error']:.2e}, "
                  f"avg ratio: {avg_ratio:.3f}")
    
    # Plot convergence analysis
    fig, axes = plt.subplots(len(test_functions), 2, figsize=(15, 5*len(test_functions)))
    
    if len(test_functions) == 1:
        axes = axes.reshape(1, -1)
    
    colors = ['blue', 'green', 'orange']
    
    for i, (func_name, func_data) in enumerate(convergence_analysis.items()):
        # Plot error convergence
        ax1 = axes[i, 0]
        for j, (method, data) in enumerate(func_data.items()):
            if data['errors']:
                iterations = range(len(data['errors']))
                ax1.semilogy(iterations, data['errors'], 'o-', color=colors[j], 
                           linewidth=2, markersize=3, label=f"{method}")
        
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('||x - x*||')
        ax1.set_title(f'{func_name} - Error Convergence')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot convergence ratios
        ax2 = axes[i, 1]
        method_names = list(func_data.keys())
        ratios = [func_data[method]['avg_ratio'] for method in method_names]
        rates = [func_data[method]['convergence_rate'] for method in method_names]
        
        x_pos = np.arange(len(method_names))
        width = 0.35
        
        bars1 = ax2.bar(x_pos - width/2, ratios, width, label='Avg Ratio', alpha=0.7)
        bars2 = ax2.bar(x_pos + width/2, rates, width, label='Conv Rate', alpha=0.7)
        
        ax2.set_xlabel('Method')
        ax2.set_ylabel('Rate')
        ax2.set_title(f'{func_name} - Convergence Analysis')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(method_names)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            if height < 10:  # Only label reasonable values
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # Summary
    print(f"\n{'='*60}")
    print("CONVERGENCE ANALYSIS SUMMARY")
    print("=" * 60)
    print(f"{'Function':<12} {'Method':<6} {'Iterations':<12} {'Final Error':<15} {'Avg Ratio':<12}")
    print("-" * 75)
    
    for func_name, func_data in convergence_analysis.items():
        for method, data in func_data.items():
            print(f"{func_name:<12} {method:<6} {data['iterations']:<12} "
                  f"{data['final_error']:<15.2e} {data['avg_ratio']:<12.3f}")
    
    return convergence_analysis

# Run convergence analysis
convergence_results = analyze_quasi_newton_convergence()
```

</details>

---

## üìù **B√†i t·∫≠p 2: Secant Equation v√† Curvature Conditions**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Nocedal & Wright, Section 6.1)
Analyze secant equation v√† curvature conditions:

a) **Secant equation** verification v√† analysis
b) **Curvature condition** importance
c) **Hessian approximation** quality metrics
d) **Update formula** derivation v√† comparison

**Y√™u c·∫ßu:**
1. Secant equation implementation
2. Curvature condition analysis
3. Hessian approximation quality assessment
4. Mathematical derivation verification

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üìù **B√†i t·∫≠p 3: Advanced Quasi-Newton Applications**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Practical Applications)
Apply quasi-Newton methods to real-world problems:

a) **Machine learning** optimization
b) **Parameter estimation** problems
c) **Large-scale** scientific computing
d) **Constrained** quasi-Newton methods

**Y√™u c·∫ßu:**
1. ML optimization applications
2. Parameter estimation frameworks
3. Large-scale implementations
4. Constrained optimization extensions

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi implement quasi-Newton methods:**
- Always check curvature condition s^T y > 0
- Use appropriate initial Hessian approximation
- Implement robust line search
- Monitor condition number c·ªßa Hessian approximation

#### **Khi choose between methods:**
- **BFGS**: Generally most robust v√† efficient
- **DFP**: Good for some specific problems
- **SR1**: Can handle indefinite Hessians
- **L-BFGS**: Best cho large-scale problems

#### **Khi handle numerical issues:**
- Skip updates when curvature condition violated
- Use regularization cho ill-conditioned approximations
- Implement safeguards cho line search
- Monitor convergence carefully

#### **Khi apply to specific problems:**
- Exploit problem structure when possible
- Use appropriate scaling
- Consider memory constraints
- Validate convergence properties

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Nocedal, J., & Wright, S. J.** (2006). *Numerical Optimization*. Springer.
   - Chapter 6: Quasi-Newton Methods

2. **Dennis, J. E., & Schnabel, R. B.** (1996). *Numerical Methods for Unconstrained Optimization and Nonlinear Equations*. SIAM.

3. **Fletcher, R.** (2000). *Practical Methods of Optimization*. Wiley.
