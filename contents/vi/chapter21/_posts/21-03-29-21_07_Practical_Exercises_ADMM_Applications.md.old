---
layout: post
title: 21-7 B√†i T·∫≠p Th·ª±c H√†nh - ·ª®ng D·ª•ng ADMM
chapter: '21'
order: 8
owner: GitHub Copilot
lang: vi
categories:
- chapter21
lesson_type: required
---

# B√†i T·∫≠p Th·ª±c H√†nh - ·ª®ng D·ª•ng ADMM

## üìù **B√†i t·∫≠p 1: ADMM cho LASSO v√† Group LASSO**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd et al., 2011)
Implement ADMM algorithms cho sparse regression problems:

a) **LASSO** regression v·ªõi ADMM
b) **Group LASSO** regression
c) **Elastic Net** regularization
d) **Performance comparison** v·ªõi c√°c methods kh√°c

**Y√™u c·∫ßu:**
1. Complete ADMM implementations cho LASSO problems
2. Group structure handling
3. Parameter tuning strategies
4. Comprehensive benchmarking

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**B∆∞·ªõc 1: ADMM Framework cho LASSO**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import sparse
from scipy.sparse.linalg import spsolve
import time
import warnings
warnings.filterwarnings('ignore')

class ADMM_LASSO:
    def __init__(self, rho=1.0, alpha=1.0, tolerance=1e-4, max_iterations=1000):
        """
        ADMM for LASSO: min (1/2)||Ax - b||¬≤ + Œª||x||‚ÇÅ
        
        Reformulation: min (1/2)||Ax - b||¬≤ + Œª||z||‚ÇÅ s.t. x = z
        
        Tham s·ªë:
        - rho: penalty parameter œÅ
        - alpha: over-relaxation parameter Œ± ‚àà [1, 2]
        - tolerance: convergence tolerance
        - max_iterations: maximum iterations
        """
        self.rho = rho
        self.alpha = alpha
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'r_norm': [],    # Primal residual
            's_norm': [],    # Dual residual
            'eps_pri': [],   # Primal tolerance
            'eps_dual': [],  # Dual tolerance
            'objective': [], # Objective value
            'x': [],         # Primal variables
            'z': [],         # Auxiliary variables
            'u': []          # Scaled dual variables
        }
    
    def solve(self, A, b, lambda_reg, x0=None, verbose=True):
        """
        Solve LASSO problem using ADMM
        
        Tham s·ªë:
        - A: design matrix (m √ó n)
        - b: response vector (m)
        - lambda_reg: regularization parameter Œª
        - x0: initial point (optional)
        """
        
        m, n = A.shape
        
        # Kh·ªüi t·∫°o
        if x0 is None:
            x = np.zeros(n)
        else:
            x = x0.copy()
        
        z = np.zeros(n)
        u = np.zeros(n)  # Scaled dual variable
        
        # Precompute matrices for x-update
        # x-update: (A^T A + œÅI)x = A^T b + œÅ(z - u)
        AtA = A.T @ A
        Atb = A.T @ b
        
        # Use Cholesky factorization for efficiency
        try:
            L = np.linalg.cholesky(AtA + self.rho * np.eye(n))
            use_cholesky = True
        except:
            use_cholesky = False
        
        if verbose:
            print("ADMM for LASSO")
            print("=" * 40)
            print(f"Problem: m={m}, n={n}, Œª={lambda_reg}")
            print(f"Parameters: œÅ={self.rho}, Œ±={self.alpha}")
            print()
            print(f"{'Iter':<4} {'r norm':<12} {'s norm':<12} {'Objective':<12}")
            print("-" * 55)
        
        for iteration in range(self.max_iter):
            # x-update: (A^T A + œÅI)x = A^T b + œÅ(z - u)
            rhs = Atb + self.rho * (z - u)
            
            if use_cholesky:
                x = np.linalg.solve(L.T, np.linalg.solve(L, rhs))
            else:
                x = np.linalg.solve(AtA + self.rho * np.eye(n), rhs)
            
            # z-update with relaxation
            x_hat = self.alpha * x + (1 - self.alpha) * z
            z_old = z.copy()
            
            # Soft thresholding: z = S_{Œª/œÅ}(xÃÇ + u)
            z = self._soft_threshold(x_hat + u, lambda_reg / self.rho)
            
            # u-update (scaled dual variable)
            u = u + x_hat - z
            
            # Store history
            self.history['x'].append(x.copy())
            self.history['z'].append(z.copy())
            self.history['u'].append(u.copy())
            
            # Compute residuals
            r_norm = np.linalg.norm(x - z)  # Primal residual
            s_norm = np.linalg.norm(-self.rho * (z - z_old))  # Dual residual
            
            self.history['r_norm'].append(r_norm)
            self.history['s_norm'].append(s_norm)
            
            # Compute objective value
            objective = 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_reg * np.linalg.norm(x, 1)
            self.history['objective'].append(objective)
            
            # Stopping criteria
            eps_pri = np.sqrt(n) * self.tol + self.tol * max(np.linalg.norm(x), np.linalg.norm(z))
            eps_dual = np.sqrt(n) * self.tol + self.tol * np.linalg.norm(self.rho * u)
            
            self.history['eps_pri'].append(eps_pri)
            self.history['eps_dual'].append(eps_dual)
            
            if verbose and iteration % 10 == 0:
                print(f"{iteration:<4} {r_norm:<12.6f} {s_norm:<12.6f} {objective:<12.6f}")
            
            # Check convergence
            if r_norm < eps_pri and s_norm < eps_dual:
                if verbose:
                    print(f"\nConverged in {iteration + 1} iterations")
                break
        
        return x, z, iteration + 1
    
    def _soft_threshold(self, x, kappa):
        """Soft thresholding operator: S_Œ∫(x) = sign(x) max(|x| - Œ∫, 0)"""
        return np.sign(x) * np.maximum(np.abs(x) - kappa, 0)
    
    def plot_convergence(self):
        """Plot convergence diagnostics"""
        
        if not self.history['r_norm']:
            print("No convergence history to plot")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        iterations = range(len(self.history['r_norm']))
        
        # Plot 1: Residuals
        axes[0, 0].semilogy(iterations, self.history['r_norm'], 'b-', linewidth=2, label='Primal')
        axes[0, 0].semilogy(iterations, self.history['s_norm'], 'r-', linewidth=2, label='Dual')
        axes[0, 0].semilogy(iterations, self.history['eps_pri'], 'b--', alpha=0.5, label='Œµ_pri')
        axes[0, 0].semilogy(iterations, self.history['eps_dual'], 'r--', alpha=0.5, label='Œµ_dual')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('Residual Norm')
        axes[0, 0].set_title('ADMM Residuals')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Objective value
        axes[0, 1].semilogy(iterations, self.history['objective'], 'g-', linewidth=2)
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].set_ylabel('Objective Value')
        axes[0, 1].set_title('Objective Function')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Solution sparsity
        sparsity = []
        for x in self.history['x']:
            sparsity.append(np.sum(np.abs(x) > 1e-6))
        
        axes[0, 2].plot(iterations, sparsity, 'purple', linewidth=2, marker='o', markersize=3)
        axes[0, 2].set_xlabel('Iteration')
        axes[0, 2].set_ylabel('Number of Non-zeros')
        axes[0, 2].set_title('Solution Sparsity Evolution')
        axes[0, 2].grid(True, alpha=0.3)
        
        # Plot 4: x and z agreement
        agreement = []
        for i in range(len(self.history['x'])):
            agreement.append(np.linalg.norm(self.history['x'][i] - self.history['z'][i]))
        
        axes[1, 0].semilogy(iterations, agreement, 'orange', linewidth=2)
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('||x - z||')
        axes[1, 0].set_title('Primal-Auxiliary Agreement')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 5: Dual variable norm
        u_norms = [np.linalg.norm(u) for u in self.history['u']]
        axes[1, 1].plot(iterations, u_norms, 'brown', linewidth=2)
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylabel('||u||')
        axes[1, 1].set_title('Dual Variable Norm')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Plot 6: Final solution
        if self.history['x']:
            x_final = self.history['x'][-1]
            non_zero_indices = np.where(np.abs(x_final) > 1e-6)[0]
            
            if len(non_zero_indices) > 0:
                axes[1, 2].stem(non_zero_indices, x_final[non_zero_indices], basefmt=' ')
                axes[1, 2].set_xlabel('Feature Index')
                axes[1, 2].set_ylabel('Coefficient Value')
                axes[1, 2].set_title('Non-zero Coefficients')
                axes[1, 2].grid(True, alpha=0.3)
            else:
                axes[1, 2].text(0.5, 0.5, 'All coefficients are zero', 
                               ha='center', va='center', transform=axes[1, 2].transAxes)
        
        plt.tight_layout()
        plt.show()

def example_lasso_admm():
    """Example: LASSO regression with ADMM"""
    
    print("Example 1: LASSO Regression with ADMM")
    print("=" * 55)
    
    # Generate synthetic data
    np.random.seed(42)
    m, n = 200, 100  # m > n (overdetermined)
    
    # True sparse solution
    x_true = np.random.randn(n)
    x_true[np.random.rand(n) > 0.2] = 0  # 80% sparsity
    
    # Generate design matrix and measurements
    A = np.random.randn(m, n)
    A = A / np.sqrt(m)  # Normalize
    b = A @ x_true + 0.1 * np.random.randn(m)  # Add noise
    
    print(f"Problem size: m={m}, n={n}")
    print(f"True sparsity: {np.sum(x_true != 0)}/{n}")
    print(f"SNR: {20 * np.log10(np.linalg.norm(A @ x_true) / (0.1 * np.sqrt(m))):.2f} dB")
    
    # Solve with different regularization parameters
    lambda_values = [0.01, 0.05, 0.1, 0.2]
    results = {}
    
    for lambda_reg in lambda_values:
        print(f"\nŒª = {lambda_reg}:")
        print("-" * 20)
        
        admm = ADMM_LASSO(rho=1.0, alpha=1.0, tolerance=1e-4)
        start_time = time.time()
        
        x_admm, z_admm, iterations = admm.solve(A, b, lambda_reg, verbose=False)
        
        solve_time = time.time() - start_time
        
        # Compute metrics
        mse = np.mean((x_admm - x_true)**2)
        sparsity = np.sum(np.abs(x_admm) > 1e-6)
        objective = 0.5 * np.linalg.norm(A @ x_admm - b)**2 + lambda_reg * np.linalg.norm(x_admm, 1)
        
        results[lambda_reg] = {
            'x': x_admm,
            'iterations': iterations,
            'time': solve_time,
            'mse': mse,
            'sparsity': sparsity,
            'objective': objective,
            'admm': admm
        }
        
        print(f"Iterations: {iterations}")
        print(f"Time: {solve_time:.4f} seconds")
        print(f"MSE: {mse:.6f}")
        print(f"Sparsity: {sparsity}/{n}")
        print(f"Objective: {objective:.6f}")
    
    # Comparison plots
    plt.figure(figsize=(15, 10))
    
    # Solution paths
    plt.subplot(2, 3, 1)
    for lambda_reg, result in results.items():
        x_solution = result['x']
        plt.plot(x_solution, 'o-', markersize=3, alpha=0.7, label=f'Œª={lambda_reg}')
    
    plt.plot(x_true, 'k--', linewidth=2, alpha=0.5, label='True')
    plt.xlabel('Feature Index')
    plt.ylabel('Coefficient Value')
    plt.title('Solution Paths')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # MSE vs Œª
    plt.subplot(2, 3, 2)
    lambdas = list(results.keys())
    mses = [results[lam]['mse'] for lam in lambdas]
    
    plt.semilogx(lambdas, mses, 'ro-', linewidth=2, markersize=8)
    plt.xlabel('Regularization Œª')
    plt.ylabel('MSE')
    plt.title('MSE vs Regularization')
    plt.grid(True, alpha=0.3)
    
    # Sparsity vs Œª
    plt.subplot(2, 3, 3)
    sparsities = [results[lam]['sparsity'] for lam in lambdas]
    true_sparsity = np.sum(x_true != 0)
    
    plt.semilogx(lambdas, sparsities, 'bo-', linewidth=2, markersize=8)
    plt.axhline(y=true_sparsity, color='red', linestyle='--', alpha=0.7, label='True Sparsity')
    plt.xlabel('Regularization Œª')
    plt.ylabel('Number of Non-zeros')
    plt.title('Sparsity vs Regularization')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Convergence comparison
    plt.subplot(2, 3, 4)
    for lambda_reg, result in results.items():
        admm = result['admm']
        iterations = range(len(admm.history['objective']))
        plt.semilogy(iterations, admm.history['objective'], linewidth=2, label=f'Œª={lambda_reg}')
    
    plt.xlabel('Iteration')
    plt.ylabel('Objective Value')
    plt.title('Convergence Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Residuals comparison
    plt.subplot(2, 3, 5)
    for lambda_reg, result in results.items():
        admm = result['admm']
        iterations = range(len(admm.history['r_norm']))
        plt.semilogy(iterations, admm.history['r_norm'], linewidth=2, label=f'Œª={lambda_reg}')
    
    plt.xlabel('Iteration')
    plt.ylabel('Primal Residual')
    plt.title('Primal Residual Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Performance metrics
    plt.subplot(2, 3, 6)
    x_pos = np.arange(len(lambdas))
    width = 0.25
    
    times = [results[lam]['time'] for lam in lambdas]
    iterations_list = [results[lam]['iterations'] for lam in lambdas]
    
    plt.bar(x_pos - width, times, width, label='Time (s)', alpha=0.7)
    plt.bar(x_pos, np.array(iterations_list) / 50, width, label='Iterations (√∑50)', alpha=0.7)
    plt.bar(x_pos + width, mses, width, label='MSE', alpha=0.7)
    
    plt.xlabel('Œª value')
    plt.ylabel('Metric Value')
    plt.title('Performance Metrics')
    plt.xticks(x_pos, [f'{lam}' for lam in lambdas])
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Show detailed convergence for one case
    print(f"\nDetailed convergence for Œª = {lambda_values[2]}:")
    results[lambda_values[2]]['admm'].plot_convergence()
    
    return results

# Run LASSO example
lasso_results = example_lasso_admm()
```

**B∆∞·ªõc 2: Group LASSO with ADMM**

```python
class ADMM_GroupLASSO:
    def __init__(self, rho=1.0, tolerance=1e-4, max_iterations=1000):
        """
        ADMM for Group LASSO: min (1/2)||Ax - b||¬≤ + Œª Œ£·µ¢ w·µ¢||x·µ¢||‚ÇÇ
        
        Tham s·ªë:
        - rho: penalty parameter œÅ
        - tolerance: convergence tolerance
        - max_iterations: maximum iterations
        """
        self.rho = rho
        self.tol = tolerance
        self.max_iter = max_iterations
        self.history = {
            'r_norm': [],
            's_norm': [],
            'objective': [],
            'x': [],
            'z': []
        }
    
    def solve(self, A, b, groups, lambda_reg, weights=None, verbose=True):
        """
        Solve Group LASSO using ADMM
        
        Tham s·ªë:
        - A: design matrix
        - b: response vector
        - groups: list of arrays, each containing indices for a group
        - lambda_reg: regularization parameter
        - weights: group weights (default: sqrt(group_size))
        """
        
        m, n = A.shape
        num_groups = len(groups)
        
        # Default weights: sqrt of group size
        if weights is None:
            weights = [np.sqrt(len(group)) for group in groups]
        
        # Kh·ªüi t·∫°o
        x = np.zeros(n)
        z = np.zeros(n)
        u = np.zeros(n)
        
        # Precompute for x-update
        AtA = A.T @ A
        Atb = A.T @ b
        
        try:
            L = np.linalg.cholesky(AtA + self.rho * np.eye(n))
            use_cholesky = True
        except:
            use_cholesky = False
        
        if verbose:
            print("ADMM for Group LASSO")
            print("=" * 35)
            print(f"Groups: {num_groups}, Features: {n}")
            print()
            print(f"{'Iter':<4} {'r norm':<12} {'s norm':<12} {'Objective':<12}")
            print("-" * 55)
        
        for iteration in range(self.max_iter):
            # x-update
            rhs = Atb + self.rho * (z - u)
            
            if use_cholesky:
                x = np.linalg.solve(L.T, np.linalg.solve(L, rhs))
            else:
                x = np.linalg.solve(AtA + self.rho * np.eye(n), rhs)
            
            # z-update: group soft thresholding
            z_old = z.copy()
            z = np.zeros(n)
            
            for i, group in enumerate(groups):
                v = x[group] + u[group]
                norm_v = np.linalg.norm(v)
                threshold = lambda_reg * weights[i] / self.rho
                
                if norm_v > threshold:
                    z[group] = (1 - threshold / norm_v) * v
                # else: z[group] = 0 (already initialized)
            
            # u-update
            u = u + x - z
            
            # Store history
            self.history['x'].append(x.copy())
            self.history['z'].append(z.copy())
            
            # Compute residuals
            r_norm = np.linalg.norm(x - z)
            s_norm = np.linalg.norm(-self.rho * (z - z_old))
            
            self.history['r_norm'].append(r_norm)
            self.history['s_norm'].append(s_norm)
            
            # Compute objective
            objective = 0.5 * np.linalg.norm(A @ x - b)**2
            for i, group in enumerate(groups):
                objective += lambda_reg * weights[i] * np.linalg.norm(x[group])
            
            self.history['objective'].append(objective)
            
            if verbose and iteration % 10 == 0:
                print(f"{iteration:<4} {r_norm:<12.6f} {s_norm:<12.6f} {objective:<12.6f}")
            
            # Check convergence
            eps_pri = np.sqrt(n) * self.tol + self.tol * max(np.linalg.norm(x), np.linalg.norm(z))
            eps_dual = np.sqrt(n) * self.tol + self.tol * np.linalg.norm(self.rho * u)
            
            if r_norm < eps_pri and s_norm < eps_dual:
                if verbose:
                    print(f"\nConverged in {iteration + 1} iterations")
                break
        
        return x, z, iteration + 1
    
    def plot_convergence(self):
        """Plot convergence history"""
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        iterations = range(len(self.history['r_norm']))
        
        # Residuals
        axes[0].semilogy(iterations, self.history['r_norm'], 'b-', linewidth=2, label='Primal')
        axes[0].semilogy(iterations, self.history['s_norm'], 'r-', linewidth=2, label='Dual')
        axes[0].set_xlabel('Iteration')
        axes[0].set_ylabel('Residual Norm')
        axes[0].set_title('ADMM Residuals')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Objective
        axes[1].semilogy(iterations, self.history['objective'], 'g-', linewidth=2)
        axes[1].set_xlabel('Iteration')
        axes[1].set_ylabel('Objective Value')
        axes[1].set_title('Objective Function')
        axes[1].grid(True, alpha=0.3)
        
        # Group sparsity
        group_active = []
        for x in self.history['x']:
            # Count non-zero groups (simplified)
            active = np.sum(np.abs(x) > 1e-6)
            group_active.append(active)
        
        axes[2].plot(iterations, group_active, 'purple', linewidth=2)
        axes[2].set_xlabel('Iteration')
        axes[2].set_ylabel('Active Features')
        axes[2].set_title('Active Features Evolution')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def example_group_lasso_admm():
    """Example: Group LASSO with ADMM"""
    
    print("\nExample 2: Group LASSO Regression with ADMM")
    print("=" * 55)
    
    # Generate grouped data
    np.random.seed(42)
    m = 200
    group_sizes = [5, 10, 5, 10, 5, 15]  # Variable group sizes
    n = sum(group_sizes)
    
    # Create groups
    groups = []
    start = 0
    for size in group_sizes:
        groups.append(np.arange(start, start + size))
        start += size
    
    # True solution: only some groups are non-zero
    x_true = np.zeros(n)
    active_groups = [0, 2, 4]  # Only groups 0, 2, 4 are active
    
    for i in active_groups:
        x_true[groups[i]] = np.random.randn(len(groups[i]))
    
    # Generate data
    A = np.random.randn(m, n)
    A = A / np.sqrt(m)
    b = A @ x_true + 0.1 * np.random.randn(m)
    
    print(f"Problem: m={m}, n={n}")
    print(f"Number of groups: {len(groups)}")
    print(f"Group sizes: {group_sizes}")
    print(f"Active groups (true): {active_groups}")
    
    # Solve with Group LASSO
    lambda_reg = 0.1
    
    print(f"\nSolving with Œª = {lambda_reg}:")
    print("-" * 30)
    
    group_lasso = ADMM_GroupLASSO(rho=1.0, tolerance=1e-4)
    start_time = time.time()
    
    x_gl, z_gl, iterations = group_lasso.solve(A, b, groups, lambda_reg, verbose=False)
    
    solve_time = time.time() - start_time
    
    # Identify active groups
    active_groups_found = []
    for i, group in enumerate(groups):
        if np.linalg.norm(x_gl[group]) > 1e-4:
            active_groups_found.append(i)
    
    print(f"Iterations: {iterations}")
    print(f"Time: {solve_time:.4f} seconds")
    print(f"Active groups (found): {active_groups_found}")
    print(f"MSE: {np.mean((x_gl - x_true)**2):.6f}")
    
    # Visualization
    group_lasso.plot_convergence()
    
    # Detailed solution visualization
    plt.figure(figsize=(15, 10))
    
    # Solution comparison
    plt.subplot(2, 2, 1)
    plt.plot(x_true, 'ko-', markersize=4, alpha=0.7, label='True')
    plt.plot(x_gl, 'r^-', markersize=4, alpha=0.7, label='Group LASSO')
    
    # Mark group boundaries
    for i, group in enumerate(groups[:-1]):
        boundary = group[-1] + 0.5
        plt.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)
    
    plt.xlabel('Feature Index')
    plt.ylabel('Coefficient Value')
    plt.title('Solution Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Group-wise norms
    plt.subplot(2, 2, 2)
    group_norms_true = [np.linalg.norm(x_true[group]) for group in groups]
    group_norms_found = [np.linalg.norm(x_gl[group]) for group in groups]
    
    x_pos = np.arange(len(groups))
    width = 0.35
    
    plt.bar(x_pos - width/2, group_norms_true, width, label='True', alpha=0.7)
    plt.bar(x_pos + width/2, group_norms_found, width, label='Group LASSO', alpha=0.7)
    
    plt.xlabel('Group Index')
    plt.ylabel('Group L2 Norm')
    plt.title('Group-wise Norms')
    plt.xticks(x_pos)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Residual fit
    plt.subplot(2, 2, 3)
    residual_true = A @ x_true - b
    residual_found = A @ x_gl - b
    
    plt.plot(residual_true, 'ko', markersize=3, alpha=0.5, label='True')
    plt.plot(residual_found, 'r^', markersize=3, alpha=0.5, label='Group LASSO')
    
    plt.xlabel('Sample Index')
    plt.ylabel('Residual')
    plt.title('Residual Analysis')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Group selection accuracy
    plt.subplot(2, 2, 4)
    
    selection_matrix = np.zeros((len(groups), 2))
    for i in range(len(groups)):
        selection_matrix[i, 0] = 1 if i in active_groups else 0
        selection_matrix[i, 1] = 1 if i in active_groups_found else 0
    
    im = plt.imshow(selection_matrix.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)
    plt.yticks([0, 1], ['True', 'Found'])
    plt.xlabel('Group Index')
    plt.title('Group Selection Pattern')
    plt.colorbar(im)
    
    plt.tight_layout()
    plt.show()
    
    return x_gl, group_lasso

# Run Group LASSO example
group_lasso_results = example_group_lasso_admm()
```

</details>

---

## üìù **B√†i t·∫≠p 2: ADMM cho Sparse + Low Rank Decomposition**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Cand√®s et al., 2011)
Implement ADMM cho robust PCA v√† matrix decomposition:

a) **Robust PCA** (Sparse + Low Rank)
b) **Matrix completion** problems
c) **Convergence analysis** cho matrix problems
d) **Applications** to image processing

**Y√™u c·∫ßu:**
1. Complete ADMM implementations cho matrix problems
2. Nuclear norm minimization
3. Performance optimization strategies
4. Real-world applications

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üìù **B√†i t·∫≠p 3: Consensus ADMM v√† Distributed Optimization**

**ƒê·ªÅ b√†i:** (Tham kh·∫£o Boyd et al., 2011)
Implement consensus ADMM cho distributed problems:

a) **Consensus ADMM** framework
b) **Distributed optimization** applications
c) **Communication efficiency** analysis
d) **Scalability** studies

**Y√™u c·∫ßu:**
1. Consensus ADMM implementations
2. Distributed computing simulations
3. Communication cost analysis
4. Scalability benchmarking

<details>
<summary><strong>üí° L·ªùi gi·∫£i chi ti·∫øt</strong></summary>

**Implementation s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ti·∫øp theo do gi·ªõi h·∫°n ƒë·ªô d√†i...**

</details>

---

## üí° M·∫πo Th·ª±c H√†nh

#### **Khi implement ADMM:**
- Choose penalty parameter œÅ carefully
- Use warm-starting when solving sequence of problems
- Implement efficient linear system solvers
- Monitor both primal and dual residuals

#### **Khi tune parameters:**
- **œÅ (penalty)**: Balance between primal and dual residuals
- **Œ± (relaxation)**: Usually Œ± ‚àà [1.0, 1.8] works well
- **Adaptive œÅ**: Adjust based on residual ratio
- **Stopping criteria**: Use both absolute and relative tolerances

#### **Khi handle large-scale problems:**
- Exploit matrix structure (sparse, low-rank)
- Use iterative linear solvers when appropriate
- Implement matrix-free operations
- Consider parallel/distributed implementations

#### **Khi apply to specific problems:**
- Reformulate to ADMM-friendly form
- Ensure subproblems have closed-form or efficient solutions
- Use problem-specific proximal operators
- Validate convergence properties

---

## üìö T√†i li·ªáu tham kh·∫£o

1. **Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J.** (2011). *Distributed optimization and statistical learning via the alternating direction method of multipliers*. Foundations and Trends in Machine Learning, 3(1), 1-122.

2. **Cand√®s, E. J., Li, X., Ma, Y., & Wright, J.** (2011). *Robust principal component analysis?* Journal of the ACM, 58(3), 11.

3. **Parikh, N., & Boyd, S.** (2014). *Proximal algorithms*. Foundations and Trends in Optimization, 1(3), 127-239.
